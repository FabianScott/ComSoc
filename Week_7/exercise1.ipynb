{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#if os.getcwd()[-1] in '0123456789':\n",
    "#    path_parent = os.path.dirname(os.getcwd())\n",
    "#    os.chdir(path_parent)\n",
    "\n",
    "paper_abstracts = pd.read_csv(\"paper_abstract_dataset.csv\")\n",
    "paper_abstracts = paper_abstracts[paper_abstracts['Abstract'].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import re\n",
    "\n",
    "tokens = []\n",
    "\n",
    "def get_tokens(abstract):\n",
    "    abstract = abstract.lower()\n",
    "    tokens_raw = word_tokenize(abstract)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokens_raw:\n",
    "        if bool(re.search(r'\\d+', token)):\n",
    "            continue\n",
    "        elif token in punctuation or token in stopwords:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        ps.stem(token)\n",
    "        tokens.append(token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_abstracts.insert(2, \"Tokens\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 4\u001b[0m paper_abstracts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 27\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(abstract)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m punctuation \u001b[38;5;129;01mor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m stopwords:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/stem/porter.py:676\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    674\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step4(stem)\n\u001b[1;32m    675\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step5a(stem)\n\u001b[0;32m--> 676\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step5b(stem)\n\u001b[1;32m    678\u001b[0m \u001b[39mreturn\u001b[39;00m stem\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/stem/porter.py:650\u001b[0m, in \u001b[0;36mPorterStemmer._step5b\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_step5b\u001b[39m(\u001b[39mself\u001b[39m, word):\n\u001b[1;32m    640\u001b[0m     \u001b[39m\"\"\"Implements Step 5a from \"An algorithm for suffix stripping\"\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[39m    From the paper:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m                                roll           ->  roll\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_rule_list(\n\u001b[1;32m    651\u001b[0m         word, [(\u001b[39m\"\u001b[39;49m\u001b[39mll\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39ml\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mlambda\u001b[39;49;00m stem: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_measure(word[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m)]\n\u001b[1;32m    652\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/stem/porter.py:248\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[39mreturn\u001b[39;00m word[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(suffix)] \u001b[39m+\u001b[39m replacement\n\u001b[0;32m--> 248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_rule_list\u001b[39m(\u001b[39mself\u001b[39m, word, rules):\n\u001b[1;32m    249\u001b[0m     \u001b[39m\"\"\"Applies the first applicable suffix-removal rule to the word\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[39m    Takes a word and a list of suffix-removal rules represented as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m    or None if the rule is unconditional.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m rules:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,row in enumerate(paper_abstracts['Abstract']):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    paper_abstracts[\"Tokens\"][i] = get_tokens(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_abstracts.to_csv(\"paper_abstract_dataset_with_tokens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pl/7f1cr2657p3bnpdt3bnm_9w00000gn/T/ipykernel_2116/3044418545.py:1: DtypeWarning: Columns (0,1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  paper_abstracts = pd.read_csv(\"paper_abstract_dataset_with_tokens.csv\")\n"
     ]
    }
   ],
   "source": [
    "paper_abstracts = pd.read_csv(\"paper_abstract_dataset_with_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "bigrams_list = [list(bigrams(row)) for row in paper_abstracts[\"Tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "p_values = []\n",
    "for i, bigrams in enumerate(bigrams_list):\n",
    "    len_bigrams = len(bigrams)\n",
    "    bigrams1 = np.array([bg[0] for bg in bigrams])\n",
    "    bigrams2 = np.array([bg[1] for bg in bigrams])\n",
    "    for bigram in bigrams:\n",
    "        first_value_true = bigram[0] == bigrams1\n",
    "        second_value_true = bigram[1] == bigrams2\n",
    "        contingency_table = np.empty((2,2))\n",
    "        contingency_table[0,0] = np.sum(np.logical_and(first_value_true, second_value_true))\n",
    "        contingency_table[1,0] = np.sum(np.logical_and(first_value_true, np.logical_not(second_value_true)))\n",
    "        contingency_table[0,1] = np.sum(np.logical_and(np.logical_not(first_value_true), second_value_true))\n",
    "        contingency_table[1,1] = np.sum(np.logical_and(np.logical_not(first_value_true), np.logical_not(second_value_true)))\n",
    "        p_value = scipy.stats.chi2_contingency(contingency_table)\n",
    "        p_values.append([i, bigram, p_value[1], len_bigrams])\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "        with open('p_values.pickle', 'wb') as handle:\n",
    "            pickle.dump(p_value, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

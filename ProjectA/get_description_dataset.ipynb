{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data from all the starwars characters from the official Starwars Databank, we first need to know all the possible URLs to get the data from. This is done by getting the list of all the characters from the 'starwars-databank-server.vercel.app' API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_CHARACTERS = \"https://starwars-databank-server.vercel.app/api/v1/characters\"\n",
    "\n",
    "def getResponse(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to get data. Status code: {response.status_code}\")\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "def cleanString(var):\n",
    "    return str(var).replace(\"\\\"\", \"\\'\")\n",
    "\n",
    "def getNextUrl(data):\n",
    "    BASE_URL = \"https://starwars-databank-server.vercel.app/\"\n",
    "    try: return (BASE_URL + data[\"info\"][\"next\"])\n",
    "    except: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [00:54<00:02,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "character_df_old = None\n",
    "temp_url = URL_CHARACTERS\n",
    "character_name_list = []\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    data = getResponse(temp_url)\n",
    "    character_name_list += [cleanString(character[\"name\"]) for character in data[\"data\"]]\n",
    "    \n",
    "    # Since the getResponse function doesn't return all characters\n",
    "    # we need to loop through the different API \"pages\" with getNextUrl\n",
    "    temp_url = getNextUrl(data)\n",
    "    if temp_url is None:\n",
    "        print(\"Done!\")\n",
    "        break\n",
    "\n",
    "url_name_list = [\"-\".join(name.lower().replace(\"\\'\", \"\").split()) for name in character_name_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of character names we can then start getting the character data we actually want from the official Star Wars databank website. We achieve this by appending these names to the end of the base URL. From the resulting HTML page we can then extract the desired data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.starwars.com/databank/\"\n",
    "URL_ADDITIONS = url_name_list\n",
    "\n",
    "errors = []\n",
    "number_of_errors = 0\n",
    "\n",
    "def add_one_error():\n",
    "    global number_of_errors\n",
    "    number_of_errors += 1\n",
    "\n",
    "def get_page(url):\n",
    "    try: response = requests.get(url)\n",
    "    except: errors.append(url); add_one_error(); return None\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "def check_if_error_404(soup):\n",
    "    try: soup.find_all(\"p\", class_=\"desc\")[0].text; return False\n",
    "    except: return True\n",
    "\n",
    "def get_properties(soup):\n",
    "    stats = soup.find_all(\"section\", class_=\"module stats span-full-screen content-span-full-screen secondary-theme dark\")\n",
    "    categories = stats[0].find_all(\"div\", class_=\"category\")\n",
    "    properties = {}\n",
    "    for category in categories:\n",
    "        heading = category.find_all(\"div\", class_=\"heading\")[0].text\n",
    "        property_names = [p.text for p in category.find_all(\"div\", class_=\"property-name\")]\n",
    "        properties[heading] = property_names\n",
    "    return properties\n",
    "\n",
    "def get_description(soup):\n",
    "    discription = soup.find_all(\"p\", class_=\"desc\")[0].text\n",
    "    discription = discription.replace(\"\\n\", \"\")\n",
    "    return discription\n",
    "\n",
    "def get_history(soup):\n",
    "    all_referals = []\n",
    "    all_text = soup.find_all(\"div\", class_=\"rich-text-output\")\n",
    "    if len(all_text) == 0:\n",
    "        return None, None\n",
    "    minor_text_samples = all_text[0].find_all(\"p\")\n",
    "    all_history_text = \"\"\n",
    "    for sample in minor_text_samples:\n",
    "        referals = sample.find_all(\"a\")\n",
    "        for referal in referals:\n",
    "            all_referals.append(referal.text.strip())\n",
    "        all_history_text += \" \" + sample.text\n",
    "    return all_history_text, all_referals\n",
    "\n",
    "def get_descriptions(url):\n",
    "    soup = get_page(url)\n",
    "    if soup is None or check_if_error_404(soup):\n",
    "        errors.append(url)\n",
    "        add_one_error()\n",
    "        return None\n",
    "    description = get_description(soup)\n",
    "    history, referal = get_history(soup)\n",
    "    properties = get_properties(soup)\n",
    "    return description, referal, properties, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all the correct URL's before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_working_url(url):\n",
    "    soup = get_page(url)\n",
    "    if soup is None or check_if_error_404(soup):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 964/964 [11:32<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_urls = []\n",
    "errors = []\n",
    "\n",
    "for i in tqdm(range(len(character_name_list))):\n",
    "    name = character_name_list[i]\n",
    "    url_addition = URL_ADDITIONS[i]\n",
    "    full_url = BASE_URL + url_addition\n",
    "    if check_if_working_url(full_url):\n",
    "        all_urls.append((name, full_url))\n",
    "    else:\n",
    "        errors.append((name, full_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat cleaned errors and all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentially_cleaned_urls = []\n",
    "\n",
    "cleaned_errors = [lst.split(\"\\n\") for lst in open(\"cleaned_errors.csv\", \"r\").readlines()]\n",
    "for cleaned_error in cleaned_errors:\n",
    "    potentially_cleaned_urls.append(cleaned_error[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:41<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "for i in tqdm(range(len( potentially_cleaned_urls))):\n",
    "    name = literal_eval(potentially_cleaned_urls[i])[0]\n",
    "    full_url = literal_eval(potentially_cleaned_urls[i])[1]\n",
    "    if check_if_working_url(full_url):\n",
    "        all_urls.append((name, full_url))\n",
    "    else:\n",
    "        errors.append((name, full_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"errors.csv\", \"w\") as f:\n",
    "    for line in errors:\n",
    "        f.write(str(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 958/958 [10:44<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors:  0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe objects:\n",
    "new_names = []\n",
    "errors = []\n",
    "descriptions = []\n",
    "referals = []\n",
    "properties = []\n",
    "histories = []\n",
    "number_of_errors = 0\n",
    "\n",
    "for i in tqdm(range(len(all_urls))):\n",
    "    name = all_urls[i][0]\n",
    "    full_url = all_urls[i][1]\n",
    "    descriptions_tuple = get_descriptions(full_url)\n",
    "\n",
    "    if descriptions_tuple is None:\n",
    "        continue \n",
    "    \n",
    "    description, referal, property, history = descriptions_tuple\n",
    "    new_names.append(name)\n",
    "    descriptions.append(description)\n",
    "    referals.append(referal)\n",
    "    properties.append(property)\n",
    "    histories.append(history)\n",
    "\n",
    "    all_urls.append((name, full_url))  \n",
    "\n",
    "print(\"Number of errors: \", number_of_errors)\n",
    "print(*errors, sep=\"\\n\") \n",
    "\n",
    "cleaned_descriptions = [description.replace(\"\\n\", \"\").replace(\"\\r\", \"\") for description in descriptions]\n",
    "df = pd.DataFrame({\"name\": new_names, \"description\": cleaned_descriptions, \"referals\": referals, \"properties\": properties, \"history\": history})\n",
    "df.to_csv(\"descriptions.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of errors: \", number_of_errors)\n",
    "with open(\"errors.csv\", \"w\") as f:\n",
    "    for line in errors:\n",
    "        f.write(line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

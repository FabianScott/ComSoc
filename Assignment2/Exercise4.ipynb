{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netwulf as nw\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Fixing the path:\n",
    "if os.getcwd()[-1] in '0123456798':\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stylized_network, config, G \u001b[39m=\u001b[39m nw\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mBigData/Graph.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Finding the louvain communities of our Graph:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m set_seed \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\netwulf\\io.py:92\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m _f:\n\u001b[1;32m---> 92\u001b[0m         \u001b[39mreturn\u001b[39;00m _read(_f)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\netwulf\\io.py:34\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     32\u001b[0m G \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mGraph\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m G \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mnode_link_graph(data[\u001b[39m'\u001b[39;49m\u001b[39mGraph\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m stylized_network, config, G\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\json_graph\\node_link.py:324\u001b[0m, in \u001b[0;36mnode_link_graph\u001b[1;34m(data, directed, multigraph, attrs, source, target, name, key, link)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m multigraph:\n\u001b[0;32m    323\u001b[0m     edgedata \u001b[39m=\u001b[39m {\u001b[39mstr\u001b[39m(k): v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m d\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m source \u001b[39mand\u001b[39;00m k \u001b[39m!=\u001b[39m target}\n\u001b[1;32m--> 324\u001b[0m     graph\u001b[39m.\u001b[39madd_edge(src, tgt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39medgedata)\n\u001b[0;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     ky \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mget(key, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stylized_network, config, G = nw.load(\"BigData/Graph.json\")\n",
    "\n",
    "# Finding the louvain communities of our Graph:\n",
    "set_seed = 42\n",
    "communities = nx_comm.louvain_communities(G, seed=set_seed)\n",
    "\n",
    "\n",
    "# Importing the paper_dataset from a few weeks back:\n",
    "paper_dataset = pd.read_csv(\"BigData/paper_dataset.csv\", converters={\"authorIds\": literal_eval, \"authorNames\": literal_eval})\n",
    "# Converting the paper_dataset into an authorId lookup by exploding and then setting a new index:\n",
    "author_lookup = paper_dataset.explode([\"authorIds\", \"authorNames\"]).set_index(\"authorIds\").rename(columns={'authorIds': 'authorId'}).dropna()\n",
    "# Removing the incorrectly exploded authorNames:\n",
    "author_lookup = author_lookup[author_lookup.index.notnull()]\n",
    "\n",
    "# Importing the tokenized paper abstracts:\n",
    "paper_tokens_lookup = pd.read_csv(\"BigData/paper_tokens_dataset.csv\", index_col=0, converters={\"Tokens\": literal_eval})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "### What does TF stand for?\n",
    "TF stands for Term Frequency. It is simply the frequency of a specific term appearing in a document. Therefore the TF is the number of times a word appears in a document, divided by the total number of words in the document.\n",
    "\n",
    "### What does IDF stand for?\n",
    "IDF stands for Inverse Document Frequency. The purpose of IDF is to emphasize rarer words, and avoiding the common words like \"the\" and \"and\" etc. The IDF is the log of the number of documents divided by the number of documents that contain the word. The more documents that contain a word, the lower the word's IDF, and the less important that word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global counters to keep track of missing authors and papers:\n",
    "missing_authors = 0\n",
    "missing_papers = 0\n",
    "\n",
    "# Get a list of all the papers authored by the authors in a community:\n",
    "def get_papers_to_check(community: list) -> list:\n",
    "    global missing_authors\n",
    "    papers_to_check = []\n",
    "    print(\"Getting papers from authorId...\")\n",
    "    for author_id in tqdm(community):\n",
    "        try: papers_to_check.extend(author_lookup.loc[str(author_id)][\"paperId\"].tolist())\n",
    "        except: missing_authors += 1\n",
    "    return papers_to_check\n",
    "\n",
    "# Get a list of all the tokens from each paper in a list:\n",
    "def get_tokens_from_papers(papers_to_check: list) -> list:\n",
    "    global missing_papers\n",
    "    total_tokens = []\n",
    "    print(\"Getting tokens from papers...\")\n",
    "    for paper_id in tqdm(papers_to_check):\n",
    "        try: total_tokens.extend(paper_tokens_lookup.loc[paper_id][\"Tokens\"])\n",
    "        except: missing_papers += 1\n",
    "    return total_tokens\n",
    "\n",
    "# Get the total tokens for each community in the list:\n",
    "def generate_token_groups(communities):\n",
    "    paper_groups = []\n",
    "    token_groups = []\n",
    "    for idx, community in enumerate(communities):\n",
    "        print(f\"Generating token groups... {idx + 1} of {len(communities)}\\n\")\n",
    "        papers_to_check = get_papers_to_check(list(community))\n",
    "        paper_groups.extend(papers_to_check)\n",
    "        token_groups.extend(get_tokens_from_papers(papers_to_check))\n",
    "        clear_output()\n",
    "    print(f\"All {len(communities)} communities successfully generated.\")\n",
    "    return token_groups, paper_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 communities by author count: [2594, 2469, 1819, 1574, 1326, 1026, 1006, 1005, 981, 977]\n"
     ]
    }
   ],
   "source": [
    "# It takes way too much time to genereate all the ~600 token groups, so I opted to just generate the top 10 and ignore the rest.\n",
    "# Finding the top 10 communities by author count:\n",
    "sorted_args = np.array([len(community) for community in communities]).argsort()[::-1]\n",
    "top_communities = [communities[i] for i in sorted_args[:10]]\n",
    "\n",
    "print(\"Top 10 communities by author count:\", [(len(communities[i])) for i in sorted_args][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 10 successfully generated.\n"
     ]
    }
   ],
   "source": [
    "# Generating token groups for each of the top communities:\n",
    "top_communities_tokens, top_communities_papers = generate_token_groups(top_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the top_communities_tokens and saving it:\n",
    "\n",
    "# Zips the all unique tokens with their # of occurences in the list:\n",
    "zipped_unique = lambda lister: zip(*np.unique(lister, return_counts=True))\n",
    "# Sorts the zipped list by the second element in descending order:\n",
    "sorter = lambda zipper: sorted(zipper, key=lambda x: x[1], reverse=True)\n",
    "# Making the sorted dicts for each top community:\n",
    "top_communities_tokens_uniquecount = [dict(sorter(zipped_unique(i))) for i in top_communities_tokens]\n",
    "\n",
    "# Making it into a dataframe:\n",
    "tokens_df = pd.DataFrame({\"Communities\": top_communities, \"PaperIds\": top_communities_papers, \"Tokens\": top_communities_tokens_uniquecount})\n",
    "# tokens_df.to_csv(\"BigData/top_community_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Communities</th>\n",
       "      <th>PaperIds</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{1409253380, 8085509, 12926984, 115777544, 308...</td>\n",
       "      <td>[19c03d3a03dd21e1dd74c3cd9ca57825d7440d88, b54...</td>\n",
       "      <td>{'patients': 5145, 'data': 4470, 'health': 401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{118390786, 35053571, 1867785, 2079686669, 281...</td>\n",
       "      <td>[3528385c6eef96422b4cf7d3a7f87ef59ea12ac1, cc3...</td>\n",
       "      <td>{'data': 5550, 'model': 3909, 'results': 3103,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{2252806, 49479696, 1679379, 2809876, 35332118...</td>\n",
       "      <td>[6f80d1ade43ae048763d65c6e8e913d9a31de4be, 8f9...</td>\n",
       "      <td>{'data': 7595, 'information': 3213, 'paper': 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{103301121, 2555924, 2117787677, 46702624, 170...</td>\n",
       "      <td>[34503c0b6a615124eaf82cb0e4a1dab2866e8980, 094...</td>\n",
       "      <td>{'models': 9180, 'model': 8473, 'tasks': 7899,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{79101961, 11493385, 2301965, 88317978, 144179...</td>\n",
       "      <td>[c117553b2eac5d02eaac3c9bc33a44fe2e1c3ca7, 857...</td>\n",
       "      <td>{'patients': 341, 'usa': 290, 'ffr': 177, 'cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{103362561, 104681475, 144484357, 113463301, 2...</td>\n",
       "      <td>[dd491b812f8acfec86f855e1cac8ed72ca062b53, 050...</td>\n",
       "      <td>{'gamma-ray': 3693, 'emission': 3463, 'data': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{2090979328, 46567426, 66150403, 2174097413, 1...</td>\n",
       "      <td>[fbfc15492c8e114f2884d7cc11ba21f7f350285f, eca...</td>\n",
       "      <td>{'workshop': 1272, 'intelligence': 1268, 'rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{2153465857, 32350210, 13398019, 3948545, 5103...</td>\n",
       "      <td>[14692c6785ce842eae91eb7cf6fffb0c7bbb805f, 868...</td>\n",
       "      <td>{'model': 1617, 'data': 1364, 'models': 1229, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{1484736514, 3121155, 4771844, 104284168, 2068...</td>\n",
       "      <td>[c7293f9dd2ec3d34edf5331eb5a6ad7614723197, 2ff...</td>\n",
       "      <td>{'emotions': 83466, 'interventions': 56043, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{145199105, 1761281, 48494598, 51390475, 49647...</td>\n",
       "      <td>[b13799435551d4f2b45f46ebb59e481baf42b11c, e8f...</td>\n",
       "      <td>{'research': 10436, 'health': 10034, 'original...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Communities  \\\n",
       "0  {1409253380, 8085509, 12926984, 115777544, 308...   \n",
       "1  {118390786, 35053571, 1867785, 2079686669, 281...   \n",
       "2  {2252806, 49479696, 1679379, 2809876, 35332118...   \n",
       "3  {103301121, 2555924, 2117787677, 46702624, 170...   \n",
       "4  {79101961, 11493385, 2301965, 88317978, 144179...   \n",
       "5  {103362561, 104681475, 144484357, 113463301, 2...   \n",
       "6  {2090979328, 46567426, 66150403, 2174097413, 1...   \n",
       "7  {2153465857, 32350210, 13398019, 3948545, 5103...   \n",
       "8  {1484736514, 3121155, 4771844, 104284168, 2068...   \n",
       "9  {145199105, 1761281, 48494598, 51390475, 49647...   \n",
       "\n",
       "                                            PaperIds  \\\n",
       "0  [19c03d3a03dd21e1dd74c3cd9ca57825d7440d88, b54...   \n",
       "1  [3528385c6eef96422b4cf7d3a7f87ef59ea12ac1, cc3...   \n",
       "2  [6f80d1ade43ae048763d65c6e8e913d9a31de4be, 8f9...   \n",
       "3  [34503c0b6a615124eaf82cb0e4a1dab2866e8980, 094...   \n",
       "4  [c117553b2eac5d02eaac3c9bc33a44fe2e1c3ca7, 857...   \n",
       "5  [dd491b812f8acfec86f855e1cac8ed72ca062b53, 050...   \n",
       "6  [fbfc15492c8e114f2884d7cc11ba21f7f350285f, eca...   \n",
       "7  [14692c6785ce842eae91eb7cf6fffb0c7bbb805f, 868...   \n",
       "8  [c7293f9dd2ec3d34edf5331eb5a6ad7614723197, 2ff...   \n",
       "9  [b13799435551d4f2b45f46ebb59e481baf42b11c, e8f...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  {'patients': 5145, 'data': 4470, 'health': 401...  \n",
       "1  {'data': 5550, 'model': 3909, 'results': 3103,...  \n",
       "2  {'data': 7595, 'information': 3213, 'paper': 3...  \n",
       "3  {'models': 9180, 'model': 8473, 'tasks': 7899,...  \n",
       "4  {'patients': 341, 'usa': 290, 'ffr': 177, 'cor...  \n",
       "5  {'gamma-ray': 3693, 'emission': 3463, 'data': ...  \n",
       "6  {'workshop': 1272, 'intelligence': 1268, 'rese...  \n",
       "7  {'model': 1617, 'data': 1364, 'models': 1229, ...  \n",
       "8  {'emotions': 83466, 'interventions': 56043, 'r...  \n",
       "9  {'research': 10436, 'health': 10034, 'original...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = pd.read_csv(\"BigData/top_community_tokens.csv\", index_col=0, converters={\"Communities\": literal_eval, \"PaperIds\": literal_eval, \"Tokens\": literal_eval})\n",
    "tokens_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe similarities and differences between the communities:\n",
    "There are some generic similarities between the communities, all of them have the \"data\" token in their top most frequent, similarly \"model\", \"models\", \"patients\", \"research\", \"paper\", etc. are also words that come up a lot and are all clearly words generally related to the topic of scientific research. But in the top 3 there are also some differences. Community 5 mentions \"gamma-ray\" and \"emission\", while community 6 \n",
    "mentions \"workshop\" and \"intelligence\", and community 8 focuses on \"emotions\" and \"interventions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 in first community:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patients', 0.5563587643536777),\n",
       " ('data', 0.48336708973001735),\n",
       " ('health', 0.4341652942429574),\n",
       " ('results', 0.378475349900461),\n",
       " ('model', 0.3625793852046416)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF formula:\n",
    "TF = lambda term_num, N: (term_num / N) * 100\n",
    "\n",
    "communities_TFs = []\n",
    "for token_group in tqdm(tokens_df['Tokens']):\n",
    "    counts = Counter(token_group)\n",
    "    TF_dict = dict([(token, TF(count, sum(counts.values()))) for token, count in counts.items()])\n",
    "    communities_TFs.append(TF_dict)\n",
    "\n",
    "# Just a little teaser:\n",
    "print(\"first 5 in first community:\")\n",
    "list(communities_TFs[0].items())[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why aren't the TFs not necessarily a good description of the communities?\n",
    "As mentionend when describing the similarities, there are a lot of generic words related to the topic of scientific research that are to be expected in an abstract. So looking purely at the frequency of words doesn't tell you much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we calculate IDF for every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 in first community:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patients', 0.20131482909240864),\n",
       " ('data', 0.2623926850589239),\n",
       " ('health', 0.3090146585761606),\n",
       " ('results', 0.3686321638405847),\n",
       " ('model', 0.38726665476204025)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDF formula:\n",
    "IDF = lambda term_occurences, N: np.log10(N / term_occurences)\n",
    "\n",
    "communities_IDFs = []\n",
    "for i in tqdm(range(len(tokens_df['Tokens']))):\n",
    "    counts, papers = Counter(tokens_df['Tokens'][i]), tokens_df[\"PaperIds\"][i]\n",
    "    IDF_dict = dict([(token, IDF(count, len(papers))) for token, count in counts.items()])\n",
    "    communities_IDFs.append(IDF_dict)\n",
    "    \n",
    "# Just a little teaser:\n",
    "print(\"first 5 in first community:\")\n",
    "list(communities_IDFs[0].items())[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What base logarithm did you use to calculate the IDF? Is that important?\n",
    "The base of the logarithm is important. Since how the IDF \"dampens\" depends on the logarithm base, i.e. a smaller base would dampen the frequent terms more, while a bigger base could be preffered if you want to give more weight to rare terms. I chose to go with np.log10 which is the logartithm with base 10, because it felt appropriate for this case, making generic terms less apparent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're ready to calculate TF-IDF. Do that for the top 9 communities (by number of authors). Then for each community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 in first community:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('models', 0.14113037359939387),\n",
       " ('research', 0.14106781219015055),\n",
       " ('study', 0.14102358371643073),\n",
       " ('mortality', 0.1409867230188593),\n",
       " ('methods', 0.14058421302837504)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_TF_IDFs = []\n",
    "for idx, token_group in enumerate([list(i) for i in tokens_df[\"Tokens\"]]):\n",
    "    group_TF_IDF = []\n",
    "    for token in token_group:\n",
    "        # An error in some cases causes negative IDF values, so I just set them to 0.\n",
    "        group_TF_IDF.append(communities_TFs[idx][token] * max(0,communities_IDFs[idx][token]))\n",
    "    communities_TF_IDFs.append(dict(sorter(zip(token_group, group_TF_IDF))))\n",
    "    \n",
    "# Just a little teaser:\n",
    "print(\"first 5 in first community:\")\n",
    "list(communities_TF_IDFs[0].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the last round of neighbors was foud without getting their names and such info, I can\n",
    "# only display the author ids:\n",
    "def print_community(community_idx):\n",
    "    print(f\"COMMUNITY {community_idx + 1}:\\n\")\n",
    "    print(\"Top 10 TF terms:\")\n",
    "    print(list(communities_TFs[community_idx])[:10])\n",
    "    print(\"Top 10 TF-IDF terms:\")\n",
    "    print(list(communities_TF_IDFs[community_idx])[:10])\n",
    "    print(\"Top 3 authors:\")\n",
    "    print(list(tokens_df[\"Communities\"][community_idx])[:3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMUNITY 1:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['patients', 'data', 'health', 'results', 'model', 'study', 'models', 'research', 'mortality', 'methods']\n",
      "Top 10 TF-IDF terms:\n",
      "['models', 'research', 'study', 'mortality', 'methods', 'model', 'social', 'using', 'results', 'use']\n",
      "Top 3 authors:\n",
      "[1409253380, 8085509, 12926984]\n",
      "\n",
      "COMMUNITY 2:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['data', 'model', 'results', 'paper', 'children', 'social', 'study', 'systems', 'system', 'health']\n",
      "Top 10 TF-IDF terms:\n",
      "['results', 'model', 'paper', 'children', 'social', 'study', 'systems', 'system', 'health', 'different']\n",
      "Top 3 authors:\n",
      "[118390786, 35053571, 1867785]\n",
      "\n",
      "COMMUNITY 3:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['data', 'information', 'paper', 'model', 'results', 'system', 'social', 'performance', 'problem', 'using']\n",
      "Top 10 TF-IDF terms:\n",
      "['results', 'system', 'social', 'performance', 'problem', 'using', 'model', 'study', 'models', 'paper']\n",
      "Top 3 authors:\n",
      "[2252806, 49479696, 1679379]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_community(0)\n",
    "print_community(1)\n",
    "print_community(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMUNITY 5:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['patients', 'usa', 'ffr', 'coronary', 'lesions', 'p', 'uk', 'risk', 'clinical', 'group']\n",
      "Top 10 TF-IDF terms:\n",
      "['coronary', 'lesions', 'p', 'uk', 'ffr', 'risk', 'clinical', 'group', 'disease', 'bifurcation']\n",
      "Top 3 authors:\n",
      "[79101961, 11493385, 2301965]\n",
      "\n",
      "COMMUNITY 6:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['gamma-ray', 'emission', 'data', 'telescope', 'fermi', 'energy', 'large', 'γ-ray', 'gev', 'flux']\n",
      "Top 10 TF-IDF terms:\n",
      "['information', 'network', 'high', 'different', 'x-ray', 'paper', 'p.', 'j.', 'distribution', 'present']\n",
      "Top 3 authors:\n",
      "[103362561, 104681475, 144484357]\n",
      "\n",
      "COMMUNITY 7:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['workshop', 'intelligence', 'research', 'artificial', 'new', 'learning', 'model', 'systems', 'language', 'workshops']\n",
      "Top 10 TF-IDF terms:\n",
      "['data', 'pp', 'reviewed', 'tasks', 'models', 'ai', 'games', 'us', 'safety', 'analysis']\n",
      "Top 3 authors:\n",
      "[2090979328, 46567426, 66150403]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_community(4)\n",
    "print_community(5)\n",
    "print_community(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMUNITY 8:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['model', 'data', 'models', 'results', 'new', 'social', 'tasks', 'language', 'using', 'cancer']\n",
      "Top 10 TF-IDF terms:\n",
      "['original', 'across', 'replication', 'scale', 'development', 'systems', 'research', 'present', 'big-bench', 'performance']\n",
      "Top 3 authors:\n",
      "[2153465857, 32350210, 13398019]\n",
      "\n",
      "COMMUNITY 9:\n",
      "\n",
      "Top 10 TF terms:\n",
      "['emotions', 'interventions', 'reappraisal', 'one', 'positive', 'negative', 'effects', 'original', 'effect', 'pandemic']\n",
      "Top 10 TF-IDF terms:\n",
      "['low', 'lab', 'among', 'important', 'greater', 'engagement', 'direct', 'extremely', 'showed', 'future']\n",
      "Top 3 authors:\n",
      "[1484736514, 3121155, 4771844]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_community(7)\n",
    "print_community(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "In some cases they are, for example the word \"data\" has in all cases gone a few places down the list, but community 6 it has unfortunately lost some of its uniqueness. But I'd still argue that the TF-IDF has helped dampen these most common/generic words, and the new top 10 is a little more informative than the TF top 10.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "376dd7633afddb7e11c0a9ffa5f656d1a19ddfafae55033d07193535982b765e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

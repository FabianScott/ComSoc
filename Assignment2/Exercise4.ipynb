{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netwulf as nw\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Fixing the path:\n",
    "if os.getcwd()[-1] in '0123456798':\n",
    "    path_parent = os.path.dirname(os.getcwd())\n",
    "    os.chdir(path_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network, config, G = nw.load(\"BigData/Graph.json\")\n",
    "\n",
    "# Finding the louvain communities of our Graph:\n",
    "set_seed = 42\n",
    "communities = nx_comm.louvain_communities(G, seed=set_seed)\n",
    "\n",
    "\n",
    "# Importing the paper_dataset from a few weeks back:\n",
    "paper_dataset = pd.read_csv(\"BigData/paper_dataset.csv\", converters={\"authorIds\": literal_eval, \"authorNames\": literal_eval})\n",
    "# Converting the paper_dataset into an authorId lookup by exploding and then setting a new index:\n",
    "author_lookup = paper_dataset.explode([\"authorIds\", \"authorNames\"]).set_index(\"authorIds\").rename(columns={'authorIds': 'authorId'}).dropna()\n",
    "# Removing the incorrectly exploded authorNames:\n",
    "author_lookup = author_lookup[author_lookup.index.notnull()]\n",
    "\n",
    "# Importing the tokenized paper abstracts:\n",
    "paper_tokens_lookup = pd.read_csv(\"BigData/paper_tokens_dataset.csv\", index_col=0, converters={\"Tokens\": literal_eval})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "### What does TF stand for?\n",
    "TF stands for Term Frequency. It is simply the frequency of a specific term appearing in a document. Therefore the TF is the number of times a word appears in a document, divided by the total number of words in the document.\n",
    "\n",
    "### What does IDF stand for?\n",
    "IDF stands for Inverse Document Frequency. The purpose of IDF is to emphasize rarer words, and avoiding the common words like \"the\" and \"and\" etc. The IDF is the log of the number of documents divided by the number of documents that contain the word. The more documents that contain a word, the lower the word's IDF, and the less important that word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global counters to keep track of missing authors and papers:\n",
    "missing_authors = 0\n",
    "missing_papers = 0\n",
    "\n",
    "# Get a list of all the papers authored by the authors in a community:\n",
    "def get_papers_to_check(community: list) -> list:\n",
    "    global missing_authors\n",
    "    papers_to_check = []\n",
    "    print(\"Getting papers from authorId...\")\n",
    "    for author_id in tqdm(community):\n",
    "        try: papers_to_check.extend(author_lookup.loc[str(author_id)][\"paperId\"].tolist())\n",
    "        except: missing_authors += 1\n",
    "    return papers_to_check\n",
    "\n",
    "# Get a list of all the tokens from each paper in a list:\n",
    "def get_tokens_from_papers(papers_to_check: list) -> list:\n",
    "    global missing_papers\n",
    "    total_tokens = []\n",
    "    print(\"Getting tokens from papers...\")\n",
    "    for paper_id in tqdm(papers_to_check):\n",
    "        try: total_tokens.extend(paper_tokens_lookup.loc[paper_id][\"Tokens\"])\n",
    "        except: missing_papers += 1\n",
    "    return total_tokens\n",
    "\n",
    "# Get the total tokens for each community in the list:\n",
    "def generate_token_groups(communities):\n",
    "    paper_groups = []\n",
    "    token_groups = []\n",
    "    for idx, community in enumerate(communities):\n",
    "        print(f\"Generating token groups... {idx + 1} of {len(communities)}\\n\")\n",
    "        papers_to_check = get_papers_to_check(list(community))\n",
    "        paper_groups.extend(papers_to_check)\n",
    "        token_groups.extend(get_tokens_from_papers(papers_to_check))\n",
    "        clear_output()\n",
    "    print(f\"All {len(communities)} communities successfully generated.\")\n",
    "    return token_groups, paper_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 communities by author count: [2594, 2469, 1819, 1574, 1326, 1026, 1006, 1005, 981, 977]\n"
     ]
    }
   ],
   "source": [
    "# It takes way too much time to genereate all the ~600 token groups, so I opted to just generate the top 10 and ignore the rest.\n",
    "# Finding the top 10 communities by author count:\n",
    "sorted_args = np.array([len(community) for community in communities]).argsort()[::-1]\n",
    "top_communities = [communities[i] for i in sorted_args[:10]]\n",
    "\n",
    "print(\"Top 10 communities by author count:\", [(len(communities[i])) for i in sorted_args][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 10 successfully generated.\n"
     ]
    }
   ],
   "source": [
    "# Generating token groups for each of the top communities:\n",
    "top_communities_tokens, top_communities_papers = generate_token_groups(top_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the top_communities_tokens and saving it:\n",
    "\n",
    "# Zips the all unique tokens with their # of occurences in the list:\n",
    "zipped_unique = lambda lister: zip(*np.unique(lister, return_counts=True))\n",
    "# Sorts the zipped list by the second element in descending order:\n",
    "sorter = lambda zipper: sorted(zipper, key=lambda x: x[1], reverse=True)\n",
    "# Making the sorted dicts for each top community:\n",
    "top_communities_tokens_uniquecount = [dict(sorter(zipped_unique(i))) for i in top_communities_tokens]\n",
    "\n",
    "# Making it into a dataframe:\n",
    "tokens_df = pd.DataFrame({\"Communities\": top_communities, \"PaperIds\": top_communities_papers, \"Tokens\": top_communities_tokens_uniquecount})\n",
    "# tokens_df.to_csv(\"BigData/top_community_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Communities</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{1409253380, 8085509, 12926984, 115777544, 308...</td>\n",
       "      <td>{'patients': 5145, 'data': 4470, 'health': 401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{118390786, 35053571, 1867785, 2079686669, 281...</td>\n",
       "      <td>{'data': 5550, 'model': 3909, 'results': 3103,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{12759041, 51884035, 1732611, 1637421061, 2116...</td>\n",
       "      <td>{'data': 7595, 'information': 3213, 'paper': 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{103301121, 22532100, 2166793, 3182604, 939745...</td>\n",
       "      <td>{'models': 9180, 'model': 8473, 'tasks': 7899,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{6590464, 39137288, 79101961, 11493385, 153317...</td>\n",
       "      <td>{'patients': 341, 'usa': 290, 'ffr': 177, 'cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{103362561, 104681475, 113463301, 25157637, 15...</td>\n",
       "      <td>{'gamma-ray': 3693, 'emission': 3463, 'data': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{2090979328, 46567426, 66150403, 2174097413, 1...</td>\n",
       "      <td>{'workshop': 1272, 'intelligence': 1268, 'rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{2153465857, 32350210, 13398019, 3948545, 5103...</td>\n",
       "      <td>{'model': 1617, 'data': 1364, 'models': 1229, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{1484736514, 3121155, 4771844, 104284168, 2068...</td>\n",
       "      <td>{'emotions': 83466, 'interventions': 56043, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{145199105, 1761281, 48494598, 51390475, 49647...</td>\n",
       "      <td>{'research': 10436, 'health': 10034, 'original...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Communities  \\\n",
       "0  {1409253380, 8085509, 12926984, 115777544, 308...   \n",
       "1  {118390786, 35053571, 1867785, 2079686669, 281...   \n",
       "2  {12759041, 51884035, 1732611, 1637421061, 2116...   \n",
       "3  {103301121, 22532100, 2166793, 3182604, 939745...   \n",
       "4  {6590464, 39137288, 79101961, 11493385, 153317...   \n",
       "5  {103362561, 104681475, 113463301, 25157637, 15...   \n",
       "6  {2090979328, 46567426, 66150403, 2174097413, 1...   \n",
       "7  {2153465857, 32350210, 13398019, 3948545, 5103...   \n",
       "8  {1484736514, 3121155, 4771844, 104284168, 2068...   \n",
       "9  {145199105, 1761281, 48494598, 51390475, 49647...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  {'patients': 5145, 'data': 4470, 'health': 401...  \n",
       "1  {'data': 5550, 'model': 3909, 'results': 3103,...  \n",
       "2  {'data': 7595, 'information': 3213, 'paper': 3...  \n",
       "3  {'models': 9180, 'model': 8473, 'tasks': 7899,...  \n",
       "4  {'patients': 341, 'usa': 290, 'ffr': 177, 'cor...  \n",
       "5  {'gamma-ray': 3693, 'emission': 3463, 'data': ...  \n",
       "6  {'workshop': 1272, 'intelligence': 1268, 'rese...  \n",
       "7  {'model': 1617, 'data': 1364, 'models': 1229, ...  \n",
       "8  {'emotions': 83466, 'interventions': 56043, 'r...  \n",
       "9  {'research': 10436, 'health': 10034, 'original...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df = pd.read_csv(\"BigData/top_community_tokens.csv\", index_col=0, converters={\"Tokens\": literal_eval})\n",
    "tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting papers from authorId...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 223/26831 [00:20<35:08, 12.62it/s]"
     ]
    }
   ],
   "source": [
    "[get_papers_to_check(list(community)) for community in tokens_df[\"Communities\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe similarities and differences between the communities:\n",
    "There are some generic similarities between the communities, all of them have the \"data\" token in their top most frequent, similarly \"model\", \"models\", \"patients\", \"research\", \"paper\", etc. are also words that come up a lot and are all clearly words generally related to the topic of scientific research. But in the top 3 there are also some differences. Community 5 mentions \"gamma-ray\" and \"emission\", while community 6 \n",
    "mentions \"workshop\" and \"intelligence\", and community 8 focuses on \"emotions\" and \"interventions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict([(item[0], item[1] / sum(list(tokens_df[\"Tokens\"][0].values())) * 100) for item in tokens_df[\"Tokens\"][0].items()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why aren't the TFs not necessarily a good description of the communities?\n",
    "As I mentionend when describing the similarities, there are a lot of generic words related to the topic of scientific research that you're basically expected to use in your abstract. So looking purely at the frequency of words doesn't tell you much.\n",
    "\n",
    "### Next, we calculate IDF for every word.\n",
    "\n",
    "\n",
    "### What base logarithm did you use? Is that important?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're ready to calculate TF-IDF. Do that for the top 9 communities (by number of authors). Then for each community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"These are the top 9 communities:\")\n",
    "\n",
    "\n",
    "display(\"These are the top 10 TF words:\")\n",
    "\n",
    "display(\"These are the top 10 TF-IDF words:\")\n",
    "\n",
    "display(\"These are the top 3 authors (by degree):\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "376dd7633afddb7e11c0a9ffa5f656d1a19ddfafae55033d07193535982b765e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

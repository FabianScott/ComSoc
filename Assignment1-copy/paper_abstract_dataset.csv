PaperId,Abstract
18b71d35e6d059a2faa5ddc51b93b207f4df7d15,"Goal: Smartphone and wearable devices may act as powerful tools to remotely monitor physical function in people with neurodegenerative and autoimmune diseases from out-of-clinic environments. Detection of progression onset or worsening of symptoms is especially important in people living with multiple sclerosis (PwMS) in order to enable optimally adapted therapeutic strategies. MS symptoms typically follow subtle and fluctuating disease courses, patient-to-patient, and over time. Current in-clinic assessments are often too infrequently administered to reflect longitudinal changes in MS impairment that impact daily life. This work, therefore, explores how smartphones can administer daily two-minute walking assessments to monitor PwMS physical function at home. Methods: Remotely collected smartphone inertial sensor data was transformed through state-of-the-art Deep Convolutional Neural Networks, to estimate a participant's daily ambulatory-related disease severity, longitudinally over a 24-week study. Results: This study demonstrated that smartphone-based ambulatory severity outcomes could accurately estimate MS level of disability, as measured by the EDSS score ($r^{2}$: 0.56,$p< $0.001). Furthermore, longitudinal severity outcomes were shown to accurately reflect individual participants' level of disability over the study duration. Conclusion: Smartphone-based assessments, that can be performed by patients from their home environments, could greatly augment standard in-clinic outcomes for neurodegenerative diseases. The ability to understand the impact of disease on daily-life between clinical visits, through objective digital outcomes, paves the way forward to better measure and identify signs of disease progression that may be occurring out-of-clinic, to monitor how different patients respond to various treatments, and to ultimately enable the development of better, and more personalised care."
1d34691f6cbb2c7d1d4191b0b8c6ef0ce2d59c8c,"Goal : Smartphone and wearable devices may act as powerful tools to remotely monitor physical function in people with neurodegenerative and autoimmune diseases from out-of-clinic environments. Detection of progression onset or worsening of symptoms is especially important in people living with multiple sclerosis (PwMS) in order to enable optimally adapted therapeutic strategies. MS is a disease whose symptoms typically follow subtle and fluctuating disease courses, patient-to-patient, and over time. Current in-clinic assessments are often too infrequently administered to reflect longitudinal changes in MS impairment that impact daily life. This work, therefore, explores how smartphones can administer daily two-minute walking assessments to monitor PwMS physical function at home. Methods : Remotely collected smartphone inertial sensor data was transformed through state-of-the-art Deep Convolutional Neural Networks, to estimate a participant’s daily ambulatory-related disease severity, longitudinally over a 24-week study. Results : This study demonstrated that smartphone-based ambulatory severity outcomes could accurately estimate MS level of disability, as measured by the EDSS score ( r 2 : 0.56, p < 0.001). Furthermore, longitudinal severity outcomes were shown to accurately reflect individual participants’ level of disability over the study duration. Conclusion : Smartphone-based assessments, that can be performed by patients from their home environments, could greatly augment standard in-clinic outcomes for neurodegenerative diseases. The ability to understand the impact of disease on daily-life between clinical visits, through objective digital outcomes, paves the way forward to better measure and identify signs of disease progression that may be occurring out-of-clinic, to monitor how different patients respond to various treatments, and to ultimately enable the development of better, and more personalised care."
240bbfb2b2e3df5086a83778ea682e0bf32178f1,"Objectives To examine the socioeconomic and demographic drivers associated with polypharmacy (5–9 medicines), extreme polypharmacy (9–20 medicines) and increased medication count. Design, setting and participants A total of 5509 participants, from two waves of the English North West Coast, Household Health Survey were analysed Outcome measures Logistic regression modelling was used to find associations with polypharmacy and extreme polypharmacy. A negative binomial regression identified associations with increased medication count. Descriptive statistics explored associations with medication management. Results Age and number of health conditions account for the greatest odds of polypharmacy. ORs (95% CI) were greatest for those aged 65+ (3.87, 2.45 to 6.13) and for those with ≥5 health conditions (10.87, 5.94 to 19.88). Smaller odds were seen, for example, in those prescribed cardiovascular medications (3.08, 2.36 to 4.03), or reporting >3 emergency attendances (1.97, 1.23 to 3.17). Extreme polypharmacy was associated with living in a deprived neighbourhood (1.54, 1.06 to 2.26). The greatest risk of increased medication count was associated with age, number of health conditions and use of primary care services. Relative risks (95% CI) were greatest for those aged 65+ (2.51, 2.23 to 2.82), those with ≥5 conditions (10.26, 8.86 to 11.88) or those reporting >18 primary care visits (2.53, 2.18 to 2.93). Smaller risks were seen in, for example, respondents with higher levels of income deprivation (1.35, 1.03 to 1.77). Polypharmic respondents were more likely to report medication management difficulties associated with taking more than one medicine at a time (p<0.001). Furthermore, individuals reporting a mental health condition, were significantly more likely to consistently report difficulties managing their medication (p<0.001). Conclusion Age and number of health conditions are most associated with polypharmacy. Thus, delaying or preventing the onset of long-term conditions may help to reduce polypharmacy. Interventions to reduce income inequalities and health inequalities generally could support a reduction in polypharmacy, however, more research is needed in this area. Furthermore, increased prevention and support, particularly with medication management, for those with mental health conditions may reduce adverse medication effects."
325e41989bf94a686dc06625ec310c8a4412ffba,"The circadian clock controls many aspects of physiology, but it remains undescribed whether extracellular vesicles (EVs), including exosomes, involved in cell-cell communications between tissues are regulated in a circadian pattern. We demonstrate a 24-hour rhythmic abundance of individual proteins in small EVs using liquid chromatography–mass spectrometry in circadian-synchronized tendon fibroblasts. Furthermore, the release of small EVs enriched in RNA binding proteins was temporally separated from those enriched in cytoskeletal and matrix proteins, which peaked during the end of the light phase. Last, we targeted the protein sorting mechanism in the exosome biogenesis pathway and established (by knockdown of circadian-regulated flotillin-1) that matrix metalloproteinase 14 abundance in tendon fibroblast small EVs is under flotillin-1 regulation. In conclusion, we have identified proteomic time signatures for small EVs released by tendon fibroblasts, which supports the view that the circadian clock regulates protein cargo in EVs involved in cell-cell cross-talk."
485804bdf0e2b0b44c986896a3fdcbca40adf6ea,
48963ee66acffc068211b0915e9a456100ea5bdf,To validate the smartphone sensor‐based Draw a Shape Test – a part of the Floodlight Proof‐of‐Concept app for remotely assessing multiple sclerosis‐related upper extremity impairment by tracing six different shapes.
b854e4dcd760838334e71d6e8e294a95786fa2fb,
eb70c4d0b1b1a1ac66073153e300c97414fbd56b,"—Personalized longitudinal disease assess- ment is central to quickly diagnosing, appropriately managing, and optimally adapting the therapeutic strategy of multiple sclerosis (MS). It is also important for identifying the idiosyncratic subject-speciﬁc disease proﬁles. Here, we design a novel longitudinal model to map individual disease trajectories in an automated way using sensor data that may contain missing values. First, we collect digital measurements related to gait and balance, and up- per extremity functions using sensor-based assessments administered on a smartphone. Next, we treat missing data via imputation. We then discover potential markers of MS by employing a generalized estimation equation. Subse- quently, parameters learned from multiple training datasets are ensembled to form a simple, uniﬁed longitudinal predic- tive model to forecast MS over time in previously unseen people with MS. To mitigate potential underestimation for individuals with severe disease scores, the ﬁnal model incorporates additional subject-speciﬁc ﬁne-tuning using data from the ﬁrst day. The results show that the proposed model is promising to achieve personalized longitudinal MS assessment; they also suggest that features related to gait and balance as well as upper extremity function, remotely collected from sensor-based assessments, may be useful digital markers for predicting MS over time."
3eff8f9ba51b355afb008026faf641fdf2913aed,"Signs and symptoms of movement disorders can be remotely measured at home through sensor-based assessment of gait. However, sensor noise may impact the robustness of such assessments, in particular in a Bring-Your-Own-Device setting where the quality of sensors might vary. Here, we propose a framework to study the impact of inertial measurement unit noise on sensor-based gait features. This framework includes synthesizing realistic acceleration signals from the lower back during a gait cycle in OpenSim, estimating the magnitude of sensor noise from five smartphone models, perturbing the synthesized acceleration signal with the estimated noise in a Monte Carlo simulation, and computing gait features. In addition, we show that realistic levels of sensor noise have only a negligible impact on step power, a measure of gait."
689eef847d84b8a37c384260525707bc7df30cc2,
720e039241cc0970a3de0c4b01b01d58a274d3ae,"Abstract The human gut microbiome has been shown to be associated with a variety of human diseases, including cancer, metabolic conditions and inflammatory bowel disease. Current approaches for detecting microbiome associations are limited by relying on specific measures of ecological distance, or only allowing for the detection of associations with individual bacterial species, rather than the whole microbiome. In this work, we develop a novel hierarchical Bayesian model for detecting global microbiome associations. Our method is not dependent on a choice of distance measure, and is able to incorporate phylogenetic information about microbial species. We perform extensive simulation studies and show that our method allows for consistent estimation of global microbiome effects. Additionally, we investigate the performance of the model on two real-world microbiome studies: a study of microbiome-metabolome associations in inflammatory bowel disease, and a study of associations between diet and the gut microbiome in mice. We show that we can use the method to reliably detect associations in real-world datasets with varying numbers of samples and covariates."
cc36c1873f4c83b3c89ef743769015656de8384c,"Circadian rhythms shift toward an evening preference during adolescence, a developmental period marked by greater focus on the social domain and salience of social hierarchies. The circadian system influences maturation of cognitive architecture responsible for motivation and reward, and observation of responses to reward cues has provided insights into neurocognitive processes that underpin adolescent social development. The objective was to investigate whether circadian phase of entrainment (chronotype) predicted both reward‐related response inhibition and social status, and to explore whether mediator and moderator relationships existed between chronotype, reward processing, and social status outcomes."
cd5b405a63b562bbb97bdfdd0bc9505f773a6aa6,"
 Background/Aims 
 Body composition changes are associated with changes in bone mineral density (BMD). Composite measures of body compartments, such as weight and body mass index (BMI) have a positive association with BMD. The aim was to study average percent fat from dual energy X-ray absorptiometry (DEXA) as a potentially useful clinical measurement.
 Methods 
 BMD data in grams/cenitmetre2 was collected from DEXA scans after referral from secondary care to the Royal Lancaster Infirmary from 2004-2010. BMD data related to the left and right hip (the neck, Ward’s area, trochanter, and total hip), and the spine (L1-L4) was measured. Data was collected longitudinally, and BMD in g/cm2 was modelled at the regions of the hip and spine using mixed effects linear models. Average percent fat and weight (kg) were used as explanatory variables, whilst adjusting for age at scan, gender, and other risk factors such as FRAX risk factors.
 Results 
 7910 patients (88% female) were included, all with average percent fat and weight measurements. The results of the models (Table 1) all have a P value<0.05. Average percent fat had a significant negative association at all regions of the left and right hip, but a significant positive association at the spine. Weight showed a significant positive association with BMD at the hip and spine. P119 Table 1:Effect size estimates from mixed effects models of BMD at regions of the hip, and the spine.Anatomical locationEffect size estimate for average percent fat (95% confidence intervals)Effect size estimate for weight (95% confidence intervals)Left neck-6.63x10-4 (-9.69x10-4, -3.56x10-4)2.07x10-3 (1.91x10-3, 2.23x10-3)Left total-1.03x10-3 (-1.32x10-3, -7.41x10-4)3.45x10-3 (3.29x10-3, 3.61x10-3)Left Ward’s-1.07x10-3 (-1.38x10-3, -7.65x10-4)1.85x10-3 (1.69x10-3, 2.02x10-3)Left trochanter-1.15x10-3 (-1.46x10-3, -8.47x10-4)3.65x10-3 (3.48x10-3, 3.81x10-3)Right neck-6.91x10-4 (-9.94x10-4, -3.88x10-4)1.97x10-3 (1.81x10-3, 2.14x10-3)Right total-1.19x10-3 (-1.48x10-3, -8.96x10-4)3.39x10-3 (3.23x10-3, 3.55x10-3)Right Ward’s-1.07x10-3 (-1.38x10-3, -7.65x10-4)1.85x10-3 (1.69x10-3, 2.02x10-3)Right trochanter-1.19x10-3 (-1.52x10-3, -8.72x10-4)3.65x10-3 (3.39x10-3, 3.73x10-3)Spine (averaged L1-L4)1.76x10-3 (1.46x10-3, 2.05x10-3)1.42x10-3 (1.16x10-3, 1.68x10-3)
 Conclusion 
 The negative association seen with average percent fat at the hip could reflect the potential negative endocrine effects of fat, and the effect of localized inflammation at the hip. Increased adiposity is also linked to sarcopenia, and further body compositional changes. However, the potential negative effects of increased adiposity at the abdomen and spine are potentially overridden by the increased biomechanical loading generated by the increased adiposity, highlighted by weight’s positive association with BMD at the spine. The average percent fat results are not mirrored with weight at the hip. This highlights that composite measures are not specific enough to measure changes in body composition compartments, and their resulting change in risk related to BMD.
 Disclosure 
 C. Thurston: None. J. Kerns: None. F. Dondelinger: None. A. Hale: None. M. Bukhari: None."
1626bd976dbcaddcb98c29abc572cfc10a7d2127,
236e72a66e08eaab576aa02cbd400bbaf2ea53fc,"Drug high-throughput screenings across large molecular-characterised cancer cell line panels enable the discovery of biomarkers, and thereby, cancer precision medicine. The ability to experimentally generate drug response data has accelerated. However, this data is typically quantified by a summary statistic from a best-fit dose response curve, whilst neglecting the uncertainty of the curve fit and the potential variability in the raw readouts. Here, we model the experimental variance using Gaussian Processes, and subsequently, leverage this uncertainty for identifying associated biomarkers with a new statistical framework based on Bayesian testing. Applied to the Genomics of Drug Sensitivity in Cancer, in vitro screening data on 265 compounds across 1,074 cell lines, our uncertainty models identified 24 clinically established drug response biomarkers, and in addition provided evidence for 6 novel biomarkers. We validated our uncertainty estimates with an additional drug screen of 26 drugs, 10 cell lines with 8 to 9 replicates. Our method is applicable to drug high-throughput screens without replicates, and enables robust biomarker discovery for new cancer therapies."
238747c098b23d6e033cabbf00b5d0b231e2cfc7,"Objective To characterise the clinical features of patients admitted to hospital with coronavirus disease 2019 (covid-19) in the United Kingdom during the growth phase of the first wave of this outbreak who were enrolled in the International Severe Acute Respiratory and emerging Infections Consortium (ISARIC) World Health Organization (WHO) Clinical Characterisation Protocol UK (CCP-UK) study, and to explore risk factors associated with mortality in hospital. Design Prospective observational cohort study with rapid data gathering and near real time analysis. setting A case report ISARIC time"
2499dcaa930d5b80913bf404de69826c1030cc4a,"Objective To characterise the clinical features of patients admitted to hospital with coronavirus disease 2019 (covid-19) in the United Kingdom during the growth phase of the first wave of this outbreak who were enrolled in the International Severe Acute Respiratory and emerging Infections Consortium (ISARIC) World Health Organization (WHO) Clinical Characterisation Protocol UK (CCP-UK) study, and to explore risk factors associated with mortality in hospital. Design Prospective observational cohort study with rapid data gathering and near real time analysis. setting A case report ISARIC time"
26bb1c7c2f9ec582324ff5cc240a8466cabb0bef,"Objective: To characterize the clinical features of patients with severe COVID-19 in the UK. Design: Prospective observational cohort study with rapid data gathering and near real-time analysis, using a pre-approved questionnaire adopted by the WHO. Setting: 166 UK hospitals between 6th February and 18th April 2020. Participants: 16,749 people with COVID-19. Interventions: No interventions were performed, but with consent samples were taken for research purposes. Many participants were co-enrolled in other interventional studies and clinical trials. Results: The median age was 72 years [IQR 57, 82; range 0, 104], the median duration of symptoms before admission was 4 days [IQR 1,8] and the median duration of hospital stay was 7 days [IQR 4,12]. The commonest comorbidities were chronic cardiac disease (29%), uncomplicated diabetes (19%), non-asthmatic chronic pulmonary disease (19%) and asthma (14%); 47% had no documented reported comorbidity. Increased age and comorbidities including obesity were associated with a higher probability of mortality. Distinct clusters of symptoms were found: 1. respiratory (cough, sputum, sore throat, runny nose, ear pain, wheeze, and chest pain); 2. systemic (myalgia, joint pain and fatigue); 3. enteric (abdominal pain, vomiting and diarrhoea). Overall, 49% of patients were discharged alive, 33% have died and 17% continued to receive care at date of reporting. 17% required admission to High Dependency or Intensive Care Units; of these, 31% were discharged alive, 45% died and 24% continued to receive care at the reporting date. Of those receiving mechanical ventilation, 20% were discharged alive, 53% died and 27% remained in hospital. Conclusions: We present the largest detailed description of COVID-19 in Europe, demonstrating the importance of pandemic preparedness and the need to maintain readiness to launch research studies in response to outbreaks. Trial documentation: Available at https://isaric4c.net/protocols . Ethical approval in England and Wales (13/SC/0149), and Scotland (20/SS/0028). ISRCTN (pending)."
28266a88e8164cec64c4008964000afd13af3ca6,
413911c8b0b245767f99fc257ec7cf3f38596095,"Large-scale longitudinal data are often heterogeneous, spanning latent subgroups such as disease subtypes. In this paper, we present an approach called {it longitudinal joint cluster regression} (LJCR) for penalized mixed modelling in the latent group setting. LJCR captures latent group structure via a mixture model that includes both the multivariate distribution of the covariates and a regression model for the response. The longitudinal dynamics of each individual are modeled using a random effect intercept and slope model. Inference is done via a profile likelihood approach that can handle high-dimensional covariates via ridge penalization. LJCR is motivated by questions in neurodegenerative disease research, where latent subgroups may reflect heterogeneity with respect to disease presentation, progression and diverse subject-specific factors. We study the performance of LJCR in the context of two longitudinal datasets: a simulation study and a study of amyotrophic lateral sclerosis (ALS). LJCR allows prediction of progression as well as identification of subgroups and subgroup-specific model parameters."
470b1fcf2269626dfb8237b85862e4c7c69e4c33,"Motivation The human gut microbiome has been shown to be associated with a variety of human diseases, including cancer, metabolic conditions and inflammatory bowel disease. Current statistical techniques for microbiome association studies are limited by relying on measures of ecological distance, or only allowing for the detection of associations with individual bacterial species, rather than the whole microbiome. Results In this work, we develop a novel Bayesian multi-task approach for detecting global microbiome associations. Our method is not dependent on a choice of distance measure, and is able to incorporate phylogenetic information about microbial species. We apply our method to simulated data and show that it allows for consistent estimation of global microbiome effects. Additionally, we investigate the performance of the model on two real-world microbiome studies: a study of microbiome-metabolome associations in inflammatory bowel disease (Beamish, 2017), and a study of associations between diet and the gut microbiome in mice (Turnbaugh et al., 2009). We show that we can use the method to reliably detect associations in real-world datasets with varying numbers of samples and covariates. Availability Our method is implemented using the R interface to the Stan Hamiltonian Monte Carlo sampler. Software for running our methods is available at https://github.com/FrankD/MicrobiomeGlobalAssociations. Contact f.dondelinger@lancaster.ac.uk"
7ee38006777c99bc0b7ece47bddb9f8304a8b64c,"
 
 
 We present our package, mrbayes, for the open source software environment R. The package implements Bayesian estimation for inverse variance weighted (IVW) and MR-Egger models, including the radial MR-Egger model, for summary-level data in Mendelian randomization (MR) analyses.
 
 
 
 We have implemented a choice of prior distributions for the model parameters, namely; weakly informative, non-informative, a joint prior for the MR-Egger model slope and intercept, and an informative prior (pseudo-horseshoe prior), or the user can specify their own prior distribution.
 
 
 
 Users have the option of fitting the models using either JAGS or Stan software packages with similar prior distributions; the option for the user-defined prior distribution is only in our JAGS functions. We show how to use the package through an applied example investigating the causal effect of body mass index (BMI) on acute ischaemic stroke.
 
 
 
 The package is freely available, under the GNU General Public License v3.0, on GitHub [https://github.com/okezie94/mrbayes] or CRAN [https://CRAN.R-project.org/package=mrbayes].
"
9d8e0d66de8c0d98a37d3c8adeda45b819676ca4,"A decreased body mass index (BMI) is associated with poorer bone health, a decreased bone mineral density (BMD), and an increased fracture risk. Cardiovascular (CVS) data has shown that the waist:hip ratio is a more robust measurement for CVS outcomes than BMI (1). Waist:hip ratio has never been evaluated as an outcome measure for bone health. Dual-energy x-ray absorptiometry (DEXA) has the capacity to measure average percentage fat in the L1-L4 region and at the hip, and directly relates to the measurement of waist:hip ratio.To evaluate the relationship between BMD and average percent fat in a cohort referred for DEXA scanning.We analysed data routinely collected from patients referred for DEXA between 2004 and 2010 at the Royal Lancaster Infirmary in the North of England. Data collected for these patients included DEXA scans of BMD at the left and right hip, and at the lumbar spine, as well as average percent far and other risk factors for osteoporosis, including the FRAX risk factors. We used only the measures collected at baseline (time of first scan). We modelled the T scores of the BMD measurements using a linear regression model including percentage fat and BMI as explanatory variables, and adjusting for gender, age at scan, and other known risk factors for osteoporosis, including the FRAX risk factors. BMI and average percent fat were standardised.The number of patients included was 33037, (82% female). Results of both regression models are shown in table 1 below. We show the standardised effect size estimates for average percent fat and BMI.Anatomical locationEffect size estimate for average percent fat (95% confidence intervals)P valueEffect size estimate for BMI (95% confidence intervals)P valueLeft neck-0.156 (-0.171, -0.141)<0.001-0.0255 (-0.0441, -0.00701)0.00692Left total-0.225 (-0.241, -0.208)<0.001-0.0680 (-0.0882, -0.0477)<0.001Left Ward’s-0.181 (-0.196, -0.166)<0.001-0.0268 (-0.0456, -0.00813)0.00493Left trochanter-0.263 (-0.281, -0.246)<0.001-0.0667 (-0.0882, -0.0451)<0.001Right neck-0.139 (-0.154, -0.124)<0.001-0.0131 (-0.0317, 0.00549)0.167Right total-0.221 (-0.237, -0.204)<0.001-0.0611 (-0.0811, -0.0411)<0.001Right Ward’s-0.180 (-0.196, -0.165)<0.001-0.0193 (-0.0381, -0.000586)0.0433Right trochanter-0.261 (-0.278, -0.243)<0.001-0.0598 (-0.0810, -0.0386)<0.001Spine (averaged L1-L4)0.219 (0.195, 0.242)<0.001-0.00846 (-0.0379, 0.0206)0.563The analysis shows that average percent fat is a statistically significant predictor for BMD at different anatomical locations, and a larger predictor in comparison to BMI when evaluated in the same model. In the right hip neck and the spine, BMI was not predictive of changes in BMD. Higher average percent fat increases the BMD in the spine, compared to a decline at the hip. Further research is needed to characterise the relationship more precisely and identify whether there is a causal link.[1]Obes Rev. 2012 Mar;13(3):275-86. doi: 10.1111/j.1467-789X.2011.00952.xNone declared"
b3f594a558496ed094d36ff3bbeb21ef76201671,
ce00cc8fa20d7eeb873407539ea1e0c4ed1e6c51,"The increasing levels of pesticide resistance in agricultural pests and disease vectors represents a threat to both food security and global health. As insecticide resistance intensity strengthens and spreads, the likelihood of a pest encountering a sub-lethal dose of pesticide dramatically increases. Here, we apply dynamic Bayesian networks to a transcriptome time-course generated using sub-lethal pyrethroid exposure on a highly resistant Anopheles coluzzii population. The model accounts for circadian rhythm and ageing effects allowing high confidence identification of transcription factors with key roles in pesticide response. The associations generated by this model show high concordance with lab-based validation and identifies 44 transcription factors regulating insecticide-responsive transcripts. We identify six key regulators, with each displaying differing enrichment terms, demonstrating the complexity of pesticide response. The considerable overlap of resistance mechanisms in agricultural pests and disease vectors strongly suggests that these findings are relevant in a wide variety of pest species."
f4f6ef1fa128d466200bdd09a5759c748ead894c,"Abstract Objective To characterise the clinical features of patients admitted to hospital with coronavirus disease 2019 (covid-19) in the United Kingdom during the growth phase of the first wave of this outbreak who were enrolled in the International Severe Acute Respiratory and emerging Infections Consortium (ISARIC) World Health Organization (WHO) Clinical Characterisation Protocol UK (CCP-UK) study, and to explore risk factors associated with mortality in hospital. Design Prospective observational cohort study with rapid data gathering and near real time analysis. Setting 208 acute care hospitals in England, Wales, and Scotland between 6 February and 19 April 2020. A case report form developed by ISARIC and WHO was used to collect clinical data. A minimal follow-up time of two weeks (to 3 May 2020) allowed most patients to complete their hospital admission. Participants 20 133 hospital inpatients with covid-19. Main outcome measures Admission to critical care (high dependency unit or intensive care unit) and mortality in hospital. Results The median age of patients admitted to hospital with covid-19, or with a diagnosis of covid-19 made in hospital, was 73 years (interquartile range 58-82, range 0-104). More men were admitted than women (men 60%, n=12 068; women 40%, n=8065). The median duration of symptoms before admission was 4 days (interquartile range 1-8). The commonest comorbidities were chronic cardiac disease (31%, 5469/17 702), uncomplicated diabetes (21%, 3650/17 599), non-asthmatic chronic pulmonary disease (18%, 3128/17 634), and chronic kidney disease (16%, 2830/17 506); 23% (4161/18 525) had no reported major comorbidity. Overall, 41% (8199/20 133) of patients were discharged alive, 26% (5165/20 133) died, and 34% (6769/20 133) continued to receive care at the reporting date. 17% (3001/18 183) required admission to high dependency or intensive care units; of these, 28% (826/3001) were discharged alive, 32% (958/3001) died, and 41% (1217/3001) continued to receive care at the reporting date. Of those receiving mechanical ventilation, 17% (276/1658) were discharged alive, 37% (618/1658) died, and 46% (764/1658) remained in hospital. Increasing age, male sex, and comorbidities including chronic cardiac disease, non-asthmatic chronic pulmonary disease, chronic kidney disease, liver disease and obesity were associated with higher mortality in hospital. Conclusions ISARIC WHO CCP-UK is a large prospective cohort study of patients in hospital with covid-19. The study continues to enrol at the time of this report. In study participants, mortality was high, independent risk factors were increasing age, male sex, and chronic comorbidity, including obesity. This study has shown the importance of pandemic preparedness and the need to maintain readiness to launch research studies in response to outbreaks. Study registration ISRCTN66726260."
fdf2447af2a72689da98226eb97dc44ec990c18c,"Cancer treatments can be highly toxic and frequently only a subset of the patient population will benefit from a given treatment. Tumour genetic makeup plays an important role in cancer drug sensitivity. We suspect that gene expression markers could be used as a decision aid for treatment selection or dosage tuning. Using in vitro cancer cell line dose-response and gene expression data from the Genomics of Drug Sensitivity in Cancer (GDSC) project, we build a dose-varying regression model. Unlike existing approaches, this allows us to estimate dosage-dependent associations with gene expression. We include the transcriptomic profiles as dose-invariant covariates into the regression model and assume that their effect varies smoothly over the dosage levels. A two-stage variable selection algorithm (variable screening followed by penalised regression) is used to identify genetic factors that are associated with drug response over the varying dosages. We evaluate the effectiveness of our method using simulation studies focusing on the choice of tuning parameters and cross-validation for predictive accuracy assessment. We further apply the model to data from five BRAF targeted compounds applied to different cancer cell lines under different dosage levels. We highlight the dosage-dependent dynamics of the associations between the selected genes and drug response, and we perform pathway enrichment analysis to show that the selected genes play an important role in pathways related to tumourgenesis and DNA damage response. Author Summary Tumour cell lines allow scientists to test anticancer drugs in a laboratory environment. Cells are exposed to the drug in increasing concentrations, and the drug response, or amount of surviving cells, is measured. Generally, drug response is summarized via a single number such as the concentration at which 50% of the cells have died (IC50). To avoid relying on such summary measures, we adopted a functional regression approach that takes the dose-response curves as inputs, and uses them to find biomarkers of drug response. One major advantage of our approach is that it describes how the effect of a biomarker on the drug response changes with the drug dosage. This is useful for determining optimal treatment dosages and predicting drug response curves for unseen drug-cell line combinations. Our method scales to large numbers of biomarkers by using regularisation and, in contrast with existing literature, selects the most informative genes by accounting for drug response at untested dosages. We demonstrate its value using data from the Genomics of Drug Sensitivity in Cancer project to identify genes whose expression is associated with drug response. We show that the selected genes recapitulate prior biological knowledge, and belong to known cancer pathways."
0403783d81262c53f81ae0f5779744e2298b0d7d,"We present our package, mrbayes, for the open source software environment R. The package implements
Bayesian estimation for IVW and MR-Egger models, including the radial MR-Egger model, for
summary-level data in Mendelian randomization analyses. Users have the option of fitting the models
using either JAGS or Stan software packages. We have implemented a choice of prior distributions for the
model parameters, namely; weakly informative, non-informative, a joint prior for the MR-Egger model
slope and intercept, and an informative prior (pseudo-horseshoe prior), or the user can specify their own
prior. Similar prior distributions are included using the Stan software with the exception of a user-defined
prior. We include We show how to use the package through an applied example investigating the causal
effect of BMI on acute ischemic stroke. In future work, we plan to provide functions for Multivariable
MR estimation."
0cd08667c1db72f06d2e531a37a66e45f4583407,"Regularized regression models are well studied and, under appropriate conditions, offer fast and statistically interpretable results. However, large data in many applications are heterogeneous in the sense of harboring distributional differences between latent groups. Then, the assumption that the conditional distribution of response Y given features X is the same for all samples may not hold. Furthermore, in scientific applications, the covariance structure of the features may contain important signals and its learning is also affected by latent group structure. We propose a class of mixture models for paired data pX,Y q that couples together the distribution of X (using sparse graphical models) and the conditional Y | X (using sparse regression models). The regression and graphical models are specific to the latent groups and model parameters are estimated jointly (hence the name “regularized joint mixtures”). This allows signals in either or both of the feature distribution and regression model to inform learning of latent structure and provides automatic control of confounding by such structure. Estimation is handled via an expectation-maximization algorithm, whose convergence is established theoretically. We illustrate the key ideas via empirical examples. An R package is available at https://github.com/k-perrakis/regjmix."
38923bba3a60968f16babf2f67782a5a2fd3237a,"Mixture models are a standard approach to dealing with heterogeneous data with non-i.i.d. structure. However, when the dimension $p$ is large relative to sample size $n$ and where either or both of means and covariances/graphical models may differ between the latent groups, mixture models face statistical and computational difficulties and currently available methods cannot realistically go beyond $p \! \sim \! 10^4$ or so. We propose an approach called Model-based Clustering via Adaptive Projections (MCAP). Instead of estimating mixtures in the original space, we work with a low-dimensional representation obtained by linear projection. The projection dimension itself plays an important role and governs a type of bias-variance tradeoff with respect to recovery of the relevant signals. MCAP sets the projection dimension automatically in a data-adaptive manner, using a proxy for the assignment risk. Combining a full covariance formulation with the adaptive projection allows detection of both mean and covariance signals in very high dimensional problems. We show real-data examples in which covariance signals are reliably detected in problems with $p \! \sim \! 10^4$ or more, and simulations going up to $p = 10^6$. In some examples, MCAP performs well even when the mean signal is entirely removed, leaving differential covariance structure in the high-dimensional space as the only signal. Across a number of regimes, MCAP performs as well or better than a range of existing methods, including a recently-proposed $\ell_1$-penalized approach; and performance remains broadly stable with increasing dimension. MCAP can be run ""out of the box"" and is fast enough for interactive use on large-$p$ problems using standard desktop computing resources."
7262f9ea7bb59707d58bd350d8b5033a3eb45352,"We present our package, mrbayes, for the open source software environment R. The package implements Bayesian estimation for IVW and MR-Egger models, including the radial MR-Egger model, for summary-level data in Mendelian randomization analyses. Users have the option of fitting the models using either JAGS or Stan software packages. We have implemented a choice of prior distributions for the model parameters, namely; weakly informative, non-informative, a joint prior for the MR-Egger model slope and intercept, and an informative prior (pseudo-horseshoe prior), or the user can specify their own prior. Similar prior distributions are included using the Stan software with the exception of a user-defined prior. We include We show how to use the package through an applied example investigating the causal effect of BMI on acute ischemic stroke. In future work, we plan to provide functions for Multivariable MR estimation."
c87a41f9c94d7edd3b90164a05ca7d5e27920031,
e81fea928b659e05d32f64033b371732bd122d03,"Regression modelling typically assumes homogeneity of the conditional distribution of responses Y given features X. For inhomogeneous data, with latent groups having potentially different underlying distributions, the hidden group structure can be crucial for estimation and prediction, and standard regression models may be severely confounded. Worse, in the multivariate setting, the presence of such inhomogeneity can easily pass undetected. To allow for robust and interpretable regression modelling in the heterogeneous data setting we put forward a class of mixture models that couples together both the multivariate marginal on X and the conditional Y | X to capture the latent group structure. This joint modelling approach allows for group-specific regression parameters, automatically controlling for the latent confounding that may otherwise pose difficulties, and offers a novel way to deal with suspected distributional shifts in the data. We show how the latent variable model can be regularized to provide scalable solutions with explicit sparsity. Estimation is handled via an expectation-maximization algorithm. We illustrate the key ideas via empirical examples."
61404658f0b26ca8ddfc179f939f575838e82e2b,
bda0730fe88e30e4ad8b15216091f506c0abd7d9,"Abstract We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where subsets of samples, representing for example disease subtypes, may differ with respect to underlying regression models. In the high-dimensional setting, estimating a different model for each subgroup is challenging due to limited sample sizes. Focusing on the case in which subgroup-specific models may be expected to be similar but not necessarily identical, we treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} }{}$\ell_1$\end{document} term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer’s disease, amyotrophic lateral sclerosis, and cancer datasets. These examples demonstrate the gains joint estimation can offer in prediction as well as in providing subgroup-specific sparsity patterns."
1a6382db2bcd251f842e77e4d05c5d6bd65dbd03,
321c784059f78983b185dd1bda546084f0f980d8,
89d6cf4567bdaa2e18c418387f23460f8f4b003f,"Motivation: Molecular pathways and networks play a key role in basic and disease biology. An emerging notion is that networks encoding patterns of molecular interplay may themselves differ between contexts, such as cell type, tissue or disease (sub)type. However, while statistical testing of differences in mean expression levels has been extensively studied, testing of network differences remains challenging. Furthermore, since network differences could provide important and biologically interpretable information to identify molecular subgroups, there is a need to consider the unsupervised task of learning subgroups and networks that define them. This is a nontrivial clustering problem, with neither subgroups nor subgroup‐specific networks known at the outset. Results: We leverage recent ideas from high‐dimensional statistics for testing and clustering in the network biology setting. The methods we describe can be applied directly to most continuous molecular measurements and networks do not need to be specified beforehand. We illustrate the ideas and methods in a case study using protein data from The Cancer Genome Atlas (TCGA). This provides evidence that patterns of interplay between signalling proteins differ significantly between cancer types. Furthermore, we show how the proposed approaches can be used to learn subtypes and the molecular networks that define them. Availability and implementation: As the Bioconductor package nethet. Contact: staedler.n@gmail.com or sach.mukherjee@dzne.de Supplementary information: Supplementary data are available at Bioinformatics online."
91f100917f7501dd87a442bf06aa772d78ee9d4e,"The package deGradInfer is an R package for efficient parameter inference in systems of ordinary differential equations (ODEs), using adaptive gradient matching via Gaussian processes. This implementation follows from the work in Calderhead et al. (2009); Dondelinger et al. (2013) and Macdonald (2017), although the software has been extended with several crucial features to make it robust and useful for practical applications. Features include:"
6312eabbc8af7f32ed5a731777cb425d92e5e136,"We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where disease subtypes, for example, may differ with respect to underlying regression models, but sample sizes at the subgroup-level may be limited. We focus on the case in which subgroup-specific models may be expected to be similar but not necessarily identical. Our approach is to treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an ℓ1 term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer’s disease, amyotrophic lateral sclerosis and cancer datasets. These examples demonstrate the gains our approach can offer in terms of prediction and the ability to estimate subgroup-specific sparsity patterns."
66c7489d2b7fe2c21f8ffa940f49e9388996dd33,"Simplifying the process greatly, signalling proteins known as kinases can be unphosphorylated (inactive) or phosphorylated (active). Cell signalling uses the phosphorylation machinery to pass messages from the exterior of the cell to the interior where they will be acted upon. This message passing is achieved via a relay of kinases and other proteins (the signalling pathway), which can be thought of as a network."
e4ceebc7018187b0c17e6d390801af0e9fb371db,
3a16c1f9294a9b1b02830d94c81fac50b8daac21,
a8fe764ee996495b47e9ad27e1376a171892626e,"Motivation: Networks are widely used as structural summaries of biochemical systems. Statistical estimation of networks is usually based on linear or discrete models. However, the dynamics of biochemical systems are generally non-linear, suggesting that suitable non-linear formulations may offer gains with respect to causal network inference and aid in associated prediction problems. Results: We present a general framework for network inference and dynamical prediction using time course data that is rooted in non-linear biochemical kinetics. This is achieved by considering a dynamical system based on a chemical reaction graph with associated kinetic parameters. Both the graph and kinetic parameters are treated as unknown; inference is carried out within a Bayesian framework. This allows prediction of dynamical behavior even when the underlying reaction graph itself is unknown or uncertain. Results, based on (i) data simulated from a mechanistic model of mitogen-activated protein kinase signaling and (ii) phosphoproteomic data from cancer cell lines, demonstrate that non-linear formulations can yield gains in causal network inference and permit dynamical prediction and uncertainty quantification in the challenging setting where the reaction graph is unknown. Availability and implementation: MATLAB R2014a software is available to download from warwick.ac.uk/chrisoates. Contact: c.oates@warwick.ac.uk or sach@mrc-bsu.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online."
476d4a22c2d030181dce79696bea34e482a4624a,
486b39d2e113aadbc8d0aa2b6f980432e90cf548,"Parameter inference in mathematical models of complex biological 
systems, expressed as coupled ordinary differential equations (ODEs), is a challenging problem. These depend on kinetic parameters, which cannot all be measured and have to be ascertained a different way. However, the computational 
costs associated with repeatedly solving the ODEs are often staggering, making 
many techniques impractical. Therefore, aimed at reducing this cost, new concepts using gradient matching have been proposed. This paper combines current 
adaptive gradient matching approaches, using Gaussian processes, with a parallel tempering scheme, in order to compare 2 different paradigms using the same 
nonlinear regression method. We use 2 ODE systems to assess our technique, 
showing an improvement over the recent method in Calderhead et al. (2008)."
6dc71505c9c15c914f375da0b83956aeb7dedc2e,"In this doctoral thesis, I present my research into applying machine learning techniques for reconstructing species interaction networks in ecology, reconstructing molecular signalling pathways and gene regulatory networks in systems biology, and inferring parameters in ordinary differential equation (ODE) models of signalling pathways. Together, the methods I have developed for these applications demonstrate the usefulness of machine learning for reconstructing networks and inferring network parameters from data. The thesis consists of three parts. The first part is a detailed comparison of applying static Bayesian networks, relevance vector machines, and linear regression with L1 regularisation (LASSO) to the problem of reconstructing species interaction networks from species absence/presence data in ecology (Faisal et al., 2010). I describe how I generated data from a stochastic population model to test the different methods and how the simulation study led us to introduce spatial autocorrelation as an important covariate. I also show how we used the results of the simulation study to apply the methods to presence/absence data of bird species from the European Bird Atlas. The second part of the thesis describes a time-varying, non-homogeneous dynamic Bayesian network model for reconstructing signalling pathways and gene regulatory networks, based on Lèbre et al. (2010). I show how my work has extended this model to incorporate different types of hierarchical Bayesian information sharing priors and different coupling strategies among nodes in the network. The introduction of these priors reduces the inference uncertainty by putting a penalty on the number of structure changes among network segments separated by inferred changepoints (Dondelinger et al., 2010; Husmeier et al., 2010; Dondelinger et al., 2012b). Using both synthetic and real data, I demonstrate that using information sharing priors leads to a better reconstruction accuracy of the underlying gene regulatory networks, and I compare the different priors and coupling strategies. I show the results of applying the model to gene expression datasets from Drosophila melanogaster and Arabidopsis thaliana, as well as to a synthetic biology gene expression dataset from Saccharomyces cerevisiae. In each case, the underlying network is time-varying; for Drosophila melanogaster, as a consequence of measuring gene expression during different developmental stages; for Arabidopsis thaliana, as a consequence of measuring gene expression for circadian clock genes under different conditions; and for the synthetic biology dataset, as a consequence of changing the growth environment. I show that in addition to inferring sensible network structures, the model also successfully predicts the locations of iii changepoints. The third and final part of this thesis is concerned with parameter inference in ODE models of biological systems. This problem is of interest to systems biology researchers, as kinetic reaction parameters can often not be measured, or can only be estimated imprecisely from experimental data. Due to the cost of numerically solving the ODE system after each parameter adaptation, this is a computationally challenging problem. Gradient matching techniques circumvent this problem by directly fitting the derivatives of the ODE to the slope of an interpolant. I present an inference procedure for a model using nonparametric Bayesian statistics with Gaussian processes, based on Calderhead et al. (2008). I show that the new inference procedure improves on the original formulation in Calderhead et al. (2008) and I present the result of applying it to ODE models of predator-prey interactions, a circadian clock gene, a signal transduction pathway, and the JAK/STAT pathway. The material within this thesis is partly based on my published papers and book chapters: • Chapter 2, on reconstructing ecological networks, is based on Faisal et al. (2010). • Chapters 3 and 4, on reconstructing signalling pathways and gene regulatory networks with information sharing, are based on Dondelinger et al. (2010), Husmeier et al. (2010), Dondelinger et al. (2012a), Lèbre et al. (2012) and Dondelinger et al. (2012b). • Chapter 5, on inferring parameters in ODE models of biological systems, is partly based on Dondelinger et al. (2012c). • Appendix C is based on Dondelinger et al. (2011). • Appendix E is based on part of Lin et al. (2010)."
a7e2c643c06a42ef86a4854d69b0aa9ea1b21cbc,"Parameter inference in mechanistic models based on systems of coupled differential equa- tions is a topical yet computationally chal- lenging problem, due to the need to fol- low each parameter adaptation with a nu- merical integration of the differential equa- tions. Techniques based on gradient match- ing, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differen- tial equations, offer a computationally ap- pealing shortcut to the inference problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes due to Calderhead et al. (2008), and shows how inference in this model can be substantially improved by consistently inferring all parameters from the joint dis- tribution. We demonstrate the efficiency of our adaptive gradient matching technique on three benchmark systems, and perform a de- tailed comparison with the method in Calder- head et al. (2008) and the explicit ODE inte- gration approach, both in terms of parameter inference accuracy and in terms of computa- tional efficiency."
29ff1555b7538d012ae8d4441e71dc64118d7218,
d943dbda74ee76936f744fab79365a0b7cd4a5df,
e4c12f0d4175d962c87c632772bceeb934b0c09c,
fd1f461ff2cee0451d86b2448521eb76f7e5d25b,"A challenging problem in systems biology is parameter inference in mechanistic models of signalling pathways. In the present article, we investigate an approach based on gradient matching and nonparametric Bayesian modelling with Gaussian processes. We evaluate the method on two biological systems, related to the regulation of PIF4/5 in Arabidopsis thaliana, and the JAK/STAT signal transduction pathway."
4ccecde0ab313b0fdebfdcc7e2b9414d3d234316,
501ea63f5318adc11d661f35f90d52db08e81d9c,"We propose a Bayesian regression and multiple changepoint model for reverse engineering gene regulatory networks from high-throughput gene expression profiles. We report results from a recently held international gene network reconstruction competition, in which our method was objectively assessed in a blind study. While we did not win the competition, the scores indicate that the proposed method favourably compares with the majority of competing approaches and clearly belongs to the group of highest-ranked performers."
6fdbef525b5c7d1a55cae2066364aef74afe1b6c,
967f9330815b2bc3fbd01f575ed2abd6a6db186f,"Classical dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption and cannot deal with heterogeneity and non-stationarity in temporal processes. Various approaches to relax the homogeneity assumption have recently been proposed. The present paper aims to improve the shortcomings of three recent versions of heterogeneous DBNs along the following lines: (i) avoiding the need for data discretization, (ii) increasing the flexibility over a time-invariant network structure, (iii) avoiding over-flexibility and overfitting by introducing a regularization scheme based in inter-time segment information sharing. The improved method is evaluated on synthetic data and compared with alternative published methods on gene expression time series from Drosophila melanogaster."
a8175ca8f2999ce3dc03a3a858b021c3ccdcf87a,
c9fc88aec6eeccdd09fb6de66cebaa64f52a108b,"Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this flexibility leads to the risk of overfitting and inflated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of five genes in yeast."
7094cbfd6c477bbd756c81c51fd9a2f9ed35b8da,"Network reconstruction methods are commonly used in molecular biology to construct regulatory networks from information about the expression values of genes in a cell. In ecology, we are presented with a similar problem when trying to reconstruct species interaction networks based on species abundance data. The aim of this project was to see if the methods that proved successful in molecular biology could be applied to this new problem. The methods that I applied and compared were Least Absolute Shrinkage and Selection Operator (LASSO), Sparse Bayesian Regression (SBR), Graphical Gaussian Models (GGMs) and Bayesian Networks (BNs). To evaluate the performance of these methods, I used data derived from a realistic population simulation model, based on a niche model of species interactions. The experiments showed that these methods were successful at reconstructing the species interaction networks, and that the comparatively simple LASSO, SBR and GGM methods performed as well if not better than the more complex Bayesian Network approaches."
877c8c61faa43fad53fca7ad512d726bff20f683,"Mouad El Khoudri*, Iliass Lahmass, Mohammed Ammari, Dounia El Adak, Mohamed Ahlalouch, Abdelkarim Chamrar, Laïla Ben Allal [1] Research Team: Materials, Environment and Sustainable Development (MEDD), Faculty of Sciences and Techniques of Tangier, BP 416 – Tangier, Morocco [2] Laboratory of biochemistry and biotechnology, University 1st, Oujda, Morocco [3] Laboratory of applied geosciences, University Mohammed 1, Oujda, Morocco"
39c4d555d58e389477e518ef43e6681ce96761b9,
955f6c7ba515dee35ef4da0eedcf68f3cefdfd85,
1d30bb5dce8549f76ef4534f8780da16198f2c02,"Incrementaldynamic analysisIDA is a nonlineardynamic analysisthat offersa hugeopportunity to evaluatethe seismicresponseof the structureand takes into accountthe effects ofthe energy, thedurationand the frequencycontained in seismic record and product theeffectof these parametersas an inter-story drift ratio and maximumdisplacementinastructure,unlikethePUSHOVERanalysiswhichis a staticanalysisthat provides only anoverview oflesserseismicresponseson the structure.The occurrence of damage with varying degrees is a random process;therefore, the only appropriate tool to predict the distribution of expected losses would be probabilistic.Therefore we propose to establish fragility curves based on the data of the incremental dynamic analysis to precisely define a sequential pattern ofdamage and establish a level of structural damage of different seismic intensities of Moroccan real estate"
6a2b209e3b80308126a21e211809927344a3e916,
47ccedb7023bf259863b7ac46e604ba4c9cce301,
faa79737a9ac7eaf1f1f760ff775a224fde523ea,"Mass media can spread information and disinformation, but its impact is hard to rigorously measure. Using a two-level randomized controlled trial covering 5 million people, we test both exposure to mass media (with 1,500 women receiving radios) and the impact of a high-quality, intensive 2.5 year, family planning mass media campaign in Burkina Faso (8 of 16 local radio stations received the campaign). We find women who received a radio in noncampaign areas reduced contraception use by 5.2 percentage points (p=0.039) and had more conservative gender attitudes. In contrast, modern contraceptive use rose 5.9 percentage points (p=0.046) in campaign areas and 5.8 percentage points (p=0.030) among those given radios in campaign areas. Births fell 10%. The campaign changed beliefs about contraception but not preferences, and encouraged existing users to use more consistently. We estimate the nationwide campaign scale-up led to 225,000 additional women using modern contraception, at a cost of US$7.7 per additional user. JEL codes: J13, J16, L82"
522cde1c52d512e397a071532c66a49fb5a4560d,
d89e629a8c2e8a5bd2733d3a98894ab3d21ac2c9,
7840f03454f3789555e989d74b45e07a3a0065fc,
2e2acf02d72e5059822559f89b73ee57e31be156,
8e2452a8fcd229a7cda441c900a3949f637df5f5,
04a8d7bbd3c86d4d6c3a9ceff4f1766fdf85681a,
070a1739d043d6aad679b92f26c40f19b6834339,
3f7ad1c90938fe550459abf5be7470985978ad01,
49d0f69d60e38f54a2a06e870e58b1f6859715a5,
934b30ededc7f9c6b15da3469e9700ae7f076f2d,
d30fd5f59489a74f048403732bc5814c653c338b,
09df77d3f1329000b060d5a1ea9f917c3d025a8d,
0d69eac0b0df75476367f451781ef26b0bece8ba,
3641b6d6da4956fa716df3db117c81ffe9ac5782,
474db1a7ae4f71d55a3a9416409260b943e796f2,"The growing number of wireless device users and the proliferation of new wireless applications are imposing increasingly greater performance demands on today's wireless networks. As a result, much effort has been directed towards more efficient or heavily resourced physical layers, aimed at improving performance by reducing communication overheads and increasing spatial reuse. Fundamentally, however, distance and decreasing signal strength limit the performance gains hardware approaches can achieve; clients distant from an Internet gateway (e.g., an access point) will experience poor performance. 
This dissertation explores a complementary MAC layer approach to increase the throughput of clients distant from their access points. This approach employs multi-hop relaying to leverage short, high-rate links when network participants can forward traffic for one another, and multiple rates are supported. I develop analytic models to evaluate the potential performance benefits of this approach across a range of design choices including relay placement, spatial reuse, power control, and multiple channels and radios. I find that with relatively few relays, multi-hop relaying can yield order of magnitude throughput speedups for many clients. Surprisingly, I find that the simplest design choices (namely single radio nodes without spatial reuse or power control) offer benefits nearly as large as much more sophisticated ones. I use simulations in an 802.11 network to study the effects of practical constraints (e.g., discrete rates, ACKs, CSMA/CA) on multi-hop relaying, and argue that these performance benefits can be achieved in practice."
60982c1e6ebb9681e2630bf6b3743d767d21f2c5,"We ask if the ubiquity of WiFi can be leveraged to provide cheap connectivity from moving vehicles for common applications such as Web browsing and VoIP. Driven by this question, we conduct a study of connection quality available to vehicular WiFi clients based on measurements from testbeds in two different cities. We find that current WiFi handoff methods, in which clients communicate with one basestation at a time, lead to frequent disruptions in connectivity. We also find that clients can overcome many disruptions by communicating with multiple basestations simultaneously. These findings lead us to develop ViFi, a protocol that opportunistically exploits basestation diversity to minimize disruptions and support interactive applications for mobile clients. ViFi uses a decentralized and lightweight probabilistic algorithm for coordination between participating basestations. Our evaluation using a two-month long deployment and trace-driven simulations shows that its link-layer performance comes close to an ideal diversity-based protocol. Using two applications, VoIP and short TCP transfers, we show that the link layer performance improvement translates to better application performance. In our deployment, ViFi doubles the number of successful short TCP transfers and doubles the length of disruption-free VoIP sessions compared to an existing WiFi-style handoff protocol."
370d150db36ea70681a561af4b686907612fea5e,"Using measurements from VanLAN, a modest-size testbed that we have deployed, we analyze the fundamental characteristics of WiFi-based connectivity between basestations and vehicles in urban settings. Our results uncover a more complex picture than previous work which was conducted in more benign settings. The interval between a vehicle coming into and going out of range of a basestation is often marred by intermittent periods of very poor connectivity. These ""gray periods"" are hard to reliably predict because their arrival is not signaled by metrics such as signal strength, loss rate, speed or distance from the basestation. At the same time, they also do not consistently occur at the same spot. Our analysis suggests that gray periods are not caused by the motion of the vehicle per se but by the variability in the urban radio environment combined with the vehicle traversing locations that are poorly covered by the basestation. We also find that knowledge of past connectivity can be used to identify regions where gray periods are more likely to occur as well as regions where the vehicle is likely to experience good connectivit."
3a3a3a68940291899fa1d5626a73fc651fcaa46d,"Despite its obvious success, robustness, and scalability, the Internet suffers from a number of end-to-end performance and availability problems. In this paper, we attempt to quantify the Internet' s inefficiencies and then we argue that Internet behavior can be improved by spreading intelligent routers at key access and interchange points to actively manage traffic. Our Detour prototype aims to demonstrate practical benefits to end users, without penalizing non-Detour users, by aggregating traffic information across connections and using more efficient routes to improve Internet performance."
445e4e5f64ac8cad38b342b986c0951410ac40c8,"We present Wit, a non-intrusive tool that builds on passive monitoring to analyze the detailed MAC-level behavior of operational wireless networks. Wit uses three processing steps to construct an enhanced trace of system activity. First, a robust merging procedure combines the necessarily incomplete views from multiple, independent monitors into a single, more complete trace of wireless activity. Next, a novel inference engine based on formal language methods reconstructs packets that were not captured by any monitor and determines whether each packet was received by its destination. Finally, Wit derives network performance measures from this enhanced trace; we show how to estimate the number of stations competing for the medium. We assess Wit with a mix of real traces and simulation tests. We find that merging and inference both significantly enhance the originally captured trace. We apply Wit to multi-monitor traces from a live network to show how it facilitates 802.11 MAC analyses that would otherwise be difficult or rely on less accurate heuristics."
50e6f5c4c5684e735553e1d643a405c897af9820,
680c819b14aab08e41b969c6eab5b42c27ef08be,"In this paper, we present the design, implementation, and evaluation of a novel endpoint congestion control system that achieves near-optimal performance in all likely circumstances. Our approach, called the Probe Control Protocol (PCP), emulates network-based control by using explicit short probes to test and temporarily acquire available bandwidth. Like TCP, PCP requires no network support beyond plain FIFO queues. Our initial experiments show that PCP, unlike TCP, achieves rapid startup, small queues, and low loss rates, and that the efficiency of our approach does not compromise eventual fairness and stability. Further, PCP is compatible with sharing links with legacy TCP hosts, making it feasible to deploy."
81887be9e2a33b163ac3494cf995221f1b80e2a1,"We present practical models for the physical layer behaviors of packet reception and carrier sense with interference in static wireless networks. These models use measurements of a real network rather than abstract RF propagation models as the basis for accuracy in complex environments. Seeding our models requires N trials in an N node network, in which each sender transmits in turn and receivers measure RSSI values and packet counts, both of which are easily obtainable. The models then predict packet delivery and throughput in the same network for different sets of transmitters with the same node placements. We evaluate our models for the base case of two senders that broadcast packets simultaneously. We find that they are effective at predicting when there will be significant interference effects. Across many predictions, we obtain an RMS error for 802.11a and 802.11b of a half and a third, respectively, of a measurement-based model that ignores interference."
bbfb3aed1d46f23a50e264c4cb52ddf33f721bfd,
1ca0cae985578fd738d3e1ab74d0a2b5206b6470,"We analyze wireless measurements taken during the SIGCOMM 2004 conference to understand how well 802.11 operates in real deployments. We find that the overhead of 802.11 is high, with only 40% of the transmission time spent in sending original data. Most of the remaining time is consumed by retransmissions due to packet losses that are caused by both contention and transmission errors. Our analysis also shows that wireless nodes adapt their transmission rates with an extremely high frequency. We comment on the difficulties and opportunities of working with wireless traces, rather than the wired traces of wireless activity that are presently more common."
6207156e8e45c05dbb170c3d7f52336de18d48ff,"The availability of a P2P service is a function of the individual peers' availabilities, and it is often desirable to estimate how available a particular P2P service will be given the availability of its peers. Prior work in this area has widely used the fraction of time the average peer is available as the basis for this estimate. It is shown that this approach has serious drawbacks. The authors developed a different measure, which is called presence-based availability, which takes into account the availability of the individual peers. Using traces of live P2P systems taken from the literature, the authors demonstrated that presence-based availability is a more reliable indicator of potential performance than prior methods. It is shown that this metrics successfully estimate the availability of a P2P file-sharing system. Then, using presence-based measures to make a better estimate of a parameter in a highly-available system, we achieve a 75% decrease in resource usage relative to an existing technique relying on traditional metrics."
d2dc14ccf05a0010a3da9679735b40fce2d2821a,"Multi-hop wireless networks are vulnerable to free-riders because they require nodes to forward packets for each other. Deployed routing protocols ignore this issue while proposed solutions incorporate complicated mechanisms with the intent of making free-riding impossible. We present Catch, a protocol that falls between these extremes. It achieves nearly the low mechanism requirements of the former while imposing nearly as effective barriers to free-riding as the latter. Catch is made possible by novel techniques based on anonymous messages. These techniques enable cooperative nodes to detect nearby free-riders and disconnect them from the rest of the network. Catch has low overhead and is broadly applicable across routing protocols and traffic workloads. We evaluate it on an 802.11 wireless testbed as well as through simulation."
893292d0fdb5db7c1a3dd582a11f4ab9cb8e3945,We applied techniques from game theory to help formulate and analyze solutions to two systems problems: discouraging selfishness in multi-hop wireless networks and enabling cooperation among ISPs in the Internet. It proved difficult to do so. This paper reports on our experiences and explains the issues that we encountered. It describes the ways in which the straightforward use of results from traditional game theory did not fit well with the requirements of our problems. It also identifies an important characteristic of the solutions we did eventually adopt that distinguishes them from those available using game theoretic approaches. We hope that this discussion will help to highlight formulations of game theory which are well-suited for problems involving computer systems.
a9900a14562fdd76cb0a979565c874a4e18ea06b,"We present Catch, a protocol that encourages cooperation in multi-hop wireless networks comprised of autonomous nodes. While these nodes depend on each other to relay packets, scarce bandwidth and energy resources may motivate some nodes to cheat and avoid packet forwarding. Catch employs a novel technique based on anonymous messages to significantly increase the difficulty of cheating with impunity. Existing routing protocols simply assume that nodes will act cooperatively and have no mechanisms to encourage this behavior or punish cheaters. Catch imposes minimal requirements and overheads on the system, and so is broadly applicable across routing protocols and system workloads. We show that Catch makes cheating counter-productive, thus encouraging the cooperative behavior on which multi-hop wireless networks are predicated. We evaluate Catch on an 802.11 wireless testbed, as well as through simulation."
2ddb1afc5e81cb5c62845a1479595525d0caf19f,"Peer-to-peer (P2P) file sharing accounts for an astonishing volume of current Internet traffic. This paper probes deeply into modern P2P file sharing systems and the forces that drive them. By doing so, we seek to increase our understanding of P2P file sharing workloads and their implications for future multimedia workloads. Our research uses a three-tiered approach. First, we analyze a 200-day trace of over 20 terabytes of Kazaa P2P traffic collected at the University of Washington. Second, we develop a model of multimedia workloads that lets us isolate, vary, and explore the impact of key system parameters. Our model, which we parameterize with statistics from our trace, lets us confirm various hypotheses about file-sharing behavior observed in the trace. Third, we explore the potential impact of locality-awareness in Kazaa.Our results reveal dramatic differences between P2P file sharing and Web traffic. For example, we show how the immutability of Kazaa's multimedia objects leads clients to fetch objects at most once; in contrast, a World-Wide Web client may fetch a popular page (e.g., CNN or Google) thousands of times. Moreover, we demonstrate that: (1) this ""fetch-at-most-once"" behavior causes the Kazaa popularity distribution to deviate substantially from Zipf curves we see for the Web, and (2) this deviation has significant implications for the performance of multimedia file-sharing systems. Unlike the Web, whose workload is driven by document change, we demonstrate that clients' fetch-at-most-once behavior, the creation of new objects, and the addition of new clients to the system are the primary forces that drive multimedia workloads such as Kazaa. We also show that there is substantial untapped locality in the Kazaa workload. Finally, we quantify the potential bandwidth savings that locality-aware P2P file-sharing architectures would achieve."
f4ea8c30580336e3717ba8c45c4fb9ef9b4a0d67,"Sloop-SMOK is a toolkit designed to improve the student design experience in a machine organization course taken by undergraduates in their first year as computer science majors. Students in this course have had some programming experience, and may have taken a one-quarter digital design course. Before Sloop-SMOK, assignments in this course were typically assembly language program implementations of functions related to architecture. The major goals in building Sloop-SMOK were to improve the relevance of homework assignments to machine organization and to emphasize some fundamental concepts of modern processors not easily addressed previously.Sloop-SMOK has two components. The Sloop is a machine architecture designed to allow implementation of a modern version of the 6502, the processor used in one of the early Atari game stations. The Sloop defines a RISC ISA and a set of on-the-fly translations from the original 6502 CISC ISA into Sloop instructions. The Sloop Machine Organization Kit (SMOK) is a general-purpose software machine organization simulator. The components of a SMOK model are at the level of detail found in typical machine organization texts: ALUs, register files, logic gates, and the like. SMOK provides a graphical interface to construct and debug models.As homework assignments, students use SMOK to build Sloop machines that successfully run most original 6502 games. Extensions to SMOK provide specific help with these Sloop models. Of particular importance is support for debugging: SMOK, when used to build a Sloop machine, runs a software 6502 simulator and compares the behavior of the student's Sloop machine against the simulator on a per-memory-operation basis. This feature simplifies debugging of Sloop machine organization by raising an error at the earliest cycle at which the student's machine is known to deviate from correct behavior.Sloop-SMOK, including downloadable software, documentation, and course assignments, is available at http://www.cs.washington.edu/software/SMOK"
0f696521e0c2c4e964a6d85932a7321a36608be2,"Two recent techniques for multicast or broadcast delivery of streaming media can provide immediate service to each client request, yet achieve considerable client stream sharing which leads to significant server and network bandwidth savings. The paper considers: 1) how well these recently proposed techniques perform relative to each other and 2) whether there are new practical delivery techniques that can achieve better bandwidth savings than the previous techniques over a wide range of client request rates. The principal results are as follows: First, the recent partitioned dynamic skyscraper technique is adapted to provide immediate service to each client request more simply and directly than the original dynamic skyscraper method. Second, at moderate to high client request rates, the dynamic skyscraper method has required server bandwidth that is significantly lower than the recent optimized stream tapping/patching/controlled multicast technique. Third, the minimum required server bandwidth for any delivery technique that provides immediate real-time delivery to clients increases logarithmically (with constant factor equal to one) as a function of the client request arrival rate. Furthermore, it is (theoretically) possible to achieve very close to the minimum required server bandwidth if client receive bandwidth is equal to two times the data streaming rate and client storage capacity is sufficient for buffering data from shared streams. Finally, we propose a new practical delivery technique, called hierarchical multicast stream merging (HMSM), which has a required server bandwidth that is lower than the partitioned dynamic skyscraper and is reasonably close to the minimum achievable required server bandwidth over a wide range of client request rates."
4bd22678cf2d6505b26fb4660c76f17bfdb6573c,"We describe DDDDRRaW, a prototype toolkit for distributed real-time rendering on commodity clusters. In constrast to most work on cluster computing, DDDDRRaW supports a repeated, low-latency computation, the drawing of frames which must take place on a time scale of 30-100 ms. DDDDRRaW employs image layer decomposition, a rendering-specific work partitioning algorithm described and evaluated using simulation. In this paper we address implementation issues. In particular, one important issue we explore is how to exploit the potential parallelism afforded by the multiple hardware resources of each node: the CPU, the network adapter and the video card. We evaluate DDDDRRaW's live performance on two small workstation clusters representing different points in the technology spectrum. Our results show that DDDDRRaW effectively exploits cluster resources to improve real-time rendering performance and should scale well to moderately sized clusters."
6e7d041a364d34474698af5f9b51474a7249b2fb,"SLOOP-SMOK is a toolkit designed to improve the student design experience in an undergraduate machine organization course. The major goals were to improve the relevance of homework assignments to machine organization in general, and to emphasize some fundamental concepts of modern processors not easily addressed previously. SLOOP-SMOK has two components. The SLOOP Machine Organization Kit (SMOK) is a general-purpose software machine organization simulator. The components of a SMOK model are at the level of detail found in typical machine organization texts: ALUs, register files, logic gates, and the like. SMOK provides a graphical interface to construct and debug machine models. The SLOOP is a set of components designed to allow implementation of a modern version of the 6502, the processor used in one of the early Atari game stations. The SLOOP defines a RISC ISA and a set of on-the-fly translations from the original 6502 CISC ISA into SLOOP instructions. Students build SLOOP machines that successfully run most original 6502 games. Extensions to SMOK provide specific help with SLOOP models. In particular, SMOK provides graphical output simulating the game station display, and passes through input events to the SLOOP machine. Additionally, when used with SLOOP, SMOK runs a software 6502 simulator and compares the behavior of the student's SLOOP machine against the simulator on a per-memory-operation basis. This feature simplifies debugging of SLOOP machine organizations by raising an error at the earliest cycle at which the student's machine is known to deviate from correct behavior. SLOOP-SMOK, including downloadable software, documentation, and course assignments, is available at http://www.cs.washington.edu/faculty/zahorjan/SMOK."
3c32d5575867d4346bc667789239cff86a8b90ec,"The work in this paper is motivated by our effort to build a dis tributed rendering system that uses the multiple hardware graphics accelerators available in a network of workstations/PCs to provide r altime rendering performance that is one or two generations ahead o f what can be achieved using only a single commodity machine. Specifically, we address one impo rtant aspect of building such a system, how to partition and assign the overall rendering work. We pr opose a novel approach called Image Layer Decomposition(ILD). ILD has several advantages over previous partitioni ng algorithms for our targeted environment, including its compatibility with the use of ha rdware graphics accelerators, decoupling of communication bandwidth requirement from scene complexit y, and reduced communication bandwidth growth as the system size increases. Furthermore, ILD tries to optimize the rendering of a sequence of frames (of an interactive application) instead of only indi vi ual frames. Using traces taken from a VRML viewer, we simulate a long-run ning off-line version of ILD to evaluate its potential for achieving good load balancing and scalability. Our results show that ILD can be expected to work well (up to moderately sized clusters) when given scene objects of reasonable granularity. Furthermore, our results show that ILD outperforms sort-last, a common partitioning approach, because of its smaller communication bandwidth requiremen t. We also simulate a two-phase greedy online ILD partitioning algorithm. While not optimal, our res ults show that this on-line algorithm performs only slightly worse than the off-line algorithm, showing go od potential for load balancing and scalability for most of the scenes that we examined."
6522c6a31ffa7824e7556e8c5efe5fbbf084bf48,WebCrawler: Finding What People Want
a2134d587ada1e97180926a9cf3b8b847ccc5481,"We propose a novel work partitioning technique, image layer decomposition (ILD), designed specifically to support distributed real-time rendering on commodity clusters. ILD has several advantages over previous partitioning algorithms for our targeted environment, including its compatibility with the use of hardware graphics accelerators, decoupling of communication bandwidth requirement from scene complexity, and reduced communication bandwidth growth as the system size increases. Furthermore, ILD tries to optimize the rendering of a sequence of frames (of an interactive application) instead of only individual frames. We simulate ILD using traces taken from a VRML viewer Our results show that ILD can be expected to work well up to moderately sized clusters and to outperform sort-last, a common partitioning approach, because of its smaller communication bandwidth requirement."
2fa333171df02aa39e4b737f4aafd3262e67dde0,"This paper proposes a new technique for on-demand delivery of streaming media. The idea is to hold in reserve, or `skim', a portion of the client reception bandwidth that is sufficiently small that display quality is not impacted significantly, and yet that is nonetheless enough to support substantial reductions in server and network bandwidth through near-optimal hierarchical client stream merging. In this paper we show that this objective is feasible, and we develop practical techniques that achieve it. The results indicate that server and network bandwidth can be reduced to on the order of the logarithm of the number of clients who are viewing the object, using a small `skim' (e.g., 15%) of client reception bandwidth. These low server and network bandwidths are achieved for every media file, while providing immediate service to each client, and without having to pre-load initial portions of the video at each client."
69f623f4b2df4ca1ba18c81d0dcd1eca701abdd1,"Despite its obvious success, the Internet suffers from end-to-end performance and availability problems. We believe that intelligent routers at key access and interchange points could improve Internet behavior by actively managing traffic. We describe the inefficiencies in routing and transport protocols in the modern Internet. We are constructing a prototype, called Detour, a virtual Internet, in which routers tunnel packets over the commodity Internet instead of using dedicated links."
f22d0546705150784a1c6c89585a7fbd5882d7cf,"The simplest video-on-demand (VOD) delivery policy is to allocate a new media delivery stream to each client request when it arrives. This policy has the desirable properties of “immediate service” (there is minimal latency between the client request and the start of playback, assuming that sufficient server bandwidth is available to start the new stream), of placing minimal demands on client capabilities (the client receive bandwidth required is the media playback rate, and no client local storage is required), and of being simple to implement. However, the policy is untenable because it requires server bandwidth that scales linearly with the number of clients that must be supported simultaneously, which is too expensive for many applications."
fea57bcf312300a120e7cade1ab88250d0d66ce5,"Both inherently sequential code and limitations of analysis techniques prevent full parallelization of many applications by parallelizing compilers. Amdahl's Law tells us that as parallelization becomes increasingly effective, any unparallelized loop becomes an increasingly dominant performance bottleneck. We present a technique for speeding up the execution of unparallelized loops by cascading their sequential execution across multiple processors: only a single processor executes the loop body at any one time, and each processor executes only a portion of the loop body before passing control to another. Cascaded execution allows otherwise idle processors to optimize their memory state for the eventual execution of their next portion of the loop, resulting in significantly reduced overall loop body execution times. We evaluate cascaded execution using loop nests from wave5, a Spec95fp benchmark application, and a synthetic benchmark. Running on a PC with 4 Pentium Pro processors and an SGI Power Onyx with 8 R10000 processors, we observe an overall speedup of 1.35 and 1.7, respectively, for the wave5 loops we examined and speedups as high as 4.5 for individual loops. Our extrapolated results using the synthetic benchmark show a potential for speedups as large as 16 on future machines."
b89d728c18d6b8b075c54201731a4756b4252a49,"We consider the problem of scheduling the rendering component of 3D multimedia applications on a cluster of workstations connected via a local area network. Our goal is to meet a periodic real-time constraint.In abstract terms, the problem we address is how best to schedule tasks with unpredictable service times on distinct processing nodes so as to meet a real-time deadline, given that all communication among nodes entails some (possibly large) overhead. We consider two distinct classes of schemes, static, in which task reallocations are scheduled to occur at specific times, and dynamic, in which reallocations are triggered by some processor going idle. For both classes we further examine both global reassignments, in which all nodes are rescheduled at a rescheduling moment, and local reassignments, in which only a subset of the nodes engage in rescheduling at any one time.We show that global dynamic policies work best over a range of parameterizations appropriate to such systems. We introduce a new policy, Dynamic with Shadowing, that places a small number of tasks in the schedules of multiple workstations to reduce the amount of communication required to complete the schedule. This policy is shown to dominate the other alternatives considered over most of the parameter space."
0c6b7308ccaf07bba6d6ea469e152908241ccedf,
446e11d5e0d592e9c48e9126d5a2ea04b96e5f5f,
5e1d985c78e0b5da12e025e9c51cf8583e2c5c71,
4fffd6a2a9386ff5d161aee9122ee159d7f573e5,
75135ecb33d3ae7ffdadac7aaa748db5c444fd3f,
841b34bfb3ac8bd05971bcfbfa05cf423d566502,"Addresses the problem of maximizing application speedup through run-time self-selection of an appropriate number of processors on which to run. Automatic run-time selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data- or time-dependent. We propose the use of a run-time system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job's processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our run-time system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation."
217a00366ffbe2d1add65cea616e1c6fffe3a886,"Multimedia documents may contain a rich mixture of static media, such as text and images, and dynamic media, such as audio and video. The presence of dynamic media introduces time as an axis along which information can be presented. Thus, multimedia documents require both a spatial layout, which specifies where media should appear, and a temporal layout, which indicates when events in the document should occur. 
In this dissertation, I argue that document creation and maintenance can be simplified by providing mechanisms for automatically producing temporal layouts. The approach that I describe is analogous to using document formatters, such as T$\sb {\rm E}$X or Microsoft Word, to automatically produce spatial layouts for traditional documents. 
I begin by presenting a new temporal specification model, based on events and temporal constraints. Events mark either predictable points in time, such as the start of a lion's roar in an audio recording, or unpredictable points in time, such as when a user presses the help button. Temporal constraints specify the temporal ordering of pairs of events, such as specifying that the lion's roar should occur simultaneously with the appearance of a lion in a video. 
I also introduce a two-phase temporal formatter that consists of a compiletime component called the scheduler and a runtime component called the runtime system. The scheduler uses a constraint satisfaction algorithm, based on linear programming, to position media in time so as to conform to the temporal constraints specified by the author. It produces a partial temporal layout in which the predictable portions have definite times and the unpredictable portions have event-relative times. Conceptually, the scheduler is similar to T$\sb {\rm E}$X's spatial layout algorithm because it permits time to be stretched or shrunk within media segments. The runtime system completes a document's temporal layout by merging the predictable and unpredictable portions together in response to the occurrence of unpredictable events. 
I also present a framework for understanding automatic temporal formatters and exploring the issues surrounding them. This framework incorporates the preceding temporal formatter's strength in handling predictable and unpredictable temporal behavior, and its ability to attempt to correct temporal mismatches before a document is presented. It can also describe the presentation of multimedia documents in heterogeneous environments, as well as the adjustment of a document's temporal layout to account for media delays. The result is the definition of a more powerful and more comprehensive class of temporal formatters."
749fe3105cc8caa102ea24ebc30e332497fc01e5,We consider the problem of multiprocessor scheduling of jobs whose memory requirements place lower bounds on the fraction of the machine required in order to execute. We address three primary questions in this work:1. How can a parallel machine be multiprogrammed with minimal overhead when jobs have minimum memory requirements?2. To what extent does the inability of an application to repartition its workload during runtime affect the choice of processor allocation policy?3. How rigid should the system be in attempting to provide equal resource allocation to each runnable job in order to minimize average response time?This work is applicable both to parallel machines and to networks of workstations supporting parallel applications.
893d1bd3b2527c80d60a940d1be47630b980e3fe,"It is increasingly important that optimizing compilers restructure programs for data locality to obtain high performance on today's powerful architectures. In this paper, we focus on array restructuring , a technique that improves the spatial locality exhibited by array accesses in nested loops. Speci cally, we address the following question: Given a set of such accesses, how should the array elements be laid out in memory to match the access pattern and thus maximize locality? Our approach is based on an invertible linear transformation of array index vectors. We present algorithms to choose a suitable transformation, and hence array layout, given the set of array accesses. Our analysis places no restrictions on the loop's nesting structure or dependence pattern. Although we focus on cases where the array indexing expressions are a ne functions of loop variables, our techniques can be applied to the non-a ne case as well. We have implemented our technique in the SUIF compiler [17]. Experimental results show that array restructuring improves loop execution performance substantially, often with no or little runtime overhead. Moreover, the performance improvement of array restructuring compares favorably with that achieved by loop restructuring."
db1cc31a155274db6b1ec8ae8d1793b0316247e3,"The Mach 3 operating system makes extensive use of remote procedure calls (RPCs) to provide services to client applications. Although the existing Mach 3 RPC facility is highly optimized, the incorporation of a Lightweight Remote Procedure Call (LRPC) facility further reduces this critical cost. This paper describes the implementation of LRPC in the Mach 3 operating system, reviewing the original LRPC concept and implementation in the TAOS operating system, the issues involved with the Mach 3 microkernel operating system, and the resulting design of LRPC Mach3. Performance measurements indicate that the resulting implementation provides a 31% reduction in latency for a minimal RPC, with even more significant benefits for more complicated RPCs."
e9400bc9cdac705d8c0db239217876a70fbf06b8,"When static analysis of a sequential loop fails to yield reliable information on its dependence structure, a parallelizing compiler is left with three alternatives: it can take the conservative option of emitting code for a sequential execution; it can optimistically emit code to speculatively execute the loop as a DOALL [6, 7]; or it can emit inspector-executor code to determine the actual dependence structure at runtime and to respect it in a parallel execution [8, 9]. The rst approach is certain to yield a slow execution. The second approach gives very good results when the loop can in fact be executed as a DOALL, but is of no help otherwise. In this paper we concentrate on the nal approach, runtime parallelization through the inspector-executor method. We have two goals in this work. The rst is to expand the class of loop to which the approach may be applied by removing restrictions on the loop dependence structures that it can handle. To achieve this goal, we introduce new forms of the inspector and executor that together remove all restrictions on the loop dependence structure. Thus, we show how to parallelize a class of loop that previously would have compelled the compiler to emit sequential code. Our second goal is to improve the performance of the inspector-executor approach through specializa-tions applicable when static analysis yields some (weak) information about the array indexing functions used in assignments. We validate our work through a set of examples designed to illustrate the fundamental performance tradeos characterizing the specialized implementations, using results taken from executions on 32 processors of a KSR1."
04326b6ee41ae1879b0eac6ca792cf215bcd41ae,"When multiple jobs compete for processing resources on a parallel computer, the operating system kernel's processor allocation policy determines how many and which processors to allocate to each. In this paper we investigate the issues involved in constructing a processor allocation policy for large scale, message-passing parallel computers supporting a scientific workload. We make four specific contributions: We define the concept of efficiency preservation as a characteristic of processor allocation policies. Efficiency preservation is the degree to which the decisions of the processor allocator degrade the processor efficiencies experienced by individual applications relative to their efficiencies when run alone. We identify the interplay between the kernel processor allocation policy and the application load distribution policy as a determinant of efficiency preservation. We specify the details of two families of processor allocation policies, called Equipartition and Folding. Within each family, different member policies cover a range of efficiency preservation values, from very high to very low. By comparing policies within each family as well as between families, we show that high efficiency preservation is essential to good performance, and that efficiency preservation is a more dominant factor in obtaining good performance than is equality of resource allocation."
21647908f083accb7a603f1212a098f0f4ece51f,"We describe Adhara, a runtime system specialized for dynamic space-based applications, such as particle-in-cell simulations, molecular dynamics problems and adaptive grid simulations. Adhara facilitates the programming of such applications by supporting spatial data structures (e.g., grids and particles), and facilitates obtaining good performance by performing automatic data partitioning and dynamic load balancing. We demonstrate the effectiveness of Adhera by efficiently parallelizing a specific plasma physics application. The development of the parallel program involved the addition of very few lines of code beyond those required to develop a sequential version of the application, and executed at 90% efficiency on 16 nodes of an Intel Paragon.<<ETX>>"
302226f939a7ebda422523f8448c483f62998287,"In a sequential program, data are often structured in a way that is optimized for a sequential execution. However, when the program is parallelized, the data access pattern may change drastically. If the structure of the data is not changed accordingly, parallel performance will su er. In this paper, we consider this problem in the context of runtime loop parallelization [8, 9], which is a general technique to parallelize loops not amenable to compile-time analysis. In a parallel execution of a loop, iterations may be performed in a very di erent order than in the sequential execution. This may result in undesirable cache e ects on distributed shared-memory multiprocessors, unless the structure of the arrays accessed by these iterations is changed accordingly. We discuss what these problems are and how they arise. We then describe two data restructuring techniques to address them: the restructuring of read-write arrays to reduce inter-processor communication due to false sharing, and the restructuring of read-only arrays to improve spatial locality. We also report experiments on a KSR1 [3] to evaluate the e ectiveness of these techniques and the preprocessing and postprocessing overheads they entail. The results show that the restructuring techniques can substantially improve performance of the parallelized loop. When restructuring overheads are ignored, we see a doubling of parallel speedups. While restructuring overheads can be quite signi cant, they can often be amortized across multiple loop executions so that they do not outweigh the performance bene ts. In our experiments, it takes only two loop executions to achieve this."
3de2af4bad21fcec842f6e0899d3f29d93b0b8fb,
fc6d50161d3d765a9939209e2fb91b90fac7949f,"The technical challenges that mobile computing must surmount to achieve its potential are hardly trivial. Some of the challenges in designing software for mobile computing systems are quite different from those involved in the design of software for today's stationary networked systems. The authors focus on the issues pertinent to software designers without delving into the lower level details of the hardware realization of mobile computers. They look at some promising approaches under investigation and also consider their limitations. The many issues to be dealt with stem from three essential properties of mobile computing: communication, mobility, and portability. Of course, special-purpose systems may avoid some design pressures by doing without certain desirable properties. For instance portability would be less of a concern for mobile computers installed in the dashboards of cars than with hand-held mobile computers. However, the authors concentrate on the goal of large-scale, hand-held mobile computing as a way to reveal a wide assortment of issues.<<ETX>>"
124d411210b2d288c79aaf9c7336df8b5e171fcc,"Parallel computing is increasingly important in the solution of large-scale numerical problems. The difficulty of efficiently hand-coding parallelism, and the limitations of parallelizing compilers, have nonetheless restricted its use by scientific programmers.
In this paper we propose a new paradigm, chores, for the run-time support of parallel computing on shared-memory multiprocessors. We consider specifically uniform memory access shared-memory environments, although the chore paradigm should also be appropriate for use within the clusters of a large-scale nonuniform memory access machine.
We argue that chore systems attain both the high efficiency of compiler approaches for the common case of data parallelism, and the flexibility and performance of user-level thread approaches for functional parallelism. These benefits are achieved within a single, simple conceptual model that almost entirely relieves the programmer and compiler from concerns of granularity, scheduling, and enforcement of synchronization constraints. Measurements of a prototype implementation demonstrate that the chore model can be supported more efficiently than can traditional approaches to either data or functional parallelism alone."
2a2de0458196ee446e320e9e8864171b69fa7ece,"57] W. Sanders and J. Meyer. Reduced base model construction methods for stochastic activity networks. 58] L. Schmickler. MEDA { mixed Erlang distributions as phase-type representation of empirical functions. Numerical evaluation of performability measures and job completion time in repairable fault-tolerant systems. 37] M. Malhotra. A computationally eecient technique for transient analysis of repairable Markovian systems.tolerant methods for transient analysis of stii Markov chains. 39] M. Malhotra and A. Reibman. Selecting and implementing phase approximations for semi-Markov models. A characterization of the stochastic process underlying a stochastic Petri net. 17] P. Courtois. Computable bounds for conditional steady-state probabilities in large Markov chain and queueing models. 19] A. Cumani. ESP { A package for the evaluation of stochastic Petri nets with phase-type distributed transition times. and identiication of non-exponential distributions by homogeneous Markov processes. 22 6 Conclusion We discussed several types of modeling techniques used in dependability and performability analysis, with a particular emphasis on approaches based on the (entire or partial) generation of the state-space. The common underlying formalisms we consider, continuous-time Markov chains (CTMCs) and Markov reward models (MRMs), are capable of modeling a large class of systems, but they result in large models, diicult to describe and analyze. The description problem is solved by using higher-level formalisms, such as reliability graphs, fault trees, queueing networks, generalized stochastic Petri nets, and stochastic reward nets. With the appropriate software modeling tools, these can then be automatically translated into CTMCs or MRMs. The solution problem, though, remains, since the size of the underlying stochastic process grows combi-natorially. In addition, when modeling activities with very diierent timescales , such as failure and repair of components, and performance-related behavior, such as arrival and departure of jobs, stiiness arises. Advanced numerical techniques, and exact or approximate approaches such as truncation, aggregation, composition , and uid models, can then be eeectively used to obtain numerical solutions. 21 Johnson and Taae 27, 28] have considered matching the rst three moments of mixtures of two Erlang distributions. For more references on this topic, refer to 7]. Generation of the overall CTMC. After the parameters of phase approximations for all the non-exponential distributions have been tted (or estimated), the overall CTMC is generated. This may require the cross-product of phase approximations 39]. A few software packages implementing this approach have been developed. Phase approximations were used in the SURF package 16], although SURF was intended only for a …"
947a79c57e348889f0f602c0e87e40dd74b010c7,"When the inter-iteration dependency pattern of the iterations of a loop cannot be determined statically, compile time parallelization of the loop is not possible. In these cases, runtime parallelization [8] is the only alternative. The idea is to transform the loop into two code fragements: the inspector and the executor. When the program is run, the inspector examines the iteration dependencies and constructs a parallel schedule. The executor subsequently uses that schedule to carry out the actual computation in parallel.
In this paper, we show how to reduce the overhead of running the inspector through its parallel execution. We describe two related approaches. The first, which emphasizes inspector efficiency, achieves nearly linear speedup relative to a sequential execution of the inspector, but produces a schedule that may be less efficient for the executor. The second technique, which emphasizes executor efficiency, does not in general achieve linear speedup of the inspector, but is guaranteed to produce the best achievable schedule. We present these techniques, show that they are correct, and compare their performance to existing techniques using a set of experiments.
Because in this paper we are optimizing inspector time, but leaving the executor unchanged, the techniques we present have most dramatic effect when the inspector must be run for each invocation of the source loop. In a companion paper [3], we explore techniques that build upon those developed here to also improve executor performance."
ff81ad322a2d01f00baae7f61279fc9b539c6e50,"We propose and evaluate empirically the performance of a dynamic processor-scheduling policy for multiprogrammed shared-memory multiprocessors. The policy is dynamic in that it reallocates processors from one parallel job to another based on the currently realized parallelism of those jobs. The policy is suitable for implementation in production systems in that:
—It interacts well with very efficient user-level thread packages, leaving to them many low-level thread operations that do not require kernel intervention.
—It deals with thread blocking due to user I/O and page faults.
—It ensures fairness in delivering resources to jobs.
—Its performance, measured in terms of average job response time, is superior to that of previously proposed schedulers, including those implemented in existing systems.
It provides good performance to very short, sequential (e.g., interactive) requests.
We have evaluated our scheduler and compared it to alternatives using a set of prototype implementations running on a Sequent Symmetry multiprocessor. Using a number of parallel applications with distinct qualitative behaviors, we have both evaluated the policies according to the major criterion of overall performance and examined a number of more general policy issues, including the advantage of “space sharing” over “time sharing” the processors of a multiprocessor, and the importance of cooperation between the kernel and the application in reallocating processors between jobs. We have also compared the policies according to other criteia important in real implementations, in particular, fairness and respone time to short, sequential requests. We conclude that a combination of performance and implementation considerations makes a compelling case for our dynamic scheduling policy."
4f7aecbb201a94f10912fd24771f4b62454e1514,"When a loop in a sequential program is parallelized, it is normally guaranteed that all ow dependencies and anti-dependencies are respected so that the result of parallel execution is always the same as sequential execution. In some cases, however, the algorithm implemented by the loop allows the iterations to be executed in a di erent sequential order than the one speci ed in the program. This opportunity can be exploited to expose parallelism that exists in the algorithm but is obscured by its sequential program implementation. In this paper, we show how parallelization of this kind of loop can be integrated into the runtime parallelization scheme of Saltz et al. [17, 18]. Runtime parallelization is a general technique appropriate for loops whose dependency structures cannot be determined at compile time. The compiler generates two pieces of code: the inspector examines dependencies at run time and computes a parallel schedule; the executor executes iterations in parallel according to the computed schedule. In our case, the inspector has to solve two problems: choosing an appropriate sequential order for the iterations and computing a parallel schedule. The two problems are treated as a single graph coloring problem, which is solved heuristically. Two methods to do so are described. Furthermore, the basic runtime parallelization scheme for shared-memory multiprocessors pays no attention to locality when scheduling iterations onto processors. One of our methods takes locality consideration into account when making these decisions. The performance implications of reordering are examined experimentally on a KSR1 parallel machine as well as through a simple analytic model of execution time."
8270154aab5e528f63f869d8002df768a6524146,"The authors analyze three approaches to scheduling mixed batch and interactive workloads on a supercomputer: (i) fixed partition, in which memory resources are statically allocated between the workloads: (ii) no partition, in which the interactive workload preempts resources as needed from the batch workload, and (iii) no partition with grouped admission, in which the interactive workload preempts resources only when the number of waiting interactive jobs reaches a threshold value. The authors also investigate the potential benefits of using virtual memory to perform the automatic overlay of jobs too large to fit in the amount of real memory instantaneously available to them. Using analytic tools, they compare the different policies according to the average speedup achieved by the batch workload given that a mean interactive job response time objective must be met by each. They show that, under a wide variety of conditions, fixed partition performs better than the other policies.<<ETX>>"
150d3765a36edfc2bc1412147ca2cd7c7f4da1da,
16182f33dd78bc2788b10a8848a4d5fbc4b900a1,
6f299051377092099526dae9115fa20be2c74455,"The authors present a parallel simulation protocol for performance Petri nets (PPNs), nets in which transition firings take randomly selected amounts of time. This protocol is interesting for two reasons. First, application of standard conservative or optimistic parallel simulation to PPNs results in either unnecessarily low (possibly no) parallelism or simply fails to produce correct results. Thus, this protocol may be thought of as addressing a class of models not amenable to standard parallel simulation, with PPNs being a particular example. Second, PPNs are currently analyzed using numerical techniques that have time and space requirements exponential in the size of the net. Simulation, particularly parallel simulation, is thus a practical alternative analysis method for these models, as is shown by measurement of execution times. The authors introduce a technique called selective receive that loosens a fundamental rule of conservative parallel simulation by allowing model components to sometimes ignore certain of their input channels and thus to determine their local clock times based on only a subset of their potential inputs.<<ETX>>"
993b49eb74a2105bd7207dfb9cf17f7f742d5886,"In a shared memory multiprocessor with caches, executing tasks develop ""affinity"" to processors by filling their caches with data and instructions during execution. A scheduling policy that ignores this affinity may waste processing power by causing excessive cache refilling.Our work focuses on quantifying the effect of processor reallocation on the performance of various parallel applications multiprogrammed on a shared memory multiprocessor, and on evaluating how the magnitude of this cost affects the choice of scheduling policy.We first identify the components of application response time, including processor reallocation costs. Next, we measure the impact of reallocation on the cache behavior of several parallel applications executing on a Sequent Symmetry multiprocessor. We also measure, the performance of these applications under a number of alternative allocation policies. These experiments lead us to conclude that on current machines processor affinity has only a very weak influence on the choice of scheduling discipline, and that the benefits of frequent processor reallocation (in response to the changing parallelism of jobs) outweigh the penalties imposed by such reallocation. Finally, we use this experimental data to parameterize a simple analytic model, allowing us to evaluate the effect of processor affinity on future machines, those containing faster processors and larger caches."
b960c36c6755bce567f19129164a6cd0b7cea506,"Spinning, or busy waiting, is commonly employed in parallel processors when threads of execution must wait for some event, such as synchronization with another thread. Because spinning is purely overhead, it is detrimental to both user response time and system throughput. The effects of two environmental factors, multiprogramming and data-dependent execution times, on spinning are discussed, and it is shown how the choice of scheduling discipline can be used to reduce the amount of spinning in each case. >"
3b17dc59a1cf8ff9d5828886c6fb71b3e7ec5702,"Oracle simulation is used to compare different load measures in terms of their impact on the performance of load balancing policies. During oracle simulation, the system is simulated and the performance measure of interest is evaluated in the future for each possible decision in the current state. By comparing the value of the performance measure in the future, the optimal decision for the current state can be reached. It is shown that the obvious load measures can be classified into two sets. The first set includes the instantaneous queue length, average queue length, utilization, response ratio, and total unfinished work. Any of these measures can greatly improve system performance. The other set includes arrival rate and throughput. These measures have only a slight effect on performance. The performance of practical load balancing policies that do not use future information supports the same conclusion.<<ETX>>"
817a2a797a594b094d42a3cd36ad9a7b727cab1e,"Existing work indicates that the commonly used “single queue of runnable tasks” approach to scheduling shared memory multiprocessors can perform very poorly in a multiprogrammed parallel processing environment. A more promising approach is the class of “two-level schedulers” in which the operating system deals solely with allocating processors to jobs while the individual jobs themselves perform task dispatching on those processors.
In this paper we compare two basic varieties of two-level schedulers. Those of the first type, static, make a single decision per job regarding the number of processors to allocate to it. Once the job has received its allocation, it is guaranteed to have exactly that number of processors available to it whenever it is active. The other class of two-level scheduler, dynamic, allows each job to acquire and release processors during its execution. By responding to the varying parallelism of the jobs, the dynamic scheduler promises higher processor utilizations at the cost of potentially greater scheduling overhead and more complicated application level task control policies.
Our results, obtained via simulation, highlight the tradeoffs between the static and dynamic approaches. We investigate how the choice of policy is affected by the cost of switching a processor from one job to another. We show that for a wide range of plausible overhead values, dynamic scheduling is superior to static scheduling. Within the class of static schedulers, we show that, in most cases, a simple “run to completion” scheme is preferable to a round-robin approach. Finally, we investigate different techniques for tuning the allocation decisions required by the dynamic policies and quantify their effects on performance.
We believe our results are directly applicable to many existing shared memory parallel computers, which for the most part currently employ a simple “single queue of tasks” extension of basic sequential machine schedulers. We plan to validate our results in future work through implementation and experimentation on such a system."
8ecd35ef9be1770b922f507387dac8687eaa5c21,
fffaea39e290105d311a0358baad1d81adad825c,"The demands on a heterogeneous distributed file system are outlined, and the design and implementation of a prototype to meet these demands are described. This prototype, the heterogeneous computer systems file system (HFS), provides a network-wide file system supporting a simple record-oriented file model. Through this standard file model, the HFS provides global access to files stored locally in many different file types. The HFS is implemented as a set of HFS servers, one running on each participating host. Each HFS server extends its host's local file system by fielding remote requests for files stored locally, translating those requests into the appropriate local file system calls, and returning any information so obtained. This prototype HFS implementation is used on a network composed of VAX systems running Unix, Sun systems running 4.2BSD Unix, and Xerox Dandelions running XDE.<<ETX>>"
6b365a4c41719eecfe1713dfffdf4a691d4d8d2f,
7d5479fec1bc2ae7ebfa58dbba2967dc2fc79774,"Now that manufacturing has become a respectable topic in industry, an obvious question is how human factors/ergonomics can contribute to the improvement of manufacturing. The traditional route for ergonomics intervention has been a Project route, with a set of objectives agreed between the human factors engineer and people within the company. Projects, however, do not ask the question of whether human factors intervention is likely to have an impact on the company's strategic objectives, for example, remaining in the manufacturing of a particular product. Case studies in a variety of industries are used to contrast the project approach with a more strategic approach. It is concluded that the project may represent sub-optimization in that a successful outcome of the project may have no impact upon company survival without a careful examination of the strategic plans of the company."
9bc6f4adc167a0c58c808f6eabcfe86590c7d7ef,"The tradeoff between speedup and efficiency that is inherent to a software system is investigated. The extent to which this tradeoff is determined by the average parallelism of the software system, as contrasted with other, more detailed, characterizations, is shown. The extent to which both speedup and efficiency can simultaneously be poor is bound: it is shown that for any software system and any number of processors, the sum of the average processor utilization (i.e. efficiency) and the attained fraction of the maximum possible speedup must exceed one. Bounds are given on speedup and efficiency, and on the incremental benefit and cost of allocating additional processors. An explicit formulation, as well as bounds, are given for the location of the knee of the execution time-efficiency profile, where the benefit per unit cost is maximized. >"
ac58e1d19e3fe97de6ba045f2b9aca0364bc75f4,"Load sharing has been the focus of a great deal of research as a means of enhancing the performance of distributed systems. In this paper we evaluate the potential benefits of load sharing in hierarchical distibuted systems. A hierarchical system consists of sub-systems, each of which is a distributed system in its own right. Our goal is to determine the level(s) at which load sharing may be exercised to improve the system performance at minimum overhead. Our simulation results indicate that most of the potential benefits can be achieved by employing load sharing locally. The additional benefit gained from global load sharing is minimal and does not justify the cost associated with it."
bc7083ab6fd282e67d45d8470152ac544c00c6c5,"The problem of sharing jobs among a set of parallel queues is discussed. The system is heterogeneous in the sense that different servers may have different speeds. Socially optimal policies that minimize the mean response time of all jobs ar of interest. Using semi-Markov decision processes, it is shown that an optimal policy that uses the instantaneous queue length independent of system utilization does not exist. Rather, the optimal decision of assigning a job to a server depends on the workload intensity. At light loads, the optimal policy tends to assign most jobs to fast servers. At heavy loads, slower servers are used to offload fast ones. Simulation results indicate that a simple heuristic, i.e., a generalization of the optimal policy for homogeneous systems derived from the analytic results, yields substantial performance improvement compared with no load sharing and outperforms the join-shortest-queue policy.<<ETX>>"
e4178fa9e176fbefec50885e67ed9bcd3ed5f548,"The authors take efficient kernel-level support as a given, and study the performance implications of design alternatives one level up-in the stubs, which insulate the client and server from details about network communication. These alternatives represent a collection of approaches to achieving standard remote procedure call of semantics. Consideration is given to the performance implications of compiled vs. interpreted stubs, procedural vs. inline code for moving data to/from packet buffers, block copy vs. individual data item copy moving data to/from packet buffers, and the presence or absence of byte swapping.<<ETX>>"
ee51d67633b5d602d942c8ce14cd39dff12e2d2f,"Load sharing has been the focus of a great deal of research as a means of enhancing the performance of distributed systems. In this paper we evaluate the potential benefits of load sharing in hierarchical distibuted systems. A hierarchical system consists of sub-systems, each of which is a distributed system in its own right. Our goal is to determine the level(s) at which load sharing may be exercised to improve the system performance at minimum overhead. Our simulation results indicate that most of the potential benefits can be achieved by employing load sharing locally. The additional benefit gained from global load sharing is minimal and does not justify the cost associated with it."
facfc5042bf5c416e82d17d49c3f69a1731e0e98,"As VLSI technology continues to improve, circuit area is gradually being replaced by pin restrictions as the limiting factor in design. Thus, it is reasonable to anticipate that on-chip memory will become increasingly inexpensive since it is a simple, regular structure than can easily take advantage of higher densities. In this paper we examine one way in which this trend can be exploited to improve the performance of multistage interconnection networks (MINs). In particular, we consider the performance benefits of placing significant memory in each MIN switch. This memory is used in two ways: to store (the unique copies of) data items and to maintain directories. The data storage function allows data to be placed nearer processors that reference it relatively frequently, at the cost of increased distance to other processors. The directory function allows data items to migrate in reaction to changes in program locality. We call our MIN architecture the Memory Hierarchy Network (MHN), In a preliminary investigation of the merits of this design [8] we examined the performance of MHNs under the simplifying assumption that an unlimited amount of memory was available in each switch. We found that despite the longer switch processing times of the MHN, system performance is improved over simpler, conventional schemes based on caching. In this paper we refine the earlier model to account for practical storage limitations. We study ways to reduce the amount of directory storage required by keeping only partial information regarding the current location of data items. The price paid for this reduction in memory requirement is more complicated (and in some circumstances slower) protocols. We obtain comparative performance estimates in an environment containing a single global memory module and a tree-structured MIN. Our results indicate that the MHN organization can have substantial perfor- mance benefits and so should be of increasing interest as the enabling technology becomes available."
0afefa8809cdc44b61b1317cf2d95ac12ff57941,"A family of dynamic cache-consistency-protocols for shared-bus multiprocessor systems is considered. A modeling approach, based on the specification and the iterative solution of sets of equations that express the mean values of interesting performance measures in terms of the mean values of certain model inputs, is presented. The equations are intuitive, in the sense that each can be explained simply in terms of the mechanics of the architecture being modeled. The solution technique is extremely efficient, requiring on the order of one second of CPU time for systems of arbitrary size. This makes it possible to explore a large design space quickly and interactively. The results are essentially as accurate as those of the previously existing techniques, which took hours on I-MIPS processors.<<ETX>>"
3aacc26b9c057f33af46a423b5a433e239b00b49,A software structure created by the Heterogeneous Computer Systems (HCS) Project at the University of Washington was designed to address the problems of heterogeneity that typically arise in research computing environments.
e40ed43ddacda12cc555d382f8d0ea1480fcf1ea,"<italic>Load sharing</italic> in a distributed system is the process of transparently sharing workload among the nodes in the system to achieve improved performance. In <italic>non-migratory</italic> load sharing, jobs may not be transferred once they have commenced execution. In load sharing with <italic>migration</italic>, on the other hand, jobs in execution may be interrupted, moved to other nodes, and then resumed.
In this paper we examine the performance benefits offered by migratory load sharing beyond those offered by non-migratory load sharing. We show that while migratory load sharing can offer modest performance benefits under some fairly extreme conditions, there are <italic>no</italic> conditions under which migration yields <italic>major</italic> performance benefits."
ef5b8978ddb032dc7e0d7857e0fa012d2ead3c17,
e565c25b63cac99072439a24f7de34d540f7553e,
f9d7cfb8f3ab655fa43e843970cfe42c607b5a56,"A prototype implementation has been built as part of the Heterogeneous Computer Systems project at the University of Washington. This service supports RPC binding and other applications in our heterogeneous environment. Measurements of the performance of this prototype show that it is close to that of the underlying name services, due largely to the use of specialized caching techniques."
43e29325171094c94d72daffa630990df72a51cb,"Rather than proposing a specific load sharing policy for implementation, the authors address the more fundamental question of the appropriate level of complexity for load sharing policies. It is shown that extremely simple adaptive load sharing policies, which collect very small amounts of system state information and which use this information in very simple ways, yield dramatic performance improvements. These policies in fact yield performance close to that expected from more complex policies whose viability is questionable. It is concluded that simple policies offer the greatest promise in practice, because of their combination of nearly optimal performance and inherent stability."
54231be1facc049c485a73539f1f9ceea5f682ce,"This paper reviews recent developments in the use of practical queueing­ based tools, particularly queueing network models, to study the performance of computer systems. We emphasize those research results that have been responsible for the successful transfer of this technology-the widespread use of these tools in computer system design, sizing, and capacity planning by people whose expertise lies in these application areas, rather than in modeling or mathematics. Queueing theory as a discipline dates from the 1920s. Almost without exception, the results of the early decades concern models of single conges­ tion points. These results encompass a wide variety of specific assumptions about the arrival process, the service process, the scheduling discipline, the queue size limitation, etc. Applications of these single resource models are found in many areas, particularly operations research, management science, and industrial engineering. In computer system engineering, these models have met with great success when used by skilled practitioners for the ""one-time"" analysis of subsystems, e.g. a specific communication network"
95935af81af254b47d1f9a6be73def59e2de66b5,"This paper studies the performance of single-user workstations that access files remotely over a local area network. From the environmental, economic, and administrative points of view, workstations that are diskless or that have limited secondary storage are desirable at the present time. Even with changing technology, access to shared data will continue to be important. It is likely that some performance penalty must be paid for remote rather than local file access. Our objectives are to assess this penalty and to explore a number of design alternatives that can serve to minimize it.
 Our approach is to use the results of measurement experiments to parameterize queuing network performance models. These models then are used to assess performance under load and to evahrate design alternatives. The major conclusions of our study are: (1) A system of diskless workstations with a shared file server can have satisfactory performance. By this, we mean performance comparable to that of a local disk in the lightly loaded case, and the ability to support substantial numbers of client workstations without significant degradation. As with any shared facility, good design is necessary to minimize queuing delays under high load. (2) The key to efficiency is protocols that allow volume transfers at every interface (e.g., between client and server, and between disk and memory at the server) and at every level (e.g., between client and server at the level of logical request/response and at the level of local area network packet size). However, the benefits of volume transfers are limited to moderate sizes (8-16 kbytes) by several factors. (3) From a performance point of view, augmenting the capabilities of the shared file server may be more cost effective than augmenting the capabilities of the client workstations. (4) Network contention should not be a performance problem for a lo-Mbit network and 100 active workstations in a software development environment."
4172693a3df2cb25bbdb46385b2255bca43d02e0,"One goal of locally distributed systems is to facilitate resource sharing. Most current locally distributed systems, however, share primarily data, data storage devices, and output devices; there is little sharing of computational resources. Load sharing is the process of sharing computational resources by transparently distributing the system workload. System performance can be improved by transferring work from nodes that are heavily loaded to nodes that are lightly loaded.
Load sharing policies may be either static or adaptive. Static policies use only information about the average behavior of the system; transfer decisions are independent of the actual current system state. Static policies may be either deterministic (e.g., “transfer all compilations originating at node A to server B”) or probabilistic (e.g., “transfer half of the compilations originating at node A to server B, and process the other half locally”).
Numerous static load sharing policies have been proposed. Early studies considered deterministic rules [Stone 1977, 1978; Bokhari 1979]. More recently, Tantawi and Towsley [1985] have developed a technique to find optimal probabilistic rules.
The principal advantage of static policies is their simplicity: there is no need to maintain and process system state information. Adaptive policies , by contrast, are more complex, since they employ information on the current system state in making transfer decisions. This information makes possible significantly greater performance benefits than can be achieved under static policies. This potential was clearly indicated by Livny and Melman [1982], who showed that in a network of homogeneous, autonomous nodes there is a high probability that at least one node is idle while tasks are queued at some other node, over a wide range of network sizes and average node utilizations.
In previous work [Eager, Lazowska & Zahorjan 1984] we considered the appropriate level of complexity for adaptive load sharing policies. (For example, how much system state information should be collected, and how should it be used in making transfer decisions?) Rather than advocating specific policies, we considered fairly abstract strategies exhibiting various levels of complexity. We demonstrated that the potential of adaptive load sharing can in fact be realized by quite simple strategies that the use only small amounts of system state information. This result is important because of a number of practical concerns regarding complex policies: the effect of the overhead required to administer a complex policy, the effect of the inevitable inaccuracies in detailed information about system state and workload characteristics, and the potential for instability. (We consciously use the phrase “load sharing” rather than the more common “load balancing” to highlight the fact that load balancing, with its implication of attempting to equalize queue lengths system-wide, is not an appropriate objective.)
Adaptive load sharing policies can employ either centralized or distributed control. Distributed control strategies can be of two basic types (although intermediate strategies also are conceivable): sender-initiated (in which congested nodes search for lightly loaded nodes to which work may be transferred), and receiver-initiated (in which lightly loaded nodes search for congested nodes from which work may be transferred). Our earlier paper considered distributed, sender-initiated policies - a sufficiently rich class to allow us to answer the fundamental questions of policy complexity that we were addressing. In the course of understanding the reasons for the degradation of these policies at high system loads, we were led to consider receiver-initiated policies as a possible alternative. The comparison of receiver-initiated and sender-initiated adaptive load sharing is the purpose of the present paper.
There have been several experimental studies, using prototypes and simulation models, of specific (typically fairly complex) adaptive load sharing policies [Bryant & Finkel 1981; Livny & Melman 1982; Kreuger & Finkel 1984; Barak & Shiloh 1984]. Both sender-initiated policies and receiver-initiated policies have been considered. However, there has not previously been a rigorous comparison of these two strategies. Such a comparison is made difficult by the problem of choosing appropriate representative policies of each type, and by the potentially quite different costs incurred in effecting transfers. (Receiver-initiated policies typically will require the transfer of executing tasks, which incurs substantial costs in most systems [Powell & Miller 1983]. Sender-initiated policies naturally avoid such costly transfers, since tasks can be transferred upon arrival, prior to beginning execution.)
Our present paper is similar to our previous work in that our purpose, rather than to advocate specific policies, is to address a fundamental question concerning policies in general: How should system state information be collected and load sharing actions initiated - by potential receivers of work, or by potential senders of work? In studying this question we consider a set of abstract policies that represent only the essential aspects of receiver-initiated and sender-initiated load sharing strategies. These policies are investigated using simple analytic models. Our objective is not to determine the absolute performance of particular load sharing policies, but rather to gain intuition regarding the relative merits of the different approaches under consideration.
We represent locally distributed systems as collections of identical nodes, each consisting of a single processor. The nodes are connected by a local area network (e.g., an Ethernet). All nodes are subjected to the same average arrival rate of tasks, which are of a single type.
In contrast to most previous papers on load sharing, we represent the cost of task transfer as a processor cost rather than as a communication network cost. It is clear from measurement and analysis [Lazowska et al. 1984] that the processor costs of packaging data for transmission and unpackaging it upon reception far outweigh the communication network costs of transmitting the data.
We study three abstract load sharing policies, comparing their performance to each other and to that of a system in which there is no load sharing. The Sender policy is used a representative of sender-initiated load sharing strategies. The Receiver and Reservation policies are used as representatives of receiver-initiated load sharing strategies; unlike the Receiver policy, the Reservation policy will transfer only newly arriving tasks. In a bit more detail:SenderIn our earlier work concerning the appropriate level of complexity for adaptive load sharing schemes, we identified two sub-policies of sender-initiated strategies. The transfer policy determines whether a task should be processed locally or remotely. The location policy determines to which node a task selected for transfer should be sent.
In that previous study, we considered threshold transfer policies, in which each node uses only local state information. An attempt is made to transfer a task originating at a node if and only if the number of tasks already in service or waiting for service (the node queue length) is greater than or equal to some threshold T.
We considered various location policies spanning a range of complexity. We found that the use of a complex location policy yields only slight improvement over the use of a simple location policy that, like the transfer policy, uses threshold information. In this threshold location policy, a node is selected at random and probed to determine whether the transfer of a task to that node would place the node above the threshold T. If not, then the task is transferred. If so, then another node is selected at random and probed in the same manner. This continues until either a suitable destination node is found, or the number of probes reaches a static probe limit, Lp. In the latter case, the originating node must process the task. (The use of probing with a fixed limit, rather than broadcast, ensures that the cost of executing the load sharing policy will not be prohibitive even in large networks. The performance of this policy was found to be suprisingly insensitive to the choice of probe limit: the performance with a small probe limit, e.g., 3 or 5, is nearly as good as the performance with a large probe limit, e.g., 20.)
The sender-initiated policy with a threshold transfer policy and a threshold location policy was found to yield performance not far from optimal, particularly at light to moderate system loads. For this reason, and because of its simplicity, we choose this policy to serve as the representative of sender-initiated strategies for the comparison that is the subject of the present paper, and term it here the Sender policy.

ReceiverTo facilitate comparison between sender-initiated strategies and receiver-initiated strategies, a representative policy of the latter class should be as similar as possible to the Sender policy. In particular, it should utilize threshold-type state information, and have a bound Lp on the number of remote nodes whose state can be examined when making a task transfer decision.
In the Receiver policy, a node attempts to replace a task that has completed processing if there are less than T tasks remaining at the node. A remote node is selected at random and probed to determine whether the transfer of a task from that node would place its queue length below the threshold value T. If not, and if the node is not already in the process of transferring a task, a task is transferred to the node initiating the probe. Otherwise, another node is selected at random and probed in the same manner. This continues until either a node is found from which a task can be obtained, or the number of probes reaches a static probe limit, Lp. In the latter case, the nod"
4b62ae3baec7d1457a3f5ce3123e5c32dcee67bb,
fafe3f68e85aa94acb11dfc6136e159a1e98d438,
02544882276ff1a35f4b6f1a8504a972b8df4087,"Reading a book is also kind of better solution when you have no enough money or time to get your own adventure. This is one of the reasons we show the quantitative system performance computer system analysis using queuing network models as your friend in spending the time. For more representative collections, this book not only offers it's strategically book resource. It can be a good friend, really good friend with much knowledge."
cce23747d45c1d7e92fd855c01bf630d09455dd1,
fbc1913c3fb6ac3f73c1fe7b2dffb9f5d32eafee,"Queueing network performance modelling technology has made tremendous strides in recent years. Two of the most important developments in facilitating the modelling of large and complex systems are hierarchical modelling, in which a single load dependent server is used as a surrogate for a subsystem, and approximate mean value analysis, in which reliable approximate solutions of separable models are efficiently obtained. Unfortunately, there has been no successful marriage of these two developments; that is, existing algorithms for approximate mean value analysis do not accommodate load dependent servers reliably.
 This paper presents a successful technique for incorporating load dependent servers in approximate mean value analysis. We consider multiple class models in which the service rate of each load dependent server is a function of the queue length at that server. In other words, load dependent center k delivers “service units” at a total rate of f@@@@ (n@@@@) when n@@@@ customers are present. We present extensive experimental validation which indicates that our algorithm contributes an average error in response times of less than 1% compared to the (much more expensive) exact solution.
 In addition to the practical value of our algorithm, several of the techniques that it employs are of independent interest."
07f4ad1f15f0c82bb971cca1760db0262b741d39,"There are two basic representations of workload populations in load independent, separable queueing network models. These correspond to the notions of open and closed classes, an open class being one in which customers may arrive and depart the model, and a closed class being one in which the number of customers is fixed. This paper examines the effect on mean system performance measures of the workload representation chosen. Open and closed representations are compared under the equivalency constraints that they result in identical system throughput or mean system population level for the class being considered. It is shown formally for a limited class of networks that the open representation results in larger system response times than equivalent closed representations, and that one of the closed representations results in the smallest system response time of those considered. Extensive numerical results show for a more general class of models that there is a strict ordering (in terms of system response time) of the natural class representations considered.
 These results can be used in at least two ways. One is to guide the initial representation of computer system workloads in performance models. The other application is as a component of approximate analysis techniques for queueing models that decompose individual networks into multiple submodels, each of which is then solved in isolation. Here the goal is to represent customer classes in each submodel in a way that is convenient computationally, and that results in performance measures closely matching those observed in the full network. It is this latter application that we assume in this paper. An example of the application of our results to an existing approximation technique is given."
6b21d3904fbdbee68963826bb83e405d44ce1a83,
6b467b3be575916a5e8b0e9b3dd7aa8342e0fcd0,
1aa0c3b2b0a32fd10d737777ed4514cf5fa0423b,"Most computer systems have a memory constraint: a limit on the number of requests that can actively compete for processing resources, imposed by finite memory resources. This characteristic violates the conditions required for queueing network performance models to be <i>separable,</i> i.e., amenable to efficient analysis by standard algorithms. Useful algorithms for analyzing models of memory constrained systems have been devised only for models with a single customer class.
 In this paper we consider the multiple class case. We introduce and evaluate an algorithm for analyzing multiple class queueing networks in which the classes have independent memory constraints. We extend this algorithm to situations in which several classes share a memory constraint. We sketch a generalization to situations in which a subsystem within an overall system model has a population constraint.
 Our algorithm is compatible with the extremely time- and space-efficient iterative approximate solution techniques for separable queueing networks. This level of efficiency is mandatory for modelling large systems."
4377ec028cd8e463526c98106b24200df893d0e4,"References I. Bard, Y. A model of shared DASD and multipathing. Comm. ACM 23,10 (Oct. 1980), 564-572. 2. Bard, Y. Some extensions to multiclass queueing network analysis. In Performance of Computer Systems. Arato, M., Butrimenko, A., and Gelenbe, E. (Eds.), North Holland, Amsterdam, 1979. 3. Bard, Y. The VM/370 performance predictor. Comput. Surv. 10,8 (Sept. 1978), 333-342. 4. Brown, R.M., Browne, J.C., and Chandy, K.M. Memory management and response time. Comm. ACM 20,3 (March 1977), 153-165. 5. Buzen, J.P., and Denning, P.J. Measuring and calculating queue length distributions. Computer 13,4 (April 1980), 33--46. 6. Chandy, K.M., Herzog, U., and Woo, L. Approximate analysis of general queueing networks. IBM J. Res. Develop. 19,1 (Jan. 1975), 43-49. 7. Comput. Surv. 10,3 (Sept. 78), Special Issue on Queueing Network Models of Computer System Performance. 8. Lavenberg, S.S., and Reiser, M. Stationary state probabilities of arrival instants for closed queueing networks with multiple types of customers. Res. Rep. RC 7592, IBM Corp., April 1979. IBM Thomas J. Watson Research Center, Yorktown Heights, NY. 9. Little, J.D.C. A proof of the queueing formula L = ~ W. Oper. Res. 9 (1961), 383-387. 10. Reiser, M., and Lavenberg, S.S. Mean-value analysis of closed multichain queueing networks..L ACM 27,2 (April 1980), 313-322. II. Sauer, C.H., and Chandy, K.M. Computer System Performance Modeling: A Primer. Prentice-Hall, Englewood Cliffs, N J, 1980. 12. Schweitzer, P. Approximate analysis of multiclass closed networks of queues. Presented at the Int. Conf. Stochastic Control and Optimization, Amsterdam, 1979. 13. Sevcik, K.C., and Mitrani, I. The distribution of queueing network states at input and output instants. J. A CM 28,2 (April 1981), 358-371. 14. Zahorjan, J. The approximate solution of large queueing network models. Ph.D. Thesis, Univ. Toronto, Toronto, Ont., Canada, 1980; also Tech. Rep. CSRG-122, Univ. Toronto, Toronto, Ont., Canada August 1980. Applications of queueing network models to computer system performance prediction typically involve the computation of exact equilibrium solutions. This procedure can be quite expensive. In actual modeling studies, many alternative systems must be considered and a model of each developed. The expense of computing the exact solutions of these models may not be warranted by the accuracy required at the initial modeling stages. Instead, bounds on performance are often sufficient. We present a new technique for obtaining performance bounds with only a few arithmetic operations (whereas an exact solution of the model requires a number of arithmetic operations proportional to the product of the number of devices and number of customers). These bounds are often tighter than previously known bounds, although they require somewhat more restrictive assumptions to be applicable."
7d2e0df5a5666d5fa17e19db20a10d76b2fa570b,
0849ed78a4d6a2182de6562e379f9b861b60b01c,"Applications of queueing network models to computer system performance prediction typically involve the computation of their equilibrium solution. When numerous alternative systems are to be examined and the numbers of devices and customers are large, however, the expense of computing the exact solutions may not be warranted by the accuracy required. In such situations, it is desirable to be able to obtain bounds on the system solution with very little computation. Asymptotic bound analysis (ABA) is one technique for obtaining such bounds. In this paper, we introduce another bounding technique, called balanced job bounds (BJB), which is based on the analysis of systems in which all devices are equally utilized. These bounds are tighter than ABA bounds in many cases, but they are based on more restrictive assumptions (namely, those that lead to separable queueing network models)."
b905f9b15cd9d2275c1fef005a8b282c22a77878,"This is a summary of a tutorial presented during the conference discussing a number of approaches to representing disk I/O subsystems in analytic models of computer systems.
 As in any analytic modelling study, the fundamental objective in considering an I/O subsystem is to determine which devices should be represented in the model, and what their loadings should be. The device loadings represent the service required by jobs, and are the basic parameters needed by the computational algorithm which calculates performance measures for the model. To set these parameters, knowledge of service times at the various devices in the I/O subsystem is required.
 The tutorial begins by distinguishing analytic modelling from alternative approaches, by identifying the parameter values that are required for an analytic modelling study, and by explaining the role of the computational algorithm that is employed (Denning & Buzen [1978] provide a good, although lengthy, summary).
 We then consider a sequence of models of increasingly complex I/O subsystems.
 Next we discuss I/O subsystems with rotational position sensing.
 We then discuss approaches to modelling shared DASD, emphasizing hierarchical techniques in which highlevel models of each system can be analyzed in isolation. We also mention recent techniques for modelling complex I/O subsystems involving multipathing.
 Finally, we discuss the analysis of I/O subsystems based on broadcast channels such as Ethernet."
c85d6aa11dd17600e5b29b0e3f6670a7e79617b9,"Because it is more intuitively understandable than the previously existing convolution algorithms, Mean Value Analysis (MVA) has gained great popularity as an exact solution technique for separable queueing networks. However, the derivations of MVA presented to date apply only to closed queueing network models. Additionally, the problem of the storage requirement of MVA has not been dealt with satisfactorily. In this paper we address both these problems, presenting MVA solutions for open and mixed load independent networks, and a storage maintenance technique that we postulate is the minimum possible of any “reasonable” MVA technique."
b65b1fc474bc61108acb9e11c7f9d5b3fd93ff34,"In this thesis we investigate the problem of obtaining solutions for large, separable queueing network models of computer systems. To this end, we begin with a detailed analysis of the time and space requirements of the two major solution techniques. Subsequently, we develop three approximations based on aggregation of network components: aggregation of classes, aggregation of customers, and aggregation of service centers. 
Each approximation technique reduces the cost of solving the network (approximately) by reducing the number of some component in it that contributes to the high cost of solution. In general, the approximations transform a large separable network into a smaller, also separable network (using aggregation) that reflects the behavior of the original network. The exact solution for this reduced model is then obtained using standard techniques, and estimates of the performance measures of the full network are made from those of the reduced network. 
We present with each technique an explanation of its relation to the state space of the full network. This formal study is augmented by example solutions using the approximations for representative sets of models to gain some insight into the error behavior of the techniques. 
Finally, we discuss an alternative approximation technique not based on aggregation. A comparison of all techniques is given, with the problem of selecting an appropriate technique for a given network in mind."
3b98e1dfa6fb59adc0acf697f0d19fbdc4504281,"In this paper we present a convolution algorithm for the full class of closed, separable queueing networks. In particular, the algorithm represents an alternative method to those already known for the solution of networks with class changes, and is the first efficient algorithm to deal with Lam-type networks [11].
 As an application of the algorithm, we study a simple queueing network with disk I/O devices connected to a single CPU through a single channel. The algorithm is then used to develop a simple, accurate approximation for the blocking of disk devices that takes place when a customer using a disk is waiting for or in service at the channel."
e6a953f7604c7f63cdc58aa7241f4b6460d67889,"AbstractIn this paper we present a general model of disk storage systems equipped with rotational position sensing (RPS). These systems are distinguished by sector scheduling at the channel. We show that the number of revolutions spent waiting for the channel due to the RPS feature is not geometrically distributed, as was assumed in most previous models. In place of the geometric assumption we give an approximate formula for the mean and variance of the number of revolutions required, and use this formula to develop an open, single queue model representing one disk module of the multi-module system. This model is then compared to simulations and to a previously developed analytic model. Finally, we develop a closed queueing network model using the single queue model as a component, and an approximate solution method is described by which to obtain performance measurements from the closed model."
277c0bebff89924a25fd8c17845b84a8db7dc560,
907a3e3af83577abf1196f2c9612aaead0e875c7,"[57] W. Sanders and J. Meyer. Reduced base model construction methods for stochastic activity networks. [58] L. Schmickler. MEDA { mixed Erlang distributions as phase-type representation of empirical functions. [29] H. Kantz and K. Trivedi. Reliability modeling of MARS system : A case study in the use of dierent tools and techniques. Numerical evaluation of performability measures and job completion time in repairable fault-tolerant systems. [37] M. Malhotra. A computationally ecient technique for transient analysis of repairable Markovian systems.tolerant methods for transient analysis of sti Markov c hains. [39] M. Malhotra and A. Reibman. Selecting and implementing phase approximations for semi-Markov models. [40] M. Malhotra and K. Trivedi. Higher-order methods for transient analysis of sti Markov c hains. In [17] P. Courtois. Computable bounds for conditional steady-state probabilities in large Markov c hain and queueing models. [19] A. Cumani. ESP { A package for the evaluation of stochastic Petri nets with phase-type distributed transition times. [8] A. Bobbio and K. Trivedi. An aggregation technique for the transient analysis of sti Markov c hains. 22 6 Conclusion We discussed several types of modeling techniques used in dependability and performability analysis, with a particular emphasis on approaches based on the (entire or partial) generation of the state-space. The common underlying formalisms w e consider, continuous-time Markov c hains (CTMCs) and Markov reward models (MRMs), are capable of modeling a large class of systems, but they result in large models, dicult to describe and analyze. The description problem is solved by using higher-level formalisms, such as reliability graphs, fault trees, queueing networks, generalized stochastic Petri nets, and stochastic reward nets. With the appropriate software modeling tools, these can then be automatically translated into CTMCs or MRMs. The solution problem, though, remains, since the size of the underlying stochastic process grows combi-natorially. In addition, when modeling activities with very dierent timescales , such as failure and repair of components, and performance-related behavior, such as arrival and departure of jobs, stiness arises. Advanced numerical techniques, and exact or approximate approaches such as truncation, aggregation, composition , and uid models, can then be eectively used to obtain numerical solutions. 21 Johnson and Tae [27, 28] have considered matching the rst three moments of mixtures of two Erlang distributions. For more references on this topic, refer to [7]. Generation of the overall CTMC. After the parameters of phase approximations for all the non-exponential distributions have been tted …"
4e1dd4c20253264f984bd56b1ba6512eac5ef7cb,"
Purpose
Neighbourhood policing is central to supporting public confidence in England and Wales. However, the delivery of neighbourhood policing models is increasingly fragmented and under pressure from austerity measures and from changes to demand and priorities. This research aims to understand the current state of neighbourhood policing in the county of “Rackhamshire” and its ability to support public confidence.


Design/methodology/approach
The authors conducted six focus groups, three with officers who were part of Community Policing Teams (CPTs) and three with members of the community who were actively engaged with community policing and local concerns. These were supplemented by two interviews with senior officers (35 participants in total).


Findings
Austerity-driven changes to policing in Rackhamshire have damaged the capacities of CPTs and residents have begun to lose confidence in the ability of the police to respond to their fears. The authors argue that reforms intended to make policing more efficient and effective appear to have the opposite effect on community policing, by preventing it from working in a way that can support public confidence and that this could have longer-term consequences.


Originality/value
The effects of austerity on the mechanisms by which neighbourhood policing supports confidence have been relatively neglected. By exploring the state of these mechanisms in one English constabulary, this research has exposed serious weaknesses in the way that community policing is able to support public confidence and suggests practical operational responses. In light of these findings, this study argues for the urgent reinstatement of earlier models of neighbourhood policing.
"
03e6af225f23686ae5941f0bfd508efb79b82b7d,"ABSTRACT New South Wales (NSW) was the first Australian state to introduce drug detection dogs as a street-level policing strategy. In 2006, the NSW Ombudsman released damning evidence that challenged the dogs’ effectiveness. Over a decade later, drug detection dogs remain a core policing policy in NSW, and the powers surrounding their use have expanded. This study provides the first comprehensive analysis of drug dog deployment since the NSW Ombudsman Review. Specifically, it analyses unit-record data on all recorded criminal incidents and persons of interest (POIs) involving drug detection dogs that led to a formal police response in NSW from June 2008 to June 2018. The analysis shows that the main target group has remained young males detected for use/possession offences, albeit that the dogs have detected a small but potentially significant population of drug suppliers, and that the circumstances for their detection differ markedly to that for consumers. The results further show that there has been a small reduction in the number of overall detections recorded by police. However, this trend has not been driven by a decrease in use/possession offences detected, and thus large numbers of use/possession offenders, as opposed to drug suppliers, continue to be policed via this policy each year. This paper discusses the implications of these findings for policy and practice."
3898187c3cb3bd5b388975496baf5b507acf738e,"This article presents a map and summary of the landscape of a systematic search of the police recruit training literature. Following the process of searching, screening, and coding both published and grey literature, a total of 109 studies met our inclusion criteria. Thematic analysis of the 109 studies led to the emergence of six broad themes and associated subthemes. The two most prevalent themes focused on ‘examining academic and/or field training’ and ‘examining a specific aspect of the training programme’, each containing 36 studies. Most of the studies were based in the USA, (n = 67). Grey literature such as dissertations, theses, and reports made up nearly half of all included studies (n = 51) and published journal articles made up the bulk of the remaining studies (n = 50). Furthermore, 56 studies (50%) used a quantitative design, 36 studies (33%) adopted mixed methods, and 19 studies (17%) employed a qualitative approach. The 109 studies were double-blind quality appraised using recognized quality appraisal tools and revealed a wide variation in the nature and quality of studies. Overall, the strength of the evidence was fragile; only 13 studies (12%) were of a ‘strong’ quality, 55 studies (50%) were ‘weak’, and the rest (41, i.e. 38%) were of ‘moderate’ quality. The article concludes with recommendations for guiding future research in police recruit training."
593355af2eba73c3ce1efb03cc876df1bac85af1,
cf6df77b64afb8ee23786e4fa301757c97d10253,
25c965c63535b3ef856bd425004f948cb8327a10,"ABSTRACT Policing in the UK is currently undergoing changes to the training and attained qualifications of entry level police officers. The College of Policing is in the process of developing three entry routes into the profession, one of which is a graduate conversion course for those new recruits who hold, at a minimum, a Bachelor’s degree. The objective of this research was to gather evidence on existing conversion courses to inform the development of this route. A rapid evidence assessment was undertaken to a narrative synthesis of the literature on graduate conversion courses in other professions. Fifty-one studies were identified and coded. Four main themes emerged from the available evidence as being central. These themes – learning styles, translating theory into practice, teaching methods and assessment were cross cut by pedagogical sub-themes of collaborative learning, and reflective practice. Policy and practice implications for a policing conversion course are drawn out to guide development and delivery of police training and education in the 21st century."
b64a11181acaafdb797e7d0f9d1a88231ee1f105,
a441e1c255dff57caea64ddde7544775ae1032ea,
f62e012d0542819e07a02999c8439cb816b83a1e,"Results: In the first 12 months of the operation of the It Stops Here: Safer Pathway Program, the DVSAT was administered to a total of 102,605 victims of domestic violence. One in five reported victims of intimate partner violence and one in 10 reported victims of non-intimate partner violence were classified as ‘at serious threat’. Threat level was strongly related to scores on the DVSAT but there was also evidence that referrer risk ratings were determined by other factors. Regardless of the DVSAT results, alleged victims were initially classified as ‘at serious threat’ if they had reportedly experienced three or more prior domestic violence incidents. This arrangement was found to create a high level of service demand and has since been dropped."
fafe374435f2da252874ae8c0b2bac01cc499a17,"Aim: To examine the experience of court users of two metropolitan courthouses in New South Wales. Method: 311 court users were surveyed regarding their awareness, understanding, contacts, and satisfaction with services at court. The sample included defendants (33.1%) , victims (10.6%), witnesses (8.0%) and supporters (48.2%). Court professionals were not sampled. Results: Nearly all (90.6%) court users had previously attended court. Court users who reported contacts with court staff (64.1%) typically found these contacts very helpful. Court users who were well informed about support options at court (31.9%) were much more likely to rate other measures of court experience positively. Court experiences varied somewhat with user role; notably, victims, witnesses and their supporters were much more likely to receive proactive assistance than defendants and their supporters (37.0% vs. 20.7%, p"
19085df76b70ea0a69deac2dae4a69b454767aea,"Overview Aim: To provide an overview of adult and juvenile re-offending over the last 10 years in New South Wales (NSW). Method: Descriptive analysis of data from the NSW Re-offending Database (ROD). Results and conclusion: Most offenders convicted in the NSW criminal courts were reconvicted of a further offence within 10 years of their offence, and this was especially so for juveniles and Indigenous offenders. Those reconvicted tended to be reconvicted for a variety of offences."
567afeb691a9ca6ce2b1a4ea4764f58d12de4782,"In the past few years in NSW, alcohol related violence and assaults on licensed premises have gained much public, media and political attention. A number of legislative reforms have been enacted in an attempt to reduce the scale of alcoholrelated violence in and around licensed premises. Many of these reforms involve the imposition of trading restrictions on licensed premises that are repeatedly the site of alcohol-related violence (see Roth 2014 for a detailed discussion of the reforms)."
c6b85da78c16db493961f66e3f21898688d571e4,"What can be learned from a detailed reading of the statements made by firms in response to Section 54 of the UK’s Modern Slavery Act 2015? This paper presents an in-depth analysis of the development of the statements over time for two large companies in different sectors and with contrasting characteristics. We show that the statements progress in sophistication over time, but have serious limitations. We demonstrate that although they present interesting information about the management of working conditions in the firms’ supply chains, they do little to address the problems of modern slavery per se. Furthermore, we show that the statements can contrast strongly with other documents published by the firm, and with established patterns of business practice. For both firms, we show that a deep understanding of the context of the organization is essential for interpreting the statements – but this contextual information is unlikely to be available to the casual reader. We conclude that the statements, as they are currently specified may in fact provide a way of deflecting the engagement of Civil Society Organisations, in a way not envisaged in the formulation of the Act. This paper is an extended version of a section of the report Hsin, L., New, S.J., Pietropaoli, I. and Smit, L. (2021). Accountability, Monitoring and the Effectiveness of Section 54 of the Modern Slavery Act: Evidence and Comparative Analysis. London: Modern Slavery Policy and Evidence Centre, providing greater detail than was possible to include in that report."
3d36f42234f8b58af770667c7d8a974782c4f5cd,"The scourge of modern slavery has led to legislation in various countries requiring firms to engage in a particular form of supply chain transparency. However, these regulatory initiatives have been widely perceived to be, by themselves, a weak response to such a serious challenge. This chapter argues that assessment of these initiatives hinges on the interpretation of modern slavery itself: Are extreme forms of labor exploitation aberrations of an otherwise blameless system, or are they intrinsic to the functioning of contemporary global capitalism? If the latter, then new types of transparency might be needed that go beyond firms reporting on their policies and efforts. The chapter introduces the idea of interrogational transparency as a mechanism by which civil society actors (including consumers, activists, and researchers) can develop accountability dialogues with powerful corporations. To illustrate these points, the chapter examines emergent patterns of transparency within the food giant Nestlé."
a1ba019de0f03a97e17be25999bf428750933da4,"This collection raises important questions and is suggestive of new directions for both theory and action. It doesn’t seek to provide either a comprehensive review or a coherent agenda; it does give a vivid snapshot of work on the role of organized labour in settings of precarious work. Here I will pick out two contrasts between the world described in the book and issues that arise in the more ‘managerial’ perspectives commonly found in business schools. It barely needs to be said that the well-discussed divide between industrial relations and the ‘people management’ as taught on MBA programmes has become a generational thing; not only do most business schools barely mention unions and collective bargaining in their courses, but – unlike their greyer colleagues – many of today’s emerging scholars in Human Resource Management and Organizational Behaviour haven’t served an apprenticeship in the territory either. The silo walls have got taller. So this book sits squarely in an intellectual field that has become – sadly – disconnected from the main audience who ought to be interested in its contents. This, it should be emphasized, is a criticism of business schools, and there are of course honourable exceptions. My own sub-field (Operations and Technology Management) has – mostly – been resolutely ignorant of/disconnected from the industrial relations agenda. There is much in this book that should inform that discourse. One important element is the de-centring of US/ European large business practice as ‘normal’. Here, the contributions on, for example, domestic work, Uruguayan waste pickers, female Indian construction workers are illuminating. But the silo criticism can cut both ways. Maybe this book also represents an unhelpful narrowness of vision, and perhaps underplays two key themes. The first issue is the significance of global supply chains. In many contexts, the role of workers’ organizations is made complex by the overlapping roles of other companies in the chain both exerting pressure for improvement in working standards, and sometimes simultaneously reinforcing anti-union practices. This raises the issue of the role of workers’ organizations in collaborating across the chain (a point mentioned in Jamie Woodcock’s standout chapter on digital labour) but also in playing a role in the overall 903045WES0010.1177/0950017020903045Work, Employment and SocietyBook review Book Review Symposium2020"
0a8da4bf13bafbcbf87400a62dd8e3ad75c893a4,"Security and privacy concerns represent a significant hindrance to the widespread adoption of cloud computing services. While cloud adoption mitigates some of the existing information technology (IT) risks, research shows that it introduces a new set of security risks linked to multi-tenancy, supply chain and system complexity. Assessing and managing cloud risks can be a challenge, even for cloud service providers (CSPs), due to the increased numbers of parties, devices and applications involved in cloud service delivery. The limited visibility of security controls down the supply chain, further exacerbates this risk assessment challenge. As such, we propose the Cloud Supply Chain Cyber Risk Assessment (CSCCRA) model, a quantitative risk assessment model which is supported by supplier security posture assessment and supply chain mapping. Using the CSCCRA model, we assess the risk of a SaaS application, mapping its supply chain, identifying weak links in the chain, evaluating its security risks and presenting the risk value in monetary terms (£), with this, promoting cost-effective risk mitigation and optimal risk prioritisation. We later apply the Core Unified Risk Framework (CURF) in comparing the CSCCRA model with already established methods, as part of evaluating its completeness."
56b0153da2445005546c6260eb140500939662a9,"Cloud computing is widely believed to be the future of computing. It has grown from being a promising idea to one of the fastest research and development paradigms of the computing industry. However, security and privacy concerns represent a significant hindrance to the widespread adoption of cloud computing services. Likewise, the attributes of the cloud such as multi-tenancy, dynamic supply chain, limited visibility of security controls and system complexity, have exacerbated the challenge of assessing cloud risks. In this paper, we conduct a real-world case study to validate the use of a supply chaininclusive risk assessment model in assessing the risks of a multicloud SaaS application. Using the components of the Cloud Supply Chain Cyber Risk Assessment (CSCCRA) model, we show how the model enables cloud service providers (CSPs) to identify critical suppliers, map their supply chain, identify weak security spots within the chain, and analyse the risk of the SaaS application, while also presenting the value of the risk in monetary terms. A key novelty of the CSCCRA model is that it caters for the complexities involved in the delivery of SaaS applications and adapts to the dynamic nature of the cloud, enabling CSPs to conduct risk assessments at a higher frequency, in response to a change in the supply chain."
70f14fbc6a8c94cd5e4680151c77aace6cad363a,
b17d82accc00d3696ded8393a1e0d690cf144414,
10537ba23bd717d0be2e5193af19e3191d2323d5,
584cd551ebedc52129dbf2512bdda46310a0fd17,
6452affa605dfe269a9dcc6bea5bb5e774c221f0,"Small increases to checks at borders can result in very large and unpredictable delays to freight.Large and unpredictable delays have serious practical consequences for industrial practice. The second-order effects of these consequences – ‘cascade effects’ – may lead to devastating consequences for particular geographies and industries. The Government therefore needs to address port-related delays and congestion immediately, facing up to the reality that this will require very substantial public investment."
7caebb1958cf6ec8c1b7bec013f67204ee0aae2c,"Cloud computing represents a significant paradigm shift in the delivery of information technology (IT) services. The rapid growth of the cloud and the increasing security concerns associated with the delivery of cloud services has led many researchers to study cloud risks and risk assessments. Some of these studies highlight the inability of current risk assessments to cope with the dynamic nature of the cloud, a gap we believe is as a result of the lack of consideration for the inherent risk of the supply chain. This paper, therefore, describes the cloud supply chain and investigates the effect of supply chain transparency in conducting a comprehensive risk assessment. We conducted an industry survey to gauge stakeholder awareness of supply chain risks, seeking to find out the risk assessment methods commonly used, factors that hindered a comprehensive evaluation and how the current state-of-the-art can be improved. The analysis of the survey dataset showed the lack of flexibility of the popular qualitative assessment methods in coping with the risks associated with the dynamic supply chain of cloud services, typically made up of an average of eight suppliers. To address these gaps, we propose a Cloud Supply Chain Cyber Risk Assessment (CSCCRA) model, a quantitative risk assessment model which is supported by decision support analysis and supply chain mapping in the identification, analysis and evaluation of cloud risks."
8f5983619383e58dac0e456284a32525e67cd1db,
3a4988b9d8b9c668a07e44a75a8bc962e1d6c9ea,"As organisations move sensitive data to the cloud, their risk profile increases due to the integrated supply chain utilised in cloud computing. The risk is made visible in situations where a cloud offering is federated, with customer data located in multiple datacenters, under the control of multiple providers and sub-providers in different jurisdictions. This problem is further exacerbated by the disposition of cloud providers to keep details of suppliers, data location, architecture, and security of infrastructure confidential from the cloud customers. As such, the shallowness of transparency amongst cloud providers makes it difficult for customers to assess the risk of cloud adoption. In this study, we report on our research into finding out how much customers know about their supply chain. We evaluate the transparency of cloud providers based on their published information and determine the resultant risk of limited visibility of the supply chain. In the course of the research, we identified eight transparency features, which, at a minimum, cloud providers should make available to their current or prospective customers, which we argue had no adverse impact on the competitiveness or profitability of the provider. The study concludes that ultimately, cloud supply chain transparency remains a customer-driven process."
61691fdb723c9c71e54bf8bf85e6a846c377b4f2,
70dc8df52407f9266a8cffd17132031bcfba62d9,"Importance: Patient safety improvement interventions usually address either work systems or team culture. We do not know which is more effective, or whether combining approaches is beneficial. Objective: To compare improvement in surgical team performance after interventions addressing teamwork culture, work systems, or both. Design: Suite of 5 identical controlled before–after intervention studies, with preplanned analysis of pooled data for indirect comparisons of strategies. Setting: Operating theatres in 5 UK hospitals performing elective orthopedic, plastic, or vascular surgery Participants: All operating theatres staff, including surgeons, nurses, anaesthetists, and others Interventions: 4-month safety improvement interventions, using teamwork training (TT), systems redesign and standardization (SOP), Lean quality improvement, SOP + TT combination, or Lean + TT combination. Main Outcomes and Measures: Team technical and nontechnical performance and World Health Organization (WHO) checklist compliance, measured for 3 months before and after intervention using validated scales. Pooled data analysis of before—after change in active and control groups, comparing combined versus single and systems versus teamwork interventions, using 2-way ANOVA. Results: We studied 453 operations, (255 intervention, 198 control). TT improved nontechnical skills and WHO compliance (P < 0.001), but not technical performance; systems interventions (Lean & SOP, 2 & 3) improved nontechnical skills and technical performance (P < 0.001) but improved WHO compliance less. Combined interventions (4 & 5) improved all performance measures except WHO time-out attempts, whereas single approaches (1 & 2 & 3) improved WHO compliance less (P < 0.001) and failed to improve technical performance. Conclusions & Relevance: Safety interventions combining teamwork training and systems rationalization are more effective than those adopting either approach alone. This has important implications for safety improvement strategies in hospitals."
8e17f547ad1f84e790025dc0fe47b863ea5d7e1b,"AIMS AND OBJECTIVES
This study designed and evaluated the use of a specific implementation strategy to deliver a nursing staff-led Intentional Rounding intervention to reduce inpatient falls.


BACKGROUND
Patient falls are a common cause of harm during hospital treatment. Intentional Rounding has been proposed as a potential strategy for prevention, but has not received much objective evaluation. Previous work has suggested that logical interventions to improve patient care require an integrated implementation strategy, using teamwork training and systems improvement training, to instigate positive change and improvement.


METHODS
Customised Intentional Rounding was implemented and evaluated as part of a staff-led quality improvement intervention to reduce falls on a neuroscience ward. Intentional Rounding was instigated using a prespecified implementation strategy, which comprised of: (1) engagement and communication activities, (2) teamwork and systems improvement training, (3) support and coaching and (4) iterative Plan-Do-Check-Act cycles. Process (compliance with hourly visiting to patients by staff) and outcome (incidence of falls) measures were recorded pre- and postintervention. Falls measured on the active ward were compared with incidence of falls in 50 wards across the rest of the same Trust.


RESULTS
There was a 50% reduction in patient falls on the active ward vs. a minimal increase across the rest of the Trust (3·48%). Customised Intentional Rounding, designed by staff specifically for the context, appeared to be effective in reducing patient falls.


CONCLUSIONS
Improvement programmes based on integrating teamwork training and staff-led systems redesign, together with a preplanned implementation strategy, can deliver effective change and improvement.


RELEVANCE TO CLINICAL PRACTICE
This study demonstrates, through the implementation of a specific strategy, an effective improvement intervention to reduce patient falls. It provides insight into the effective design and practical implementation of integrated improvement programmes to reduce risk to patients at the frontline."
d7d1e866e5a97ffff8d5b867d1ad0104d7cea862,
0f7c6b3ddbd3d504baa3a23ee1e30b77e771b920,"Objectives To examine the effectiveness of a “systems” approach using Lean methodology to improve surgical care, as part of a programme of studies investigating possible synergy between improvement approaches. Setting A controlled before-after study using the orthopaedic trauma theatre of a UK Trust hospital as the active site and an elective orthopaedic theatre in the same Trust as control. Participants All staff involved in surgical procedures in both theatres. Interventions A one-day “lean” training course delivered by an experienced specialist team was followed by support and assistance in developing a 6 month improvement project. Clinical staff selected the subjects for improvement and designed the improvements. Outcome Measures We compared technical and non-technical team performance in theatre using WHO checklist compliance evaluation, “glitch count” and Oxford NOTECHS II in a sample of directly observed operations, and patient outcome (length of stay, complications and readmissions) for all patients. We collected observational data for 3 months and clinical data for 6 months before and after the intervention period. We compared changes in measures using 2-way analysis of variance. Results We studied 576 cases before and 465 after intervention, observing the operation in 38 and 41 cases respectively. We found no significant changes in team performance or patient outcome measures. The intervention theatre staff focused their efforts on improving first patient arrival time, which improved by 20 minutes after intervention. Conclusions This version of “lean” system improvement did not improve measured safety processes or outcomes. The study highlighted an important tension between promoting staff ownership and providing direction, which needs to be managed in “lean” projects. Space and time for staff to conduct improvement activities are important for success."
1586e2b1de2d91d501ea014cb52df5bd44e8035d,
23205015f59f3e13c54e168859d78e7b8e0c9604,
3f1c381224bf74363d92016a6f2a581d06a8a2bf,
89631afed26f4f75ab761858c8d40a491b26a13b,"Objective: To analyze the challenges encountered during surgical quality improvement interventions, and explain the relative success of different intervention strategies. Summary Background Data: Understanding why and how interventions work is vital for developing improvement science. The S3 Program of studies tested whether combining interventions addressing culture and system was more likely to result in improvement than either approach alone. Quantitative results supported this theory. This qualitative study investigates why this happened, what aspects of the interventions and their implementation most affected improvement, and the implications for similar programs. Methods: Semistructured interviews were conducted with hospital staff (23) and research team members (11) involved in S3 studies. Analysis was based on the constant comparative method, with coding conducted concurrently with data collection. Themes were identified and developed in relation to the program theory behind S3. Results: The superior performance of combined intervention over single intervention arms appeared related to greater awareness and ability to act, supporting the S3 hypothesis. However, we also noted unforeseen differences in implementation that seemed to amplify this difference. The greater ambition and more sophisticated approach in combined intervention arms resulted in requests for more intensive expert support, which seemed crucial in their success. The contextual challenges encountered have potential implications for the replicability and sustainability of the approach. Conclusions: Our findings support the S3 hypothesis, triangulating with quantitative results and providing an explanatory account of the causal relationship between interventions and outcomes. They also highlight the importance of implementation strategies, and of factors outside the control of program designers."
9b501fbe8b04c08e7c203c8934c88ec164191df8,
a40393f293295e83e4c037e9a886e2ed1753f41e,
b9374679b8a4ad59aa5ee6615a7197e35e4b388c,
bc3bba78a84627c9da31529928fbaa70fd3be4df,
bcf103fabe0b502853268d7673ece3126f95baeb,"Background High rates of iatrogenic harm have been confirmed in observational studies of surgery. Most interventions designed to reduce this have been targeted at either workplace culture or operational systems. We hypothesised that an integrated intervention addressing both culture and system might be more effective than either approach alone. Objective To evaluate interventions designed to improve surgical team performance by impacting culture or systems in isolation or combination. Design Five controlled intervention experiments, addressing system, culture or both, were performed in operating theatres. A final whole-system intervention study integrated approaches that showed benefit in these experiments. The five linked studies were subjected to a pre-planned pooled analysis to identify the effects of interventions, combinations and confounders. A qualitative interview study provided explanatory data on the mechanisms of intervention success and failure. An economic analysis was conducted. Setting Operating theatres in five hospitals, performing orthopaedic, trauma, vascular and plastic surgery were used for the linked studies. The final study occurred in a tertiary referral neurosurgery unit. Participants The main study subjects were clinical staff. Patient outcomes, both clinical and patient reported, were collected as secondary outcome measures. Interventions The interventions tested were (1) teamwork training (TT) based on the aviation crew resource management model, (2) the development of a set of standard operating procedures (SOPs), (3) a safety improvement programme based on lean principles, (4) TT plus SOPs and (5) TT plus lean. The final intervention used elements of all three strategies. Main outcome measures Primary outcomes were team non-technical skills [as measured by the Oxford Non-Technical Skills (NOTECHS) II scale score] and team technical performance (via the ‘glitch count’). Secondary outcomes were compliance with the World Health Organization (WHO)’s checklist procedures, patient length of stay, readmissions, 30-day mortality, complications and patient-reported outcome measures [as measured by the European Quality of Life-5 Dimensions (EQ-5D)]. A qualitative interview study provided explanatory data on the mechanisms of intervention success and failure. An economic analysis was conducted. Data sources Direct observation of whole operations, clinical records, hospital information systems and EQ-5D questionnaires. The qualitative study used semistructured interviews. Statistical methods Individual studies were analysed using two-way analysis of variance, and an overall individual patient pooled analysis was performed. Methods validation studies and other analyses used chi-squared test, correlation and regression methods as appropriate. Results We studied 453 operations. The results of single interventions were inconsistent. TT alone improved non-technical skills and WHO compliance (p Conclusions A combination of TT plus systems improvement training appears more effective in improving team performance than either approach alone. An implementation strategy based on an understanding of the barriers to change in hospitals is important for success. Future work More work is required to understand and measure barriers to safety improvement. Implementation strategies need to be tested empirically. Methods for delivering integrated interventions on a larger scale need development. A cluster randomised trial of the integrated-systems/culture-improvement approach is warranted. Funding The National Institute for Health Research Programme Grants for Applied Research programme."
d8d7684919b03bce1e1baa5eda3cad0ce55adb9d,
e6f4e9350d79e8fdf67ca7cb251605a047e3e2fb,
0f9db01b35479f7393af7bdb8e9879eb4f83e4f1,
20cb9e03e480c189d2b15fec890b0d948d5d396d,"Background To investigate the effectiveness of combining teamwork training and lean process improvement, two distinct approaches to improving surgical safety. We conducted a controlled interrupted time series study in a specialist UK Orthopaedic hospital incorporating a plastic surgery team (which received the intervention) and an Orthopaedic theatre team acting as a control. Study Design We used a 3 month intervention with 3 months data collection period before and after it. A combined teamwork training and lean process improvement intervention was delivered by an experienced specialist team. Before and after the intervention we evaluated team non-technical skills using NOTECHS II, technical performance using the glitch rate and WHO checklist compliance using a simple 3 point scale. We recorded complication rate, readmission rate and length of hospital stay data for 6 months before and after the intervention. Results In the active group, but not the control group, full compliance with WHO Time Out (T/O) increased from 14 to 71% (p = 0.032), Sign Out attempt rate (S/O) increased from 0% to 50% (p<0.001) and Oxford NOTECHS II scores increased after the intervention (P = 0.058). Glitch rate decreased in the active group and increased in the control group (p = 0.001). Complications and length of stay appeared to rise in the control group and fall in the active group. Conclusions Combining teamwork training and systems improvement enhanced both technical and non-technical operating team process measures, and were associated with a trend to better safety outcome measures in a controlled study comparison. We suggest that approaches which address both system and culture dimensions of safety may prove valuable in reducing risks to patients."
5aae1f1b3f53f377f20e564eff38969e8e3b7ef0,
7531c3c1234151fb02e69ffafc5a721f8add7e6e,
775f5bd350a8679ecde593436d569d88db63ecde,
8a47a92c071c17671b74b9682ae2b14a5c0d11db,"Objectives To evaluate the effectiveness of aviation-style teamwork training in improving operating theatre team performance and clinical outcomes. Setting 3 operating theatres in a UK district general hospital, 1 acting as a control group and the other 2 as the intervention group. Participants 72 operations (37 intervention, 35 control) were observed in full by 2 trained observers during two 3-month observation periods, before and after the intervention period. Interventions A 1-day teamwork training course for all staff, followed by 6 weeks of weekly in-service coaching to embed learning. Primary and secondary outcome measures We measured team non-technical skills using Oxford NOTECHS II, (evaluating the whole team and the surgical, anaesthetic and nursing subteams, and evaluated technical performance using the Glitch count. We evaluated compliance with the WHO checklist by recording whether time-out (T/O) and sign-out (S/O) were attempted, and whether T/O was fully complied with. We recorded complications, re-admissions and duration of hospital stay using hospital administrative data. We compared the before–after change in the intervention and control groups using 2-way analysis of variance (ANOVA) and regression modelling. Results Mean NOTECHS II score increased significantly from 71.6 to 75.4 in the active group but remained static in the control group (p=0.047). Among staff subgroups, the nursing score increased significantly (p=0.006), but the anaesthetic and surgical scores did not. The attempt rate for WHO T/O procedures increased significantly in both active and control groups, but full compliance with T/O improved only in the active group (p=0.003). Mean glitch rate was unchanged in the control group but increased significantly (7.2–10.2/h, p=0.002) in the active group. Conclusions Teamwork training was associated with improved non-technical skills in theatre teams but also with a rise in operative glitches."
8ca4156d3fdf2597c8e81c40e3e4299db7e406ca,"Purpose – This conceptual paper aims to examine modern slavery in the supply chain, showing how the issue challenges conventional thinking and practice in corporate social responsibility (CSR). Design/methodology/approach – The paper considers the differences between modern slavery and other concerns within CSR. It examines legal attempts to encourage supply chain transparency and the use of corporate CSR methods. An example of forced labour in UK agriculture is used to develop a critique of these approaches. The paper examines the challenges facing research in this important area. Findings – The paper shows that the distinctive characteristics of modern slavery may make conventional supply chain CSR practices relatively ineffective. A holistic perspective may be needed in future research. Research limitations/implications – Researchers need to focus less on the espoused policies of corporations, and more on the enacted practice. Social implications – Modern slavery is universally accepted as a shameful b..."
98a1f9c96937417df36456162b03482280decbae,"There is substantial evidence that the supply chain perspective is crucial for understanding the causes of and the solutions to modern slavery. During the passage of the UK Modern Slavery Act, a consultation was arranged to explore issues relating to the operationalisation of sections of the Act relating to transparency in supply chains. This document is a response to that consultation."
b82bc860bd9ba9968a80b7259a63c1d52c63e907,
be1815cc757c29f9def0ff2ac2093fe0de5c0d62,"This is a prepublication version of the paper published in Supply Chain Management: An International Journal, Vol. 20 Iss 6 pp. 697 – 707. An earlier version of this paper was presented at the EurOMA Sustainable Operations and Supply Chain Forum, Barcelona, Spain, 23-24 March 2015. The author would like to acknowledge the contribution of Laurence Cranmer to this work. MODERN SLAVERY AND THE SUPPLY CHAIN:"
e51b615c1c5fa644fe54a7d7d4c7e6dfad46d773,"Will Hutton’s (2015) ""How Good We Can Be"" is reviewed, with comparisons made to other recent works. The potential efficacy of Hutton’s recommendations concerning changes to the UK Companies Act are discussed, particularly in the light of the effectiveness of non-executive directors in regulating large companies."
fbe3bca0bf4135224a88a337907324c0f7c8a8b9,
3788a2225c81e59996ba950717d1b587bc50167f,"(2014) Oxford NOTECHS II : a modified theatre team non-technical skills scoring system. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work of researchers of the University of Warwick available open access under the following conditions. This article is made available under the Creative Commons Attribution 4.0 International license (CC BY 4.0) and may be reused according to the conditions of the license. For more details see: A note on versions: The version presented in WRAP is the published version, or, version of record, and may be cited as it appears here. Abstract Background: We previously developed and validated the Oxford NOTECHS rating system for evaluating the non-technical skills of an entire operating theatre team. Experience with the scale identified the need for greater discrimination between levels of performance within the normal range. We report here the development of a modified scale (Oxford NOTECHS II) to facilitate this. The new measure uses an eight-point instead of a four point scale to measure each dimension of non-technical skills, and begins with a default rating of 6 for each element. We evaluated this new scale in 297 operations at five NHS sites in four surgical specialities. Measures of theatre process reliability (glitch count) and compliance with the WHO surgical safety checklist were scored contemporaneously, and relationships with NOTECHS II scores explored."
536086504737b27199fa70d4a707334ae2331e05,"Background Teamwork training and system standardisation have both been proposed to reduce error and harm in surgery. Since the approaches differ markedly, there is potential for synergy between them. Methods Design: Controlled interrupted time series with a 3 month intervention and observation phases before and after. Setting: Operating theatres conducting elective orthopaedic surgery in a single hospital system (UK Hospital Trust). Intervention: Teamwork training based on crew resource management plus training and follow-up support in developing standardised operating procedures. Focus of subsequent standardisation efforts decided by theatre staff. Measures: Paired observers watched whole procedures together. We assessed non-technical skills using NOTECHS II, technical performance using glitch rate and compliance with WHO checklist using a simple quality tool. We measured complication and readmission rates and hospital stay using hospital administrative records. Before/after change was compared in the active and control groups using two-way ANOVA and regression models. Results 1121 patients were operated on before and 1100 after intervention. 44 operations were observed before and 50 afterwards. Non-technical skills (p=0.002) and WHO compliance (p<0.001) improved significantly after the intervention in the active versus the control group. Glitch count improved in both groups and there was no significant effect on clinical outcomes. Discussion Combined training in teamwork and system improvement causes marked improvements in team behaviour and WHO performance, but not technical performance or outcome. These findings are consistent with the synergistic hypothesis, but larger controlled studies with a strong implementation strategy are required to test potential outcome effects."
8e1e2fb000af5e8b6c22a0243d6dfc228e2a6fc8,"Background We previously developed and validated the Oxford NOTECHS rating system for evaluating the non-technical skills of an entire operating theatre team. Experience with the scale identified the need for greater discrimination between levels of performance within the normal range. We report here the development of a modified scale (Oxford NOTECHS II) to facilitate this. The new measure uses an eight-point instead of a four point scale to measure each dimension of non-technical skills, and begins with a default rating of 6 for each element. We evaluated this new scale in 297 operations at five NHS sites in four surgical specialities. Measures of theatre process reliability (glitch count) and compliance with the WHO surgical safety checklist were scored contemporaneously, and relationships with NOTECHS II scores explored. Results Mean team Oxford NOTECHS II scores was 73.39 (range 37–92). The means for surgical, anaesthetic and nursing sub-teams were 24.61 (IQR 23, 27); 24.22 (IQR 23, 26) and 24.55 (IQR 23, 26). Oxford NOTECHS II showed good inter-rater reliability between human factors and clinical observers in each of the four domains. Teams with high WHO compliance had higher mean Oxford NOTECHS II scores (74.5) than those with low compliance (71.1) (p = 0.010). We observed only a weak correlation between Oxford NOTECHS II scores and glitch count; r = −0.26 (95% CI −0.36 to −0.15). Oxford NOTECHS II scores did not vary significantly between 5 different hospital sites, but a significant difference was seen between specialities (p = 0.001). Conclusions Oxford NOTECHS II provides good discrimination between teams while retaining reliability and correlation with other measures of teamwork performance, and is not confounded by technical performance. It is therefore suitable for combined use with a technical performance scale to provide a global description of operating theatre team performance."
92cf9c034f051ab757e0e3a1387835703537f96e,"Increasing awareness of the intrinsically complex nature of supply networks has brought the field of supply chain management into the domain of network science. However, due to the difficulties of acquiring large-scale and consistent empirical data sets, a more complete picture of a real-world supply network has remained remarkably elusive. In this paper, we present novel data that characterize the Toyota supply network, and identify key structural features using measures from social network analysis and the more recent field of network science. We show that the network structure for the Toyota supply network departs widely from the simplified models on which much previous work is based. Our analysis reveals the heterogeneous composition of the network and identifies key firms. Further analysis reveals the existence of constituent sub-networks, and we show that their structures reflect various factors, such as product categorization, geographical closeness and business alignment. Mapping the topology, geography, and distribution of productive capabilities for this supply network provides a critical first step for developing a more empirically-grounded theory of distributed production."
bfcfc7d27d04352cd80710b6e60c00d37a8b68a3,"Background Standard operating procedures (SOPs) should improve safety in the operating theatre, but controlled studies evaluating the effect of staff-led implementation are needed. Methods In a controlled interrupted time series, we evaluated three team process measures (compliance with WHO surgical safety checklist, non-technical skills and technical performance) and three clinical outcome measures (length of hospital stay, complications and readmissions) before and after a 3-month staff-led development of SOPs. Process measures were evaluated by direct observation, using Oxford Non-Technical Skills II for non-technical skills and the ‘glitch count’ for technical performance. All staff in two orthopaedic operating theatres were trained in the principles of SOPs and then assisted to develop standardised procedures. Staff in a control operating theatre underwent the same observations but received no training. The change in difference between active and control groups was compared before and after the intervention using repeated measures analysis of variance. Results We observed 50 operations before and 55 after the intervention and analysed clinical data on 1022 and 861 operations, respectively. The staff chose to structure their efforts around revising the ‘whiteboard’ which documented and prompted tasks, rather than directly addressing specific task problems. Although staff preferred and sustained the new system, we found no significant differences in process or outcome measures before/after intervention in the active versus the control group. There was a secular trend towards worse outcomes in the postintervention period, seen in both active and control theatres. Conclusions SOPs when developed and introduced by frontline staff do not necessarily improve operative processes or outcomes. The inherent tension in improvement work between giving staff ownership of improvement and maintaining control of direction needs to be managed, to ensure staff are engaged but invest energy in appropriate change."
cc78aedf7504a0079a995f3143db15d4a405f490,
d6c1ac181bc42a502df3ddb431661075c0567cc7,"Abstract This paper explores an important but unexplored theme in the development of 'total quality' relationships in the supply chain: why suppliers may exhibit resistance to quality initiatives, and why what seems to customers as cooperation can appear as unwelcome interference or even exploitation to suppliers. This is a question of great relevance to all those seeking to understand current business practice, and also to those seeking to bring about practical improvements in supply chain quality. Too much of the supply chain and quality literature assumes the issue of supplier's compliance with customer initiatives is unproblematic (unable to understand this sentence); experience suggests it is one of the major obstacles in developing the quality-oriented, integrated supply chain."
12d9f1549654777c836d8635ed285b9286143042,"The World Health Organization (WHO) Surgical Safety Checklist is reported to reduce surgical morbidity and mortality, and is mandatory in the UK National Health Service. Hospital audit data show high compliance rates, but direct observation suggests that actual performance may be suboptimal."
4c5baaf238e50d49132793e0bed0126f886fea14,"Objectives To develop a sensitive, reliable tool for enumerating and evaluating technical process imperfections during surgical operations. Design Prospective cohort study with direct observation. Setting Operating theatres on five sites in three National Health Service Trusts. Participants Staff taking part in elective and emergency surgical procedures in orthopaedics, trauma, vascular and plastic surgery; including anaesthetists, surgeons, nurses and operating department practitioners. Outcome measures Reliability and validity of the glitch count method; frequency, type, temporal pattern and rate of glitches in relation to site and surgical specialty. Results The glitch count has construct and face validity, and category agreement between observers is good (κ=0.7). Redundancy between pairs of observers significantly improves the sensitivity over a single observation. In total, 429 operations were observed and 5742 glitches were recorded (mean 14 per operation, range 0–83). Specialty-specific glitch rates varied from 6.9 to 8.3/h of operating (ns). The distribution of glitch categories was strikingly similar across specialties, with distractions the commonest type in all cases. The difference in glitch rate between specialty teams operating at different sites was larger than that between specialties (range 6.3–10.5/h, p<0.001). Forty per cent of glitches occurred in the first quarter of an operation, and only 10% occurred in the final quarter. Conclusions The glitch method allows collection of a rich dataset suitable for analysing the changes following interventions to improve process safety, and appears reliable and sensitive. Glitches occur more frequently in the early stages of an operation. Hospital environment, culture and work systems may influence the operative process more strongly than the specialty."
6463fa6eaa93157f4bd44c21fb5d4e722d5c87de,
67ab88c54ddb2c0ab2bed61b13d698634760a01e,"We read with interest Captain Harden's article on the benefits of teamwork training and checklists in the plastic surgery clinic.1 Despite the author's confidence in the benefits of these strategies, there remains doubt in the medical community as to their effect on both process and outcome. In previous work, we found some benefit with teamwork training …"
7cdb6035989a5e167cd03b68db09963882080b54,
9029952310f60ca8e1cf72a8dd2d5526d5d7d6e9,
b382968977eb57af175eb4a3ecd56b974cd3fc38,
e722753a8e9c5c8ef1f29ca9ecfb2a6aa448a3df,"Objective To assess the effectiveness of a multifaceted intervention based on industrial process improvement to identify and sustainably correct deficiencies in thromboprophylaxis delivery. Summary background data Deep vein thrombosis and pulmonary embolism are major causes of morbidity and mortality in surgical patients, but effective prophylactic treatments are available. Ensuring reliable delivery of the intended thromboprophylaxis is, however, a long-standing problem. Methods Delivery of thromboprophylactic treatment on an emergency general surgery admissions ward was targeted during a multidisciplinary intervention to improve process reliability using industrial quality improvement approaches. Delivery was audited against guidelines before and after 3- month intervention. Clinical outcome was evaluated by reviewing all radiological investigations for suspected Deep Vein Thrombosis (DVT) or Pulmonary Embolism (PE) from patients admitted to the unit in the 1 year immediately before and that immediately after intervention. Results Delivery of thromboprophylaxis according to guidelines was improved from 35% before to 87% 3 months after intervention (χ2=87.412, p<0.0001) and sustained at 86% 10 months after intervention. Radiologically identified thromboembolic events occurring up to 60 days after admission in patients admitted for over 48 h fell from 23/3075 (0.75%) before to 9/3080 (0.29%) after intervention (HR 0.39, CI 0.29 to 0.53, χ2=6.18, p=0.01292). The risk of thromboembolism in the two groups diverged during follow-up to 60 days, before converging again. Conclusions A quality improvement process resulted in major sustainable improvements in the delivery of thromboprophylaxis associated with a 61% reduction in radiologically detected clinical episodes of thromboembolism 2 months after admission. Further study of this approach to improving care quality is warranted."
83219631d156ba4c9d05977a1d6a4c3d541c0dcd,
fc655ab18ee43f34418ea098f6a877b5e7c7246e,"In this short report we explore some key aspects of supply chain risk. Drawing on interviews with seven major UK-based companies in the construction, property, utilities and rail sectors, we review companies’ approaches to ‘REEL’ supply chain risks: reputational, ethical, environmental and legal. These risks arise where the actions of a supplier may damage its customer – even if the customer is separated from the supplier by several links in the chain.Examples of this kind of risk include the reputational damage inflicted on a firm when one of its suppliers is found to be illegally or unduly damaging the environment, or hiring illegal labour, or engaging in some other breach of acceptable business practice. Fairly or unfairly, large companies may be held to account as if they were the culpable party, and are seen as sharing at least some of the responsibility for the supplier’s behaviourWhat firms should actually do in the face of these challenges remains unclear. By exploring these issues we suggest a series of insights that may help organizations navigate this complex terrain."
3d45faf983e580223db781a8dba3df30848df70f,
704051ecd0c842c1f75eb99ebbbcf1790a7643e9,
88aa20d065715969a3ced009129d99b974dda4c3,"We assemble a large‐scale empirical dataset that allows us to examine the local and global topology of relationships between firms in the Toyota supply network. On this basis we propose novel measures that allow us to characterise the resilience of the entire supply network. Our findings show that simple linear supply chain models are inadequate, because they neglect important lateral dependencies between suppliers. Hence, we argue that it is necessary to describe and model the supply chain as a complex network. We observe that the degree distribution for this network scales exponentially, so that disruptions at randomly chosen suppliers have little impact but vulnerability to disruptions at highly connected suppliers is significant. These potential vulnerabilities are mitigated by the network’s ‘small‐world’ structure, where the average path to any given supplier via other suppliers is very low, and the number of relations between suppliers that produce the same product types is high, especially within the Kyoho‐kai supplier association. Membership of the tightly‐knit Kyoho‐kai supplier association is positively correlated with an increase in the number of connections that a supplier has, leading to a segmentation that favours highly connected hubs. The network also exhibits a high degree of product type redundancy, where multiple suppliers offer similar product types. When we examine product diversity along the chain we find that upper tiers show higher diversification given their vulnerability to changing customer demands. Our analysis of a unique, large‐scale empirical dataset aims to move the field of supply chain management beyond stylised facts, and to demonstrate how methods from interdisciplinary work on complex networks can contribute novel insights."
8a4f8ce60ac29f712a8d6d18f1e36e0625d051ca,"We assemble a large‐scale empirical dataset that allows us to examine the local and global topology of relationships between firms in the Toyota supply network. On this basis we propose novel measures that allow us to characterise the resilience of the entire supply network. Our findings show that simple linear supply chain models are inadequate, because they neglect important lateral dependencies between suppliers. Hence, we argue that it is necessary to describe and model the supply chain as a complex network. We observe that the degree distribution for this network scales exponentially, so that disruptions at randomly chosen suppliers have little impact but vulnerability to disruptions at highly connected suppliers is significant. These potential vulnerabilities are mitigated by the network’s ‘small‐world’ structure, where the average path to any given supplier via other suppliers is very low, and the number of relations between suppliers that produce the same product types is high, especially within the Kyoho‐kai supplier association. Membership of the tightly‐knit Kyoho‐kai supplier association is positively correlated with an increase in the number of connections that a supplier has, leading to a segmentation that favours highly connected hubs. The network also exhibits a high degree of product type redundancy, where multiple suppliers offer similar product types. When we examine product diversity along the chain we find that upper tiers show higher diversification given their vulnerability to changing customer demands. Our analysis of a unique, large‐scale empirical dataset aims to move the field of supply chain management beyond stylised facts, and to demonstrate how methods from interdisciplinary work on complex networks can contribute novel insights."
8a6056aec6917880ecb32a4023c9ca5496c08601,
9689a1d5e1ef7bbcee40c8e62ff62db1f1ce6bca,
a025096ac5af4766c84f7143aa7b76c237c4b464,"We assemble a large‐scale empirical dataset that allows us to examine the local and global topology of relationships between firms in the Toyota supply network. On this basis we propose novel measures that allow us to characterise the resilience of the entire supply network. Our findings show that simple linear supply chain models are inadequate, because they neglect important lateral dependencies between suppliers. Hence, we argue that it is necessary to describe and model the supply chain as a complex network. We observe that the degree distribution for this network scales exponentially, so that disruptions at randomly chosen suppliers have little impact but vulnerability to disruptions at highly connected suppliers is significant. These potential vulnerabilities are mitigated by the network’s ‘small‐world’ structure, where the average path to any given supplier via other suppliers is very low, and the number of relations between suppliers that produce the same product types is high, especially within the Kyoho‐kai supplier association. Membership of the tightly‐knit Kyoho‐kai supplier association is positively correlated with an increase in the number of connections that a supplier has, leading to a segmentation that favours highly connected hubs. The network also exhibits a high degree of product type redundancy, where multiple suppliers offer similar product types. When we examine product diversity along the chain we find that upper tiers show higher diversification given their vulnerability to changing customer demands. Our analysis of a unique, large‐scale empirical dataset aims to move the field of supply chain management beyond stylised facts, and to demonstrate how methods from interdisciplinary work on complex networks can contribute novel insights."
b7bd726c946f26239d671fa7f8e3070eccb9ef58,
c120affd3740c29a47c819edf6322fafed9aa97d,
c30113aaf4e328f82c3006290c538d579a35ab75,"In this article we examine a key business question: how much should organizations know about their extended supply base, and what should they do with this information? This question has become of urgent practical significance for firms across industry sectors, as they square up to the complex inter-connectedness of the modern world. We present a series of challenges that firms can use to frame an initial review of their own practices."
0036c71ce3964f4dfff5354609d7c85b40c786df,"The article discusses technology for supply chain management, focusing on tools for making information on products transparent to both managers and consumers. Among the technologies examined are radio frequency identification tags embedded in products, online databases available for consumers and Webcasting of supplier operations. The complexity of managing this information is acknowledged. The marketing advantages of presenting consumers with information on supply chain practices and products are considered. The use of such information in supply chain management is discussed."
6461297a97d96c89b347c80c5da5534df9082be1,
767272866ef865bb584002ab11d2e731e0273d3f,"The default position on the ward now is that patients should be wearing TEDS unless there is a specific contraindication. Even without a doctors signature, the nursing staff are happy to use the shift checking system to ensure their patients receive TEDS. Redundancy has been built into the system through several independent check points. None add significantly to the workload of staff. FIRST RE-AUDIT N=24 65% Compliance"
8ce29ff185f630b0681d47be47fb2ebd07626c83,"PROBLEM
Emergency surgical patients are at high risk for harm because of errors in care. Quality improvement methods that involve process redesign, such as “Lean,” appear to improve service reliability and efficiency in healthcare.


DESIGN
Interrupted time series.


SETTING
The emergency general surgery ward of a university hospital in the United Kingdom.


KEY MEASURES FOR IMPROVEMENT
Seven safety relevant care processes.


STRATEGY FOR CHANGE
A Lean intervention targeting five of the seven care processes relevant to patient safety.


EFFECTS OF CHANGE
969 patients were admitted during the four month study period before the introduction of the Lean intervention (May to August 2007), and 1114 were admitted during the four month period after completion of the intervention (May to August 2008). Compliance with the five process measures targeted for Lean intervention (but not the two that were not) improved significantly (relative improvement 28% to 149%; P<0.007). Excellent compliance continued at least 10 months after active intervention ceased. The proportion of patients requiring transfer to other wards fell from 27% to 20% (P<0.000025). Rates of adverse events and potential adverse events were unchanged, except for a significant reduction in new safety events after transfer to other wards (P<0.028). Most adverse events and potential adverse events were owing to delays in investigation and treatment caused by factors outside the ward being evaluated.


LESSONS LEARNT
Lean can substantially and simultaneously improve compliance with a bundle of safety related processes. Given the interconnected nature of hospital care, this strategy might not translate into improvements in safety outcomes unless a system-wide approach is adopted to remove barriers to change."
daff2149b62d25276a7670ad467c79d8a91e302c,
182b3abe76baf0b0aaa85255ba5b4c57cb91f842,"Objective:To evaluate patient safety in an emergency surgical unit using process and outcome measures in parallel. Background:Patient harm from errors in care is common in modern surgical practice. Measurement of the problem is essential to any solution, but current methods of evaluating patient harm are either impractical or inadequate. We have therefore analyzed compliance with safety-relevant care processes, with the aim of developing a process-based system for evaluating ward safety. Methods:Adverse events (AE), potential adverse events (PAE), and 7 safety-relevant processes were measured on a 38-bed surgical emergency unit over a 16-week period. AE, PAE, and process measures were studied by prospective direct observation in large convenience samples, using objective measures. Possible influences on AE and PAE risk were analyzed. Results:Compliance with the 7 processes studied ranged from 23% to 89%. The AE and PAE rates were 11.9% and 13.8% in a 63% sample of admissions (n = 607). Length of stay was significantly associated with both AE (P < 0.001) and PAE (P < 0.001). Having an operation was also associated with AE (P = 0.001) but not with PAE. No other factors appeared to influence AE/PAE rates. Delays were the commonest causes of both AE and PAE. Conclusions:Compliance with individual care processes on a ward with average levels of patient harm is poor. Length of hospital stay increases the risk of both AE and PAE, suggesting a system defect. A bundle of care processes may be useful for monitoring safety improvement."
4ca65d36fe0ee874cab9d7468e08395b96928f9d,
71f14462b80195c64147ee5e0d6e60207888b323,
c9fafe46d11f983771f047f2882a493059585f6d,"Patientharmfromerrorsincareiscommoninmodernsurgicalpractice. Measurement of the problem is essential to any solution, but currentmethods of evaluating patient harm are either impractical or inadequate. Wehave therefore analyzed compliance with safety-relevant care processes, withthe aim of developing a process-based system for evaluating ward safety."
ff4f1c9fe3b5db9d1daa39744d364040b8c0eb77,
028f69d3a3879de3d735293a36968d8367eb4cea,
0a2689c84fb1a26091d330e0ad480367dfa739ac,
4bfaea024de12823316fe7feb1274ded96a8a15f,
4df15787134cb5d87940df0a2ea144c2c6578c16,
5e3dd4537e21fc8eebd8786e1fb11aa0edec8684,
80c1a019bab850d2d611adcfd9f78e791a629b41,"The idea of a supply chain is one of the most important concepts to emerge in management research and practice in recent years. In simple terms, a supply chain might be defined as a sequence of organizations - such as suppliers, wholesalers, retailers, distributors, and transport and storage facilities - that participate in the production of a particular product or service. Supply chain management (SCM) recognizes that businesses and other organizations cannot function successfully in isolation and is concerned with the direction and regulation of materials, information, and finances as they process along such a chain.The burgeoning academic and professional interest in SCM can be traced to a variety of causes including the pressures on organizations to outsource non-core activities; globalization; a growing appreciation of the dynamics of 'lean supply' Japanese sourcing practices; the growth of the use of IT for collaborative logistics planning; and, more recently, the frenzy around 'B2B' e-commerce. This has led to an unprecedented growth of practitioner-orientated publications and a similar development in academic activity. 
 
The sheer scale of the growth in SCM research output makes this collection especially timely and welcome.Furthermore, ideas at the heart of SCM have also become the concern of much academic work that goes on under labels other than business and management, and the collection will include insights and research from these different disciplinary perspectives. Edited by a leading SCM researcher at the University of Oxford's Said Business School, this collection brings together carefully selected key historical papers along with cutting-edge research. The organization of the four volumes - on operational integration, relational and functional integration, explanatory frameworks, and the ethics, environment and social impacts of SCM - emphasizes the important key themes and interconnections in the field.Together with the editor's newly written introductory essays, this organization will enable users to make sense of the wide range of approaches, theories, and concepts that have informed SCM thinking and practice to date. It is an essential collection destined to be valued as a vital research resource by all scholars and students of the subject."
8510accec23355eaa89fd19d5ac8b2293082dd15,
af01df446042a1093bc836b581ed01a786557b53,"This paper summarizes the events surrounding the dramatic product recalls of Mattel toys during 2007. Drawing on press reports and official documents, the paper explores the way in which issues relating supply chain integration, product inspection and quality management become part of political discourse. We observe that the public discourse surrounding these events can become disconnected from the operational reality, and lead to potentially dysfunctional regulatory responses."
b34fc66ef4e262a0aaf2b5f10a85cab29c2e65bb,
b802f85636e4d9b31d488ff4ea42ed8f90208223,"In 2007, Mattel, one of the world’s leading toy makers, suffered a series of unprecedented product recall disasters in which millions of items had to be withdrawn from sale. The story is interesting not just as a first-order 
phenomenon of quality management, logistics and marketing, but also in terms of the way in which it became an important media event, and the focus of extensive public discourse. In particular, the story became swept up in a wider political and economic debate about, among other things: the reliance of western (and particularly, American) economies on Chinese manufacturing; the effect of globalization on consumer safety; the scope of corporate social responsibility; and, the nature of quality control itself."
bc85000a4b0f6c2cb559db673852ede3449a97e4,
c72409c860029fd3f9fb68e08245646831b521de,
c8e7e49807bed8f9d9336837d24d2c3dec4a0753,
d8c97278e7aa262ad95db8bdf145b0add673ae4d,
e4776d262abc7cad7eb5d5361f6371fa132d3182,
f69b78aa0da63df5fb05ffcb674ab63ae382f8ed,
c2e2a9d3ceb003d994657c738f50ed10aacf2b54,
4464fc22d19ff31266edd6edf4ba65b4ea35fe45,
b0b933cab363393734021be46bf7f8baf1009787,
bc6f05fcabddacd79f93f0ab1fa686073451b4ce,
894e9f752d01bb1be92e4c0f4c99c2898c3383bc,
e26b1ce707c4157d8244c48ec839d74b29aee602,
14d7188a2f982f01f9a36547304fd4c27185a52b,
1681f46ef18e6d773245e36432adcd47f09ef39b,
95e264af799a04f89e19db7c1d17a5059f9420ba,
c3c22b9ad659c49250afd337405bdbb7f3e38e43,
da5f0cbab333037fc20ec1d675cbd20bade4c2dd,"The Supply Chain concept is one of the most important ideas to emerge in management research and practice in the last twenty five years. Organizations do not exist in isolation. Any organization, whether a large corporation, public body, or small business, which aims to meet the needs of its various customers and stakeholders will need resources in order to do this, and will acquire many of its materials, equipment, and supplies from other organizations. The performance of an organization is thus influenced to a greater or lesser degree by the actions of the organizations that make up the Supply Chain. There is no doubt that the emergence of Supply Chain Management has been a major development in management thinking and practice. It has become an established feature of management education, and a professional field with its own magazines and journals - a field with its own distinctive perspectives. However, many writers observe that it is a field characterized by imprecise terminology, sloppily applied metaphors, and conflated or confused concepts. The slightest skim of the many literatures that use the term reveals a wide range of interpretations, hundreds of different formulations, nuances, and taxonomies for the 'Supply Chain', and dozens of near synonyms. The purpose of this volume is to bring together insights from the leading researchers and thinkers on supply chain management to help move the field forward. It provides a survey of the key theoretical concepts which underpin the field, and presents critical evaluations of the underlying ideas and approaches. It will be an important resource for those active in researching in or applying the ideas of supply chain management, and for advanced students and their teachers. Contributors to this volume - John Bessant, Cranfield School of Management. Nigel Caldwell, Centre for Research in Strategic Purchasing and Supply, School of Management, University of Bath. Ani Calinescu, Oxford University Computing Laboratory. Martin Christopher, Cranfield School of Management, Cranfield University. Paul Cousins, Queen's School of Management and Economics, Queen's University, Belfast. Simon Croom, Warwick Business School, University of Warwick. Stephen Disney, Cardiff Business School, Cardiff University. Janet Efstathiou, Department of Engineering Science, University of Oxford. Mihalis Giannakis, Middlesex University Business School. Christine Harland, Centre for Research in Strategic Purchasing and Supply, School of Management, University of Bath. Luisa Huaccho Huatuco, Department of Engineering Science, University of Oxford. P. Fraser Johnson, Richard Ivey School of Business, University of Western Ontario. Robert D. Klassen, Richard Ivey School of Business, University of Western Ontario. Louise Knight, Centre for Research in Strategic Purchasing and Supply, School of Management, University of Bath. Richard Lamming, School of Management, University of Southampton. Mohamed Naim, Cardiff Business School, Cardiff University. Guido Nassimbeni, Dipartimento Di Ingegneria Elettrica Gestionale E Meccanica (DIEGM) via delle Scienze, University of Udine. Steve New, Said Business School, University of Oxford. Wendy Phillips, Centre for Research in Strategic Purchasing and Supply, School of Management, University of Bath. Nigel Slack, Warwick Business School, University of Warwick. Suja Sivadasan, RAND Europe. Denis Towill, Cardiff Business School, Cardiff University. Roy Westbrook, Said Business School, University of Oxford."
fe36d64bbb448d37db962732cff826588b3dff4f,"There has been a wealth of investigation into the use of online multiple-choice questions as a means of summative assessment, however the research into the use of formative MCQs by the same mode of delivery still remains patchy. Similarly, research and implementation has been largely concentrated within the Sciences and Medicine rather than the more discursive subjects within the Humanities and Social Sciences. The INQUIRE (Interactive Questions Reinforcing Education) Evaluation Project was jointly conducted by two groups at the University of Oxford—the Said Business School and the Academic Computing Development Team to evaluate the use of online MCQs as a mechanism to reinforce and extend student learning. This initial study used a small set of highly focused MCQ tests that were designed to complement an introductory series of first-year undergraduate management lectures. MCQ is a simple and well-established technology, and hence the emphasis was very much on situating the tests within the student experience. The paper will cover how the online MCQs are intended to fit into the Oxford Undergraduate study agenda, and how a simple evaluation was executed and planned to investigate their usage and impact. The chosen method of evaluation was to combine focus groups with automated online methods of tracking, and the paper discusses the findings of both of these. DOI: 10.1080/0968776042000259564"
52b878235b1ab582e9051ec1174cbe99e3c3f71a,"About the book: Major advances have been made recently in environmental social science but the context and importance of this research has also changed. Social and natural science studies of the environment have begun to interact more closely with each other and many analysts now agree that an understanding of environmental problems often depends on an understanding of the attitudes and behaviour of people and organisations. Moreover, policy and public debates have also shown that many assumptions that underpin arguments about sustainable development need to be reconsidered and re-framed."
62a4a73125ac87bf9d17325dab5b2a8ce69bdb6b,"This paper reports the study of kaizen as practised in a selection of Japanese companies. After discussing the general understanding of kaizen and proposing a clear definition, the paper describes the methodology of the study, and presents findings from the research, taking Nippon Steel Corporation (NSC) as a base model and comparing this with the data from other companies. The development of kaizen activity in NSC is presented together with a description of the current nature of kaizen, which is compared with other firms in the steel and automotive industries to assess uniformity. The paper concludes that kaizen evolves uniquely within each organisation, following changes to the organisation's business environment. Detailed implementations vary considerably between organisations, but all rely on kaizen to achieve targets as an integral element in the operations management system. This yields insights into kaizen's sustainability, and points to its vulnerability to external economic conditions."
9acd449666b2244dc2b61ed14d3aabba8fbe2f30,"Geographically‐dispersed, global service organisations must find cost‐effective ways of delivering consistent service quality while recognising local differences in service culture. For such organisations, the management of operations cannot be separated from issues of training, corporate culture, and organisational identity. This paper presents an analysis of the experience of Avis Europe, a leading car rental firm, in developing a multimedia system for training frontline staff. The case illustrates how multimedia technologies enable a new sort of organisational text that has implications for the way in which operations may be documented and enacted. In particular, multimedia allows texts that are complex enough to address issues of what Hochschild describes as “emotional labour” in service operations. These developments have great significance for both practice and research in operations management."
aa758fb443d7c41957fa32fd568e9d48b5b30768,
f15145d15e10be6205624eed789eb5748e949c9b,"The impact of the Internet on the supply chain is one of the most important areas of research for Operations Management. A major element of the so-called ‘dot.com’ boom was the emergence of new forms of intermediary organisations (called variously “B2B exchanges”, “hubs” and “marketplaces”) through which industrial buyers and sellers could interact. This paper presents a small selection of data from two aspects of a broader investigation into the nature of B2B e-commerce. Firstly, it presents data from the analysis of 302 B2B e-marketplaces, and focuses in particular on the patterns of birth and death, and on the epistemological problems of investigating the demography of B2B. This is complemented by a brief discussion of three major industry-based B2B initiatives (Covisint, Exostar and Transora)."
fd84f0028f19fdbda32eca4ef12bb34f5741539e,
d001f07fc9c6b619ae1ecb39857297239a94840d,"This paper examines differences and similarities between private and public sectors regarding green supply: the incorporation of environmental considerations into procurement and supply chain relationships. While there are considerable differences between the sectors, there are two key areas of similarity. Firstly, responses in both sectors are heavily influenced by organisational structure and patterns of decision-making and information flow. Secondly, the success of green supply initiatives appears to be heavily dependent on organisation’s ability to align activity with dominant corporate objectives."
4689abeaefc83cfea7748eabc6831f85306c7bc9,
5cd7c71a31c4e90b1966490236605a2f6fac1772,
5d418141d2888dc36729977980cd5b17af38eb9f,
8ed5925f223d92e17b5275897a65a943c9a33463,"Geographically-dispersed, global service organisations must find cost-effective ways of delivering consistent service quality while recognising local differences in service culture. For such organisations, the management of operations cannot be separated from issues of training, corporate culture, and organisational identity. This paper presents an analysis of the experience of Avis Europe, a leading car rental firm, in developing a multimedia system for training frontline staff. The case illustrates how multimedia technologies enable a new sort of organisational text that has implications for the way in which operations may be documented and enacted. In particular, multimedia allows texts that are complex enough to address issues of what Hochschild describes as “emotional labour” in service operations. These developments have great significance for both practice and research in operations management."
f3efb79ac602059ce4e03b06561bde1284e27557,
1eb5e38c67f3d9337414509dd7be5402b498fa54,
3164eb618c09492895097df4deb5a68358014655,"The use of consumer pressure in greening the economy has long been advocated by environmentalists. This article takes the view that the traditional image of the consumer as the primary agent of environmental change is inadequate. Efforts to green the economy require an understanding of corporations and public organizations as consumers as well as an understanding of individuals as consumers. The article sets out the arguments for treating all organizations as consumers and as a dominant but underemphasized force in greening the economy. It then considers organizational consumption in the context of supply chains, with respect to the issue of agency within the organization and with respect to the transmissions of market signals for innovation. The discussion makes clear the importance of considering the interorganizational context and ways in which this context both constrains and enables green purchasing initiatives. Reference is made to examples from a range of organizations."
33006f8e6183b8b26aa79fb4f8ccc747ded29544,"The use of consumer pressure in greening the economy has long been advocated by environmentalists. This article takes the view that the traditional image of the consumer as the primary agent of environmental change is inadequate. Efforts to green the economy require an understanding of corporations and public organizations as consumers as well as an understanding of individuals as consumers. The article sets out the arguments for treating all organizations as consumers and as a dominant but underemphasized force in greening the economy. It then considers organizational consumption in the context of supply chains, with respect to the issue of agency within the organization and with respect to the transmissions of market signals for innovation. The discussion makes clear the importance of considering the interorganizational context and ways in which this context both constrains and enables green purchasing initiatives. Reference is made to examples from a range of organizations."
480128c5ea51969f7dfbeeb81e527ef9266a5e75,
925a6c530813e2030931cd5aa4de141a472f805c,
a61152f0dc7d19719b740d481652062e1db42137,
f4fc208f22ea1ddf51f6619a8daf7c8e3a37eeed,
8794114206e4544f6f2103c5d983ee67cf46660a,"Based on research in the UK into the market for routine products used in health care, this paper explores the relationship between environmental controversy, innovation, and the competitive response of firms to demand from their customers for environmentally-friendly products. This leads to an understanding of the use of environmental issues in corporate purchasing, and the action of commercial pressure in supply chains as a mechanism of corporate greening. We then consider Baxter Healthcare, one of the main companies in the market we have examined, and the production and sale of plastic, PVC-using healthcare products, particularly 'IV bags'. The analysis of this material leads us to consider three issues: (1) the balance between the 'local' and the 'global' in firms' understanding of the market; (2) the problem of highly contested environmental claims (especially over the impact of products made from PVC), and (3) the influence of structure and practice within buying and selling organizations."
8ef2856b40a17e259363a072c5aba24ddf2842c6,
972f8ac7379165bc77f4404e38824104c52e11e6,"Based on research in the UK into the market for routine products used in health care, this paper explores the relationship between environmental controversy, innovation, and the competitive response of firms to demand from their customers for environmentally-friendly products. This leads to an understanding of the use of environmental issues in corporate purchasing, and the action of commercial pressure in supply chains as a mechanism of corporate greening. We then consider Baxter Healthcare, one of the main companies in the market we have examined, and the production and sale of plastic, PVC-using healthcare products, particularly 'IV bags'. The analysis of this material leads us to consider three issues: (1) the balance between the 'local' and the 'global' in firms' understanding of the market; (2) the problem of highly contested environmental claims (especially over the impact of products made from PVC), and (3) the influence of structure and practice within buying and selling organizations."
209f1a08b7ceb85707bd1ae746148d56cd82c678,
2da45d1f89bb2f4a4267c176edbbea3962b2b735,
3ecaeffc68e67aed61190888f708cc8e7d513150,The paper uses data from a questionnaire survey of UK firms to examine the ways in which benefits and costs are divided in buyer‐supplier relationships. These results are analysed in the light of Burnes and New′s framework for considering supply chain improvement. The results indicate that some caution is required in the applicability of the notion of “win‐win” in buyer‐supplier collaborations. Conclusions are drawn for theory and practice.
5da338a0d7f57ef83e45975289f30f701aeab9c4,
7c7fee79fb4adaaa7e50915bc38e5a4dd9e7837f,
7f1a882234f6efe4afef15945f5d5da2d8cd80ff,
a6f0955cf2a196dd11935a2b15126a426c36906f,
b8ada420613502ff2a8effdf88edd8850b2f4657,
e4a8fbae2f74758abce556449d4c2fb0bdb89127,
e6f88320fbf3caae03141abab078a191da7721ca,"Asks how does green purchasing change the environmental performance of the firms in a supply chain/network and what is the influence of supply chain and industry structure on that performance? Do such changes contribute to companies’ overall environmental performance and to sustainability? Discusses these, and related questions, by exploring the activities of the UK hardware retailer, B&Q, as an example of green purchasing and supply in action."
0ce0a53a2b4a3c14c13cf07099f7555d164fe8e8,
1b078030ac3c9a10c9f6bf8782574ca9682f0c70,
251c5eaf193eb58569d35465961725c833765a2e,
6436b837e103d0043f15731aedfa807dc6bef984,
8626003b414306971d9c1de60e24ffb157d7da8d,"This article describes a case study undertaken as part of a wider project concerning the emergence of collaborative customer-supplier relationships in the United Kingdom, and looks at the Rover Group automobile manufacturer and component supplier TRW. It illustrates how effective integration at the operating level does not of itself remove other areas of potential conflict, particularly in the area of costs and pricing. This may mean that the rhetoric of “partnerships” may require some degree of decoding. However, the case also illustrates that such issues can be overcome with appropriate managerial attention."
8d15d04e16fa9cb3acbface9d4fd87e45b022301,
a168fdeb4e96069d067874fabe0b266b2da4844f,
b554f0b17d93ae2a52559d52e17623fbc0dff844,
c6b0b3e355e6a1eb80d60ad91bc2e5d51f49ac38,
cea36ee5fe2ccc219d81fe5962c59d2d23cb55e0,"Advocates an expanded scope for supply chain management research which accounts for the social function and the political and economic implications of supply chain developments. Argues that the research agenda must not be driven by the notion of efficiency alone, but should also be developed around the concept of the just supply chain. Provides a framework which sets out the range of issues which may contribute to this approach. Believes that the objectives and ideological assumptions of research need to be open to challenge and debate."
33c82937a4f984bb7954e26d56d979d9c3b159e1,
4a13fee7915d1c03bb4d0eaf3f6d33742d5c90c4,"Introduction This paper will discuss the need for a robust conceptual framework for supplychain management. It forms part of a continuing research programme at UMIST aimed at producing a broadly applicable theoretical basis for work in this important area. The discussion adopts the following structure. First it is argued that despite the explosion of interest in the field, there remain significant obstacles in formulating sensible research strategies. A simple taxonomy is then presented, which allows some of the evidence in this area to be classified; without this differentiation of approach there is a danger of mistaking different types of supply-chain “phenomena” and thus in formulating an unreliable prescription. Some examples are given which illustrate the use of the framework. Finally, the implications of the taxonomy are examined, and some avenues for further work suggested."
5d4296a80d1c70c78c09b37c1df6bcede59faf97,
60618827cf957bb85501d780ca8ebd7c539d0873,"Presents a classification scheme which allows the interpretation of supply chain improvement initiatives. While there is an abundance of anecdotal evidence, there is little systematic theoretical development in the field. This is partly because of the many different ways in which co‐operation between trading partners may be developed. The suggested framework is useful in that it highlights some important yet often ignored issues relating to the logic and outcomes of collaborative activities. Illustrates the use of the framework by the secondary analysis of some well‐known UK examples in logistics and supply‐chain management and with supplementary data gathered by questionnaire. Concludes by discussing the limitations of the scheme and its role in further research."
646200f362278b0dd6897998af424b81a1b1689c,
76c88859211f832b74e613f5d02d5136e254cc94,
9b280d625833a8bb71473ba252890c24a13871ad,
e0cf6709d7e98519141b1e652ad71e4e391a29c4,The mechanisms are examined by which environmentally informed business practices and technologies may diffuse through industry as a result of the ‘greening’ of purchasing and supply. The efforts of official bodies in the UK to raise environmental awareness among industrial purchasers are reviewed. It is then argued that the supply chain model is an important way of interpreting the industrial landscape from a green perspective and that it is in some ways a more hopeful and positive starting point for achieving industrial transformation. The results of an analysis of some UK companies practices in using their purchasing policies to ‘green’ their supply chains are presented and opportunities for further research indicated.
13f5524d8a709fc9440a9bd162d7b3a7f504b711,"In marketing and operations literature, there is an emerging consensus that the inter-firm network is of strategic significance. The assumption that supply networks are always out there waiting to be investigated is challenged. Rather than treat networks as objective artifacts, the extent to which they are invented and chosen by actor(s)/observer(s) is explored. Examples are used to illustrate this argument."
1b767f64d4f08a9d392dc97ba02969b95c1a4449,
4a3656fe836bf0b2fdac16c0ef616a15a722756f,
557a4ee4dda8525c893ae50589bc01b5335267e9,
5fbb0df1ec4b491ec59de7d095399850313d4544,
62ce72c9a2df8f0dd6306b5e74503f0afcccf2e9,
b1c4585d6ac3a8abda02d8d81f2121249f9921da,
cfadb84917040b919e487e8b4c976ca9162b09d3,
e64b21fcb455fbdbf3b3dbeee87d1e78a6772e35,"Although there is growing enthusiasm for supply chain managementand integrated logistics, much prescriptive writing rests on a flimsyempirical base. Explores the methodological dilemmas which arise inresearch in logistics practice. Presents three contrasting models ofresearch frameworks. Outlines the experiences of a novel investigationinto supply chain integration in the UK carried out in the first half of1994. Makes recommendations about the use of secondary data, andstrategies for future research."
0ff4e20214938d8145f8f800fc7da46d9f663d14,
173c60e48f9b0e019d9264fd358547013c609b28,
64f5585ee0a71f03422c9a66de9a93b829ba2584,
8390f32db1b8b4ceeff9fa9af9d38469c656e3c7,
879ef7c5a2c5d19d2dd0e55fa1252e4510a9214e,
a09bfa87375b3bf919894ec224bede3975716e28,
af42f8e47c9d61fdba97ab52e88d51cabb0b37cc,
c5bae63289d2ec5e1ea793ac076f2896ae075406,
45b2cbe32dfc257121562a63b731931d2e9c3d90,
f5131ccc53b882e79f97a863836a61e873f545fb,
f9d4e6e4c644c160710795ebed7f7844c428f85d,
3ad5de1f7cafc2d0042c8fe2ee6983c2d3ec700f,
a85678d02eb263027363d76a0cee78b97b070e8e,
e70ca9b2be3b8ff30c455c4d5018f71b66eefa2d,
f83a29857671598faa4efbb885662179dd6953e9,"Plenary Paper Outlines. The Manufacturing Futures Project (J.G. Miller). Dominate or Die: Market-Driven Restructuring (M.D. Oliff). International Operations. Comparing Manufacturing Practices in North America and Western Europe: Are There any Surprises? (G. Vastag and D.C. Whybark). A Comparative Study of JIT Implementation between UK and US Manufacturing Organisations (S. Croom-Morgan, A. Harrison and T. Billesbach). Operations Management in a Centrally Planned Economy: Manufacturing in East Germany (P. Brown). The Development of a Model for the Effective Management of Operations in Developing Countries (J. O'Hara). Dynamic Manufacturing Strategy Development for Proactive Manufacturing in Brazil (H.L. Correa and I.G.N. Gianesi). Adoption and Management of Technology in Chinese Manufacturing Enterprises (D. Bennett, W.X. Ming and H.C. Xia). Strategic Aspects of Operations. Formulating Strategy: a POM Perspective (S.A. Blenkinsop and J.P. Duberley). Manufacturing Strategy Process: a Proposed Theory and Measurement Instrument (R.G. Schroeder, E.J. Flynn and K.A. Bates). A Management Development Approach to the Integration of Competitive, Marketing and Manufacturing Strategies in an International Engineering Company (J. Saker, I.G. Smith and P. Walley). MAESTRO: Management Evaluation of Strategic Options (R. Maull, J. Bennett and D. Hughes). The State of the Art and Science of Manufacturing in the UK: Preliminary Results (L.C. Sprague and Z.T. Naji). A Study of University-Based Managerially-Oriented Manufacturing Research Centres (D.A. Elvers, W.L. Berry, A. McKinney and D. Watts). Evolution in the Rapport between Large Firms Operating on the International Market and Small Local Supplying Concerns (A. De Toni, G. Nassimbeni and S. Tonchia). From National Competitive Advantage to Dominant International Operations: the Development of Lean Supply (R.C. Lamming). Environmental Protection and Physical Distribution Management: the Removal of Chemical Waste (A. de Groene and F. Aertsen). European Integration and Changing Plant Configuration (R. van Dierdonck and A. Vereecke). Contracting out the Physical Distribution Function: the Shipper - Third Party Relationship (F. Aertsen). Issues in Managing the Organisation. World Class Performance by Design (J.P.H. Bennett, P.F. Culverhouse and D.R. Hughes). Conceptual Cost Information as an Aid to the Designer (A.R. Mileham, C.G. Currie, A.W. Miles and D.T. Bradford). The Challenges Associated with Indirect Cost Apportionment in Modern Manufacturing Industry (G. Simmons and D. Steeple). Best Practice in Managing the New Product Development Process? (B.J. Doherty). The Quality of the New Product Development Process (M. Muffatto and R. Panizzolo). The Changing Role of the Operations Manager: Challenges and Competences (C.G. Armistead and J. Mapes). Towards a Temporal Division of Labour? (J. Benders). Manufacturing Strategy Implementations in Australia (A.S. Sohal, D. Samson and L. Ramsay)."
3e3599d0a1ef64dc7dcacf5caa6b9c5425b008da,
9bff380c49d7d51b0f601748f89e39981394bab9,
f0caa07743835a45034ecad59c3c6f4765c6a1da,
151adbbc3b102b68a20643e8dacdb9d02f739b61,Part I: Signs of Change Part II: Forcing Change Part III: Driving Climate Change Part IV: Expected Consequences Part V: Responding to Change Part VI: Committing to Solutions Part VII: Climate Change Data
78a8e31f2f9d21e931504fc30d4527c4a0e8dee6,
4659c288097ddd76f0b85cc30be808f240cf7c01,
94eb01b90700a10e334da0e4f5ed5b4db37255b4,"Conservation organizations are increasingly applying adaptive capacity assessments in response to escalating climate change impacts. These assessments are essential to identify climate risks to ecosystems, prioritize management interventions, maximize the effectiveness of conservation actions, and ensure conservation resources are allocated appropriately. Despite an extensive literature on the topic, there is little agreement on the most relevant factors needed to support local scale initiatives, and additional guidance is needed to clarify how adaptive capacity should be assessed. This article discusses why adaptive capacity assessment represents a critical tool supporting conservation planning and management. It also evaluates key factors guiding conservation NGOs conducting these assessments in tropical island communities, and explores alternative priorities based on input from academic experts and key local stakeholders. Our results demonstrate that important differences exist between local stakeholders and nonlocal academic experts on key factors affecting adaptation and coping mechanisms. The exclusion of local community input affects the validity of adaptive capacity assessment findings, and has significant implications for the prioritization and effectiveness of conservation strategies and funding allocation."
01a868fc35127710f39868424aa7c0683fbef25a,"Climate change threatens tropical coastal communities and ecosystems. Governments, resource managers, and communities recognize the value of assessing the social and ecological impacts of climate change, but there is little consensus on the most effective framework to support vulnerability and adaptation assessments. The framework presented in this research is based on a gap analysis developed from the recommendations of climate and adaptation experts. The article highlights social and ecological factors that affect vulnerability to climate change; adaptive capacity and adaptation options informing policy and conservation management decisions; and a methodology including criteria to assess current and future vulnerability to climate change. The framework is intended for conservation practitioners working in developing countries, small island nations, and traditional communities. It identifies core components that assess climate change impacts on coastal communities and environments at the local scale, and supports the identification of locally relevant adaptation strategies. Although the literature supporting vulnerability adaptation assessments is extensive, little emphasis has been placed on the systematic validation of these tools. To address this, we validate the framework using the Delphi technique, a group facilitation technique used to achieve convergence of expert opinion, and address gaps in previous vulnerability assessments."
f96aeaa305cbb11a050f8a5a411cddfe6d14ebd8,"The last drought experienced in the UK occurred between 1995 and 1997. In South East England this was experienced as the most severe groundwater drought since records began. This drought prompted a government call for better drought planning on the part of the privatised water service companies. The overarching question being addressed by this paper is ‘what is the effectiveness of the new system of drought planning and the resilience of current planning strategies to changing patterns of demand and climate?’ This question relates to the climate and demand pressures acting upon the water resources management system and their current impacts on the supply demand balance, the natural environment and the profitability of the privatised water sector. The question focuses on how individuals (as water consumers) and organisations (through policy development and implementation) respond to these pressures. The paper will examine the sufficiency of the response and resistance to successful drought management. Research into climate risk and drought management in South East England has been undertaken through two European Framework 5 projects SIRCH (Societal and Institutional Responses to Climate Change and Climatic Hazards) and FIRMA (Freshwater Integrated Resource Management with Agents). This paper draws from these two projects focusing on the use of agent-based modelling to explore institutional drought management and household consumption patterns under different climate change scenarios. Agent based programming can be distinguished from procedural programming in its explicit representation of stakeholders in the modelling code. This leads to the close involvement of stakeholders in model building and validation to ensure that agents appropriately represent stakeholder behaviours. Agent-based modelling process is used in the project not just to produce a final model but as a forum for social learning by sharing viewpoints between stakeholders and testing system perceptions. This paper gives a brief outline of the issues, institutions and stakeholders involved in drought management in the UK, describes the agent-based methodologies used and discusses findings related to: agent-based modelling; climate and demand uncertainty; and water resources management."
fb23b6f53b57890d574e643f8906f6231e38bad2,
c9ed60b2806fce5a0ba4b83cd6485efa4861e47b,
cff8ca49f2c2664ce286d527c49b9fa9748fcc14,
2af588f3794adce8242abb3159589a9f588db193,
69b52c6e37e105553f22ce9cd5b5ced51c931454,"As part of the Mediterranean area, the Guadiana basin in 
Spain is particularly exposed to increasing water stress 
due to climate change. Future warmer and drier climate 
will have negative implications for the sustainability of 
water resources and irrigation agriculture, the main socio- 
economic sector in the region. This paper illustrates a 
systematic analysis of climate change impacts and adaptation in the Guadiana basin based on a two-stage modeling approach. First, an integrated hydro-economic modeling framework was used to simulate the potential 
effects of regional climate change scenarios for the period 2000-2069. Second, a participatory multi-criteria technique, namely the Analytic Hierarchy Process (AHP), was applied to rank potential adaptation measures based on agreed criteria. Results show that, in the middle-long run and under severe climate change, reduced water availability, lower crop yields and increased irrigation demands might lead to water shortages, crop failure, and up to ten percent of income losses to irrigators. AHP results show how private farming adaptation measures, including improving irrigation efficiency and adjusting crop 
varieties, are preferred to public adaptation measures, 
such as building new dams. The integrated quantitative and qualitative methodology used in this research can be 
considered a socially-based valuable tool to support adaptation decision-making."
7ba977e53521c81fa9c7677a8a90073983db03fe,"1. Introduction: setting and problem definition 
2. The Adaptation Pathway 
–2.1 Stage 1: appraising risks and opportunities 
•Step 1: Impact analysis 
•Step 2: Policy analysis 
•Step 3: Socio-institutional analysis 
–2.2 Stage 2: appraising and choosing adaptation opt 
ions 
•Step 4: identifying and prioritizing adaptation o 
ptions 
3. Conclusions"
8b372b548bc2ae427f4ac79e6cdb60d610be9def,
de98b23ba3487fc88920e5310b2fd5fa17909329,"Overview Vulnerability assessment is a key aspect of anchoring assessments of climate change impacts to present development planning. Methods of vulnerability assessment have been developed over the past several decades in natural hazards, food security, poverty analysis, sustainable livelihoods and related fields. These approaches—each with their own nuances—provide a core set of best practices for use in studies of climate change vulnerability and adaptation."
efa79a8de27f52c555418df6efa202498663b781,
89aea9e5637db77cedf9fb72d5e36e9dde7b9d78,"The shift from framing climate change adaptation as vulnerability–impacts to adaptation pathways is also a shift from a predict‐and‐provide approaches to understanding dynamic processes. Studies of the economics of adaptation relying only on the comparative statics of reference and climate impacts scenarios ignore the more challenging frontier of representing decision processes and uncertainty. The logic of the shift to dynamic‐pathway approaches is widely accepted in principal. Effective analytical tools are only beginning to appear. Further case studies are required to explore the matrix of uncertainty in future climate conditions against the range of metrics for valuing impacts in decision processes. WIREs Clim Change 2012, 3:161–170. doi: 10.1002/wcc.157"
a86c5a12bb6abba0537d8c39fd3c0767b3bcb7a2,
cac1f2d7a51df9599a0676e2895ca622584a383a,
321b6277827c1b70316a1bfc492ea0967b2cec9d,
7a2085217555f9ca52d922d564370821923636b7,
cd97a50432a66aad9f9c893ca9b343f59cd8e323,"Climate change is likely to have serious and significant impacts on human population health. The
mechanisms by which climate change may affect health are becoming better understood. Current
quantitative methods of estimating future health impacts rely on disease-specific models that
primarily describe relationships between mean values of weather variables and health outcomes
and do not address the impacts of extreme events or weather disasters. Extreme events have the
potential to disrupt community function, which is of concern for decision-makers. Estimating the
magnitude and extent of impacts from low probability high impact events is challenging because
there is often no analogue that can provide relevant evidence and that take into account the
complexity of factors determining future vulnerability and health impacts (the social determinants of
health)."
e310f4456db92e25a691a1644874e65b57b79368,
fe13adcfe285dc43c1840546977257f1b07f3347,
ff3d9f214294f17fde4d57c1ca40e76ec0f7e9b9,
10690b7f50bf967ff98e6e329bbffd3dc9c84c51,................................................................................................................................................. iv
6dfe9af8b53cc3e3cac9fa0f8c2491e10b7d82f2,"Resilience and vulnerability represent two related yet different approaches to understanding the response of systems and actors to change; to shocks and surprises, as well as slow creeping changes. Their respective origins in ecological and social theory largely explain the continuing differences in approach to social-ecological dimensions of change. However, there are many areas of strong convergence. This paper explores the emerging linkages and complementarities between the concepts of resilience and vulnerability to identify areas of synergy. We do this with regard to theory, methodology, and application. The paper seeks to go beyond just recognizing the complementarities between the two approaches to demonstrate how researchers are actively engaging with each field to coproduce new knowledge, and to suggest promising areas of complementarity that are likely to further research and action in the field."
9973ea46a927d062694d1f2ae41144a7bb8cbee9,"Resilience and vulnerability represent two related yet different approaches to understanding the response of systems and actors to change; to shocks and surprises, as well as slow creeping changes. Their respective origins in ecological and social theory largely explain the continuing differences in approach to social-ecological dimensions of change. However, there are many areas of strong convergence. This paper explores the emerging linkages and complementarities between the concepts of resilience and vulnerability to identify areas of synergy. We do this with regard to theory, methodology, and application. The paper seeks to go beyond just recognizing the complementarities between the two approaches to demonstrate how researchers are actively engaging with each field to coproduce new knowledge, and to suggest promising areas of complementarity that are likely to further research and action in the field."
a95ab604e5388018806e2570f6e253191e8c2195,"""Tipping Points in Humanitarian Crisis: From Hot Spots to Hot Systems"", edited by Xiaomeng Shen , Thomas E. Downing and Mohamed Hamza, presents the outcomes of the 4th UNU-EHS/MRF Summer Academy on Social Vulnerability (26 July - 1 August 2009). The Academy participants developed the new ""hot system approach"" that analyses how the interplay of coupled social-ecological systems increases vulnerability or resilience."
c338455e234208555a6a47bc8625311599f8b984,
e7e7a0187101fc1fe293e19f1ef7074c9949b5fb,"This monograph summarises the findings from the Adapt Cost project, which support the statement from the Tunis Roundtable. The project investigated and built evidence of the costs of adaptation, producing a range of estimates for climate adaptation in Africa using different evidence. The monograph will help African policymakers and the international climate change community to establish a collective target for financing adaptation in Africa."
1a38497c5c91debc8d43b067a2976a73f61960f7,"Adaptation was defined in 1992 by the United Nations Framework Convention on Climate Change (UNFCCC) as ‘all adjustments in socio-economic systems designed to reduce vulnerability to climate change’. Since then this concept has evolved and gained relevance along with mitigation. Whereas mitigation refers to limiting global climate change by reducing greenhouse gas (GHGs) emissions and enhancing carbon sinks, adaptation aims at moderating adverse climate change effects through a wide range of system-specific actions at the local and the regional level."
2baf738c9404e4341e1154582f2e51e186dd43f5,"Vulnerability to climate change in Africa Climate change is expected to intensify existing problems and create new combinations of risk with potentially grave consequences. This is particularly true in Africa where direct dependence on the natural environment for livelihood support combines with a lack of infrastructure and high levels of poverty to create vulnerability in the face of all types of environmental change. In regions of Eastern and Southern Africa, vulnerability is particularly high due to the large number of households that depend on the already marginalised natural resource base for their livelihood."
39be7d13ad79053cff76bccad9ab002984c69107,
3c6ea7267cfa453fa4e6e8db1f0a9ae0bef40094,
45849e196d88acdc4ed0f624c872b88565a28d05,"In arid countries worldwide, social conflicts between irrigation-based human development and the conservation of aquatic ecosystems are widespread and attract many public debates. This research focuses on the analysis of water and agricultural policies aimed to conserve groundwater resources and to maintain rural livelihoods in a basin in Spain’s central arid region. Intensive groundwater mining has caused overexploitation of the basin’s large aquifer and the degradation of internationally reputed wetlands. This has caused notable social conflicts over the years but the policies implemented have not been able to solve them so far. The methodology consists in the integration of a farm-based economic model and a basin-based hydrology model to analyze the dynamics of different water and agricultural policies under changing climate conditions. Results show that the region’s current quota-based water policies may contribute to reduce water consumption in the farms but will not be able to recover the aquifer and will inflict economic losses to the rural communities. This situation would worsen in case of drought. The long-term sustainability of the aquifer and rural livelihoods will be attained only if additional measures are put in place such as the control of illegal abstractions and the establishing of a water bank"
6dd061cbf4412977a748e1cbfc84fb7459ac614e,"Spain is the most arid country in Europe and water use as well as water depletion and environmental degradation have slowly become a matter of social concern. Balancing the two objectives of “water for rural livelihoods” and “water for nature” constitutes a difficult challenge for the water Administration. The aim of this paper is to analyze the impact of water conservation policies in the Upper Guadiana River basin (Spain), where intensive irrigated agriculture resulted in the overexploitation of the Western La Mancha aquifer and the subsequent degradation of the highly valuable wetlands. Focus is made on farmers’ vulnerability to these policies. The methodology combines qualitative and quantitative aspects by the integration of an economic model and a vulnerability analysis. The economic model simulates farmers’ behaviour facing different policy options, and the results are used as an input for the analysis of vulnerability based on farm income indicators, through the elaboration of a classification tree. The model integration presented in this work proves that, in the case of the Upper Guadiana, different farm types stand diverse policy impacts and that structural, behavioural and institutional aspects play a major role in those impacts, being small and legal farms the most vulnerable ones."
780cd797e64637d255e6af006383ad4bdfc3b55f,"There is a need to build a link between global scenarios and local situations. Although there are many global scenarios, most of them are limited in their potential to portray social realities at the local level such as different impacts of change and the changing relationships between global drivers and local impacts."
81bc5082622a40cb492e542f21b114287afed1e7,
bba41bfc3c1e25f51cd7ae331ca412667e280fcb,
c3b146f439d76114387acdd59787b2e26beb8721,
cd3e721ef5fb87355210b78f5c621432aa3c9229,
d5991683b36ff32890b4f97a8ede025d81ad1506,
37ed1f3dc7f4136fe7541e6e013efeaabddc00d1,
6de065132fcf03d4d0100cbe9d463d37bfc8fc1e,"There is an increasing interest in the economics of climate change, and the marginal damage costs of emissions, known as the Social Cost of Carbon (SCC). In 2002, the UK Government recommended an SCC for policy appraisal. A recent review of this SCC was commissioned and summarised in this paper. The authors conclude that SCC estimates span at least three orders of magnitude, reflecting uncertainties in climate change and choices of key parameters/variables (discount rate, equity weighting and risk aversion). Estimates also vary due to their coverage, and a risk matrix was developed to compare climate change eects (predictable to major events) against impacts (market, non-market and socially contingent). From several lines of evidence, the current lower SCC value is considered a reasonable lower benchmark for a global decision committed to reducing the threat of dangerous climate change. An upper benchmark was more dicult to deduce, though the risk of high values was considered significant. It is currently impossible to provide a central value with confidence. The study also reviewed the use of the SCC in policy, from project appraisal to long-term climate policy, and used stakeholder interviews to elicit views. A wide diversity of responses was found: whilst most considered some values are needed for policy appraisal, nearly all had reservations for long-term policy. From this, the authors propose a two tier approach. The economic benefits of climate change should be considered when setting long-term policy, but a wider framework is needed (i.e. than cost-benefit analysis). This should include a disaggregated analysis of economic winners and losers by region and sector, and key impact indicators such as health and ecosystems. It should also consider the full risk matrix (i.e. non-marginal/irreversible eects). Once long-term policy is set, shadow prices for appraisal across Government are useful, provided they are consistent with the long-term goal, and are applied consistently."
8c469732fb2009ae6bc67b7c6920a100a2a5c00f,"In the Mediterranean basin, irrigation agriculture is a key sector for the economy but it consumes a large proportion of all available water resources. This situation is producing an ever-mounting depletion of water resources and degradation of valuable aquatic ecosystems. In southern countries of the EU competing uses of water for agricultural production and for providing ecosystem services is calling for a revision of former water policies and for an integration of agricultural and water policies. This research focuses on the comparative effects of water polices and agricultural policies aiming to conserve water resources in an area of Spain's southern central plateau in the region of Castilla-La Mancha. In this area, agricultural production is dependent solely on groundwater and, as a consequence of lucrative CAP production-related payments, water abstractions have exceeded the recharge capacity of the aquifer. The induced over-exploitation of the aquifer has lead to long-lasting social conflicts as well as acute environmental concerns due to the derived degradation of natural wetlands of high ecological value. Regional, national and European policies have been implemented with the purpose of solving these conflicts, but the solution has not been found so far. Based on an integrated vision of water resources management, the methodology of this study consists in the integration of an agro-economic model and a hydrology model. The economic model is a farm-level mathematical programming model of constrained maximization that simulates farmers' behavior confronted to different policy scenarios. The hydrology model (WEAP, water evaluator and planning system) permits to up-scale the results of the MPM to the basin's level to assess the effects on the aquifer of the selected policies. Based on an ample field work several policy scenarios have been chosen and simulated to analyze the impacts that the different policy options would have on the different components of the system. Results of the integrated economic and hydrology models, show that all current water conservation policies applied in the area (Upper Guadiana basin), even when they can contribute to an important reduction in water consumption, will not be able to attain the recovery of the aquifer unless other additional measures, aiming to reduce water abstractions, are put into practice. We can also conclude form the stakeholders' perspective that a more integrated and coordinated application of water policies and agricultural policies is a key issue to meet the dual objective of maintaining farming activity and protecting the wetland ecosystems. In the current EU context of the WFD and the CAP larger and well targeted public participation will contribute to ensuring an effective and socially acceptable water resources management in the area."
bd1869684f753e880c825e28071d9307456db738,"Executive Summary The aim of this study is to provide external advice and recommendations to the Shadow Committee on Climate Change (SCCC) on the methodological approaches that could be used to propose UK interim carbon budgets for the period 2008-2022. These interim budgets are bounded by existing Government commitments, most notably the 26-32% reduction in CO 2 emissions by 2020 (on 1990 levels), on the way to ‗at least' a 60% CO 2 reduction by 2050. As such, the interim carbon budgets will effectively set the UK's 2020 reduction target, and this date has been the primary focus of the study. The findings are summarised below.  The setting of interim budgets is strongly influenced from the ‗top down' by: long-term global stabilisation targets; the global pathways of emissions to these long-term targets; and the UK's relative share in global emissions reductions (e.g. as a developed country). It is also influenced from the ‗bottom up' by the UK's decisions on how much of its commitment to meet from overseas through international flexible mechanisms, and the relative share of reductions by sectors within and outside the emissions trading scheme. Finally it is influenced by external political negotiations with respect to the EU's 2020 GHG targets and the burden sharing agreement. Any decision on interim targets therefore sits within a much wider set of policy considerations.  A review has found a variety of possible methods for setting carbon targets. These reflect different perspectives for framing climate-change policy, and accordingly, different methodological approaches. The entry points range from a strong economic perspective through to a non-economic/precautionary perspective. To date, most proposed long-term targets have used a precautionary approach, but most interim targets (and the UK Government's long-term target) have been set after a check for technical and economic achievability using cost-effectiveness analysis (CEA).  The challenge for the UK is to set short-term (interim) carbon targets under multiple sources of uncertainty over the future evidence and the course of policy. Many of these uncertainties will be reduced in the future. Therefore, any methodological approach should consider the extent to which interim targets keep options open for the longer term, thereby avoiding irreversible commitments. This problem is one of decision making under uncertainty, or options analysis. In the literature, this has been explored in most depth using cost-benefit analysis (CBA), though such a framework can be applied to other methods.  Each of …"
df47916ea27bb9a9dedc653161013a5ccacdf21c,
e179bf87e526df23f3084351428e23a592f130d1,
fdf8d36a47d1fddb0f60ee52b6bc313247b56913,
feee5dc606b851a36134bdd32122b3fd478c4826,
1ce2425f11fe785be943ed55578b04a46b4fd564,
5b5f0ec3895214d0d5bfc1c9c3e2991d246d0990,"In this millennium, global drylands face a myriad of problems that present tough research, management, and policy challenges. Recent advances in dryland development, however, together with the integrative approaches of global change and sustainability science, suggest that concerns about land degradation, poverty, safeguarding biodiversity, and protecting the culture of 2.5 billion people can be confronted with renewed optimism. We review recent lessons about the functioning of dryland ecosystems and the livelihood systems of their human residents and introduce a new synthetic framework, the Drylands Development Paradigm (DDP). The DDP, supported by a growing and well-documented set of tools for policy and management action, helps navigate the inherent complexity of desertification and dryland development, identifying and synthesizing those factors important to research, management, and policy communities."
7cf862af352ce872d1c3c684c322d224f6ec17b9,"Downing et al. (2006) highlight a progression in the past decade toward a vulnerability/adaptation science that recognises six key attributes of social vulnerability and four processes for understanding vulnerability. We assess the applicability of these attributes and processes to a Highlands community in Lesotho, vulnerable to a complex combination of stresses, and investigate whether such an analysis furthers understanding of local issues and identification of potential adaptation options. In attempting to integrate vulnerability in to water catchment planning, we describe: the different kinds of vulnerability that are experienced in this area; the dynamic and multiple-scale nature of this vulnerability; the attributes of vulnerable groups, which may enable them to respond to stresses and threats; and the adaptation choices that are made by different groups (Downing 2006). Such an approach may help to identify the institutional structures that are required to support adaptation factoring in the uncertainty of future predictions of climate change and its implications on the water system. The vulnerability analysis indicates that those adaptation options which are already being practised by the community are good targets for more for"
996590a488c2aa65d2163235599274b5f91dac8b,
a276828bb0e5a99919e038c6cf89c9e3c422a7b7,"There is a scientific consensus regarding the reality of anthropogenic climate change. This has led to substantial efforts to reduce atmospheric greenhouse gas emissions and thereby mitigate the impacts of climate change on a global scale. Despite these efforts, we are committed to substantial further changes over at least the next few decades. Societies will therefore have to adapt to changes in climate. Both adaptation and mitigation require action on scales ranging from local to global, but adaptation could directly benefit from climate predictions on regional scales while mitigation could be driven solely by awareness of the global problem; regional projections being principally of motivational value. We discuss how recent developments of large ensembles of climate model simulations can be interpreted to provide information on these scales and to inform societal decisions. Adaptation is most relevant as an influence on decisions which exist irrespective of climate change, but which have consequences on decadal time-scales. Even in such situations, climate change is often only a minor influence; perhaps helping to restrict the choice of ‘no regrets’ strategies. Nevertheless, if climate models are to provide inputs to societal decisions, it is important to interpret them appropriately. We take climate ensembles exploring model uncertainty as potentially providing a lower bound on the maximum range of uncertainty and thus a non-discountable climate change envelope. An analysis pathway is presented, describing how this information may provide an input to decisions, sometimes via a number of other analysis procedures and thus a cascade of uncertainty. An initial screening is seen as a valuable component of this process, potentially avoiding unnecessary effort while guiding decision makers through issues of confidence and robustness in climate modelling information. Our focus is the usage of decadal to centennial time-scale climate change simulations as inputs to decision making, but we acknowledge that robust adaptation to the variability of present day climate encourages the development of less vulnerable systems as well as building critical experience in how to respond to climatic uncertainty."
3de8347bec2bad521f5987e265448cff2cb25c8e,"Human health and wellbeing are closely linked to the health and resilience of ecosystems. When natural disasters occur in situations where natural resources have been severely degraded, it is much more diffi cult for communities to recover and for people to re-establish their lives. By examining lessons from the December 2004 tsunami, it is possible to identify the important role healthy coastal and marine ecosystems played in buff ering immediate impacts and protecting human lives, and the longer-term benefi ts gained for human health and livelihoods from sustainable use of natural resources (see Bowen et al., this issue). Whilst the role resilient ecosystems played in reducing the severe humanitarian impacts of such a powerful phenomenon should not be exaggerated (especially in Sumatra, Indonesia where wave height and force was very high; see Keim et al., this issue), the potential of healthy ecosystems to hasten the recovery of communities is clearly evident."
440b30abe5048896eaacb25344d01723bf82deb6,"Understanding of how best to support those most vulnerable to climate stress is imperative given expected changes in climate variability. This paper investigates local adaptation strategies to climate variability, focusing on agricultural decision-making in a communal irrigation scheme in Vhembe District, Limpopo Province, South Africa. Research done through interviews, surveys and participatory methods demonstrates that adaptation strategies within a community are socially differentiated and present differing objectives and priorities. These results highlight the need for intervention and policy that support a heterogeneous response to a wide range of stresses. Evidence for climate change is clear and the need for adaptation is urgent. However, adaptation measures have to be sensitively integrated with ongoing development pathways to ensure they are sustainable and relevant to local priorities."
614a9a5e90f0e3f7c962ab851375cfbb1301a085,"There is an unknown but probably small probability that the West‐Antarctic Ice Sheet (WAIS) will collapse because of anthropogenic climate change. A WAIS collapse could cause a 5–6 metre global sea level rise within centuries. In three case studies, we investigate the response of society to the most extreme yet not implausible scenario, a five‐metre sea level rise within a century, starting in 2030. The case studies combine a series of interviews with experts and stakeholders with a gaming workshop. In the Rhone delta, the most likely option would be retreat, with economic losses, perhaps social losses, and maybe ecological gains. In the Thames estuary, the probable outcome is less clear, but would probably be a mix of protection, accommodation and retreat, with parts of the city centre turned into a Venice of London. A massive downstream barrier is an alternative response. In the Rhine delta (the Netherlands), the initial response would be protection, followed by retreat from the economically less important parts of the country and, probably, from Amsterdam–Rotterdam metropolitan region as well. These impacts are large compared to other climate change impacts, but probably small compared to the impacts of the same scenario in other parts of the world. This suggests that the possibility of a anthropogenic‐climate‐change‐induced WAIS collapse would strengthen the case for greenhouse gas emission reduction."
7dde080a22f2089eadb4ff475b1b94baea2f3948,
0d5196df38882b8ffdf6f71b4b7b155d221e3ae7,
4fbfdd2c1ecbba8546dfc90daca3194464a02da7,"Adaptation involves the management of risks posed by climate change, including variability. The identification and characterisa-tion of the manner in which human and natural systems are sensitive to climate become key inputs for targeting, formulating and evaluating adaptation policies. With the guidance presented here, users should be equipped to carry out a vulnerability assessment at the appropriate level of detail and rigour. Not every Adaptation Policy Framework (APF) user will need to undertake a vulnerability assessment; those who do will likely be motivated by a specific need to raise awareness of vulnerability, to target adaptation strategies toward key vulnerabilities and to monitor exposure to climatic stresses. These users can tap the guidance outlined here to hone in on key groups, sectors, geographic areas, etc., assess current and future vulnerability, and integrate observations into adaptation planning and policy making. If we take the example of human health, climate change is likely to affect the distribution and prevalence of infectious disease vectors, which might lead to increased mortality and morbidity from diseases such as malaria and cholera. However, this outcome is dependent on non-climate factors, including environmental controls, public health systems, and the availability and use of drugs and vaccines. A first step in designing effective adaptation strategies would be to clearly establish the importance of climate change, including variability, in terms of the final health outcomes. In this instance, a vulnerability assessment would target those regions most affected by the health impacts of climatic variability, focus adaptation options on effective interventions for the most vulnerable populations, and produce baseline data and indices for monitoring responses. While a vulnerability assessment (VA) is important for responding to future climate risks (TP5), the assessment process may also help improve the management of current climate risks (TP4). For example, the vulnerability assessment can be used to address the following questions of immediate relevance to policy-makers and development planners: To what extent are the anticipated benefits from existing development projects sensitive to the risk of climate change, including vari-ability? In what way can considerations of future climate risk be incorporated into the design of development projects? These questions are particularly germane in developing countries that are witnessing the rapid build-up of long-lived civil infrastructure (such as irrigation systems, transportation systems and urban settlements) and in conditions where natural resources are rapidly degrading (such as desertification, water quality and scarcity, and the loss of other environmental services). …"
8441947507e71f98c7c4f64c69d52114e336a8ef,
85e3154424afe920b92d05c40209fac23d0559c6,"Water Management is facing major challenges due to increasing uncertainties caused by climate and 
global change and by fast changing socio-economic boundary conditions. Adaptive management is 
advocated as a timely extension of IWRM to cope with these challenges. Adaptive management aims 
at increasing the adaptive capacity of river basins based on a profound understanding of key factors 
that determine a basin’s vulnerability. More attention has to be devoted to understanding and 
managing the transition from current management regimes to more adaptive regimes that take into 
account environmental, technological, economic, institutional and cultural characteristics of river 
basins. The paper identifies major challenges for research and practice how to achieve this transition. 
The European project NeWater project is presented as one approach where new scientific methods and 
practical tools are developed for the participatory assessment and implementation of adaptive water 
management. The project puts strong emphasis on establishing science-policy dialogues at local, basin 
and global scales."
be0b8bca4c92ba1c9dd640df854748657f86d0bd,"Seasonal climate outlooks provide one tool to help decision-makers allocate resources in anticipation of poor, fair or good seasons. The aim of the ‘Climate Outlooks and Agent-Based Simulation of Adaptation in South Africa’ project has been to investigate whether individuals, who adapt gradually to annual climate variability, are better equipped to respond to longer-term climate variability and change in a sustainable manner. Seasonal climate outlooks provide information on expected annual rainfall and thus can be used to adjust seasonal agricultural strategies to respond to expected climate conditions. A case study of smallholder farmers in a village in Vhembe district, Limpopo Province, South Africa has been used to examine how such climate outlooks might influence agricultural strategies and how this climate information can be improved to be more useful to farmers. Empirical field data has been collected using surveys, participatory approaches and computer-based knowledge elicitation tools to investigate the drivers of decision-making with a focus on the role of climate, market and livelihood needs. This data is used in an agent-based social simulation which incorporates household agents with varying adaptation options which result in differing impacts on crop yields and thus food security, as a result of using or ignoring the seasonal outlook. Key variables are the skill of the forecast, the social communication of the forecast and the range of available household and community-based risk coping strategies. This research provides a novel approach for exploring adaptation within the context of climate change."
e3a1b87efec97c3efe6d323fafe3fec65199330d,"The effects of global climate change from greenhouse gas emissions (GHGs) are diverse and potentially very large, and probably constitute the most serious long-term environmental issue currently facing the world. This paper is prepared as task 1 of the project 'Modelling support for Future Actions - Benefits and Cost of Climate Change Policies and Measures', ENV.C.2/2004/0088, led by K.U.Leuven, Katholieke Universiteit Leuven. The paper provides a rapid review and analysis of the impacts and economic costs from climate change. The objective is to provide estimates of the benefits of climate change policy, i.e. from avoided impacts, for support to the Commission in considering the benefits and costs of mitigation efforts, and to support DG Environment in its report to the Spring Council 2005 and in future international negotiations on climate change."
0f52deecb76689e2257ebdc3b239a02250d4fb43,"Climate change is likely to impact more severely on the poorer people ofthe world, because they are more exposed to the weather, because they are closer to the biophysical and experience limits ofclimate, and because their adaptive capacity is lower. Estimates of aggregated impacts necessarily make assumptions on the relative importance ofsectors, countries and periods; we propose to make these assumption explicit. We introduce a Gini coefficient for climate change impacts, which shows the distribution of impacts is very skewed in the near future and will deteriorate for more than a century before becoming more egalitarian. Vulnerability to climate change depends on more than per capita income alone, so that the geographical pattern ofvulnerability is complex, and the relationship between vulnerability and development non-linear and non-monotonous. r 2004 Elsevier Ltd. All rights reserved."
18cd905a732ea812d2c32b6541938e0e2aeab478,"Scoping and designing an adaptation project 13 Task 1: Scope project and define objectives 14 Task 2: Establish project team 15 Task 3: Review and synthesise existing information on vulnerability and adaptation 15 Task 4: Design the adaptation project 16 Key issues 16 Assessing current vulnerability 17 Task 1: Assess climate risks and potential impacts 17 Task 2: Assess socioeconomic conditions 18 Task 3: Assess adaptation experience 18 Task 4: Assess vulnerability 18 Key issues 19 Assessing future climate risks 19 Task 1: Characterise climate trends, risks and opportunities 20 Task 2: Characterise socioeconomic trends, risks and opportunities 20 Task 3: Characterise natural resource and environmental trends 20 Task 4: Characterise adaptation barriers and opportunities 21 Key issues 21 Formulating an Adaption Strategy Task 1: Synthesise previous Components/studies on potential adaptation options 22 Task 2: Identify and formulate adaptation options 22 Task 3: Prioritise and select adaptation options 22 Task 4: Formulate the adaptation strategy 22 Key issues 23 Continuing the adaptation process 23 Task 1: Incorporate adaptation policies and measures into development plans 24 Task 2: Implement the adaptation strategy and institutionalise follow-ups 24 Task 3: Review, monitor and evaluate the effectiveness of policies, measures and projects 25 Key issues 25 Engaging stakeholders 25 Links with Adaptation Policy Framework Components 25 Identify key stakeholders 26 Clarify stakeholder roles 26 Manage the dialogue process 26 Key issues 26 Assessing and enhancing adaptive capacity 27 Links with Adaptation Policy Framework Components 27 Assess current adaptive capacity 27 Identify barriers to, and opportunities for, developing adaptive capacity 28 Develop strategies to integrate adaptive capacity into adaptation 28 Key issues 28 CONTENTS 9 User's Guidebook"
23fe41f290e048bfe8a63aa05ae6e8ea4b530359,
edab7f14e1fe81461757f15e9a95300c1c6c80c8,"Agent-based modelling concepts and techniques are well suited to the investigation of food system vulnerability to global environmental change. The actors in a food system are represented by agents in a computer model—responding to the environment, managing resources and learning from their experiences and each other. Thus, the particular vulnerabilities of local communities can be related to the general drivers of the stresses and shocks in social, economic and environmental change."
ffc79a78f3aedbb6b38fe8364c7a238640ae11f3,
05467244c8741ef4bc83ea8a643d91e2e1b05b81,
07c1d9fe0368dbbecf51d411686dfc8eb93c1e47,
1d79715d9a8428880a2935f2508ab2914eff72be,
1f286131216d832b2fc1e4730d4491e0a561a625,"In this chapter we focus on how local vulnerability, expressed in land degradation, is related to international and global processes. We begin with a synopsis of vulnerability in the context of land degradation and desertification. The nature of international regimes that may affect vulnerable socioeconomic groups is presented. Analogues of local–global linkages in vulnerability are summarized, including the green revolution, climate change, and intellectual property rights for biodiversity. Methodologies for local–global understanding of vulnerability are suggested. Our conclusions suggest lessons learned in research on international vulnerability and identify issues for further consideration."
51e3713781c08757d2a0c684e158ecc1eade5090,
62413004044668ea3e4ac5c616eb36f88993b67d,
960be90e5679badcea1f39313b1fde7e7d186fbd,
97b86e91b644e590e43bcca27228443c827e1c49,
bc7c2fa4eda978587b5dbc5dc5891e8829bc8275,
c250c29f565bac27635c099692addbb00401b9fa,
e756ac2e7023e065ef21f33eb4f335f4b0775564,"FOREWORD This paper was prepared for an OECD Workshop on the Benefits of Climate Policy: Improving Information for Policy Makers, held 12-13 December 2002. The aim of the Workshop and the underlying Project is to outline a conceptual framework to estimate the benefits of climate change policies, and to help organise information on this topic for policy makers. The Workshop covered both adaptation and mitigation policies, and related to different spatial and temporal scales for decision-making. However, particular emphasis was placed on understanding global benefits at different levels of mitigation-in other words, on the incremental benefit of going from one level of climate change to another. Participants were also asked to identify gaps in existing information and to recommend areas for improvement, including topics requiring further policy-related research and testing. The Workshop brought representatives from governments together with researchers from a range of disciplines to address these issues. Further background on the workshop, its agenda and participants, can be found on the internet at: www.oecd.org/env/cc The overall Project is overseen by the OECD Working Party on Global and Structural Policy (Environment Policy Committee). The Secretariat would like to thank the governments of Canada, Germany and the United States for providing extra-budgetary financial support for the work. This paper is issued as an authored "" working paper ""-one of a series emerging from the Project. The ideas expressed in the paper are those of the author alone and do not necessarily represent the views of the OECD or its Member Countries. As a working paper, this document has received only limited peer review. Some authors will be further refining their papers, either to eventually appear in the peer-reviewed academic literature, or to become part of a forthcoming OECD publication on this Project. The objective of placing these papers on the internet at this stage is to widely disseminate the ideas contained in them, with a view toward facilitating the review process. were of great help in getting the paper in its current shape. The interpretations and any errors in this paper are, however, our own."
fa159a6faf0037ed005f02a771c7668f47ed6ddb,
13a168c4d3ca8c5063f5d95a2692b742e1fb422f,Introduction 1. How Fragile is the Earth System? 2. Risks of Conflict: Resource and Population Pressures 3. Valuing the Earth: Reintegrating the Study of Humans and the Rest of Nature 4. Can Technology Save us from Global Climate Change? 5. The Role of Corporate Leadership 6. Who Governs a Sustainable World? The Role of International Courts and Tribunals 7. The Climate Change Regime: Can a Divided World Unite? 8. Protecting the Vulnerable: Climate Change and Food Security
75c80a02b108218c76068343d363ac72939520da,"A promising area of application for agent based social simulation is social policy analysis. This promise stems from the ability of ABSS models to describe key processes and relationships in actual societies. However, existing descriptive studies with ABSS have been too abstract convincingly to inform policy analysis. This paper describes the transition from one such study towards a usefully detailed and realistic model currently being developed to inform water demand management in the south of England during a period of climate change."
7e94c5d6ebdce66539a98ef37dd130236ae611d0,"In this chapter we focus on how local vulnerability, expressed in land degradation, is related to international and global processes. We begin with a synopsis of vulnerability in the context of land degradation and desertification. The nature of international regimes that may affect vulnerable socioeconomic groups is presented. Analogues of local–global linkages in vulnerability are summarized, including the green revolution, climate change, and intellectual property rights for biodiversity. Methodologies for local–global understanding of vulnerability are suggested. Our conclusions suggest lessons learned in research on international vulnerability and identify issues for further consideration."
af4b38de2eb0609158ad7473fe9350c0fdd1e566,"Although it is now widely recognized throughout societies around the world that humankind is straining the capacity of our planet, the responses of the many parties who need to come together to solve the myriad challenges have in general been sectoral in their approach. Yet the dangers presented by drawing successively on Earth's natural resources and exacerbating drought, water scarcity, hunger, health - indeed the viability of whol ecommunities and countries - coupled with the economic and social aspirations of the developed and developing world, clearly call for holistic vision leading to coordinated action. In this book, eight highly experienced authors from acaemia, intergovernmental negotiation, and diplomacy each present their own perspective and assess prospects for progress. the necessity for interdisciplinarity of approach, and for partnership and altruism on an unprecedented scale for solution, and for new negotiation and arbitration frameworks, emerge as inevitalbe conclusions. Contributors to this volume - Professor Hans-Joachim Schellnhuber, Director of teh Potsdam Institute for Climate Impacts Research, and Professor of Theoretical Physics at the University of Potsdam, Germany; Professor Robert Costanza, Department of Zoology, University of Maryland; Sir Crispin Tickell, Warden of Green College Oxford 1990-97, and British Permanent Representative to the UN 1987-90; Chairman of Erthwatch (Europe) 1994-7, Convenor, Government Panel on Sustainable Development since 1994; Dr Bert Metz, Netherlands Institute of Publish Health and the Environment. Chairman of the current Task Force III of the IPCC; Dr Joyeeta Gupta, Senior Researcher, Institute for Environmental Studies, Vrije Universiteit, Amsterdam; Professor Philippe Sands, School of Oriental & African Studies, University of London, and Global Professor of Law, New York University. Co-Director, Project on International Courts and Tribunals; Sir John Browne, Group Chief Executive, BP Amoco, non-executive director of Goldman Sachs Group and Intel Corporation, a trustee of the British Museum, a member of the supervisory board of Daimler Chrysler, and vice-president of the board of the Prince of Wales's Business Leaders' Forum"
c7beebbc3ed599b805e52268036e90c6afaaa6e6,"Scaling methods in regional integrated assessments are often adopted as givens, when in fact there are a range of methods that each have their strengths and weaknesses. Methods such as a site within a polygon, spatially uniform grids, grids with relational data on polygons, interpolation and stochastic spatial models are reviewed for crop-climate modeling of climate change impacts developed in the European Union’s Clivara project. A similar suite of methods for downscaling from global climate models to local conditions exists, and is reviewed. Up- and down-scaling issues relate to availability of data, the level of technical expertise in the project team, validation, uncertainty and risk, stakeholder participation, and modeling of actor-agents. Given the many aims of integrated assessments, no one approach is best."
e1e5d8157d299e8df0d36e6dbd2fea9bfbb933db,
52abf9c52719ee64d665b8cb26f9a8aa9381edf4,"A key challenge in applying the lessons of vulnerability analysis to policy is developing strategies for specific local situations based on an understanding of the local context together with combinations of national level data and lessons drawn from diverse case study findings. While national data can provide an overview, general guidance on the strengths and weaknesses of particular policy options must recognise the local characteristics of environmental and social stressors and draw from the experiences of many individual vulnerability reduction efforts to determine the common elements of success and failure. This paper will present preliminary results of an evaluation of vulnerability reduction efforts and discuss lessons of policy effectiveness for poverty and vulnerability reduction. Systematic comparison of case studies will be used to uncover elements of effective vulnerability reduction policy for comparable situations. It will also report on analysis of synergies between poverty and vulnerability reduction strategies in order to identify opportunities for multiple benefits and to avoid unfavourable interactions. This research builds on the approach developed by Geist and Lambin (2001) who conducted an evaluation of deforestation, in which a structured review of sub-national cases allowed for the identification and analysis of interactions among the dominant forces driving more localised patterns of deforestation. Vulnerability reduction efforts in recent years have established the necessary foundation for such an effort in the field of vulnerability research. Focussing on vulnerability to environmental change, this paper will address such issues as, differential gender vulnerability, the role of institutions in reducing vulnerability, and the interaction of multiple processes operating at different spatial and temporal scales. References Geist, Helmut J., and Eric F. Lambin. 2001. What Drives Tropical Deforestation? A meta-analysis of proximate and underlying causes of deforestation based on subnational case study evidence. Lovain-la-Neuve, Belgium: LUCC International Project Office."
63b5c9371ac8dc511e109a02947ffdff6438c179,
75ab5c75a2b8a7dd44369da6da1d395a587738e1,"The unjust distributional consequences of climate change, and its potentially negative aggregate effect on economic growth and welfare are two reasons to be concerned about climate change. Our knowledge of the impact of climate change is incomplete. Monetary valuation is difficult and controversial. The effect of other developments on the impacts of climate change is largely speculative. Nonetheless, it can be shown that poorer countries and people are more vulnerable than are richer countries and people. A modest global warming is likely to have a net negative effect on poor economics in hot climates, but may have a positive effect on rich economies in temperate climates. If one counts dollars, the world aggregate impact may be positive. If one counts people, the world aggregate effect is probably negative. For more substantial warming, negative effects become more negative, and positive effects turn negative. The marginal costs of carbon dioxide emissions are uncertain and sensitive to assumptions that partially reflect ethical and methodological positions, but are unlikely to exceed $50 per tonne of carbon. The marginal costs of methane emission are likely to be less than $250/tCH4; the marginal costs of nitrous oxide emissions are probably lower than $7000/tN2O. Global warming potentials, the official manner to trade-off the various greenhouse gases, do not reflect, conceptually or numerically, the real tradeoffs in either a cost-benefit or a cost-effectiveness framework."
817015dd7d587636c48a21e6e61a9e2af5f0238f,
839639f938255503dc4044af76da0fbdb9c5f707,
997bf099e1edb79b7d4f91a29c72554a64751491,"Abstract This workshop describes the U.S. Government Printing Office's (GPO) team-based approach to efficiently identify, catalog, and provide electronic access to online serials published by more than five hundred U.S. Government publishers. Topics include publishing practices of Federal agencies, the process of selecting serials for GPO's Electronic Collection, and requirements for authenticity and textual integrity. Additional topics include GPO's application of CONSER's single record option, use of OCLC's persistent uniform resource locator (PURLs) for electronic access, GPO's efforts to provide “permanent public access” to online resources, and its partnerships with other institutions. Government agencies are improving their Website publishing practices and more and more works are being made available only online."
af6e3c71922983b5e42e55d89a88b63854803a38,
f1a67a67251e3639c038772a6b52908e712aa609,"This paper introduces broad concepts of vulnerability, food security and famine. It argues that the concepts and theories driving development and implementation of vulnerability assessment tools are related to their utility. The review concludes that socio-geographic scale is a key issue, and challenge. It analyses three vulnerability assessment (VA) methods, using Ethiopia as a case study. Facing the challenges of vulnerability assessment and early warning requires providing accurate information at the required scale, useful for multiple decision-makers within realistic institutional capacities."
1dd96ea0f2b94613aef3165ba5f62398a072501b,
1ea15516ca1f74ff012a1b1a0eebd4f4eee09cf0,
1ea9898ae6b658b239ec0fa7f589bd3d5b9eb1c9,
42054a822da8b9065517b219c3a4263d30465501,"Author(s): Lambin, E; Chasek, P; Downing, T; Kerven, C; Kleidon, A; Leemans, R; Ludeke, M; Prince, S; Xue, Y | Editor(s): Reynolds, J; Smith, D"
5df860f60109b48c862b8904e6b579b651d3af12,
908b7f4266dc458cee2f5fc21b01a5a7aef1c885,
92c902d5fbf44a688eb0b67723819e9d0d3c28e7,
a77681e17e4701c36e924de464fcf5a9eacfef88,"Abstract Seasonal maize water-stress forecasts were derived for area averages of the primary maize-growing regions of South Africa and Zimbabwe. An agroclimatological model was used to create a historical record of maize water stress as a function of evapotranspiration for 1961–94. Water stress, the primary determinant of yield in water-limited environments such as southern Africa, was correlated with two well-known indices of the El Nino– Southern Oscillation: the Southern Oscillation index (SOI) and the Nino-3 region of the equatorial Pacific. Forecasts for South Africa using only the SOI at a 4-month lead yielded a hindcast correlation of 0.67 over 17 seasons (1961–78) and a forecast correlation of 0.69 over 16 seasons (1978–94). Forecasts for Zimbabwe were less remarkable."
c59100e64fd9c58f9e4656275358199bf579c777,
c9af04a30590a3f1de9e42a6cf750817810e0cce,
d55049347e490f387adefc6505f4a30b1e2ffdb0,"This paper presents the marginal costs of the emissions of a selected number of radiatively-active 
gases, three uniformly-mixed gases – carbon dioxide, methane, nitrous oxide – and two region-specific gases – nitrogen (from aircraft) and sulphur, which influence ozone and sulphate aerosol concentrations, respectively. The paper complements earlier research by adding a third model (FUND2.0), adding region-specific gases, and by presenting an alternative accounting framework. The discounting and valuation procedures for marginal cost estimation were refined, but the estimates for the three greenhouse gases do not substantially differ from those in earlier research. It should be noted that with the inclusion of new insights into the impacts of climate change, it can no longer be excluded that marginal costs are negative, particularly for methane. The sign of the costs is model and region dependent. Despite their short life-time, the marginal costs of nitrogen and sulphur emissions are relatively large, primarily because they are not much discounted. The results presented should not be taken as final estimates. The impacts covered by the models used are only a fraction (of unknown size) of all climate change impacts. Particularly, large scale disruptions, such as a breakdown of North Atlantic Deep Water formation or a collapse of the West-Antarctic Ice Sheet, are excluded from the analysis."
21e84fe7508624b528a1349adadceaf66b5ffad2,
3e6ea0faab0972b0a85437952355cb4dced68a80,
4f5823764300120b9061d214565d198dc900f9ee,
549a5190ee0fa0a2e73c6d6b3b4999ea5cb76734,
9d70e64605d339efcb61aa67a6b6440df3d14a32,
b83a671afe747f1f7e8aada380f4773db3679e25,
d0664a2889a608a68786d16693351c02a40fbaa8,"Foreword Preface 1. Climate, Change and Risk: Introduction 2. Climate Change, Climatic Hazards and Policy Responses in Australia 3. Assessing the Potential Effects of Climate Change on Clay Shrinkage Induced Land Subsidence 4. Agricultural Drought in Europe: Site, Regional and National Effects of Climate Change 5. Flooding in a Warmer World 6. A Concise History of Riverine Floods and Flood Management in the Dutch Rhine Delta 7. Hurricane Impacts in the Context of Climate Variability, Climatic Change and Coastal Management Policy on the Eastern US Sea-Board 8. On Assessing the Economic Impacts of Sea Level Rise on Developed Coasts 9. Tropical Cyclones in the Southwest Pacific: Impacts on Pacific Island Countries with Particular Reference to Fiji 10. Impacts of Windstorms in the Netherlands: Present Risk and Prospects for Climate Change 11. Heat Waves in a Changing Climate 12. Economic Analysis of Natural Disasters 13. An Analytical Review of Weather Insurance 14. Identifying Barriers and Opportunities for Policy Responses to Changing Climatic Risks"
e5f705a9035fd0410a8619a6807367e269e1c958,
f62e91b4acf71c4ced607268f16084364156c9d8,
2cfe671e6c0e554983fb63beb95eca83bdb107df,
a624b3acac6308eac66aac0d3d6995d1a4e66fd6,
106b9e369ee0f9bd3e9a2fe4477074e2017d33d5,
39fbbab66f009d2f6308ad2dcac2d60e242826fb,
5960feb0554703d73413b2618d164ef0e332aabe,
dbc98a89567a10981f1437bd9f937e66e6c4bdba,
82d952d60aafc9733ee7e43d71e5c0c0790f3b75,
244151da0ca6f62ff5704c783b02f057b3f520a7,
4782347ae09e40d1d78cbaa619b819e40cf584c3,
573b20b8812576e17fc364a40c5c18ea563a9230,
798141f3b3c8cb8925a1fa09d654bfba74ab7f75,
798cf1ee8bc685698b8e271c13d7bf71351c66bd,
a03221d5c0a5a5c33733621936df9b28a14a5c83,
b3f36f27cc403aab7518cd7a182bab183774874d,
f718a123d3e3e55a5796b3dc6376abbdc2496eb5,
5606468e8596688cd2c574d55d017beb87e0cd56,
733c2ce616cf952ced76ab22f8c86c06c95608af,
75c34536e4ea650369106b470a4a27e9048171ff,
a34c173cce6d9dbd55e29d66b1f5d148234263af,
b938c668b262e87fd614a72034752f7ddee39f51,
d4a02664fb4a1a5b47b46dee34837c651cbe6a07,
f290289d67b324f03192dba094bf2243b3748e4e,
19bebc3e011ed02f99b84452a7b89be71ef74589,
9b410c05ced81693a007e9968b374f116b8aa319,
e2236c7c7c06fa54d84a03049810880117615f9f,
718a61d9e711b059113f510f2fee084fa1a4d64e,
a75e7b228443b4ec20e3821847f2234649f58551,
06ad384bd8fecfd68ed05394649cc53dd2055271,
154895062ce4d27aa1aa2bf39f46645c39c63641,"The geographic and temporal scale of institutional responses to food crises suggests three levels of food information or famine early warning system: a seasonal national food balance, baseline data on household food poverty and estimates of vulnerability to climatic and economic variations, and targeted interventions based on individual entitlements and food deprivation. Stimulating the demand for food information, beyond the need to forecast famines, is a crucial factor in the adoption of improved monitoring systems. Issues in the design of food information systems are illustrated by the experience in Kenya in 1984-85. The government of Kenya responded to the 1984 drought and ensuing food crisis to prevent widespread famine, largely through timely commercial imports of yellow maize. Although qualified by the nature of the drought and Kenya's economic development, this success story emphasises the need to improve food information systems."
25c171fbf8fb136624d3f90454a00809ee597905,
8976c5c9e53bfeae56ee55af3db6e0a9ce79e39d,"This analysis of population growth and environmental changes in Central and Eastern Kenya illustrates two models of the interactions of population and resources: (1) the classical neo-Malthusian model that relates population growth to conservation of natural resources but ultimate degradation, and (2) the proposition that population growth induces intensified agricultural production and environmental conservation."
da0fc8b7743fd9d9c596503742a5072b69f27d58,
103119e5176ecb0fd34d631e2bf6a31ae2de21a1,
1bed1ff77c38b9c4fc888ad1207d2539f5a61d3c,Various conditions of food shortage comprise: 1) not enough food; 2) food poverty where they may be sufficient food but people have no access to it; 3) and food deprivation where they may be sufficient food but it is withheld from individuals nutritional needs are not met or illness prevents proper absorption. In 1985 1485 million people 30% of the world population lived in countries where the total dietary energy supply was less than that required for health growth and productive work. Of these 46 countries 20 were located in sub-Saharan Africa 8 in South and Southeast Asia and 6 in the Western hemisphere. In 1987 the New York Times reported famine in 16 countries with a combined population of 180 million people. During 1985-87 the population of famine-plagued countries averaged 170 million. Using a low estimate of Chinas food poverty (70 million) it is estimated that 950 million people or 19% of the world population were food poor in 1985. Hungers toll on children is experienced in their low birth weights in the wasting and stunting of their growth in their diarrheal-related deaths and in Asia in death or blindness caused by Vitamin A deficiency and cretinism and less severe mental/motor impairments caused by iodine deficiency. Anemia caused by iron deficiency and malaria affects all people but especially women of reproductive age and very young children. About 1/2 of the worlds pregnant woman and 1/2 of all women have low blood hemoglobin levels. Cereal food aid from July 1986 to June 1987 totaled 12.2 million metric tons amounting to 7% of world trade in cereals and 11% of the total imports of developing countries. 1/2 of the aid went to Africa including 25% to sub-Saharan Africa and much of the remainder went to other low-income food-deficit countries. UNICEF has targeted child survival with growth monitoring oral rehydration breast feeding and immunization.
6af9f3e5cc6ddfdc95d169ed951c054c7801c910,
6f4a2b31572af29c838419428d7f21fa521ab3ea,
b9a9fc3eecd15c601fbaa932914ac75b81e75e94,
cc4bd6a599c36f8ce3997afd7219a6a81ab6e7f8,
18dadd8031454423078e1f2cfbc09bcc1b8e96af,
c1c6165b182068fcab56532610841fff36fdbfc8,
837fdcae9b3976ab675007f537e1b0f05fa72880,"Although environmental problems are common in many countries or affect large regions, the threats of carbon dioxide-induced climate change and depletion of stratospheric ozone are perhaps the most credible global environmental problems presently facing humankind. While there are many differences between the CO2 and 03 problems the diffuse, global hazards share a number of common characteristics. The threats are due to anthropogenic chemical changes of a slow, cumulative nature. Early effects may be disguised by normal environmental variation which generates considerable scientific uncertainty. By the time all the mechanisms and consequences are known and evaluated, it may be too late to avoid the majority of effects which may persist for centuries. In addition, the climate or stratosphere so affected is not the property of any individual or nation. The vulnerable natural and social systems require on-going monitoring, research, and risk assessment as uncertainties are defined and, hopefully, reduced. In short these are threats to the global commons for which there is no precedent in human history. This paper examines the different national responses to the threat of chlorofluorocarbon (CFC) depletion of stratospheric ozone as the first of the global atmospheric problems for which nations have taken concrete action beyond scientific study and risk assessment."
348ee76f33454ff557187f648ed8d50c1fcaa338,
7f8940f612a0ebfbdb5d51fe0b116454d389a2bb,"We give a randomized 1+5.06/√k-approximation algorithm for the minimum k-edge connected spanning multi-subgraph problem, k-ECSM."
116da63acb9db8e4b03d154fc648f48b2a68d394,"A matroid M on a set E of elements has the α-partition property, for some α > 0, if it is possible to (randomly) construct a partition matroid P on (a subset of) elements of M such that every independent set of P is independent in M and for any weight function w : E → R≥0, the expected value of the optimum of the matroid secretary problem on P is at least an α-fraction of the optimum on M. We show that the complete binary matroid, Bd on Fd2 does not satisfy the α-partition property for any constant α > 0 (independent of d). Furthermore, we refute a recent conjecture of [BSY21] by showing the same matroid is 2/d-colorable but cannot be reduced to an α2/d-colorable partition matroid for any α that is sublinear in d. dornaa@cs.washington.edu. Research supported by NSF grant CCF-1907845 and Air Force Office of Scientific Research grant FA9550-20-1-0212. karlin@cs.washington.edu. Research supported by Air Force Office of Scientific Research grant FA9550-20-10212 and NSF grant CCF-1813135. nwklein@cs.washington.edu. Research supported in part by NSF grants DGE-1762114, CCF-1813135, and CCF-1552097. shayan@cs.washington.edu. Research supported by Air Force Office of Scientific Research grant FA9550-201-0212, NSF grant CCF-1907845, and a Sloan fellowship."
a26a07d61bdbf99dd385caeae25ce68001be7bbc,
4b31981bb53a068d9251f328f0bf3d7efbcc85d5,For some > 10−36 we give a randomized 3/2− approximation algorithm for metric TSP.
5fb82181bce25dcf3b97c19e40e600773db613fa,
9460acfb5d1265a6ab6b23d31f837c88be80ec5a,"We investigate non-adaptive algorithms for matroid prophet inequalities. Matroid prophet inequalities have been considered resolved since 2012 when [KW12] introduced thresholds that guarantee a tight 2-approximation to the prophet; however, this algorithm is adaptive. Other approaches of [CHMS10] and [FSZ16] have used non-adaptive thresholds with a feasibility restriction; however, this translates to adaptively changing an item's threshold to infinity when it cannot be taken with respect to the additional feasibility constraint, hence the algorithm is not truly non-adaptive. A major application of prophet inequalities is in auction design, where non-adaptive prices possess a significant advantage: they convert to order-oblivious posted pricings, and are essential for translating a prophet inequality into a truthful mechanism for multi-dimensional buyers. The existing matroid prophet inequalities do not suffice for this application. We present the first non-adaptive constant-factor prophet inequality for graphic matroids."
b13236f9dc3ea1deaa5ce3603a118e0c491e8a80,"Pairing Things Off: Counting Stable Matchings, Finding Your Ride, and Designing Tournaments"
f6af50f454be5deaf29ccda61b092240fbdc48dd,
a52168fac4d71a562fe2a2e96da87535c283faa7,"We design a 1.49993-approximation algorithm for the metric traveling salesperson problem (TSP) for instances in which an optimal solution to the subtour linear programming relaxation is half-integral. These instances received significant attention over the last decade due to a conjecture of Schalekamp, Williamson and van Zuylen stating that half-integral LP solutions have the largest integrality gap over all fractional solutions. So, if the conjecture of Schalekamp et al. holds true, our result shows that the integrality gap of the subtour polytope is bounded away from 3/2."
d21a528020d3ef49b435dd23eb077977647d5e42,"We study combinatorial auctions with interdependent valuations. In such settings, each agent $i$ has a private signal $s_i$ that captures her private information, and the valuation function of every agent depends on the entire signal profile, ${\bf s}=(s_1,\ldots,s_n)$. The literature in economics shows that the interdependent model gives rise to strong impossibility results, and identifies assumptions under which optimal solutions can be attained. The computer science literature provides approximation results for simple single-parameter settings (mostly single item auctions, or matroid feasibility constraints). Both bodies of literature focus largely on valuations satisfying a technical condition termed {\em single crossing} (or variants thereof). 
We consider the class of {\em submodular over signals} (SOS) valuations (without imposing any single-crossing type assumption), and provide the first welfare approximation guarantees for multi-dimensional combinatorial auctions, achieved by universally ex-post IC-IR mechanisms. Our main results are: $(i)$ 4-approximation for any single-parameter downward-closed setting with single-dimensional signals and SOS valuations; $(ii)$ 4-approximation for any combinatorial auction with multi-dimensional signals and {\em separable}-SOS valuations; and $(iii)$ $(k+3)$- and $(2\log(k)+4)$-approximation for any combinatorial auction with single-dimensional signals, with $k$-sized signal space, for SOS and strong-SOS valuations, respectively. All of our results extend to a parameterized version of SOS, $d$-SOS, while losing a factor that depends on $d$."
36654adb696391ffb30795dc592576901b630bcc,"Stable matching is a classical combinatorial problem that has been the subject of intense theoretical and empirical study since its introduction in 1962 in a seminal paper by Gale and Shapley. In this paper, we provide a new upper bound on f(n), the maximum number of stable matchings that a stable matching instance with n men and n women can have. It has been a long-standing open problem to understand the asymptotic behavior of f(n) as n→∞, first posed by Donald Knuth in the 1970s. Until now the best lower bound was approximately 2.28n, and the best upper bound was 2nlogn− O(n). In this paper, we show that for all n, f(n) ≤ cn for some universal constant c. This matches the lower bound up to the base of the exponent. Our proof is based on a reduction to counting the number of downsets of a family of posets that we call “mixing”. The latter might be of independent interest."
48e284957ac29ae6565e152cb7997d92bb7d0425,"We are given a training set S = {(xi, yi), 1 ≤ i ≤ m}, where each xi is an instance in some space X, e.g. a feature vector over Rd, and yi is a binary label (classification). For example, xi could be a set of features of an email message and y could be a label indicating whether it is spam or not. We assume that yi = f(xi) where f : X → {0, 1} is the correct labeling of the message, i.e., the ground truth. We also assume that each xi is drawn independently from some distribution D over the instance space."
7b8a03fdb29417b7f41cd4d60d064dc392930d39,
b91473d50f087f79cfe2e33f211942e81552528b,"s: Speakers should submit abstracts on the easy-to-use interactive Web form. No knowledge of L ATEX is necessary to submit an electronic form, although those who use LATEX may submit abstracts with such coding, and all math displays and similarily coded material (such as accent marks in text) must be typeset in LATEX. Visit www.ams.org/ cgi-bin/abstracts/abstract.pl/ . Questions about abstracts may be sent to absinfo@ams.org. Close attention should be paid to specified deadlines in this issue. Unfortunately, late abstracts cannot be"
d972d1f59b5b5821cebe213c79fce4b7fbd7ebcd,"We consider time-of-use pricing as a technique for matching supply and demand of temporal resources with the goal of maximizing social welfare. Relevant examples include energy, computing resources on a cloud computing platform, and charging stations for electric vehicles, among many others. A client/job in this setting has a window of time during which he needs service, and a particular value for obtaining it. We assume a stochastic model for demand, where each job materializes with some probability via an independent Bernoulli trial. Given a per-time-unit pricing of resources, any realized job will first try to get served by the cheapest available resource in its window and, failing that, will try to find service at the next cheapest available resource, and so on. Thus, the natural stochastic fluctuations in demand have the potential to lead to cascading overload events. Our main result shows that setting prices so as to optimally handle the expected demand works well: with high probability, when the actual demand is instantiated, the system is stable and the expected value of the jobs served is very close to that of the optimal offline algorithm."
1e9bc5df3995407dd22637489db9b54a04e88b74,
5ed2a73d2c0d24a4f7a9ee10506b52fb4f3972e2,"Remember that Time is Money” — Benjamin Franklin in Advice to a Young Tradesman (1748) Consider the following setting: a customer has a package and is willing to pay up to some value v to ship it, but needs it to be shipped by some deadline d. Given the joint prior distribution from which (v, d) pairs are drawn, we characterize the auction that yields optimal revenue, contributing to the limited understanding of optimal auctions beyond single-parameter settings. Our work requires a new way of combining and ironing revenue curves which illustrate why randomization is necessary to achieve optimal revenue. Finally, we strengthen the emerging understanding that duality is useful for both the design and analysis of optimal auctions in multi-parameter settings."
78ad2bd3002c61f92395f486ad8a9742073796b7,"Consider the pricing problem faced by FedEx. Each customer has a package to ship, a deadline $d$ by which he needs his package to arrive, and a value $v$ for a guarantee that the package will arrive by his deadline. FedEx can (and does) offer a number of different shipping options in order to extract more revenue from their customers. In this paper, we solve the optimal (revenue-maximizing) auction problem for the single-agent version of this problem. Our paper adds to the relatively short list of multi-parameter settings for which a closed-form solution is known."
be4dd5952ed9c914f4e83a30d48f8f640a96090b,"We consider a pricing problem where a buyer is interested in purchasing/using a good, such as an app or music or software, repeatedly over time. The consumer discovers his value for the good only as he uses it, and the value evolves with each use. Optimizing for the seller's revenue in such dynamic settings is a complex problem and requires assumptions about how the buyer behaves before learning his future value(s), and in particular, how he reacts to risk. We explore the performance of a class of pricing mechanisms that are extremely simple for both the buyer and the seller to use: the buyer reacts to prices myopically without worrying about how his value evolves in the future; the seller needs to optimize for revenue over a space of only two parameters, and can do so without knowing the buyer's risk profile or fine details of the value evolution process. We present simple-versus-optimal type results, namely that under certain assumptions, simple pricing mechanisms of the above form are approximately optimal regardless of the buyer's risk profile. Our results assume that the buyer's value per usage evolves as a martingale. For our main result, we consider pricing mechanisms in which the seller offers the product for free for a certain number of uses, and then charges an appropriate fixed price per usage. We assume that the buyer responds by buying the product for as long as his value exceeds the fixed price. Importantly, the buyer does not need to know anything about how his future value will evolve, only how much he wants to use the product right now. Regardless of the buyers' initial value, our pricing captures as revenue a constant fraction of the total value that the buyers accumulate in expectation over time."
c089ca0eb0eca16cd92e8862d82c459e049806dd,"We consider the online carpool fairness problem of Fagin–Williams (IBM J Res Dev 27(2):133–139, 1983), where an online algorithm is presented with a sequence of pairs drawn from a group of n potential drivers. The online algorithm must select one driver from each pair, with the objective of partitioning the driving burden as fairly as possible for all drivers. The unfairness of an online algorithm is a measure of the worst-case deviation between the number of times a person has driven and the number of times they would have driven if life was completely fair."
20dd97ba6ae18b5bee5c1268ca225c52e7d8d9c9,"We study revenue maximization in settings where agents' values are interdependent: each agent receives a signal drawn from a correlated distribution and agents' values are functions of all of the signals. We introduce a variant of the generalized VCG auction with reserve prices and random admission, and show that this auction gives a constant approximation to the optimal expected revenue in matroid environments. Our results do not require any assumptions on the signal distributions, however, they require the value functions to satisfy a standard single-crossing property and a concavity-type condition."
56dac1aeb24a5864b35c18c578cefec246e26722,"We consider pricing in settings where a consumer discovers his value for a good only as he uses it, and the value evolves with each use. We explore simple and natural pricing strategies for a seller in this setting, under the assumption that the seller knows the distribution from which the consumer's initial value is drawn, as well as the stochastic process that governs the evolution of the value with each use. 
We consider the differences between up-front or ""buy-it-now"" pricing (BIN), and ""pay-per-play"" (PPP) pricing, where the consumer is charged per use. Our results show that PPP pricing can be a very effective mechanism for price discrimination, and thereby can increase seller revenue. But it can also be advantageous to the buyers, as a way of mitigating risk. Indeed, this mitigation of risk can yield a larger pool of buyers. We also show that the practice of offering free trials is largely beneficial. 
We consider two different stochastic processes for how the buyer's value evolves: In the first, the key random variable is how long the consumer remains interested in the product. In the second process, the consumer's value evolves according to a random walk or Brownian motion with reflection at 1, and absorption at 0."
cf058754634fa6afa7b62e200414b9e697a9675a,"We study the dynamics of multiround position auctions, considering both the case of exogenous click-through rates and the case in which click-through rates are determined by an endogenous consumer search process. In both contexts, we demonstrate that dynamic position auctions converge to their associated static, envy-free equilibria. Furthermore, convergence is efficient, and the entry of low-quality advertisers does not slow convergence. Because our approach predominantly relies on assumptions common in the sponsored search literature, our results suggest that dynamic position auctions converge more generally."
9a67ab4e1d890114936f56c2dd9f09ac5c6a94fc,
c9bbfc84b8b18c6a0b1bc51dcf45d6a80117113b,"We introduce and study strongly truthful mechanisms and their applications. We use strongly truthful mechanisms as a tool for implementation in undominated strategies for several problems, including the design of externality resistant auctions and a variant of multi-dimensional scheduling."
dc418049358ec75620b36354ed22998e25206c2f,"Surveys conducted by human interviewers are one of the principal means of gathering data from all over the world, but the quality of this data can be threatened by interviewer fabrication. In this paper, we investigate a new approach to detecting interviewer fabrication automatically. We instrument electronic data collection software to record logs of low-level behavioral data and show that supervised classification, when applied to features extracted from these logs, can identify interviewer fabrication with an accuracy of up to 96%. We show that even when interviewers know that our approach is being used, have some knowledge of how it works, and are incentivized to avoid detection, it can still achieve an accuracy of 86%. We also demonstrate the robustness of our approach to a moderate amount of label noise and provide practical recommendations, based on empirical evidence, on how much data is needed for our approach to be effective."
f97507f8883a1a2b90382f8506a42ff01237d82f,"We consider prior-free benchmarks in non-matroid settings. In particular, we show that a very desirable benchmark proposed by Hartline and Roughgarden is too strong, in the sense that no truthful mechanism can compete with it even in a very simple non-matroid setting where there are two exclusive markets and the seller can only sell to agents in one of them. On the other hand, we show that there is a mechanism that competes with a symmetrized version of this benchmark. We further investigate the more traditional best fixed price profit benchmark and show that there are mechanisms that compete with it in any downward-closed settings."
3b9abf67a3c1922047ca5d112b6a38a5c22bc126,"Surveys are one of the principal means of gathering critical data from low-income regions. Bad data, however, may be no better—or worse—than no data at all. Interviewer data fabrication, one cause of bad data, is an ongoing concern of survey organizations and a constant threat to data quality. In my dissertation work, I build software that automatically identifies interviewer fabrication so that supervisors can act to reduce it. To do so, I draw on two tool sets from computer science, one algorithmic and the other technological. On the algorithmic side, I use two sets of techniques from machine learning, supervised classification and anomaly detection, to automatically identify interviewer fabrication. On the technological side, I modify data collection software running on mobile electronic devices to record user traces that can help to identify fabrication. I show, based on the results of two empirical studies, that the combination of these approaches makes it possible to accurately and robustly identify interviewer fabrication, even when interviewers are aware that the algorithms are being used, have some knowledge of how they work, and are incentivized to avoid detection."
6d25cf4c4d41a3c65bffc869486beab82b51ef73,"
 
 In many real-world auctions, a bidder does not know her exact value for an item, but can perform a costly deliberation to reduce her uncertainty. Relatively little is known about such deliberative environments, which are fundamentally different from classical auction environments. In this paper, we propose a new approach that allows us to leverage classical revenue-maximization results in deliberative environments. In particular, we use Myerson (1981) to construct the first non-trivial (i.e., dependent on deliberation costs) upper bound on revenue in deliberative auctions. This bound allows us to apply existing results in the classical environment to a deliberative environment. In addition, we show that in many deliberative environments the only optimal dominant-strategy mechanisms take the form of sequential posted-price auctions.
 
"
a68b027641c093594620ef5fa51cc6360827278d,"In this thesis I focus on two simple yet fundamental observations which require further investigation as the field of algorithmic game theory progresses in the context of information economics. Specifically: 1) Access to information widens an agent's strategy space, and 2) the generation and exchange of information between agents is itself a game. 
Informed Valuations: Increasingly sophisticated consumer tracking technology gives advertisers a wealth of information which they use to reach narrowly targeted consumer demographics. With targeting, advertisers are able to bid differently depending on the age, location, computer, or even browsing history of the person viewing a website. 
Using historical bidding data from a large premium advertising exchange, we show that the bidding distributions are unfavorable to the standard mechanisms. This motivates our new BIN-TAC mechanism, which is simple and effective in extracting revenue in the presence of targeting information. Bidders can ""buy-it-now"", or alternatively ""take-a-chance"" in an auction, where the top d > 1 bidders are equally likely to win. The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now. We show that for a large class of distributions, this mechanism outperforms the second-price auction, and achieves revenue performance close to Myerson's optimal mechanism. We apply structural methods to our data to estimate counterfactual revenues, and find that our BIN-TAC mechanism improves revenue by 4.5% relative to a second-price auction with optimal reserve. 
Information Acquisition: A prevalent assumption in traditional mechanism design is that the buyers know their precise value for an item; however, this assumption is rarely accurate in practice. Judging the value of a good is difficult since it may depend on many unknown parameters such as the intrinsic quality of the good, the saving it will yield in the future, or the buyer's emotional state. Additionally, the estimated value for a good is not static; buyers can ""deliberate"", i.e. spend money or time, in order to refine their estimates by acquiring additional information. This information, however, comes at some cost, either financial, computational or emotional. It is known that when deliberative agents participate in traditional auctions, surprising and often undesirable outcomes can occur. 
We consider optimal dominant strategy mechanisms for one-step deliberative setting where each user can determine their exact value for a fixed cost. We show that for single-item auctions under mild assumptions these are equivalent to a sequential posted price mechanism. Additionally, we propose a new approach that allows us to leverage classical revenue-maximization results in deliberative environments. In particular, we use Myerson (1981) to construct the first non-trivial (i.e., dependent on deliberation costs) upper bound on revenue in deliberative auctions. This bound allows us to apply existing results in the classical environment to a deliberative environment; specifically, for single-parameter matroid settings, sequential posted pricing is a 2-approximation or better. 
Exchange Networks: Information is constantly being exchanged in many forms; i.e. communication among friends, company mergers and partnerships, and more recently, selling of user information by companies such as BlueKai. Exchange markets capture the trade of information between agents for profit, and we wish to understand how these trades are agreed upon. Understanding information markets helps us determine the power and influence structure of the network. To do this, we consider a very general network model where nodes are people or companies, and weighted edges represent profitable “potential” exchanges of information, or any other good. Each node is capable of finalizing an exactly one of its possible transactions; this models the situation where some form of exclusivity is involved in the transaction. This model is in fact very general, and can capture everything from targeting information exchange to the marriage market. (Abstract shortened by UMI.)"
3fccfc78c04b9e140c5c68e118de1f5d61421ea4,"We study the design of truthful mechanisms for maximizing a seller's profit in a variety of settings that go beyond the traditional Bayesian approach from economics: 
· Prior-free mechanism design. In the first part of this thesis, we study the problem of designing truthful mechanisms to maximize a seller's profit without prior knowledge of buyers' valuations, using the competitive framework pioneered in [20] and [19], which involves defining an appropriate profit benchmark and then designing mechanisms that approximate the benchmark on every instance. Working with one of the simplest settings that was not well understood: when a seller can sell to any number of buyers in one of a number of exclusive markets, we show that the profit benchmark proposed by Hartline and Roughgarden [24] is unattainable—that is, no truthful in expectation mechanism can be constant competitive to it. On the other hand, we propose a natural, symmetrized version of this benchmark and show that it is approximable in our setting. 
· Prior-independent mechanism design. In the second part, we consider prior-independent mechanism design for unit-demand combinatorial auctions. In a these auctions, an auctioneer is selling a collection of different items to a set of agents, each has a different private value for each item and is interested in buying at most one item. We consider the problem of designing a truthful auction that maximizes the auctioneer's profit. Previously, there has been progress on this problem in the Bayesian setting. Specifically, assuming the distributions of the agents' valuations are known, it has been shown how to design auctions tailored to these distributions that achieve high expected profit. In our work, we present the first ""prior-independent"" unit-demand combinatorial auctions. These auctions are guaranteed to achieve a constant fraction of the optimal expected profit, but are designed without any knowledge of the prior distributions that the agents values come from. 
· Mechanism design in deliberative settings. Deliberative settings are those in which even the agents do not know their values with absolute certainty. Instead, they have some initial beliefs and some deliberations they can perform, at some cost to themselves, to reduce their uncertainty. 
We investigate a special case of the deliberative model and show that any dominant-strategy mechanism for single-item auctions in this model is equivalent to a sequential posted price mechanism, extending a result of [33]. Moreover, we give a connection between profit maximization mechanisms in deliberative and classical settings, thereby obtaining approximation mechanisms for a variety of special deliberative environments."
7695c4d5468274234125cbc0e3377652d6c94df7,
9727fbe9198be8acbbe46fe08c4148fa8a1d2f0b,"Thus, using (8) we receive that the competitive ratio c according to the absolute cost C is c = C(online) C(offline) = C on C off < 2 (9) Therefore, from (4) and (9) we receive that the competitive ratio of the online algorithm is c = min(1 + y 2d ; 2) 7 Discussion In this paper we have analyzed the problem of placing an application in a very large distributed system in a good location with high probability, using a small number of steps. We have shown an algorithm that achieves this goal, proved its behavior for a single application and simulated it for multiple applications, and showed that it exhibits a good competitive ratio. Also, we have presented a scalable two-step protocol for placing virtual servers near their clients, which is based on the application placement algorithms we discussed. Yet, there are some other aspects of this problem that are not addressed by this work. For example, a heuristic for making smart choices regarding when a process should move and when it should stay even if it found a better place might improve the average overall cost of an application. That is, even though we have shown a good competitive ratio, this measure only states that in the worst case our protocol is at most twice as bad as an oo-line optimal algorithm. However, it says nothing about the average or common behavior. Another issue is analyzing or simulating a situation in which multiple applications are allowed to simultaneously check the same location for its goodness number. Also, simulating the case when there are diierent weights to diierent applications, and providing a solution to the problem associated with it as presented in Section 4.1 is desirable. This includes combining our protocol with methods that can exchange two or more applications simultaneously , even if neither one would beneet if it where to move by itself. The challenge is to do so in a scalable way. Presumably, it will also have to rely on probabilistic methods. Finally, it would be highly valuable to integrate our protocol from Section 5 into symphony 7], and measure how well it works in a full scale system. Acknowledgements: We would like to thank Hadas Shachnai for helping us with the nal competitive analysis results. that the online and the optimal oo-line algorithms pay for this sequence of j steps is C 0 …"
c6005301a09fccd52ba7b822a93dff21fe51a81e,
e562157e3e5e07b75ead92c382930e8e27632230,
2b851ff961188de39c9944fa534aac5ba1e30bc5,
4be7836250d08eb9d90ab0d5029f7ed0113c2aab,
82cf965b3910e058d7b0bd6380f54e118aa9e7f1,"Sponsored search has grown to be an important share of the advertisement market and a major income source for large internet companies. Its success relies not only on the explosive internet use but also on successful implementation that made it a highly efficient and profitable concept for advertisers and search engines, in particular, through the use of keyword auctions. Naturally, it also attracted significant interest from the research community, where the focus is on modeling and analyzing the sponsored search system using the tools and methods of game theory . 
Up until now, most of the research relies on the simplest models to extract useful results. Unfortunately, practical scenarios present a variety of complicated parameters that lead us to question the practical significance of these models. 
We extend the most common models to encapsulate an aspect of the system that only recently has begun to draw attention; the effect of competing advertisements on the user's actions and subsequently on the advertisers' campaign efficiency. As most of these online campaigns are run in a highly competitive environment, for example when the user has to make a unique purchasing choice, it is known that nearby advertisements impose complicated correlated effects on each other's performance. We present models that take these effects into account while remaining simple enough for us to answer the most basic game-theoretic questions about them such as the presence of equilibria and their efficiency. We also compare our models to the most common model and show their significant advantages. 
We follow our modeling analysis by investigating another approach for improving the current implementations. Although search engines are experimenting with different forms of advertisers' bidding, all of them only allow advertisers to bid on a single auction outcome such as impressions, clicks or conversions. We propose a mechanism that allows the advertisers' to bid in all interested outcomes in a combined fashion. We show that this mechanism has analytical advantages and also showcase through simulations that it performs at least as well as the typical mechanism in the most interesting metrics."
9b22be2ca0bcb96558869ee70f037ac1edc2f777,
01a4444179a7f2f6ba5ebba886ba94f7122d02de,
878546b6d33a4e74967e995b33a0e766249fbe15,"The strength of the competition plays a significant role in the eciency of an online advertising campaign, as well as in traditional ad campaigns: the presence of a competitor’s ad makes the customer less likely to buy one’s own product. Models for keyword auctions usually assume that, for the advertiser, the value of a click is fixed and independent of the other sponsored ads in the auctions result. However in reality, all clicks are not created equal — the ones leading to a conversion or a purchase are definitely more important. How is the customer’s decision aected by the set of competing advertisements presented to him? We propose a new valuation model for keyword auctions that is competitionaware and takes into account the entire set of sponsored results present in the auction. We study properties of our model under the two most interesting mechanisms (GSP, VCG), both analytically and in simulations."
8d616a3f02b0942762e4994baf336d88446a2546,"With the development of the Internet and new technologies, network effects are recognized as a critical part of the industrial organization and play a fundamental role in expanding the economics of Information Technology industries. Network effects occur when the value or utility of a product or service to an individual varies depending on the number and possibly the set of other individuals who own the product or service. 
Our focus is to understand the role of network effects and study the following questions in different settings: (1) What is the best way to take advantage of network effects to market a new product? We study the question in a viral marketing model with diffusion of information through a social network. We model the question as an optimization problem and show a strong inapproximability result. Our result continues to hold for a few special settings and answers a complexity question proposed in [54, 119]. (2) What is the best way to reach and evaluate social optimum? Motivated by applications in online dating and kidney exchange, we study a stochastic matching problem, where nodes represent agents in a matching market with dichotomous preferences, i.e., each agent finds every other agent either acceptable or unacceptable and is indifferent between all acceptable agents. The goal is to produce a matching between acceptable agents of maximum size. We give a simple greedy strategy for selecting probes which produces a matching whose cardinality is, in expectation, at least a quarter of the size of this optimal algorithm's matching. We additionally show that variants of our algorithm can handle more complicated constraints, such as a limit on the maximum number of rounds, or the number of pairs probed in each round. 
To evaluate social optimum in terms of competition among agents, we propose the notion of cheap labor cost in ""hiring a team"" markets to characterize how much better off a market can be for a consumer by removing a subset of agents. We present tight bounds on the cheap labor cost for a variety of markets including s-t path markets, matroid markets and perfect bipartite matching markets. (3) What is the best way to maximize the total revenue given the existence of network effects? We study the question in an envy-free pricing setting with metric substitutability among items. While the general envy-free pricing problem is hard to approximate, the addition of metric substitutability constraints allows us to solve the problem exactly in polynomial time by reducing it to an instance of weighted independent set on a perfect graph. When the substitution costs do not form a metric, even in cases when a (1 + e)-approximate triangle inequality holds, the problem becomes NP-hard. Our results show that the triangle inequality is the exact sharp threshold for the problem of going from ""tractable"" to ""hard""."
999a424936e0d0f515f32be958311e6695dacb31,"This paper considers a general setting for structured procurement and the problem a buyer faces in designing a procurement mechanism to maximize profit. This brings together two agendas in algorithmic mechanism design, frugality in procurement mechanisms (e.g., for paths and spanning trees) and profit maximization in auctions (e.g., for digital goods). In the standard approach to frugality in procurement, a buyer attempts to purchase a set of elements that satisfy a feasibility requirement as cheaply as possible. For profit maximization in auctions, a seller wishes to sell some number of goods for as much as possible. We unify these objectives by endowing the buyer with a decreasing marginal benefit per feasible set purchased and then considering the problem of designing a mechanism to buy a number of sets which maximize the buyer's profit, i.e., the difference between their benefit for the sets and the cost of procurement. For the case where the feasible sets are bases of a matroid, we follow the approach of reducing the mechanism design optimization problem to a mechanism design decision problem. We give a profit extraction mechanism that solves the decision problem for matroids and show that a reduction based on random sampling approximates the optimal profit. We also consider the problem of non-matroid procurement and show that in this setting the approach does not succeed."
a587153cf96873b37c91ae2bce19bdbd65127e2a,"Recent work has addressed the algorithmic problem of allocating advertisement space for keywords in sponsored search auctions so as to maximize revenue, most of which assume that pricing is done via a first-price auction. This does not realistically model the Generalized Second Price (GSP) auction used in practice, in which bidders pay the next-highest bid for keywords that they are allocated. Towards the goal of more realistically modeling these auctions, we introduce the Second-Price Ad Auctions problem, in which bidders' payments are determined by the GSP mechanism. We show that the complexity of the Second-Price Ad Auctions problem is quite different than that of the more studied First-Price Ad Auctions problem. First, unlike the first-price variant, for which small constant-factor approximations are known, it is NP-hard to approximate the Second-Price Ad Auctions problem to any non-trivial factor, even when the bids are small compared to the budgets. Second, this discrepancy extends even to the 0-1 special case that we call the Second-Price Matching problem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no deterministic algorithm achieving a non-trivial competitive ratio and no randomized algorithm achieving a competitive ratio better than 2. This contrasts with the results for the analogous special case in the first-price model, the standard bipartite matching problem, which is solvable in polynomial time and which has deterministic and randomized online algorithms achieving better competitive ratios. On the positive side, we provide a 2-approximation for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM. The latter result makes use of a new generalization of a result on the performance of the ""Ranking"" algorithm for online bipartite matching."
e18831417ce7ee5154a70fa9c25c0e919c3438b6,
1ab048fe801c898efa116ce417d3c6207d4bf817,"Mechanism design is a subfield of game theory and microeconomics focused on incentive engineering. A mechanism is a protocol, typically taking the form of an auction, that is explicitly designed so that rational but non-cooperative agents, motivated solely by their self-interest, end up achieving the designer's goals. The challenge of mechanism design is to apply these methods to traditional computer science goals such as worst-case or competitive analysis. We consider two challenging problems related to mechanism design for profit maximization: the analysis of natural bidding strategies used by participants in sponsored search auctions, and the design of a mechanism for matroid procurement with provable performance guarantees. 
The sponsored search auctions of web search engines such as Google or Yahoo! use the generalized second-price (GSP) mechanism, in which bidders do not have a dominant strategy. We develop a framework for studying a variety of greedy bidding strategies and analyze their revenue, convergence and robustness properties. We compare the performance of greedy bidding strategies to that of a Nash equilibrium, quantifying how close these bidding strategies are to one of the most natural and rational stable points of the system. 
In the procurement problem a buyer is given a set of agents with values, along with a family of feasible sets over the agents. The goal is to procure a feasible set of maximum value, for minimum cost. Assuming that a buyer obtains a decreasing marginal benefit per feasible set procured, the problem is to determine the optimal number of feasible sets to procure in order to maximize the buyer's profit. We develop a mechanism that approximates the optimal profit to within a constant factor, when the set system is a matroid. Matroids are important structures in combinatorial optimization: for example, minimum spanning trees and node-weighted maximum matchings are both matroid problems. We also show that the well-known cost sharing revenue extraction mechanism is only truthful for matroid set systems, so that procurement problems over non-matroid set systems are not likely to be solved with current techniques."
21ee629eb652d95c242afb11279f776df86ee491,"We study markets in which consumers are trying to hire a team of agents to perform a complex task. Each agent in the market prices their labor, and based on these prices, consumers hire the cheapest available team capable of doing the job they need done. We define the cheap labor cost in such a market as the ratio of the best Nash equilibrium of the original market and the best possible Nash equilibrium of any of its submarkets, where ""best"" is defined with respect to consumers, i.e., we are looking at Nash equilibria in which the consumer pays the least. This definition is motivated by a ""Braess-style"" paradox: in certain kinds of marketplaces, competition, in the form of the availability of ""cheap labor"", can actually cause the prices paid by consumers to go up.
 We present tight bounds on the cheap labor cost for a variety of markets including s-t path markets, matroid markets and perfect bipartite matching markets. The differences in cheap labor cost across markets demonstrate the complex relationship between the combinatorial structure of the marketplace and the advantages or more precisely, disadvantages to consumers due to competition."
626027b3a13e006c569a223620944e1999ca17c1,"We study the power of ascending auctions in a scenario in which a seller is selling a collection of identical items to anonymous unit'demand bidders. We show that even with full knowledge of the set of bidders' private valuations for the items, if the bidders are ex-ante identical, no ascending auction can extract more than a constant. times the revenue of the best fixed-price scheme. This problem is equivalent to the problem of coming up with an optimal strategy for blowing up indistinguishable balloons with known capacities in order to maximize the amount of contained, air. We show that the algorithm which simply inflates all balloons to a fixed volume is close to optimal in this setting."
910be8ed006a52f3f5d7a7044e7eeb41856d6998,"How should players bid in keyword auctions such as those used by Google, Yahoo! and MSN?allWe consider greedy bidding strategies for a repeated auction on a single keyword, where in each round, each player chooses some optimal bid for the next round, assuming that the other players merely repeat their previous bid. We study the revenue, convergence and robustness properties of such strategies. Most interesting among these is a strategy we call the balanced bidding strategy (BB): it is known that BB has a unique fixed point with payments identical to those of the VCG mechanism. We show that if all players use the BB strategy and update each round, BB converges when the number of slots is at most 2, but does not always converge for 3 or more slots. On the other hand, we present a simple variant which is guaranteed to converge to the same fixed point for any number of slots. In a model in which only one randomly chosen player updates each round according to the BB strategy, we prove that convergence occurs with probability 1.We complement our theoretical results with empirical studies."
abd03ee69aede414cf610a1e8cffaa2cce9afa25,
0b6f6f0ab7c254f8a89e52aca7ed68caf01b72d5,"A concise and factual abstract of no more than 150 words is required. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself. Graphical abstract Although a graphical abstract is optional, its use is encouraged as it draws more attention to the online article. The graphical abstract should summarize the contents of the article in a concise, pictorial form designed to capture the attention of a wide readership. Graphical abstracts should be submitted as a separate file in the online submission system. Image size: Please provide an image with a minimum of 531 × 1328 pixels (h × w) or proportionally more. The image should be readable at a size of 5 × 13 cm using a regular screen resolution of 96 dpi. Preferred file types: TIFF, EPS, PDF or MS Office files. You can view Example Graphical Abstracts on our information site. Authors can make use of Elsevier's Illustration Services to ensure the best presentation of their images and in accordance with all technical requirements. Classification codes Please provide up to 6 standard JEL codes. The available codes may be accessed at JEL. Abbreviations Define abbreviations that are not standard in this field in a footnote to be placed on the first page of the article. Such abbreviations that are unavoidable in the abstract must be defined at their first mention there, as well as in the footnote. Ensure consistency of abbreviations throughout the article. Acknowledgements Collate acknowledgements in a separate section at the end of the article before the references and do not, therefore, include them on the title page, as a footnote to the title or otherwise. List here those individuals who provided help during the research (e.g., providing language help, writing assistance or proof reading the article, etc.). Formatting of funding sources List funding sources in this standard way to facilitate compliance to funder's requirements: Funding: This work was supported by the National Institutes of Health [grant numbers xxxx, yyyy]; the Bill & Melinda Gates Foundation, Seattle, WA [grant number zzzz]; and the United States Institutes of Peace [grant number aaaa]. It is not necessary to include detailed descriptions on the program or type of grants and awards. When funding is from a block grant or other resources available to a university, college, or other research institution, submit the name of the institute or organization that provided the funding. If no funding has been provided for the research, please include the following sentence:"
ded24ca8b73970002e045ac6ac78ff9ba20724ab,
32b6afc2bcc79d8c6e7f192c1b9c2e7f022c1149,"We study the problem of pricing items for sale to consumers so as to maximize the seller's revenue. We assume that for each consumer, we know the maximum amount he would be willing to pay for each bundle of items, and want to find pricings of the items with corresponding allocations that maximize seller profit and at the same time are envy-free, which is a natural fairness criterion requiring that consumers are maximally happy with the outcome they receive given the pricing. We study this problem for two important classes of inputs: unit demand consumers, who want to buy at most one item from among a selection they are interested in, and single-minded consumers, who want to buy one particular subset, but only if they can afford it.We show that computing envy-free prices to maximize the seller's revenue is APX-hard in both of these cases, and give a logarithmic approximation algorithm for them. For several interesting special cases, we derive polynomial-time algorithms. Furthermore, we investigate some connections with the corresponding mechanism design problem, in which the consumer's preferences are private values: for this case, we give a log-competitive truthful mechanism."
3e5444758ab3ad81f873b1798424fb29ef5110e2,"We study truthful mechanisms for auctions in which the auctioneer is trying to hire a team of agents to perform a complex task, and paying them for their work. As common in the field of mechanism design, we assume that the agents are selfish and will act in such a way as to maximize their profit, which in particular may include misrepresenting their true incurred cost. Our first contribution is a new and natural definition of the frugality ratio of a mechanism, measuring the amount by which a mechanism ""overpays "", and extending previous definitions to all monopoly-free set systems. After reexamining several known results in light of this new definition, we proceed to study in detail shortest path auctions and 'r-out-of-k sets"" auctions. We show that when individual set systems (e.g., graphs) are considered instead of worst cases over all instances, these problems exhibit a rich structure, and the performance of mechanisms may be vastly different. In particular, we show that the well-known VCG mechanism may be far from optimal in these settings, and we propose and analyze a mechanism that is always within a constant factor of optimal."
dd0cf18433adf6e77b8db27f696fbbbefeeb485e,"Finally theLagranage dual functionis given by g(~λ, ~ν) = inf~x L(~x,~λ, ~ν) We now make a couple of simple observations. Observation.WhenL(·, ~λ, ~ν) is unbounded from below then the dual takes the value −∞. Observation.g(~λ, ~ν) is concave1 as it is the infimum of a set of affine 2 functions. If x is feasible solution of program (10.2)(10.4), then we have the following L(x,~λ, ~ν) = f0(x) + ∑m i=1 λifi(x) + ∑p j=1 νjhj(x) ≤ f0(x) for ~λ ≥ 0 A functiong(x) is concave is for any0 ≤ α ≤ 1, αg(x) + (1− α)g(y) ≤ g(αx + (1− α)y). That is, linear in{λi} and{νj}."
48386f2b9d0357d2127ff9f2908c3ba510ac26f4,
e5842694aa609595b02da9940a58f45ddb2e9cda,"“Spectral methods” captures generally the class of algorithms which cast their input as a matrix and employ linear algebraic techniques, typically involving the eigenvectors or singular vectors of the matrix. Spectral techniques have had much success in a variety of data analysis domains, from text classification [26] to website ranking [59, 47]. However, little rigorous analysis has been applied to these algorithms, and we are left without a firm understanding of why these approaches work as well as they do. 
In this thesis, we study the application of spectral techniques to data mining, looking specifically at those problems on which spectral techniques have performed well. We will cast each problem into a common mathematical framework, giving a unified theoretical justification for the empirical success of spectral techniques in these domains. Specifically, we present models that justify the prior empirical success of spectral algorithms for tasks such as object classification, web site ranking, and graph partitioning, as well as new algorithms using these techniques for as of yet underdeveloped data mining tasks such as collaborative filtering. We will then take the understanding from this common framework and use it to unify several spectral results in the random graph literature. Finally, we will study several techniques for extending the practical applicability of these techniques, through computational acceleration, support for incremental calculation, and deployment in a completely decentralized environment."
2dbab8733137803e9631c648184c369ab4a38ed9,"We consider the study of a class of optimization problems with applications towards profit maximization. One feature of the classical treatment of optimization problems is that the space over which the optimization is being performed, i.e., the input description of the problem, is assumed to be publicly known to the optimizer. This assumption does not always accurately represent the situation in practical applications. Recently, with the advent of the Internet as one of the most important arenas for resource sharing between parties with diverse and selfish interests, this distinction has become more readily apparent. The inputs to many optimizations being performed are not publicly known in advance. Instead they must be solicited from companies, computerized agents, individuals, etc., that may act selfishly to promote their own self-interests. In particular, they may lie about their values or may not adhere to specified protocols if it benefits them. 
An auction is one of the simplest applications where the classical (a.k.a. public value) optimization approach fails to work as expected in the presence of selfish agents with private data. We consider casting profit optimization problems into the game theoretic framework of mechanism design and consider the design of auction mechanisms to maximize the profit of the auctioneer. We show how a competitive analysis can be used to gauge the performance of profit maximizing mechanisms. We develop a number of techniques for designing auctions and show how they can be extended to more complex profit maximization problems."
aeead2f05880f714c03e836dec0795e730cb6ebb,
202576fe80f8533c5c679049260b198f9924bdd8,
2821be45c7f0d7d7e32fd7e18c1db57a40e8d38b,
2c478f515fa557cfa9757916b6b17f28ea137f91,
588bc243e2b9235d760142492a2798d8f4cf2d54,
9b963f1200f67a5fe2061ff1a52739d9c4045e59,
af7c5ab7580ffb1c8c5ffcb2bb5f7394890b54e5,
fcb8bfd06d7a82f45f924bf7359f0b174dba8240,"We describe mechanisms for auctions that are simultaneously truthful (alternately known as strategy-proof or incentive compatible) and guarantee high ""net"" profit. We make use of appropriate variants of competitive analysis of algorithms in designing and analyzing our mechanisms. Thus, we do not require any probabilistic assumptions on bids.We present two new concepts regarding auctions, that of a cancellable auction and that of a generalized auction. We use cancellable auctions in the design of generalized auctions, but they are of independent interest as well. Cancellable auctions have the property that if the revenue collected does not meet certain predetermined criteria, then the auction can be cancelled and the resulting auction is still truthful. The trivial approach (run a truthful auction and cancel if needed) yields an auction that is not necessarily truthfu.Generalized auctions can be used to model many problems previously considered in the literature, as well as numerous new problems. In particular, we give the first truthful profit-maximizing auctions for problems such as conditional financing and multicast."
ffb394442bcb8b67ba2142b66f63368fa03bc095,"S u n n n a r y In this research, we address a simple question: how much benefit can be achieved by traffic-aware routing of lntemet traffic? Efficient routing on packet-switched networks has attracted considerable ~ h attention from the early days of the Interoet to the present day, yet current routing practice still relies on weighted shortest paths to route traffic, using algorithms that do not take the distribution of traffic demand into account. Compared to trafficaware routing, static muting potentially reduces Internet performance and/or increases operational and infrastructure costs, as networks must overprovision to avoid congestion."
0210fb1b6be4fb8cfa84d8c56c8a4bf14412dd04,
0485ae97c51f0e6c522c3ebf786e46a8fd3ed2c8,"The data migration problem is the problem of computing an efficient plan for moving data stored on devices in a network from one configuration to another. Load balancing or changing usage patterns could necessitate such a rearrangement of data. In this paper, we consider the case where the objects are fixed-size and the network is complete. The direct migration problem is closely related to edge-coloring. However, because there are space constraints on the devices, the problem is more complex. Our main results are polynomial time algorithms for finding a near-optimal migration plan in the presence of space constraints when a certain number of additional nodes is available as temporary storage, and a 3/2-approximation for the case where data must be migrated directly to its destination."
201262b88e8030ccb18006f27d8a8cf487627964,"We introduce backoff processes, an idealized stochastic model of browsing on the world-wide web, which incorporates both hyperlink traversals and use of the “back button.” With some probability the next state is generated by a distribution over out-edges from the current state, as in a traditional Markov chain. With the remaining probability, however, the next state is generated by clicking on the back button, and returning to the state from which the current state was entered by a “forward move”. Repeated clicks on the back button require access to increasingly distant history. We show that this process has fascinating similarities to and differences from Markov chains. In particular, we prove that like Markov chains, backoff processes always have a limit distribution, and we give algorithms to compute this distribution. Unlike Markov chains, the limit distribution may depend on the start state."
620cf63f101f20c03a2e530c287c2603839de15e,"Experimental evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications include (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering --- the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that has been observed but little theoretical study has been done. In this paper we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. These results give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking."
7b48dd041287679374e0ea4a92392e498eada54e,"A hydraulic cylinder operated by ordinary home water pressure is used to actuate a swinging water closet. In accordance with a preferred embodiment of the invention, the water pressure and water discharge is obtained by a simple modification of a standard flush valve."
b7f787454de0d600fbd6b5c81958a31c05461a50,"This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back toward their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed,” source addresses. In this paper, we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed “post mortem”—after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backward compatible, and can be efficiently implemented using conventional technology."
bd52e1810c4b44348b005d87b1fb8b5097635b5a,
c833c9bfabf2c046d4dd6d1c13aab4499c062b9c,"We present the first optimal randomized online algorithms for the TCP acknowledgment problem [5] and the Bahncard problem [7]. These problems are well-known to be generalizations of the classical online ski rental problem, however, they appeared to be harder. In this paper, we demonstrate that a number of online algorithms which have optimal competitive ratios of e/(e-1), including these, are fundamentally no more complex than ski rental. Our results also suggest a clear paradigm for solving ski rental-like problems."
de95ebd2678fcd2b4a5206266bfeb722225dd4ca,
f481c6a71b77d508c8ab51599b759702d3342b7b,"We present a model for web search that captures in a unified manner three critical components of the problem: how the link structure of the web is generated, how the content of a web document is generated, and how a human searcher generates a query. The key to this unification lies in capturing the correlations between these components in terms of proximity in a shared latent semantic space. Given such a combined model, the correct answer to a search query is well defined, and thus it becomes possible to evaluate web search algorithms rigorously. We present a new web search algorithm, based on spectral techniques, and prove that it is guaranteed to produce an approximately correct answer in our model. The algorithm assumes no knowledge of the model, and is well-defined regardless of the model's accuracy."
3e3f9345ede33f4d4274a6aaa1907f09cf583e2a,"We introduce backoffprocesses, an idealized stochastic model o f browsing on the world-wide web, which incorporates both hyperlink traversals and use o f the ""back button"" With some probability the next state is generated by a distribution over out-edges from the current state, as in a traditional Markov chain. With the remaining probability, however, the next state is generated by clicking on the back button, and returning to the state from which the current state was entered by a ""forward move"". Repeated clicks on the back button require access to increasingly distant history. We show that this process has fascinating similarities to and differences from Markov chains. In particular, we prove that like Markov chains, backoff processes always have a limit distribution, and we give algorithms to compute this distribution. Unlike Markov chains, the limit distribution may depend on the start state."
472bb7074d088185f08cb0112dbc1026b5e05498,"This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back towards their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or ``spoofed'', source addresses. In this paper we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed ``post-mortem'' -- after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backwards compatible and can be efficiently implemented using conventional technology."
0bfa9ef6bbcfdaa259c10a78a1d138b4492f9984,"Programmingerrors in parallel programs commonlyshow themselves in unsynchronized accesses to data, called data-races. Unfortunately, the complexity of discovering such bugs is very high, which gave rise to run-time detection techniques. In run-time methods the detection overhead may become very large when applied to memory fragments that are irrelevant to the checked program variables, as is the case in page-based dsms. In contrast, using MultiView we can ensure that only the relevant minipage is checked, by manipulating the protection of the associated vpage. Since the mini-pages access pattern represents the variables access pattern by the application, data race detection is eeciently integrated in the dsm by embedding a detection protocol in the minipage consistency management. We have recently used MultiView for this purpose and found a dramatic performance improvement. The overhead dropped more than two orders of magnitude, from hundreds percents reported in previous works to only a few percents in our system 7]. MultiView can help collecting garbage distributively in native application granularity, at the level of a standard dsm service. The main contribution of MultiView here is that marshaling and tracing pointer assignments are easy when diierent objects are pointed via diierent vpages. Thus, implementing counting methods, which are best suited for distributed garbage collection, become a lot easier and employ less penalty. We integrated a distributed garbage collector in our system millipage using the method developed in 9]. As was mentioned for data race detection mechanisms, tracing and collecting information regarding accesses to application-level shared variables becomes a natural service of the dsm. Once again, the basic property is that the minipages access pattern also represents the variables access pattern. Thus, it is relatively simple to implement a lightweight (remote) access logging mechanism, maintaining access histories per variable. This gives rise to all kinds of optimization techniques, which, for instance, may relocate threads and balance the load according to the true-sharing pattern 12]. memory: Where we are and where we should be headed? In"
0c4f7f0a47b94e6f91e6c056e5a5c1da3ac747ef,
20029d7ed8335820eda5d0e3f73a5997b8aed090,
68803119c1c147033440fda87c61c5c2ad8b77fe,"While algorithms for cooperative proxy caching have been widely studied, little is understood about cooperative-caching performance in the large-scale World Wide Web environment. This paper uses both trace-based analysis and analytic modelling to show the potential advantages and drawbacks of inter-proxy cooperation. With our traces, we evaluate quantitatively the performance-improvement potential of cooperation between 200 small-organization proxies within a university environment, and between two large-organization proxies handling 23,000 and 60,000 clients, respectively. With our model, we extend beyond these populations to project cooperative caching behavior in regions with millions of clients. Overall, we demonstrate that cooperative caching has performance benefits only within limited population bounds. We also use our model to examine the implications of future trends in Web-access behavior and traffic."
88faeade02f0582fbe949b6f5d398651b6b5369f,We examine the application of offline algorithms for determining the optical sequence of loads and superloads (a load of multiple consecutive cache lines) for direct-mapped caches. We evaluate potential gains in terms of miss rate and bandwidth and find that in many cases optimal superloading can noticeably reduce the miss rate without appreciably increasing bandwidth. Then we examine how this performance potential might be realized. We examine the effectiveness of a dynamic online algorithm and of static analysis (profiling) for superloading and compare these to next-line prefetching. Experimental results show improvements comparable to those of the optimal algorithm in terms of miss rates.
8bbfe5b46460e50aafc77b0dafcfbd1512e83e19,"An?-biased random source is a sequenceX=(X1,X2,?,Xn) of 0, 1-valued random variables such that the conditional probability PrXi=1|X1,X2,?,Xi?1 is always between 12??and 12+?. Given a familyS?{0,1}nof binary strings of lengthn, its?-enhanced probability Pr?(S) is defined as the maximum of PrX(S) over all?-biased random sourcesX. In this paper we establish a tight lower bound on Pr?(S) as a function of |S|,nand?."
c0ec1feb8aad3748ce20198657027ede84411462,"Suppose that we sequentially place $n$ balls into n boxes by putting each ball into a randomly chosen box. It is well known that when we are done, the fullest box has with high probability (1 + o(1))ln n/ln ln n balls in it. Suppose instead that for each ball we choose two boxes at random and place the ball into the one which is less full at the time of placement. We show that with high probability, the fullest box contains only ln ln n/ln 2 + O(1) balls---exponentially less than before. Furthermore, we show that a similar gap exists in the infinite process, where at each step one ball, chosen uniformly at random, is deleted, and one ball is added in the manner above. We discuss consequences of this and related theorems for dynamic resource allocation, hashing, and on-line load balancing."
d2f5e41b7da049a5d87827cc6e13597f85b9207e,"Performance-enhancing mechanisms in the World Wide Web primarily exploit repeated requests to Web documents by multiple clients. However, little is known about patterns of shared document access, particularly from diverse client populations. The principal goal of this paper is to examine the sharing of Web documents from an organizational point of view. An organizational analysis of sharing is important, because caching is often performed on an organizational basis; i.e., proxies are typically placed in front of large and small companies, universities, departments, and so on. Unfortunately, simultaneous multi-organizational traces do not currently exist and are difficult to obtain in practice. 
 
The goal of this paper is to explore the extent of document sharing (1) among clients within single organizations, and (2) among clients across different organizations. To perform the study, we use a large university as a model of a diverse collection of organizations. Within our university, we have traced all external Web requests and responses, anonymizing the data but preserving organizational membership information. This permits us to analyze both inter-and intra-organization document sharing and to test whether organization membership is significant. As well, we characterize a number of parameters of our data, including basic object characteristics, object cacheability, and server distributions."
c4b90e7aef8e7d888ff8f6bad543f6744fcbd324,"This paper presents cooperative prefetching and caching --- the use of network-wide global resources (memories, CPUs, and disks) to support prefetching and caching in the presence of hints of future demands. Cooperative prefetching and caching effectively unites disk-latency reduction techniques from three lines of research: prefetching algorithms, cluster-wide memory management, and parallel I/O. When used together, these techniques greatly increase the power of prefetching relative to a conventional (non-global-memory) system. We have designed and implemented PGMS, a cooperative prefetching and caching system, under the Digital Unix operating system running on a 1.28 Gb/sec Myrinet-connected cluster of DEC Alpha workstations. Our measurements and analysis show that by using available global resources, cooperative prefetching can obtain significant speedups for I/O-bound programs. For example, for a graphics rendering application, our system achieves a speedup of 4.9 over a non-prefetching version of the same program, and a 3.1-fold improvement over that program using local-disk prefetching alone."
dbbf178e9124523c8314ba30b491b9dccfc305ac,
0c3336ae99ff43c19935b8e51854b7ab3e7ac977,"New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks, small transfers are needed to provide low latency. This trend in page size is thus at odds with the use of network memory on high-speed networks.This paper studies the use of subpages as a means of reducing transfer size and latency in a remote-memory environment. Using trace-driven simulation, we show how and why subpages reduce latency and improve performance of programs using network memory. Our results show that memory-intensive applications execute up to 1.8 times faster when executing with 1K-byte subpages, when compared to the same applications using full 8K-byte pages in the global memory system. Those same applications using 1K-byte subpages execute up to 4 times faster than they would using the disk for backing store. Using a prototype implementation on the DEC Alpha and AN2 network, we demonstrate how subpages can reduce remote-memory fault time; e.g., our prototype is able to satisfy a fault on a 1K subpage stored in remote memory in 0.5 milliseconds, one third the time of a full page."
15fafd4d537219bb75b10570f6a5974f385dcf54,
1f068e83db2313d6cb6ca49054206aa8a4d5003f,"High-performance I/O systems depend on prefetching and caching in order to deliver good performance to applications. These two techniques have generally been considered in isolation, even though there are signi cant interactions between them; a block prefetched too early reduces the e ectiveness of the cache, while a block cached too long reduces the effectiveness of prefetching. In this paper we study the effects of several combined prefetching and caching strategies for systems with multiple disks. Using disk-accurate tracedriven simulation, we explore the performance characteristics of each of the algorithms in cases in which applications provide full advance knowledge of accesses using hints. Some of the strategies have been published with theoretical performance bounds, and some are components of systems that have been built. One is a new algorithm that combines the desirable characteristics of the others. We nd that when performance is limited by I/O stalls, aggressive prefetching helps to alleviate the problem; that more conservative prefetching is appropriate when signi cant I/O stalls are not present; and that a single, simple strategy is capable of doing both."
257928f41070e5b5ff8b2a604831c27838760100,"The authors consider algorithms for integrated prefetching and caching in a model with a fixed-size cache and any number of backing storage devices (disks). Previously, the single disk case was considered by Cao et al. (1995). They show that the natural extension of their aggressive algorithm to the parallel disk case is suboptimal by a factor near the number of disks in the worst case. The main result is a new algorithm, reverse aggressive, with near-optimal performance in the presence of multiple disks."
7694cdbe30344a326a48af2070e8f10c0c09e0f4,"We present and evaluate adaptive, hybrid cache coherence protocols for bus-based, shared-memory multiprocessors. Such protocols are motivated by the observation that sharing patterns vary substantially between different programs and even cache blocks within the same program. Performance measurements across a range of parallel applications indicate that the adaptive protocols we present perform well compared to both write-invalidate and write-update protocols."
a7c130d05238046f51321af7dab8ab4dae89912a,"Prefetching and caching are widely-used approaches for improving the performance of le systems. A recent study shows that it is important to integrate the two, and proposed an algorithm that performs well both in theory and in practice [2, 1]. That study was restricted to the case of a single disk. Here, we study integrated prefetching and caching strategies for multiple disks. The interaction between caching and prefetching is further complicated when a system has multiple disks, not only because it is possible to do multiple prefetches in parallel, but also because appropriate cache replacement strategies can alleviate the load imbalance among the disks. We present two o ine algorithms, one of which has provably near-optimal performance. Using tracedriven simulation, we evaluated these algorithms under a variety of data placement alternatives. Our results show that both algorithms can achieve near linear speedup when the load is distributed evenly on the disks, and our best algorithm performs well even when the placement of blocks on disks distributes the load unevenly. Our simulations also show that replicating data, even across all of the disks, o ers little performance advantage over a striped layout if prefetching is done well. Finally, we evaluated online variations of the algorithms and show that the online algorithms perform well even with moderate advance knowledge of future le accesses."
c90a77bee61a23ac3587de883b66876b9579f59b,"As the performance gap between disks and micropocessors continues to increase, effective utilization of the file cache becomes increasingly immportant. Application-controlled file caching and prefetching can apply application-specific knowledge to improve file cache management. However, supporting application-controlled file caching and prefetching is nontrivial because caching and prefetching need to be integrated carefully, and the kernel needs to allocate cache blocks among processes appropriately. This article presents the design, implementation, and performance of a file system that integrates application-controlled caching, prefetching, and disk scheduling. We use a two-level cache management strategy. The kernel uses the LRU-SP (Least-Recently-Used with Swapping and Placeholders) policy to allocate blocks to processes, and each process integrates application-specific caching and prefetching based on the controlled-aggressive policy, an algorithm previously shown in a theoretical sense to be nearly optimal. Each process also improves its disk access latency by submittint its prefetches in batches so that the requests can be scheduled to optimize disk access performance. Our measurements show that this combination of techniques greatly improves the performance of the file system. We measured that the running time is reduced by 3% to 49% (average 26%) for single-process workloads and by 5% to 76% (average 32%) for multiprocess workloads."
f0ad62661f955bf596eebe26fb524dfc08434c31,"This chapter presents an introduction to the competitive analysis of online algorithms. In an online problem, data is supplied to the algorithm incrementally, one unit at a time. The online algorithm must also produce the output incrementally: after seeing t units of input, it must output the t th unit of output. Since decisions about the output are made with incomplete knowledge about the entire input, an online algorithm often can not produce an optimal solution. Such an algorithm can only hope to approximate the performance of the optimal algorithm that sees all the inputs in advance. An online algorithm is said to be competitive if its performance is close to that of the optimal ooine algorithm on each input. We introduce the basic principles underlying the design and analysis of online algorithms and illustrate these ideas with results for three diierent problems: paging, metrical task systems and the k-server problem. We then give detailed descriptions of two recent results. The rst is the analysis of the work-function algorithm for the k-server problem ((KP94b]), and the second is the exponential function technique for virtual circuit routing ((AAF + 93]). Finally, we discuss new directions in the analysis of online algorithms intended to overcome some of the shortcomings of traditional competitive analysis."
f81957c073c9eac46c7f968a1e96b4bfe33249a4,"Prefetching and caching are widely-used approaches for improving the performance of file systems. A recent study shows that it is important to integrate the two, and proposed an algorithm that performs well both in theory and in practice [2, I]. That study waa restricted to the case of a single disk. Here, we study integrated prefetching and caching strategies for multiple disks. The interaction between caching and prefetching is further complicated when a system has multiple disks, not only because it is possible to do multiple prefetches in parallel, but also because appropriate cache replacement strategies can alleviate the load imbalance among the disks. We present two offline algorithms, one of which has provably near-optimal performance. Using tracedriven simulation, we evaluated these algorithms under a variety of data placement alternatives. Our results show that both algorithms can achieve near linear speedup when the load is distributed evenly on the disks, and our best algorithm performs well even when the placement of blocks on disks distributes the load unevenly. Our simulations also show that replicating data, even across all of the disks, offers little performance advantage over a striped layout if prefetching is done well. Finally, we evaluated online variations of the algorithms and show that the online algorithms perform well even with moderate advance knowledge of future file accesses."
0c3ca242761efb6926f3a775bd6d277d6b5bef45,"Modern microprocessors contain small translation lookaside buffers (TLBs) that maintain a cache of recently used translations. A TLB's coverage is the sum of the number of bytes mapped by each entry. Applications with working sets larger than the TLB coverage will perform poorly due to high TLB miss rates. Superpages have been proposed as a mechanism for increasing TLB coverage. A superpage is a virtual memory page with size and alignment that are a power of two multiple of the system's base page size. In this paper, we describe online policies for superpage management that monitor TLB miss traffic to decide when a superpage should be constructed. Our policies take into account both the benefit of a superpage promotion (potential for preventing future misses) and the cost (page copying). Although our approach increases the cost of each TLB miss, the net effect is to improve total execution time by eliminating a large number of misses without significantly increasing memory usage, thereby improving system performance."
13b6dd42357b77a2e01915f164ab5740a791d2f5,"Advances in network and processor technology have greatly changed the communication and computational power of local-area workstation clusters. However, operating systems still treat workstation clusters as a collection of loosely-connected processors, where each workstation acts as an autonomous and independent agent. This operating system structure makes it difficult to exploit the characteristics of current clusters, such as low-latency communication, huge primary memories, and high-speed processors, in order to improve the performance of cluster applications. This paper describes the design and implementation of global memory management in a workstation cluster. Our objective is to use a single, unified, but distributed memory management algorithm at the lowest level of the operating system. By managing memory globally at this level, all system- and higher-level software, including VM, file systems, transaction systems, and user applications, can benefit from available cluster memory. We have implemented our algorithm in the OSF/1 operating system running on an ATM-connected cluster of DEC Alpha workstations. Our measurements show that on a suite of memory-intensive programs, our system improves performance by a factor of 1.5 to 3.5. We also show that our algorithm has a performance advantage over others that have been proposed in the past."
404b462a4593867ed9395a4cd6cc6c50af91a12d,"What is the best paging algorithm if one has partial information about the possible sequences of page requests? Borodin, Irani, Raghavan and Schieber [2] proposed a framework in which such questions can be addressed, by studying the competitive analysis of paging algorithms in the access graph model. This model restricts page requests so that they conform to a notion of locality of reference, given by an arbitrary access graph. It can also be viewed as an example of a diffuse adversary model, as defined by Koutoupias and Papadimitriou [11]. We present two results in this model. The first is a randomized algorithm which is strongly competitive against oblivious adversaries for undirected access graphs. We also present strongly competitive algorithms against adversaries that can use multiple pointers into the access graph in order to construct the reference sequence. This models more closely a multiprogramming environment, or a situation in which there are multiple threads of control."
c0e643cfefea01916c0273a2cf73e5d7b2c37bea,"9 For the case of a randomized algorithm, we should rst be convinced that the adversary's power is no stronger than that of an oblivious adversary in each of the carpool instances that we have deened. Observe that the inputs to the j instance are determined by (V 1 ; V 2 ; : : :) and the decisions made by the carpool solver on instances j + 1 through`. The decisions made in instance k, for 1 k j at any point in time do not eeect the inputs to instance j. Therefore, for instance j, the adversary chooses a distribution on (V 1 ; V 2 ; : : :) and can even be given the power to make all the decisions in instances j + 1 throughànd yet all that it would be doing cannot depend on the decision at the j-th instance. Given that we know that max 1in is bounded by (n), we get that the expectation of max 1in jD(t; i; V 1 ; V 2 ; : : :V t)j is at most 2(n). A binary feedback scheme for congestion avoidance in computer networks. As in the carpool problem, in the on-line version we consider deterministic algorithms as well as randomized algorithms against the oblivious adversary. Tijdeman 23] has considered the vector rounding problem and has shown that the oo-line version has a solution of diierence 1, i.e. for every sequence of real vectors (V 1 ; V 2 ; : : :) there exist integer (Z 1 ; Z 2 ; : : :) such that for all t 1 we have max 1in j P t j=1 z i j ? P t j=1 v i j j 1. One can easily cast the carpool problem as a vector rounding problem: for a sequence (X 1 ; X 2 ; : : :) create the vectors (V 1 ; V 2 ; : : :) where for all t 1 and all 1 i n we have v i t = 1=jX t j if i 2 X t 0 if i 6 2 X t. Therefore if we can connect the performance of the 2-person carpool problem to the vector rounding problem then we will have reduced the general carpool problem to the 2-person problem. Before we show the reduction we will make some simplifying assumptions, which can be easily …"
ca59d01498422a5f7021e0f5a664361b72c21408,"Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetching and caching must satisfy, and then presents and studies two such integrated strategies, called aggressive and conservative. We prove that the performance of the conservative approach is within a factor of two of optimal and that the performance of the aggressive strategy is a factor significantly less than twice that of the optimal case. We have evaluated these two approaches by trace-driven simulation with a collection of file access traces. Our results show that the two integrated prefetching and caching strategies are indeed close to optimal and that these strategies can reduce the running time of applications by up to 50%."
d8461974f1a5b866c5a37e6a3a72422f12cdb938,"hand the Adaptive policies, that gather more information about the inter-arrival time distributions, do the best of the policies that we considered. They have only 25.8% higher cost than non-causal optimal in the holding cost model and only 34% higher cost in the paging model. The system costs for the Adaptive policies are reasonable; in the holding cost model there are roughly 10 instruction per packet on the packet forwarding path and 100 oo this path. In the paging model a page fault requires a longer computation outside the packet forwarding path; in typical cases this amounts to 1/2 ms on a 30 MIPS machine. This means that the scheme may not be as useful in situations where the average interval between page faults is very small. However the overhead may be alleviated by sharing the computation among a number of page faults. In both models the memory requirements are reasonable but somewhat larger than the Lru/Lru-based policies. In typical cases the Adaptive policies uses 47-413 KBytes of memory. Based on performance and systems cost we propose that the Adaptive policies be used, except when memory is very scarce or in the paging model if page faults occur very frequently, in which case the Lru/Lru-based policies are good alternatives. Lastly, we note that our policies for both pricing models may be useful in a wider context. In general terms, the holding cost model involves a resource that is intermittently used, and must be \open"" to be used. There is a cost for opening it, and a cost for each time unit it remains open. This scenario describes many speciic problems , for example disk management in portable computers: the \open cost"" is the loss of utility to the user while spinning up the disk, while the holding cost corresponds to depletion of battery power. Similarly the paging model can be phrased more generally: a large number of entities are competing for the use of a scarce resource. This general model is also interesting in a variety of contexts. It is an interesting direction for further study to determine whether our adaptive methods can provide a performance beneet for related applications. for help in collecting the datasets. Thanks also to Ramon Caceres for many helpful discussions and for good advice. studies of competitive spinning for a shared-memory mulipro-cessor,"" in Proc. 12 queue should be large enough to accommodate N events, where …"
fd67550525f12426ef344bea0d5b1972978791e8,
5dfa66165ab583947d95a4bac0ec667754b52f5f,"Suppose that we sequentially place n balls into n boxes by putting each ball into a randomly chosen box. It is well known that when we are done, the fullest box has with high probability lnn/lnlnn(1 + o(1)) balls in it. Suppose instead, that for each ball we choose two boxes at random and place the ball into the one which is less full at the time of placement. We show that with high probability, the fullest box contains only lnlnn/ln2 + O(1) balls - exponentially less than before. Furthermore, we show that a similar gap exists in the infinite process, where at each step one ball, chosen uniformly at random, is deleted, and one ball is added in the manner above. We discuss consequences of this and related theorems for dynamic resource allocation, hashing, and on-line load balancing."
79ea90401af3fec8fcb4c02d3ee672c723d6a3b7,"We study the robustness of the butterfly network against random static faults. Suppose that each edge of the butterfly is present independently of other edges with probability p, Our main result is that there is a O-1 law on the existence of a linearized component. More formally, there is a critical probability p~ such that for p above p*, the faulted butterfly almost surely contains a linear-sized component, whereas for p below px, the faulted butterfly almost surely does not contain a linear-sized component."
c9d15e4a3b860f08158fb5bce6b266c099dc0023,"We study the robustness of the butter y network against random static faults. Suppose that each edge of the butter y is present independently of other edges with probability p. Our main result is that there is a 0-1 law on the existence of a linearsized component. More formally, there is a critical probability p such that for p above p , the faulted butter y almost surely contains a linear-sized component, whereas for p below p , the faulted butter y almost surely does not contain a linear-sized component."
f310c5689fea9e24a75bbf5ae590c73b5cffd9d8,
8a2c219b1d0d5fbc4fe93ae3390465647eccd586,"How much can an imperfect source of randomness affect an algorithm? We examine several simple questions of this type concerning the long-term behavior of a random walk on a finite graph. In our setup, at each step of the random walk a “controller” can, with a certain small probability, fix the next step, thus introducing a bias. We analyze the extent to which the bias can affect the limit behavior of the walk. The controller is assumed to associate a real, nonnegative, “benefit” with each state, and to strive to maximize the long-term expected benefit. We derive tight bounds on the maximum of this objective function over all controller's strategies, and present polynomial time algorithms for computing the optimal controller strategy."
91696e2903530d53d740dfe182735e00a87640c2,"[61] D.D. Sleator, R.E. Tarjan. Self-adjusting binary search trees. Journal of the ACM, Vol. 32, No. 3, pages 652{686, July 1985. [62] J.R. Spirn. Program Behavior: Models and Measurements. Elsevier Computer Science Library. Elsevier, Amsterdam, 1977. [63] R.E. Tarjan. E ciency of a good but not linear set union algorithm. Journal of the ACM, Vol. 22, pages 215{225, 1975. [64] R.E. Tarjan. A class of algorithms that require nonlinear time to maintaing disjoint sets. Journal of Computer and System Sciences , 18:110-227. (1979). [65] R.E. Tarjan. Amortized copmutational complexity. SIAM J. Alg. Discrete Methods, Vol. 6, No.2, April 1985. [66] S. Vishwanathan. Randomized online coloring of graphs. 31st Annual Symposium on the Foundations of Computer Science, 1990. [67] H. Whitney. Non-seperable and planar graphs. Transactions of the American Mathematical Society, 34:339-362. [68] A. C-C. Yao. Probabilistic computations: Towards a uni ed measure of complexity. 17th Annual Symposium on Foundations of Computer Science, pages 222{227, 1977."
b127bbf34f8f5ad3e26211cf7987eb8fe1fdf25d,"The setup for the authors' problem consists of n servers that must complete a set of tasks. Each task can be handled only by a subset of the servers, requires a different level of service, and once assigned can not be re-assigned. They make the natural assumption that the level of service is known at arrival time, but that the duration of service is not. The on-line load balancing problem is to assign each task to an appropriate server in such a way that the maximum load on the servers is minimized. The authors derive matching upper and lower bounds for the competitive ratio of the on-line greedy algorithm for this problem, namely /sup (3n)2/3///sub 2/(1+o(1)), and derive a lower bound, Omega ( square root n), for any other deterministic or randomized on-line algorithm.<<ETX>>"
c30156f251186ab2069b84bdb035fed82397ff13,
cdd0bd6569ae831d5f454b6a072437931086daa5,"AN1 (formerly known as Autonet) is a local area network composed of crossbar switches interconnected by 100Mbit/second, full-duplex links. In this paper, we evaluate the performance impact of certain choices in the AN1 design. These include the use of FIFO input buffering in the crossbar switch, the deadlock-avoidance mechanism, cut-through routing, back-pressure for flow control, and multi-path routing. AN1's performance goals were to provide low latency and high bandwidth in a lightly loaded network. In this it is successful. Under heavy load, the most serious impediment to good performance is the use of FIFO input buffers. The deadlock-avoidance technique has an adverse effect on the performance of some topologies, but it seems to be the best alternative, given the goals and constraints of the AN1 design. Cut-through switching performs well relative to store-and-forward switching, even under heavy load. Back-pressure deals adequately with congestion in a lightly-loaded network; under moderate load, performance is acceptable when coupled with end-to-end flow control for bursts. Multi-path routing successfully exploits redundant paths between hosts to improve performance in the face of congestion."
f1b48c97db818c1e0ded97313774bccbdbd296c8,
f4f866813686326c953c158381d7438da3e2b473,"What is the best paging algorithm if one has partial information about the possible sequences of page requests? We give a partial answer to this question, by presenting the analysis of strongly competitive paging algorithms in the access graph model. This model restricts page requests so that they conform to a notion of locality of reference, given by an arbitrary access graph.
We first consider optimal algorithms for undirected access graphs. Borodin et al. [2] define an algorithm, called FAR, and proved that it is within a logarithmic factor of the optimal. We prove that FAR is in fact strongly competitive, i.e. within a constant factor of the optimum. For directed access graphs, we present an algorithm that is strongly competitive on all structured program graphs—graphs modeling the request sequences of structured programs."
f65a362b7ddae8cf608a4c2b4ec5051831f90611,"This paper considers the problem of paging under the assumption that the sequence of pages accessed is generated by a Markov chain. The authors use this model to study the fault-rate of paging algorithms, a quantity of interest to practitioners. They first draw on the theory of Markov decision processes to characterize the paging algorithm that achieves optimal fault-rate on any Markov chain. They address the problem of efficiently devising a paging strategy with low fault-rate for a given Markov chain. They show that a number of intuitively good approaches fail. Their main result is an efficient procedure that, on any Markov chain, will give a paging algorithm with fault-rate at most a constant times optimal. Their techniques also show that some algorithms that do poorly in practice fail in the Markov setting, despite known (good) performance guarantees when the requests are generated independently from a probability distribution.<<ETX>>"
2571aeae16e96db29026dedcd170695ba4126f01,"We consider efficient parallel algorithms for the evaluat ion of game trees. We prove an inherent limit at ion on the speedup achievable, and give an algorithm that achieves its best performance bounds on trees of the sort that are likely to arise in game-playing programs."
59ac7e2c78d066a5bc6cec866de2970bde46025d,"A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are competitive strategies, for which the performance can be shown to be no worse than some constant factor times an optimal off-line strategy. The performance of five competitive strategies is compared with that of always blocking, always spinning, or using the optimal off-line algorithm. Measurements of lock-waiting time distributions for five parallel programs were used to compare the cost of synchronization under all the strategies. Additional measurements of elapsed time for some of the programs and strategies allowed assessment of the impact of synchronization strategy on overall program performance. Both types of measurements indicate that the standard blocking strategy performs poorly compared to mixed strategies. Among the mixed strategies studied, adaptive algorithms perform better than non-adaptive ones."
3d4d1cb20e7235fd052e82ffffb16bcfa39db7ad,"A dictionary is a data structure that allows a sequence of on-line insertions, deletions, and lookup operations for keys drawn from a universe U. Typically the size of the dictionary, n, is much smaller than the size of U, and efficient dictionaries are implemented via various hashing schemes. We present a family of hashing algorithms that allow an implementation of a dictionary that is particularly advantageous when the underlying hardware allows the parallelization of hash function computations and the parallelization of memory reads in different memory banks. In this case, our scheme requires exactly one memory cycle for deletes and lookups and constant expected amortized cost for insertions. We use easy-to-compute hash functions and make no unproven assumptions about their randomness properties, or about any property of the keys. According to our analysis, with the Multilevel Hashing Algorithm it is possible to build a dictionary that is able to answer 20 parallel searches among 64,000 entries in less than 5 ps using relatively inexpensive hardware. *DEC Systems Research Center, 130 Lytton Avenue, Palo Alto, CA 94301."
6bd49cbaf3a740744897f619b5cd25fd5b5fa573,"The computational power of 2-D and 3-D processor arrays that contain a potentially large number of faults is analyzed. Both a random and a worst-case fault model are considered, and it is proved that in either scenario low-dimensional arrays are surprisingly fault tolerant. It is also shown how to route, sort, and perform systolic algorithms for problems such as matrix multiplication in optimal time on faulty arrays. In many cases, the running time is the same as if there were no faults in the array (up to constant factors). On the negative side, it is shown that any constant congestion embedding of an n*n fault-free array on an n*n array with Theta (n/sup 2/) random faults (or Theta (log n) worst-case faults) requires dilation Theta (log n). For 3-D arrays, knot theory is used to prove that the required dilation is Omega ( square root log n).<<ETX>>"
bdf524f747f3c7b4b38753671aafe63f1c03ecb6,
577c9fa45b5f8f29e06b7408d53bc085683258ea,"Aleliunas <italic>et al.</italic> [1] posed the following question: “The reachability problem for undirected graphs can be solved in logspace and <italic>O</italic>(<italic>mn</italic>) time [<italic>m</italic> is the number of edges and <italic>n</italic> is the number of vertices] by a probabilistic algorithm that simulates a random walk, or in linear time and space by a conventional deterministic graph traversal algorithm. Is there a spectrum of time-space trade-offs between these extremes?” We answer this question in the affirmative for linear-sized graphs by presenting an algorithm which is faster than the random walk by a factor essentially proportional to the size of its workspace. For denser graphs, the algorithm is faster than the random walk but the speed-up factor is smaller."
1330ce1beb9737467846e79e6859485c04b5c8fa,
5f5f958c39a69c8fe77be43fd6208266f9546fc8,"A central issue in the theory of parallel computation is the gap between the ideal models that utilize shared memory and the feasible models that consist of a bounded-degree network of processors sharing no common memory. This problem has been widely studied. Here a tight bound for the probabilistic complexity of this problem is established. The solution in this paper is based on a probabilistic scheme for implementing shared memory on a bounded-degree network of processors. This scheme, which we term parallel has/zing, enables n processors to store and retrieve an arbitrary set of n data items in O(logn) parallel steps. The items’ locations are specified by a function chosen randomly from a small class of universal hash functions. A hash function in this class has a small description and can therefore be efficiently distributed among the processors. A deterministic lower bound for the point-to-point communication model is also presented."
be52435e4445dc593b5bab2c757d4a10482a3d39,"Consider a particle that moves on a connected, undirected graphG withn vertices. At each step the particle goes from the current vertex to one of its neighbors, chosen uniformly at random. Tocover time is the first time when the particle has visited all the vertices in the graph starting from a given vertex. In this paper, we present upper and lower bounds that relate the expected cover time for a graph to the eigenvalues of the Markov chain that describes the random walk above. An interesting consequence is that regular expander graphs have expected cover time Θ(n logn)."
ee251e3360601a027f7cf5cae27d5f4470873c45,"A randomized algorithm is given for the dictionary problem with O(1) worst-case time for lookup and O(1) amortized expected time for insertion and deletion. An Omega (log n) lower bound is proved for the amortized worst-case time complexity of any deterministic algorithm in a class of algorithms encompassing realistic hashing-based schemes. If the worst-case lookup time is restricted to k, then the lower bound for insertion becomes Omega (kn/sup 1/k/).<<ETX>>"
0014c37cc57dc0ec49eb10bbf97860f56c25c171,
6264204ccf79ef095d73951d5b5da1850145268d,
c724a9f0523a1b4851c4662b9442c13c15c2ae56,"Information exchange between processors is essential for any efficient parallel computation. One very convenient mechanism for exchanging information is for processors to utilize shared variables. In this work, we present efficient algorithms for sharing variables on two very different types of distributed systems. The first is a synchronous, bounded degree network of processors each with an associated local memory. The problem we consider in this case is how to distribute and route the shared variables among the processor's local memories. The solution combines the use of universal hash functions for distributing the variables and probabilistic two-phase routing. 
The second model we consider is an asynchronous broadcast model. Here each processor has a cache and the caches are connected to each other and to a main memory over a bus. Only one variable can be transmitted over the bus per bus cycle, and shared variables may reside in any number of caches. A block retention algorithm associated with each cache monitors bus and processor activity in order to determine which blocks of data to keep and which to discard. There is a tradeoff: If a certain variable is dropped from a cache, then a subsequent read for that variable requires bus communication. On the other hand, if a variable is not dropped from a certain cache, then a write by some other processor to that variable requires a bus cycle so that the first cache will obtain the updated value. For several variants of this model we present on-line block retention algorithms which incur a communication cost that is within a constant factor of the cost to the optimal off-line algorithm."
88841a74c6a4f5a2fa18e076cfb71860b732b8b6,
acc549c4516605e57fd794a2ea13eeca46f597de,
236e00d1eed753ea76242c788542e6d59430ef9d,"The language of regular expressions is a useful one for specifying certain sequebtial processes at a very high level. They allow easy modification of designs for circuits, like controllers, that are described by patterns of events they must recognize and the responses they must make to those patterns. This paper discusses the compilation of such expressions into reasonably compact layouts. The translation of regular expressions into nondeterministic automata by two different methods is discussed, along with the advantages of each method. A major part of the compilation problem is selection of good state codes for the nondeterministic automata; one successful strategy is explained in the paper."
41d0ae4cf4f84764bf86dfd181fab44bba8aa453,"The data migration problem is the problem of computing an eÆcient plan for moving data stored on devices in a network from one con guration to another. Load balancing or changing usage patterns could necessitate such a rearrangement of data. In this paper, we consider the case where the objects are xed-size and the network is complete. The direct migration problem is closely related to edge-coloring. However, because there are space constraints on the devices, the problem is more complex. Our main results are polynomial time algorithms for nding a near-optimal migration plan in the presence of space constraints when a certain number of additional nodes is available as temporary storage, and a 3/2-approximation for the case where data must be migrated directly to its destination."
ecad5d7a27d26e043fe098afe0fb1df00f76e8b1,"This paper describes the organizational approaches and usability methodologies considered by HCI professionals to increase the strategic impact of usability research within companies. We collected the data from 134 HCI professionals at three conferences: CHI 98, CHI 99, and the Usability Professionals' Association 1999 conference. The results are the first steps towards a toolkit for the usability community that can help HCI practitioners learn from the experiences of others in similar situations."
f7c40168588e820d5b4ff734acab9aaed8d200a6,This panel asks a group of well-known usability practitioners what is keeping us from achieving the penetration of strategic usability within organizations. Eight panelists describe the lessons they learned while attempting to make usability pervasive in different organizational environments.
9649eea03f29274f39308c94090ae70e658352b5,"Stephanie Rosenbaum Judee Humburg Janice Rohn President J.L. Humburg Associates Manager, Usability Labs Tee-Ed, Inc. 377 McKendry Drive and Services P.O. Box 1905 Menlo Park, CA 940152919 Sun Microsystems, Inc. Ann Arbor, MI 48106 Voice: l-650-328-0144 2550 Garcia Avenue, MPK 18-107 Voice: l-734-995-1010 Fax: l-650-328-1327 Mountain View, CA 94043-l 100 Fax: l-734-9951025 judee@pacbell.net Voice: l-650-786-6367 Stephanie @ teced.com"
ed925a5a48dd4dd15e15b114217092121a8d176f,
0896bdb0c16803a4e42a34c9464b3add8c4c4d12,"This panel explores approaches to making usability research more strategic within organizations---not just with respect to the product development life cycle, but pervasive throughout the organization. Six panelists discuss different ways in which usability can be strategic, depending on their organizational environments or ""profiles."""
09ac1fc81711913e1fd14cacf0cd61b252ef946a,
f53982a670686beca4b6c948ae3f03786460bb42,
f699db1d2d937472e546f9dd1e7e211be6ff2004,"How can we choose among customer data collection methods when limited staff and financial resources must be spread across the whole development cycle? This tutorial helps participants understand the tradeoffs, so they can make effective choices among methods at different points during product design and development. It focuses on early user-centered intervention to gain cost-effective, reusable end-user information."
35d4558af9f11a3c39066e13abd7ab670d021de5,"This tutorial applies human factors research techniques to collecting customer data early and “building usability into” the product definition and design processes. User input contributes to designing the whole product, including the user interface and documentation. After an overview of product development models, we discuss appropriate research questions and methods for the investigation, requirements definition, and early design phases. Participants will have extensive hands-on practice in methods, including designing a new product during the tutorial."
6349c49d4652fa43f339fd56efecabe922f82c33,
78dd7b029f22c17c1aacae5e5ab40525abc9a65d,No abstract for this introductory article.
f0436d3aec597a6dd4bf54c1549cf7f6ddd5a5c1,"The Mackenzie Delta is an extensive river-mouth depocentre, the second largest delta on the Arctic Ocean, and lies in the zone of continuous permafrost. We report the first measurements of natural consolidation subsidence in a high-latitude delta with ice-bonded sediments. Several years of episodic GPS records on a network of 15 stable monuments throughout the central and outer delta reveal downward motion between 1.5 ± 0.7 and 5.3 ± 1.1 mm/yr relative to a nearby monument on bedrock. Additional shallow subsidence results from loss of near-surface excess ice with deeper seasonal thaw in a warming climate. Isostatic adjustment is a third component of subsidence, captured in the NAD83v70VG crustal velocity model. Sedimentation rates over much of the outer delta are less than the rate of subsidence combined with rising sea level. Scenarios for future inundation are evaluated using interpolated IPCC AR5 projections, NAD83v70VG, and a LiDAR DEM with realistic consolidation, thaw subsidence, and sedimentation rates, on time scales of 40 and 90 years. These reveal increases in area flooded at mean water level from 33% in 2010 to 65% or as much as 85% in 2100, depending on the emissions scenario, driving delta-front retreat and removing a large proportion of avian nesting habitat. The three components of subsidence together increase the relative sea-level rise by a factor of two to eight, depending on the scenario. Consolidation subsidence may also contribute to rising low-flow water levels in the central delta, increasing river-lake connectivity, with negative impacts on aquatic biodiversity and productivity."
ff1f8aa3d6666c88d916d84e616490d8484db4d4,"The coastline of the Inuvialuit Settlement Region (ISR) in the Mackenzie –Beaufort region of the western Canadian Arctic is characterized by rapid erosion of ice-bonded sediments with abundant excess ground ice, resulting in widespread thermal and mechanical process interactions in the shore zone. Coastal communities within the ISR are acutely aware of the rapidly eroding coastline and its impacts on infrastructure, subsistence activities, cultural or ancestral sites, and natural habitats. Tuktoyaktuk Island is a large natural barrier protecting the harbour and surrounding community from exposure to waves. It is threatened by coastal erosion, a better understanding of which will inform adaptation strategies. Using historical and recent aerial imagery, high-resolution digital elevation models, cliff geomorphology, stratigraphy, and sedimentology, including ground-ice content, this study documents erosional processes, recession rates, volume losses and sediment delivery since 1947, and projected into the future. Erosion along the northwest-facing (exposed) cliff, primarily by thermo-abrasional undercutting and block failure, has accelerated since 2000 to a mean of 1.8 ± 0.02 m/yr, a 22% increase over the previous 15 years and 17% faster than 1947-2000. Lower recession rates on the harbour side of the island increased more than two-fold. Projection of future shoreline vectors by extrapolation, using the post-2000 accelerated coastal recession rates at 284 transects, points to breaching of this vital natural harbour barrier by 2044, after which rapid realignment is expected to occur as the new inlet evolves. Further acceleration of rates, as seems highly likely, brings the breaching date closer."
7978d6e7ff8e19d3a5b5a7f8fdaf72f5e3807fd8,
7c4a7543bb446a51c98a337c6475f9ae2c2a0225,"This study investigates the postglacial sea-level history of eastern Cumberland Peninsula, a region of Baffin Island, Nunavut where submerged terraces were documented in the 1970s. The gradient in elevation of emerged postglacial marine-limit deltas and fiord-head moraines led Dyke (1979) to propose a conceptual model for continuous postglacial submergence of the eastern peninsula. Multibeam mapping over the past decade has revealed eight unequivocal submerged deltas at 19-45 m below [present] sea level (bsl) and other relict shore-zone landforms (boulder barricade, spits, and sill platform) at 16-51 m bsl. Over a distance of 115 km from Qikiqtarjuaq to Cape Dyer, the submerged coastal features increase in depth toward the east, with a slope (0.36 m/km), somewhat less than that of the marine-limit shoreline previously documented (0.58-0.62 m/km). The submerged ice-proximal deltas, deglacial ice limits, and radiocarbon ages constrain the postglacial lowstand between 9.9 and 1.4 ka cal BP. The glacial-isostatic model ICE-7G_NA (VM7) (Peltier 2020) computes a lowstand relative sea level at 8.0 ka, the depth of which increases eastward at 0.28 m/km. The difference between observed and model-derived lowstand depths ranges from 1 m in the west to 10 m in the east and the predicted tilt is significantly less than observed (p=0.0008). The model results, emerging data on Holocene glacial re-advances on eastern Baffin Island, and evidence for proglacial delta formation point to a Cockburn (9.5-8.2 ka) age for the lowstand, most likely later in this range. This study confirms the 1970s conceptual model of postglacial submergence in outer Cumberland Peninsula and provides field evidence for further refinement of glacial-isostatic adjustment models."
a7155a9d34a74dd014c47e98b2a88db33a785c0b,"The Cockburn Substage readvance marks the last major late-glacial advance of the northeast sector of the Laurentide Ice Sheet on Baffin Island. The causes of this abrupt, late reversal of retreat are still unclear, but greater chronological control may provide some insight. To date, the literature has focused on the large terminal moraines in the region, providing a date of readvance (c. 9.5-8.5 ka cal BP). In Frobisher Bay, the Cockburn Substage readvance and recession onshore are marked by a series of moraines spread over ~20 km along the inner bay. Acoustic marine mapping reveals five distinct transverse ridges, morphologically suggestive of grounding-zone wedges, and two later fields of DeGeer moraines on the floor of the inner bay. These indicate that the style of ice retreat (beginning no later than 8.5 ka cal BP) changed over time from punctuated recession of a floating ice-front (20 km over >680 years, with four pauses) to more regular tidewater ice-front retreat, reaching the head of the bay 900 years or more after withdrawal from the outer Cockburn limit. The established chronology for final recession in the region is based largely on radiocarbon dating of bulk shell samples and single shells of deposit-feeding molluscs, notably Portlandia arctica, affected by old carbon from carbonate-rich sediments. Sedimentary analysis and judicious sampling for 14C dating of glaciomarine and marine facies in seabed sediment cores enables development of a late- and postglacial lithostratigraphy that indicates final withdrawal of ice from the drainage basin by 7 ka cal BP."
aa6e1d2a1e4ce68f346338183ca39ad975f25ed9,"Tidal flats are widely distributed on high-latitude coasts, where sea ice processes have been invoked to explain the abundance and distribution of boulders. This study documents the surface morphology and sediment dynamics of a low-Arctic macrotidal system, the boulder-rich tidal flats of Koojesse Inlet, fronting the Nunavut capital, Iqaluit, on Baffin Island. This is a region of postglacial isostatic uplift and forced regression, with raised littoral, deltaic, and glaciomarine deposits. The spring-tidal range is 11.1 m and sea ice cover lasts roughly 9 months of the year. The extensive intertidal flats are up to 1 km wide, with a veneer of sand and gravel (including large boulders) resting on an erosional unconformity truncating the underlying glaciomarine mud, forming a terrace within the present tidal range. Over a three-year study, no consistent pattern of erosion or deposition was evident. Over a longer time scale, the concave hypsometry, low sediment supply, slight ebb-dominance of weak tidal currents, abrasion by wave-entrained sand, ebb-oriented ripples formed under subaerial drainage, and slumps on the terrace flanks are consistent with seaward hydraulic and gravitational sediment transport. These processes may be of greater importance than shoreward ice transport. This study underlines the importance of relict glaciomarine deposits, postglacial uplift, and falling relative sea level in the erosional development of these high-latitude tidal flats. Relative sea-level projections for Iqaluit are ambiguous, but a switch to rising sea level, if it occurs, combined with more open water and wave energy, could alter the foreshore dynamics of the system."
492fa1f665111f90a475ace34562a0d16e163478,"Coastal wetlands, such as saltmarshes and mangroves that fringe transitional waters, deliver important ecosystem services that support human development. Coastal wetlands are complex social-ecological systems that occur at all latitudes, from polar regions to the tropics. This overview covers wetlands in five continents. The wetlands are of varying size, catchment size, human population and stages of economic development. Economic sectors and activities in and around the coastal wetlands and their catchments exert multiple, direct pressures. These pressures affect the state of the wetland environment, ecology and valuable ecosystem services. All the coastal wetlands were found to be affected in some ways, irrespective of the conservation status. The main economic sectors were agriculture, animal rearing including aquaculture, fisheries, tourism, urbanization, shipping, industrial development and mining. Specific human activities include land reclamation, damming, draining and water extraction, construction of ponds for aquaculture and salt extraction, construction of ports and marinas, dredging, discharge of effluents from urban and industrial areas and logging, in the case of mangroves, subsistence hunting and oil and gas extraction. The main pressures were loss of wetland habitat, changes in connectivity affecting hydrology and sedimentology, as well as contamination and pollution. These pressures lead to changes in environmental state, such as erosion, subsidence and hypoxia that threaten the sustainability of the wetlands. There are also changes in the state of the ecology, such as loss of saltmarsh plants and seagrasses, and mangrove trees, in tropical wetlands. Changes in the structure and function of the wetland ecosystems affect ecosystem services that are often underestimated. The loss of ecosystem services impacts human welfare as well as the regulation of climate change by coastal wetlands. These cumulative impacts and multi-stressors are further aggravated by indirect pressures, such as sea-level rise."
5b9eca43610791b29026d21cd0ed000000413a17,
c04cf16bde298cfcf51efbdac255994544dac3e5,
35f5cdba7f0503f04f1e7cd0c9fe255b2b81e565,The science of geodesy and the corresponding reference systems it develops have increasingly been applied to measuring motions and slow deformations of the Earth’s crust driven by plate tectonics. ...
4db0ce44d7c6fc12fd57407ee0c78e80c90ded2d,
caaae600040ab67679f4d490a14ec8acb1a31130,
cc27dc41aaf1aeffdc1d2d31cb62b9799f352bf0,
9ed20bfbbb291139fa95ffc1986c817a607b0653,"Abstract Submarine slope failures in the nearshore waters of SE Baffin Island, eastern Canadian Arctic, present a challenge to coastal and seabed development. Submarine slope failures are a known geohazard in fjords in Norway, Chile, Alaska, British Columbia and elsewhere, but have received little attention in the coastal waters of Arctic Canada. Over the past 6 years, there has been a rapid expansion of multibeam echosounder (MBES) mapping in Canadian Arctic fjords, leading to the discovery of many submarine slope failures. One area that has been mapped in detail is inner Frobisher Bay. This macrotidal, seasonally ice-covered, semi-enclosed embayment has a glacially scoured bed, ice-contact deposits, including recessional moraines, and stratified glaciomarine and post-glacial silts and clays with abundant dropstones. The prevalence of submarine slope failures in the inner bay (one per 20 km2) appears to be anomalous. To date, MBES mapping has imaged at least 246 failures, ranging in size from 0.007 to 2.1 km2 and all within the glaciomarine and post-glacial succession. Morphometric analysis of these features based on high-resolution MBES bathymetry provides an insight into their spatial distribution, relative chronology, triggers and flow characteristics; factors essential to understanding the mechanisms underlying their abundance in this Canadian Arctic fjord."
a2c7c90f1d0eec29b4f463374c19aa3c55f6c99c,"SUMMARY Small islands can guide visualization of the diverse information requirements of future context-relevant coastal governance. On small marine islands (<20 000 km2), negative effects of coastal challenges (e.g., related to population growth, unsustainable resource use or climate change) can develop rapidly, with high intensity and extreme impacts. The smallest and most remote islands within small-island states and small islands in larger states can be threatened by intrinsic governance factors, typically resulting in access to fewer resources than larger islands or administrative centres. For these reasons, efforts to support coastal change governance are critical and need to be targeted. We propose a conceptual framework that distinguishes key governance-related components of small-island social–ecological systems (SESs). To prioritize areas of vulnerability and opportunity, physical, ecological, social, economic and governance attributes are visualized to help show the ability of different types of small-island SESs to adapt, or be transformed, in the face of global and local change. Application of the framework to an Indonesian archipelago illustrates examples of local rule enforcement supporting local self-organized marine governance. Visualization of complex and interconnected social, environmental and economic changes in small-island SESs provides a better understanding of the vulnerabilities and opportunities related to context-specific governance."
ab3643014973e1213ba5f59c4e6925141cc54720,"(1) Laboratoire CEARC, Observatoire de Versailles Saint-Quentin, Université de Versailles Saint-Quentin-en-Yvelines, Guyancourt, France, (2) Alfred Wegener Institute Helmholtz Centre for Polar and Marine Research, Potsdam, Germany, (3) Geological Survey of Canada, Natural Resources Canada, Bedford Institute of Oceanography, Dartmouth, NS, Canada„ (4) Institute of Humanities and Northern Indigenous Peoples, Siberian Branch of the Russian Academy of Sciences, Yakutsk, Sakha Republic, Russian Federation"
49d20ecc83d577da1c2fdd07ee74b1aae3f8393f,"ABSTRACT Amos, C.L.; Brylinsky, M.; Forbes, D.L.; Robertson, A.; Thompson, C.E.L., and Kassem, H., 2018. Stability of seabed sediments in the embayments of North Rustico, Prince Edward Island, Canada. This paper describes a series of 15 deployments of the annular benthic flume Sea Carousel made along the two estuaries of North Rustico, Prince Edward Island (PEI), Canada: The Hunter River estuary and the Wheatley River estuary. The study was part of a wider study on the environmental impact of coastal changes and aquaculture (Mytilus edulis) development in the region. The study site was situated on the dynamic north shore of PEI that is characterised by sandy barrier islands, inlets and beaches, and muddy lagoons. The lagoons are dominated by the seagrass Zostera marina in the muddy outer parts and by Ulva lactuca in the eutrophic inner parts. The thresholds for surface erosion of cohesive sediments (0.10 < τcrit,o < 0.75) derived from trends of suspended sediment concentration (C) increased with distance (d) into the estuary: τcrit,o = 0.50e−0.26d (±0.05) Pa. Location in the estuary is the best predictor of bed stability. The next best predictor is surface sediment wet bulk density (ρb), which takes the form: τcrit,o = 6.67 × 10−4(ρb) − 0.55 Pa. Erosion rates (E) can only be correlated (significantly) with excess bed shear stress (τex) by incorporation of a time (depth)-varying erosion threshold. A power function emerged of the form: E = Eoτexm kg/m2/s (0.28 < m < 1.06). The exponent is within the range of laboratory experiments reported in the literature. The still-water settling rates of eroded material are diagnostic of a wide range of sedimentation diameters varying from fine sand to medium silt. The deposition threshold (τd) was evaluated in a laboratory equivalent of Sea Carousel (Lab Carousel) for sites RUS11 and RUS12 and yielded a mean value of 0.51 Pa. The exponential decay constant (k) for still-water settling is strongly dependent on initial concentration and suggests a constant settling rate that fits with data from a wide variety of settings throughout Canada."
645651254c7848b31449dff8c154a5359d962b0b,"The central north shore of Prince Edward Island comprises embayments separated by subtle headlands that may constrain nearshore sediment transport. The study area includes two such embayments informally known as Brackley and Tracadie bights, both of which are sand-rich onshore and sand-starved between 20 and 50 m water depth. Storm winds and waves from the northwest and northeast are common in autumn and winter. The hydrodynamic model Delft3D is used to simulate waves, currents, water levels, and sediment transport in Brackley and Tracadie bights during 23 autumn seasons between 1955 and 2005. When compared with wave and current measurements from a field experiment in the autumn of 1999, the model successfully simulates conditions during storms and fair-weather periods. Results from the simulations show that, in autumn, the weighted mean direction of transport is to the southeast (133°). Bedload transport is directed onshore to the south (170°), and suspended load is directed offshore to the northeast (67..."
83e9fd5ef4f8835a8131dfea9021670d0bc469fd,"The Circumpolar Arctic Coastal Communities Observatory Network (CACCON) functions as the Arctic Regional Engagement Network for Future Earth Coasts. In partnership with other Arctic knowledge networks and programs, including the Exchange for Local Observations and Knowledge of the Arctic (ELOKA) and Arctic-COAST, CACCON promotes consensus and collaboration to advance local knowledge availability and accessibility for adaptation planning and sustainable development in Arctic coastal communities and regions. Components of the CACCON agenda include: integrative analyses of sustainability challenges in Arctic coastal communities using co-developed situational and sustainability indicators; solutions-oriented research for actionable, proactive adaptation policies in Arctic coastal communities; sharing insights among existing community-based research and resilience programs; responding to community-based agendas and building resilience by growing local and regional knowledge co-production and dissemination capacity. These activities support the Global Coastal Futures initiative of Future Earth Coasts, rooted in the Future Earth principles of co-design and co-production of knowledge involving a broad cross-section of stakeholders and consensus-building on pathways for transformation to more sustainable strategies for enhanced present and future well-being on Arctic coasts."
a1d555827a38f1fd273d68bbc4afd704f56d9df9,
b2c49ad2f7a75b511e1b24e79da1a9bd36902917,
35c6da269141dad82f5577373b01943881f2ac5e,
3949e1a6fe9c45fd6ad7890447e61375e2e805c2,"The City of Iqaluit, Nunavut, is an expanding urban centre with important infrastructure located in the coastal zone. This study investigates the exposure of this infrastructure to coastal hazards (rising mean sea level, extreme water levels, wave run-up, and sea ice). Using a coastal digital elevation model, we evaluate the inundation and flooding that may result from projected sea level rise. Some public and private infrastructure is already subject to flooding during extreme high water events. Using a near upper-limit scenario of 0.7 m for relative sea level rise from 2010 to 2100, we estimate that critical infrastructure will have a remaining freeboard of 0.3–0.8 m above high spring tide, and some subsistence infrastructure will be inundated. The large tidal range, limited over-water fetch, and wide intertidal flats reduce the risk of wave impacts. When present, the shorefast ice foot provides protection for coastal infrastructure. The ice-free season has expanded by 1.0–1.5 days per year since 1979, increasing the opportunity for storm-wave generation and thus exposure to wave run-up. Overtopping of critical infrastructure and displacement by flooding of subsistence infrastructure are potential issues requiring better projections of relative sea level change and extreme high water levels. These results can inform decisions on adaptation, providing measurable limits for safe development."
6b2efd75cdf208583d4073f63885ca5c89946505,"Abstract Landward retreat (marine transgression) is a common response of coastal systems to rising relative sea level. However, given sufficient sediment supply, the coast may advance seaward. The latter response of gravel barriers has been recorded in parts of southeastern and northwestern Canada, where seaward‐rising sets of beach ridges are observed in areas of Holocene RSL rise. Cape Charles Yorke, northern Baffin Island, is a 5 km long gravel foreland characterized by seaward‐rising beach‐ridge crest elevations. The prograded morphology of the Cape Charles Yorke foreland is a prime example of coastal response to a combination of rising RSL and abundant sediment supply, an unusual and little‐documented pattern in the Canadian Arctic. The main gravel supply to Cape Charles Yorke is likely from eroding bedrock and raised marine deposits southwest of the foreland. Although not the dominant sediment source, the Cape Charles Yorke delta contributed to the formation of the foreland by sheltering it from easterly storm waves and providing an anchor point for the prograding ridges. The truncation of relict ridges by the modern shoreline suggests a recent regime shift from continuous deposition to predominant erosion. The cause and timing of this shift are unknown but could result from a recent dwindling in sediment supply, increased accommodation space, increased wave energy, and/or an accelerated rise of relative sea level."
6ed330f707285d64a4b2444d113acbc37068d76d,
6f1b3c7a18f092332be45e0da1a3b4152eaf1d60,
7846e4caa0ced8010894fd5e01062baaad7ac8ab,
997c1fce419916ab0694a9f8b9913a6f9ef0bc5a,"Storm overwash is an important process constant at 46-52% over the 30-year affecting the modern Aspy Bay barri~r interval spanned by the photographs. system and washover sediments form a Scour-fill sequences were monitored in major component of the barrier deposit. two washover channels during OctoberOverwash occurs as a unidirectional epiNovember 1981 to determine, the relative sodic nonuniform surge when wave uprush importance of overwash and wind transovertops the berm. Due to rapid infilport. Re~orking of washoyer sediments tration, many overwash flows do not reach by strong winds directed both onand the lagoon; however, interstitial or offshore was not a significant factor in surface ice in winter may prevent infilthe evolution of the channels during tration, increasing the frequency and the study period. On the other hand extent of overwash. Where extensive gravel lag deposits in some areas of the overtopping occurs, washover transport barrier backshore are attributed to forms sheet-like deposits; elsewhere aeolian deflation of washover sediments. the overwash is laterally confined and Four storms during the 2-month field the washover sediments form channel fills program generated overwash scour and terminating in thin fans or deltas. fill events that resulted in deposition Although the distribution of washover of composite distal-thinning washover channels on the 4-km barrier fronting units with a maximum observed thickness North Aspy Pond remained essentially of 263 mm. Washover sediments are prefixed over a 3-year interval 1966-1969, dominantly subparallel-laminated sands major changes in the location and motdipping 1.5-1.9° toward the lagoon. The phology of washover features occurred thickness of individual sedimentation between each of four sets of air photos units ranges from order 10° to 10 1 mm. taken in 1939, 1947, 1953 and 1966. The Some units appear to exhibit inverse proportion of total barrier length subtextural grading consistent with a grainject to overwash remained relatively flow transport process. Small inbri-"
9a051aba8edbcc96ecbb51094ac2e5d8a35b1788,
a6efff4dd627603cddb1dd6e9c70d4aa39bcd975,
a84419dec6c9cad9b976ba3237a84b4e8cd97e0f,
ed46257b5b185d5eee85d0cd3dcb896c03a443b5,
eda752d0232c8a3d4d775bdd49cf93528416f3bb,
1d123dc0ebc4773648a4bbf2f4cd58d75f8d361b,
2eb7648bc1aca4bf74c15d74f92671c9b606aed1,"Abstract Polar and subpolar coasts are distinctive owing to the presence of ice on land as permafrost, ground ice and glacier ice, and in the sea as tidewater glaciers, icebergs, ice shelves and sea ice. Most of these coasts remain glaciated or are recently deglaciated so their geomorphology carries a strong glacial signature. The morphogenetic environment of polar and subpolar coasts is dominated by extreme seasonality with winter development of sea ice and a shore-fast ice foot that excludes wave activity and is primarily protective. However, sea ice may also be erosional at any time of year but is most effective as an erosional agent on polar coasts between freeze-up and break-up, when wave activity forces sea ice to repeatedly impact the shore. Depending on latitude, the short summers are characterized by wave and sea ice erosion at high latitudes and by wave activity at lower latitudes. The contribution of frost weathering to cliff and shore platform development in polar and subpolar rock coasts is unclear, but is likely to be an important influence. Rock coasts are widespread in the Arctic and sub-Arctic, including Iceland, and in the Antarctic and sub-Antarctic the limited ice-free coast is almost entirely rock-dominated."
3c2f477081925f6d953aed402a7c822227fefafb,"Integrated seabed mapping is an important prerequisite for effective management of offshore areas. With the rapidly expanding City of Iqaluit on its shores and mineral resources on nearby Hall Peninsula, Frobisher Bay will undoubtedly see new infrastructure development over the next several years. The 2014 field season marked the first of a two-year, collaborative, seabed-mapping project in the region. The purpose of the project is to improve understanding of the geology of Frobisher Bay and, ultimately, to support decision-making with respect to its seabed use. Using both legacy and newly acquired high-resolution seabed-morphology and geology data, the project will generate a suite of bathymetric and geological maps for the floor of Frobisher Bay. Initial results reveal three zones (outer, middle and inner) with distinctive seabed morphology and surficial geology, and extensive evidence of seabed-slope instability in the inner zone."
4e11f62551d3a6b603085576beb7320d9195c1f0,
6e97343cad872d511c67aa96ce3b1768c2492e04,"Abstract Cold climate exerts a clear influence on the processes of marine transgression in mid- and high-latitude coastal-plain settings, but its signature in the depositional record is much clearer at high latitude. Both cases selected for this study are influenced by the legacy of past glaciation and the pervasive effects of ongoing Holocene marine transgression. Both are affected by sea ice. The high-latitude site lies within the zone of continuous permafrost and the abundance of excess ground ice along the Beaufort coast is the dominant factor distinguishing it from the mid-latitude Gulf of St Lawrence (GSL) setting and standard models of transgressive coasts elsewhere. In the southern GSL, the transgressive unconformity (TU) is at the seabed (or buried by a very thin veneer) across the inner shelf; shoreface sand moves landward, keeping pace with the transgressive front through deposition in barriers, dunes and estuaries. The pace of transgression in the Beaufort Sea is influenced by a number of distinctive periglacial erosion processes, including thermal abrasion and thaw subsidence. Marine transgression across this landscape creates intricate breached-lake estuaries and low sandy barrier beaches with limited dunes, leaving distinctive facies suites and geometry, while seaward sediment transport buries the TU on the inner shelf."
85ac47758ef40beee3b043f8c6d0d41be8a7f0be,"Pan‐Arctic rivers strongly affect the Arctic Ocean and their vast lake‐rich deltas. Their discharges may be increasing because of an intensifying hydrological cycle driven by warming climate. We show that a previously unexplained trend toward earlier ice breakup in the Mackenzie River Delta is little affected by winter warming during the period of river‐ice growth and is unaffected by river discharge, but unexpectedly is strongly related to local spring warming during the period of river‐ice melt. These results are statistically linked to declining winter snowfall that was not expected because of an intensifying Arctic hydrological cycle. Earlier ice breakup is expected to cause declining water level peaks that will reduce off‐channel flows through the lake‐rich delta before river waters enter the ocean. Thus, local spring warming with unexpected snowfall declines, rather than warmer winters, can drive earlier ice breakup in large Arctic rivers and biogeochemical changes in their river‐ocean interface."
9e5bb80991b4955d3baefeaa3eac0d8515313424,
c63233be0912fff3a68fdb64c5cf4b75fa358b93,
c6d616e55e61aa9f157d67b0ec984c4010b54f81,
cff8ca49f2c2664ce286d527c49b9fa9748fcc14,
084471acc2de13be50ac17eb4babd2b7597f30c7,
11561aa444770108344178384e91ae385834e05e,"For the engineering design of underwater pipelines and communication cables in freezing seas, reliable estimates of the frequency and penetration depth of ice-keel scour on the seabed and shoreface are required. Underestimation of ice gouging intensity on the seabed can lead to the infrastructure damage, while overestimation leading to excessive burial depth raises the cost of construction. Here we present results from recent studies of ice gouge morphology in Baydaratskaya Bay, Kara Sea. The direct impact of ice gouging by floes on the seabed and shores is described, generalized and systematized: the depth of the gouges varies from the first centimeters up to 2 m; the most intensive ice gouging is observed near the fast ice rim, due to the maximum impact executed by ice ridges frozen into large floe. We propose a zonation of Baydaratskaya Bay based on the types of ice formation and the intensity of ice impacts on the coasts and sea floor."
3105bdd1ba320e9aae0608bb2527e8cd06590da4,
435fd5fe92ca1f684e75c9635746d33324f38bc4,"Future climate scenarios and impacts modeling predict changes in climate variables that may increase coastal landscape instability and hazard risk. Projecting the future response of the coastal land system to these changes in climate forcing is a prerequisite for an effective adaptation strategy and forms the core of this ArcticNet project. Through improved understanding of changes in climate, sea-level, sea ice, storms and wave climate, seasonal thaw depths, and other aspects of environmental forcing we will assess integrated impacts on coastal landscape stability, including fl ooding, erosion, habitat integrity, and community vulnerability. Together with northern communities and partners we plan to integrate local and external research and knowledge on climatechange trends and impacts in order to provide a common basis for decision-making at all levels, thereby enhancing community adaptive capacity. Ultimately the goal is to promote informed choices of adaptation measures and enhanced resilience in northern coastal communities."
75bade6cb3f9edee030953ec67fe1ee53b434c4c,
b7a4b95bcc13acc423b2549c261908fb5e76c939,"North‐flowing rivers of the pan‐Arctic region have important effects on the Arctic Ocean, but their river‐ocean interfaces, including some with vast deltas such as the Mackenzie, have complex hydrology and remain poorly understood. Analysis of 39 years (1973–2011) of water‐levels and river discharge at the head of the Mackenzie Delta, 48 years (1964–2011) of water‐levels in the mid‐delta, and 28 years (1984–2011) of water‐levels in the outer delta permitted evaluation of changes in the timing, duration, and magnitude of peak annual water‐levels during river‐ice breakup. The initiation date of freshet‐discharge into the delta has not changed, but the duration from freshet initiation until peak water‐levels in the central delta (i.e., duration of ice clearance) has shortened from 35 to 27 days since 1964. The height of annual water‐level peaks in the outer delta at Reindeer Channel may have declined by ∼0.4 m from 1984 to 2010, but complicating factors may be influencing this result. Winter‐discharge has increased by ∼21% from 1973 to 2011, but this amount is too small to cause a trend in total Mackenzie discharge. Breakup‐discharge (i.e., occurring during ice clearance through the central delta) has not significantly changed. The lag time from freshet‐discharge initiation into the delta until initial breakage of the river ice‐sheet has declined by 6.6 days from 1974 to 2007 and is sufficient to account for the shortened period of river‐ice clearance. Declining snow‐pack depths during April suggest that river‐ice may be melting earlier and more rapidly."
c2fc006fab907199539ff58c5e72de931769c8a5,
089a949f3afc6089c7a50f14f54ea531547b7906,This activity uses available information on vertical land motion and on global glacier and ice sheet mass balance to make projections of sea-level change (to the year 2100) for Arctic communities and vital infrastructure.
28be1332ece3f7f5fb7037dfd62fa99e56b8492f,
4f63226f9ee05606ec11b348e947978641cb596a,
554f4d4a5d6e934f5fbff836b13975f9bda68509,
82843bfefc776c0e42c4fd7745e73f9d4593de88,
958440f47ae1c1bbb97dfe053e8eaeb3577931bc,
ad90978423cb056d714c7a7cd551a2be7e11ec34,
c4da4c9405497815de954ec9e488d58f7e892ec3,
1429b0433e910c264f1caf16883d8155468e5798,
24419d5e4a663d5a2c568744bb6db4aee9f37a8c,"The Grayling Platform, which was built in 1967, is owned and operated by UNOCAL as a development platform of the Alaska’s Cook Inlet McArthur River field. The platform was built in response to the extensive oil and gas discoveries in the Cook Inlet by Standard Oil, Pan American, and Ohio Oil Companies in 1960’s. Following the discovery, further field development led to over 35 exploration wells being drilled in the Cook Inlet by various operators on other platforms."
3c7c88b6d4aaf911614b5244c45735e6f458bee1,"Abstract This paper examines the short-term evolution of a cuspate foreland with diminished sediment supply in the western Gulf of St. Lawrence. Topographic light detection and ranging and airborne light detection and ranging bathymetry are used to provide an overall analysis of the foreland system at Paspébiac, Quebec, including high-resolution digital morphology of combined subaerial and subaqueous components, between 2003 and 2006. Results indicate large differences in coastal stability around the foreland. The western barrier exhibits a stable shoreline (net change  =  +0.3 m·y−1), a moderate beach slope (0.12–0.18), and no subtidal bar system. The morphodynamic response in this sector is influenced by jetties and alongshore variability and is related to beach planform readjustments to varying wave conditions. The eastern barrier has higher wave exposure, high erosion rates (<6.7 m·y−1), wave washover, and an intermediate barred-beach profile, with higher beach slopes (β  =  0.16–0.24). The alongshore variability is controlled, at length scales of about 500 to 700 m, by differences in relaxation time between distal and proximal sections of the foreland. At shorter length scales (∼100–500 m), alongshore variation is related to inner bar morphology, higher erosion rates being observed near rip channels or in the absence of an inner bar. Sand transport patterns reflect wave energy and approach and include a reversal in transport direction along the eastern barrier under storm waves from the SW. We show that, under declining sediment supply, sediment is being lost to a shoal and deep water off the tip of the foreland and erosion on the eastern barrier is not compensated by the slow accretion on the western side of the point."
46ea0b8008f02f7213588b3b59c1304795a65dee,
4aeb23554bb2e70d30af66f3259fba062ada40b4,"Abstract ST-HILAIRE-GRAVEL, D.; FORBES, D.L., and BELL, T., 2012. Multitemporal analysis of a gravel-dominated coastline in the central Canadian Arctic Archipelago. This study assesses the stability of Arctic gravel coasts across a range of timescales, based on field and remote-sensing studies of three coastal sites near Resolute Bay, Nunavut. It considers shore-zone sensitivity to ice, wind, and wave forcing at storm-event and annual timescales within a longer-term context, including coastal emergence resulting from postglacial isostatic uplift partially counteracted by accelerating sea-level rise. Another long-term factor associated with climate change is the potential for increased seasonal depth of thaw in the beachface and nearshore. The coast in this area is ice bound on average for 10 months of the year, but the annual duration of ice cover has decreased over the past 30 years (1979–2009) by 0.95 d/y. A longer open-water season has implications for the number and timing of storm-wave events, with increased probability of storms impacting the coast later in the season when the seasonal thaw layer is approaching maximum thickness. Overall, shoreline progradation surpassed erosion in the Resolute area between 1958 and 2006, reflecting a combination of sediment supply and emergence. The coastal impacts of storms were found to be short lived and not necessarily indicative of longer-term trends. Gravel shorelines can be resilient in the face of intermittent storm impacts, but thresholds of stability in this high-latitude setting are poorly understood. If current trends of rising sea level, increasing open-water duration, and more frequent effective wave events continue, there is a heightened potential for more rapid coastal change in the region."
af6e30a9d2acc1d5005d97465f9012e33f279723,
b6adff5b2d8def9ed1c02d28a5c420288db7da83,
119f009f7273d3572bdac6c78caa8da888d33bd5,
2695ec0d99c531c7c6c9e137f79ca714f8c17a64,
3723c175f97147b57aa11917aa3eab1da2807eed,"The coast is a key interface in the Arctic environment, a locus of human activity, a rich band of biodiversity, critical habitat, and high productivity, and among the most dynamic components of the circumpolar landscape. A very large proportion of Arctic residents live on the coast and many derive their livelihood from marine resources. The coast is a region exposed to natural hazards and particularly sensitive to climate change; it is thus a high priority for change detection and awareness acknowledged by the Arctic Climate Impact Assessment (ACIA) and the Arctic Human Development Report (AHDR). Under the patronage of Land-Ocean Interaction in the coastal zone (LOICZ), the International Arctic Science Committee (IASC), the Arctic Monitoring and Assessment Programme (AMAP) and the International Permafrost Association (IPA), a new initiative was formed to fill the gap observed in existing reports and assessments and to highlight the uniqueness of Arctic coasts. Its objective is to produce the first review on the state of Arctic coasts and to provide an outlook on the fate of coastal biophysical and societal environments, underlining the complex interactions at work at the land-Ocean interface. This presentation provides an update on the progress of this report and a few excerpts from the document."
6bc75858ae488dab88c0ec221aff6f1d51402d33,"D.L. Forbes1, J. Charles2, G.K. Manson1, C. Hopkinson3, R.B. Taylor1, R. Wells2, and D. Whalen1 1. Natural Resources Canada, Bedford Institute of Oceanography, Dartmouth, Nova Scotia B2Y 4A2, Canada ¶ 2. Real Property Planning, Halifax Regional Municipality, Halifax, Nova Scotia B3J 3A5, Canada ¶ 3. Applied Geomatics Research Group (AGRG), Nova Scotia Community College, Middleton, Nova Scotia B0S 1P0, Canada"
853b965ad12769af6b8a35d6863e15f98a02378d,"Changing RSL Even though RSL is in itself a passive control, the rate and direction of RSL change is of great significance to beach evolution since it constrains the reach of wave processes. RSL rate and direction of change is a determining factor in the access to sediment source and in the rate of sediment supply (Forbes et al., 1995). For example, a higher rate of RSL change results in higher rates of sediment supply."
ad6b4372d4012ff9b2c3dcaa2384154561dc252f,"This study investigates whether raised beach sequences preserved on emergent coasts of the central Canadian Arctic Archipelago contain a proxy record of past sea-ice conditions and wave intensity. We hypothesize that periods of reduced sea ice (increased open water) expose shorelines to more prolonged and higher wave energy, leading to better-developed beach ridges. Surveys of raised beach sequences on Lowther Island revealed the following patterns: a) high, wide, single- to multi-crested barriers backed by deep swales or lagoons characterize both the active and lowest relict shorelines; b) small, narrow, discontinuous ridges of poorly sorted gravel extend from 1.0 to 7.5 m asl, except from 4.5 to 5.0 m asl; c) ridge morphology is similar to the active and first relict ridges between 7.5 and 11 m asl; d) a near-featureless zone with minor terraces and ridges above 11 m extends to above 30 m asl. These distinct morphological and sedimentary units are interpreted as a function of wave climate and thus of summer sea-ice conditions. This model suggests periods of greater wave activity from the present day back about 500 14C years (530 cal BP; Unit A), during a short interval from 1750 to 1600 14C years BP (1750–1450 cal BP; Unit B′), and earlier from 2900 to 2300 14C years BP (3030–2340 cal BP; Unit C). Units B and D are interpreted as the result of more severe ice conditions with lower wave energy from 2300 to 500 14C years BP (2340–530 cal BP) and earlier from more than 5750 to 2900 14C years BP (6540–3030 cal BP). Discrepancies with previously published interpretations of regional sea-ice history may reflect the local nature of the beach proxy record, which implies occurrences of extensive open-water fetch east and west of Lowther Island but cannot be extrapolated to a regional scale. The beach record shows distinct variation through time and provides an alternative window on past summer ice extent in central Barrow Strait."
1f1fdd1f9a40e6d42f28fba6d144a54c2506512f,
89dad0773cb5310f369db8cb5d8d657b8286fa72,"This paper demonstrates a methodology for efficient mapping of seabed composition in shallow coastal areas using airborne light detection and ranging (lidar). Employing the latest Scanning Hydrographic Operational Airborne Lidar Survey (SHOALS) bathymetric survey system, we show that bottom sediment textures and algal cover types can be discriminated and classified using shape parameters of the lidar bottom-return signal, supplemented with bathymetric data, notably small-scale roughness. The study area is a region of moderate wave energy with siliciclastic sediment along the north shore of the Baie des Chaleurs in Quebec. Environmental factors affecting the accuracy of the method include sea state, turbidity, benthic cover, patchiness, and fuzzy class boundaries. Technical factors contributing to classification errors include signal definition, water column attenuation, band selection issues, class definition, and possible errors in interpretation of validation data. This study achieved an accuracy of 67% in a complex and challenging setting. Further development of the methodology is expected to improve the quality and expand the applicability of this approach."
9955ce8aae59066f60bc7e1761601450bb25c2ff,
18b74d808694a5c9c9ca28acb1ae12035b183ebb,
317a02c5800e1aa89a9e489d625b07eef71de17e,
934b229ad6e9f5fc8b0cdbf5cbbcb943c24f13d1,
f7fa6754427385980a4cb5e279a3e1b225464c13,"Growing concern about the state of Arctic sea-ice has been highlighted in the recently published ACIA report (2005). The alarming trends of decreasing sea-ice extent and thickness in the Arctic Ocean stress the need for comprehensive studies of past sea-ice variability. The primary goal of this study is to investigate whether raised beach sequences, preserved on the emergent coastline of the central Canadian Arctic, contain a proxy record of past sea-ice intensity. The research approach is to compare variations in beach morphology and sedimentology with a proxy record of sea ice intensity derived from driftwood and whalebone occurrences on emerged beaches. It is hypothesized that periods of reduced sea ice intensity and increased open water would expose shorelines to higher and more prolonged wave energy, leading to better developed beach berms. More specifically then, the study will: a) document variability in beach morphology and sedimentology as a function of age and b) assess the extent to which variations in"
895b03f8a8bcb086e81f3fb9a20105b099ff6266,
8b67f0c826a28e417c22bdd5d6b405c1815ca9bc,
b84e9edcf84c447007adc11ec303e99b957876af,
ed0b9dafe84dcebc585b3a69b8261f6df0a0d525,"As analogs for impact of a future sea-level rise on the coast of Nova Scotia (eastern Canada), geological data and information on relative sea-level changes are examined at three different time scales. Relative sea level rose swiftly during the early Holocene, at a maximum rate of 11 m/ka at 7500 radiocarbon years BP. Freshwater, salt-marsh, and estuarine sediments that formed during this period have been located on the inner shelf. After 5000 BP the rate slackened to about 2 m/ka. Despite overall submergence and coastal retreat since that time, gravel barriers have persisted where large amounts of sediment have been added to the littoral system by erosion of glacial deposits. The barriers often display evidence of early progradational phases in the form of gravel beach ridges, partly or wholly submerged in lagoons behind contemporary storm beaches. Tide-gauge data from the past century show submergence rates averaging 3.5 mm/a, well in excess of the longterm trend. The response of the coastline to this rapid rise is complex. Unconsolidated cliffs (bluffs) retreat at up to 5 m/a during initial exposure to wave attack and during extreme storm events, but at lesser rates ( 8 m/a) in some locations, but low elsewhere, in some cases showing almost no movement over the past 10 years, and neighbouring beaches are sometimes observed to behave in completely different ways. Sediment released by coastal erosion finds its way into nearby estuaries, causing growth of flood-tidal deltas and marsh aggradation. If a global rise in sea level occurs, the processes of erosion and sedimentation operating along the coast of Nova Scotia during the Holocene are expected to continue in a similar fashion, but rates of change will increase at many locations."
114f685b407babfdf41f25dcc6ee0e7410f4b360,"The results of this work enable specification of the vertical and horizontal accuracy of LiDAR systems. The horizontal error is mainly the result of aircraft positioning and LiDAR spatial resolution limitations and is assumed to be smaller than 30 cm. LiDAR vertical errors were found to be slope-dependent, the greatest errors being observed on cliffs. Sea cliffs are in any case the most challenging features to survey and wave-cut notches constitute a major limitation to airborne surveys. The portion of the cliff that can be surveyed with an airborne LiDAR has been determined according to notch dimensions and laser beam incidence angle. Keywords-LiDAR; remote sensing; accuracy; coastal zone; erosion monitoring; sea cliffs"
137537f09acf9bbab60b56e71315ce7e3661191e,
57fd8cafc8e0b165d5fd4f30f114d4227a66c937,"Coastal flooding from storm-surge events and sea-level rise is a major issue in Atlantic Canada. Airborne light detection and ranging (lidar) has the spatial density and vertical precision required to map coastal areas at risk of flooding from water levels typically 1–2 m higher than predicted tides during storm surges. In this study, a large section of the New Brunswick coast along Northumberland Strait was surveyed in 2003 and 2004 using two lidar systems. Water levels from a major storm-surge event in January 2000 were surveyed using a global positioning system (GPS) and used as a benchmark for flood-risk maps. Maps of flood depth were also generated for all water levels and used for socioeconomic and ecosystem impact assessment. Flood-risk maps were constructed using standard geographical information system (GIS) processing routines to determine the spatial extent of inundation for a given water level. The high resolution of the lidar digital elevation model (DEM) captured embankments such as raised roadbeds that could prevent flooding inland. Where connectivity was present due to culverts or bridges, the DEM was notched across the roadbed to simulate the connection between the ocean and upstream low-lying areas in the GIS. An automated routine was then used to generate maps of flood extent for water levels at 10 cm increments from 0 to 4 m above mean sea level. Validation of the flood-risk and flood-depth maps for the January 2000 storm-surge water level by field visits indicates that the simulations are generally accurate to within 10–20 cm. The lidar data were also used to evaluate the potential for overtopping and dune erosion on a large coastal spit, La Dune de Bouctouche. This showed a high vulnerability to storm damage for critical habitats on the spit. The lidar-derived maps produced in this study are now available to coastal communities and regional planners for use in the planning process and to assist in development of long-term adaptation strategies."
6df5f948283cfa27ff1fc2fd785ce0d67a807f4c,"Well-publicized community concern about changing coastal conditions led to Sachs Harbour being chosen as one of the fi rst coastal sites for detailed study along the E-W gradient as part of ArcticNet Project 1.2. Fieldwork, including RTK coastal profi ling, and analyses of aerial photography and QuickBird images were used to assess coastal processes, rates of coastal retreat, and long-term decadal changes in coastal erosion rates near the community of Sachs Harbour, NWT. The southwestern coastline of Banks Island is dominated by low bluffs composed of frozen unlithifi ed glacially derived sediments containing segregated ice lenses and ice-rich silty"
9b6ab294ca10812e8bb63d8bf23d263388fa4e7c,
c159b124902a46d509a89262e337ae7e4854f9dd,"This paper presents an attempt at calibrating airborne LiDAR return intensities by correlating them with several geotechnical parameters. The influence of four factors on the backscattered LiDAR intensity has been illustrated. The most influential one is sediment water content, which produces an exponential decrease of return signal intensity. Laser beam incidence angle, sediment grain size, and sediment density are also important factors. Using these results, a preliminary equation is proposed to describe the relationship between return intensity and the four key factors. Keywords-LiDAR; remote sensing; coastal zone; morphology; sedimentology"
e6cef0a31147cc96832f7a3edc34dd3bf0bd00bc,
1643ad5ad79df49fc48c7ff2475fe68cc18e5b65,
39bb0d1123e4ebfde04b2277d6e932481e5854d1,
5197823cb81f0a9c12502da9921bc7b1f10c0fc6,"With observed climate warming in the western Canadian Arctic and potential increases in regional sea level, we anticipate expansion of the coastal region subject to rising relative sea level and increased flooding risk. This is a concern for coastal communities such as Holman NWT and Kugluktuk NU and for the design and safety of hydrocarbon production facilities on the Mackenzie Delta. To provide a framework in which to monitor these changes, a consistent velocity field has been determined from GPS observations throughout North America, including the Canadian Arctic Archipelago and the Mackenzie Delta region. An expanded network of continuous GPS sites and multi-epoch (episodic) sites has enabled an increased density that enhances the application to geophysical studies including the discrimination of crustal motion, other components of coastal subsidence, and sea-level rise. To obtain a dense velocity field consistent at all scales, we have combined weekly solutions of continuous GPS sites from different agencies in Canada and the USA, together with the global reference frame under the North American Reference Frame initiative. Although there is already a high density of continuous GPS sites in the conterminous United States, there are many fewer such sites in Canada. To make up for this lack of density, we have incorporated high-accuracy episodic GPS observations on stable monuments distributed throughout Canada. By combining up to ten years of repeated, episodic GPS observations at such sites, together with weekly solutions from the continuous sites, we have obtained a highly consistent velocity field with a significantly increased spatial sampling of crustal deformation throughout Canada. This exhibits a spatially coherent pattern of uplift and subsidence that is consistent with the expected rates of glacial isostatic adjustment. To determine the contribution of vertical motion to sea-level rise under climate warming in the Canadian Arctic, we have established co-located tide gauges and continuous GPS at a number of sites across the Canadian Arctic. AGU Fall Meeting San Francisco, December 5-9, 2005 Paper G11A-1203 LiDAR survey to simulate effects of subsidence in the outer delta. Lower panel shows flooding to 1 m above MSL (upper panel). This could become the mean topography through a combination of regional and local subsidence. Fjord, boulder barricade, and tidal flats, Pangnirtung (Baffin Island) NU Preliminary velocity field for combined CGPS & EGPS network in Canada Natural Resources Ressources naturelles Canada Canada"
66f03ac06f8e65aff12dd1d8fe7d4d145ecafd32,
90fdd642fc2694c151fe2176bd5de0844b2c5c84,"The coastal zone, comprising the intertidal zone and a band extending miles landward and seaward of the shoreline, is an area of constant dynamic change. It encompasses some of the world’s most valuable resources, both natural and economic. Coastal hazards include storm-surge flooding, storm-wave run-up, ice ride-up or pile-up, coastal slope failure, and shoreline erosion, among others. It is important to determine the potential for changing hazard conditions under existing rates of relative sea level rise and the possible effects of warming climate, including accelerated sea level rise, increased storminess, reduced sea ice, and other factors. Appropriate design, land management, and risk reduction depend on an adequate understanding of the hazards and realistic mapping of hazard zones. This paper describes how the coastline can no longer be considered as merely a line on paper, it should be understood as a three-dimensional landform that is subject to physical change over time. Coasts, especially those in high energy zones, can be difficult and very costly to map accurately. Recently developed remote-sensing techniques such as airborne laser altimetry provide a new capability to produce high-resolution digital elevation models of low-lying flood-prone terrain in support of other innovative approaches to risk reduction and hazard mitigation. The paper describes how recent technological development s have led to significant advances, including the evolution of geographic information systems (GIS), electronic charts, global positioning systems (GPS), high resolution multibeam swath bathymetry and airborne laser altimetry (LIDAR). Using remotely sensed data at low tide, on can acquire continuous coverage of the intertidal zone, allowing a seamless merger of land topography with bathymetric data. Potential coastal flood areas can also be accurately determined with high-resolution digital elevation modes acquired by airborne and spaceborne remote sensing."
5a83149dd6e82b9e4d6e2bc02a9c505d18ef8660,"by H. Thorleifson1, T. Anderson1, R. Betcher2, R. Bezys, W. Buhay3, S. Burbidge4, D. Cobb5, N. Courtier6, J. Doering7, G. Fisher-Smith3, D. Forbes8, W. Franzin5, K. Friesen3, D. Frobel8, D. Fuchs7, C. Gibson9, P. Henderson1, K. Jarrett10, T. James6, J. King9, H. Kling5, A. Lambert6, W. Last7, M. Lewis8, L. Lockhart5, G. Matile, T. McKinnon4, K. Moran8, E. Nielsen, S. Pullan1, F. Rack11, J. Risberg12, C. Rodrigues13, A. Salki5, C. Schröder-Adams4, M. Stainton5, A. Telka1, B. Todd14, R. Vance1 and W. Weber"
c71a34b6059f3607f71769a3aa463d0b0ae190e1,
d13b3b692033ef95c0e19fd060108ee4829cbd34,
df061a0bd6f5cf0c4b97f51f25d1a210f0607ccf,"As part of a recent project to determine coastal impacts of climate change and sea-level rise on Prince Edward Island (PEI), airborne scanning laser altimetry (lidar) was employed to acquire high-resolution digital elevation models (DEMs) and other landscape information. The study area included both the Charlottetown urban area and an extensive portion of the rural North Shore of PEI. Problems with the lidar data included data gaps and incorrect classification of ""ground"" and ""non-ground"" laser hits along the waterfront. Accurate representation of wharves and other waterfront features in the DEM was achieved by combining ""ground"" and ""non-ground"" data. The importance of calibration and validation in lidar data acquisition and interpretation was demonstrated by three independent validation exercises that uncovered and adjusted for a vertical offset attributed to calibration problems. The ground DEM was adjusted to hydrographic chart datum and used to model flood extent at three storm-surge water levels, one observed in the record storm of 21 January 2000 and two higher levels representing flood scenarios under rising sea level. Flood modelling was executed in a geographic information system (GIS) on the gridded ground DEM. The resulting binary grids were vectorized along the flooding limit. Low-lying areas isolated from free exchange with the harbour were excluded from the flood area. Vectors depicting the storm-surge water lines for the three flood scenarios were implemented on the geographic information system (GIS) in the city planning department and overlain on property boundary and assessment layers. This study demonstrated that validated DEMs derived from airborne lidar data are efficient and adequate tools for mapping flood risk hazard zones in coastal communities."
2b6a678a55e18090f911bb2fc4274f7f2ab783ae,
4817b1b7cf8690eeb23e96e97e976886b4e0eb8c,
6689273fc917fe7ed2f874e1782da9ca2804c39c,
7f0127cb9b65a3b44d91657d003386b38c57f784,"A 30-year record of sea ice in the Gulf of St. Lawrence (southeastern Canada) demonstrates quasi-periodic variation in seasonal total accumulated ice cover and length of ice season, with a statistically ambiguous trend to less ice in recent years. Ice inhibits wave generation and shorefast ice provides temporary shore protection during much of the winter storm season, although damaging ride-up and pile-up can occur during storm surges with onshore wind stress. The nearshore ice complex (NIC) typically includes an icefoot rampart landward of shorefast ice, which is stabilized by pressure ridges grounded on nearshore bars. With predictions of greatly diminished ice cover in future, the impact on shore stability will depend primarily on the sensitivity of coastal ice to climate change. The NIC is not represented in global or regional climate models simulating ice cover, nor is it covered by present monitoring activities. New methodologies and protocols are needed to document coastal ice features and extent. INTRODUCTION Winter ice in the southern Gulf of St. Lawrence (Fig. 1) is among the most southerly sea ice in the Northern Hemisphere and may be sensitive to small changes in climate. The implications for coastal erosion and management are significant, because ice limits wave generation and shorefast ice provides temporary shore protection during much of the winter storm season, when prevailing winds are onshore. Climate models project a significant decrease in sea-ice extent and length of ice season over coming decades, with significant loss of ice within 50 years (e.g. Flato et al., 2000). More extensive open water fetch and a possible increase in wind intensity are expected to result in greater winter wave energy. The impact on shore stability will depend primarily on the sensitivity of coastal ice to climate change, but past ice monitoring by the Canadian Ice Service (CIS) and others has not included the nearshore ice complex (NIC), nor is it represented in existing ice climate models. Considerable 1 Geological Survey of Canada (GSC), Bedford Institute of Oceanography, Dartmouth NS B2Y 4A2 2 Canadian Ice Service (CIS), 373 Sussex Drive E-3, Ottawa ON K1A 0H3 3 Canada Centre for Remote Sensing (CCRS), 588 Booth Street, Ottawa ON K1A 0Y7 effort has been devoted to documenting ice dynamics and surface morphology offshore in the Gulf of St. Lawrence (GSL), in part because of its importance for winter navigation, but little work has been reported on nearshore ice conditions in the region (see, however, Owens, 1976; Bouchard and Hill, 1995; Manson et al., 2002). Recent efforts to determine potential impacts of climate-change on the coasts of Prince Edward Island (PEI) and New Brunswick (NB), in the southern GSL, have provided new data on nearshore ice dynamics, the nearshore ice complex, and its relevance to coastal vulnerability. In this paper, we present an analysis of decadal-scale variation and trends in ice cover (coverage and duration), based on more than 30 years of monitoring by the Canadian Ice Service (CIS). We provide a brief description of nearshore ice features and related effects and assess the potential for monitoring nearshore ice using new imaging technology. Figure 1: Study area in southern Gulf of St. Lawrence, Canada. SEA ICE IN THE GULF OF ST. LAWRENCE Ice monitoring Charts of ice concentration and thickness are prepared by the CIS from visual shorebased and ship reports, airborne reconnaissance, and more recently using airborne and spaceborne remote sensing. Since 1968, ice charts for Canadian east coast waters have been prepared on a weekly basis in winter. Using the years 1971 to 2000, these charts provide a climatological record of sea ice in the GSL (McCulloch et al., 2002). The ice cover within a given area is represented by seasonal cumulative coverage, termed the total accumulated [ice] cover (TAC). This is calculated by summing all individual weekly ice coverage data (product of area and mean concentration) from the beginning to the end of each ice season. The TAC can be used as an ice severity index for the"
93a13e86275f00047ce02cf6ae30111f1876f3ab,
46481dc720bd1b640a9d4e6e6867039415997cf8,"New findings of paleoenvironmental change in Lake Winnipeg, southern Manitoba, reveal evidence of unexpectedly dry conditions from 7.5 to 4 ka (7500 to 4000 radiocarbon years before present), with reduced lake area in the north and a desiccated lake basin in the south. Changes in extent of this large lake, now ∼400 km long, can be explained by a combination of (1) expansion due to postglacial differential uplift (tilting), and (2) lake-area reduction due to drier climates associated with the former presence of dry-grassland vegetation. Comparing lake areas sustainable by grassland climate with computed potential lake areas based on the assumption of open (overflowing) conditions, we quantify the atmospheric moisture reduction represented by the middle Holocene dry conditions. This approach holds promise for calibrating regional models of climate change and exploring the effects of dry paleoclimates in other large lake basins such as the Laurentian Great Lakes. The ongoing postglacial tilting is of societal concern because it contributes to long-term lakeshore erosion and to the decrease in discharge capacity of the inflowing flood-prone Red River in a populated region."
71a17d8cdc57eb96a1df2f147ca1262cc4581642,"The sensitivity, adaptive capacity, and vulnerability of natural and human systems to climate change, and the potential consequences of climate change, are assessed in the report of Working Group II of the Intergovernmental Panel on Climate Change (IPCC), Climate Change 2001: Impacts, Adaptation, and Vulnerability. This report builds upon the past assessment reports of the IPCC, reexamining key conclusions of the earlier assessments and incorporating results from more recent research."
9ff1dd12ee7ea0d6b739f0241bd53bc9ea3db1e1,
b4bafb6bb2c4b84793bcb63b966b52dc8d50d4fa,"Wind, wave and current measurements were carried in the nearshore zone of the Canadian Beaufort Sea at two coastal sites having distinct morphologies. The first site is a sandy beach backed by a low bluff, while the second site consists of low-lying barriers. Computation of potential sediment transport using a numerical model for combined flow conditions (LI and AMOS, 1993) suggests that coastal morphology may play a significant role on circulation and sediment transport on the shoreface during storm events. Downwelling near-bottom currents and offshore sediment transport were observed at all sites during storm surges, but with some variations in the shoreface current patterns and sediment transport. According to the numerical model used in this study, offshore sediment transport is more significant where the beach is backed by a bluff acting as a natural barrier. Such condition appears to be favorable to the development of strong seaward-directed horizontal pressure gradients that drive offshore bottom currents. Along low barriers that are easily submerged and overwashed, sediment transport is mainly directed obliquely offshore due to more limited set-up of sea level at the coast during storm surges. These results suggest that coastal morphology may be responsible for variable offshore sediment dispersal on the shoreface during storms. Our results show that sediment may be transported offshore to depths from which fairweather waves may not be capable of returning the material onshore. Consequently, a loss of material to the offshore may be greater where overwashing is restricted due to the presence of a coastal feature that acts as a boundary for onshore-driven surface waters."
82a7f140710a509220f7fe10df1894ef0eb8909e,"Research along Nova Scotia's Eastern Shore has shown The North Atlantic Oscillation Index (NAOI), defined as that decadal-scafe coastal evolution is affected by varying the difference between mean normalized winter sea-level air relative sea-level rise, stonniness, stonn-surge frequency, and pressures in the Azores and Iceland, has been correlated to a sediment supply. Some combination of these factors may number of meteorological and oceanographic phenomena, cause a sudden shift in shoreline response from relative including decadal-scale sea-level variability across the North stability to rapid change. Two episodes of rapid coastal retreat Atlantic. Rapid sea-level rise at Halifax appears to occur are interpreted from historical charts and airphotos of during downward trends in the NAOI. The 1954-1964 stormy McNab's Jsland at the entrance to Halifax Harbour. The first period occurred during the lowest point in the NAOJ since occurred sometime between about 1750 and 1850 and the 1864. Earlier episodes of storminess reflected in the second between 1954 and I 964. Other sites along the Eastern sedimentological record of Halifax Harbour appear to Shore show accelerated retreat aft.er 1954, although the most correspond to minima in an extended NAO! constructed from rapid retreat in some places occurred in the late 1960s or a Greenland ice core. 1970s as well as in the mid 1990s. It is important to recognize that geological factors, Relative sea level measured at Halifax has been rising particularly sediment supply, also influence the shoreline since 1920 at a mean rate of 3.0 mm/a. The record shows response to varying environmental forcing. Coastal change significant decadal and multi-decadal variability. From I 920 occurs through non-linear interaction between storm waves, to 1970, the mean rate of rise was 4.0 mm/a, declining to 0.8 stonn surge, sea level, and shore-zone morphology and mm/a after I 970 and increasing again in the 1990s. Winds sediments. Limitation of supply or a vulnerable self-organ ized measured at CFB Shearwater since 1953 show an anomalous morphology· may render shorelines more susceptible to period of storminess between 1954 and 1964. This period was exceeding the threshold and change forced by storms and seacharacterized by increased frequency of storms with a level rise. Global climate change may affect the NAO!, thus southeasterly modal storm wind direction, in contrast to less stonniness and sea-level rise, with a potential impact on rates frequent stonns with a southwesterly mode from the 1970s to of shoreline recession. the early I 990s."
b7bc2058c0d5b64b56747b1e4526104b02ea55bc,
c97ca04c8c302995b3b6b5b0288abb3da6358d2b,
cba72eace654c07491b0bafc8ff8c365e6a8bd41,
cfa06a983bb5f43c6411c056156b8627ee05b5eb,
039a50d2530eee384cebc11f8641f2e5149d23ea,
0ba4339753253917f20bc8d4377349b53d41a57f,"The sea-level rise that may result from global climate change is placed within the context of past and present sea-level changes on Canadian coasts. To assess future impact, a dimensionless index of sensitivity is determined. Coasts with low, moderate, and high sensitivity constitute 67%, 30%, and 3% of the total coastline, respectively. The most sensitive regions are: (1) several parts of the Maritime Provinces; (2) two areas of the British Columbia coast; and (3) a large part of the Beaufort Sea coast. Impacts in four regions - Bay of Fundy, Beaufort Sea, Fraser Delta, and Eastern Shore of Nova Scotia - are discussed in detail. It is argued that the societal response to changes in sea level should favour retreat and accommodation strategies. 
 
 
 
Il est possible que les changements climatiques globaux provoqueront une elevation du niveau de la mer. Nous examinons ce scenario dans le contexte des changements passes et presents du niveau de la mer sur les cotes canadiennes. Pour evaluer l'impact de l'elevation prevue un indice non dimensionnel de vulnerabilite est determine. Les cotes a la vulnerabilite basse, moderee et elevee constituent, respectivement, 67%, 30%, et 3% de tout le littoral. Les regions les plus vulnerables sont: (1) plusieurs regions dans les provinces maritimes; (2) deux zones sur la cote de la Colombie britannique; et (3) la plupart de la cote de la mer de Beaufort. Nous discutons en detail les impacts dans quatre regions, soit la baie Fundy, la mer Beaufort, la delta du Fraser et la rivage dit ‘Eastern Shore’ de la Nouvelle-Ecosse. Nous estimons que la reponse sociale aux changements du niveau de la mer devrait favoriser des strategies de retraite et d'accommodement."
682adda3efabb1d34fcacdd97e8043a6259658af,
1227ac7bbc962b9478dbb9fbb355136036113313,
4ece6efb0a70361b0c39ce8c0d1ac49e32623530,"Several Pacific Island nations have been funded to undertake studies of coastal vulnerability to climate change and sea-level rise. Using examples from two such studies, in the Suva region (Fiji) and Tarawa Atoll (Kiribati), we demonstrate approaches to coastal hazard and vulnerability assessment [VA] at present sea level and under various scenarios for accelerated sea-level rise [ASLR]. Standard procedures for VA, as laid out in the IPCC Common Methodology and the US Country Studies guidelines, are inappropriate for tropical reef coasts and atolls in small island developing states. In this study, the standard guidelines were modified as appropriate for the different study sites. Emphasis was placed in the Suva VA on cyclone storm surges and flooding in relation to shore protection and urban infrastructure, and in the Tarawa VA on historical shoreline changes, sand supply, beach stability, and flooding in relation to population pressures and limited infrastructure. Environmental data were combined in a geographic information system [GIS] with data on shore type and condition, reef morphology, shore-zone topography, historical trends, and socioeconomic factors such as population density, land values, land use, infrastructure, and other development factors to determine vulnerability in a geographically referenced framework. Interannual variations in water level related to El Nino/ Southern Oscillation and other processes may be comparable to predicted water-level trends under climate change and must be incorporated in the analysis. Existing models for coastal response to rising sea level are inappropriate; new models, incorporating reef response and sand supply, are urgently needed for realistic vulnerability assessment. Improved models of reef-lagoon circulation, storm-surge, setup, runup, and wave transmission over reefs are needed for hazard zone delineation and risk assessment in settings such as Suva. Large uncertainties in the specification of physical impacts, including the qualitative response of atoll beaches to rising sea level, have obvious implications for the assessment of socioeconomic impacts and response options. Monitoring is required to detect changes and facilitate ongoing VA as an iterative process. With serious deficiencies in the baseline data required for effective VA, and significant uncertainties in the prediction of physical impacts, these issues demand attention as critical aspects of capacity building for climatechange preparedness and mitigation in the Pacific. Furthermore, the quantitative GIS approach to coastal vulnerability adopted in this study is equally appropriate for coastal hazard assessment under present conditions, a necessary prerequisite for realistic"
c1114c74546d0e77ea2944b518ccc0bbca997b19,
d2bb0fa2615ee9a8076d4ffc4481ccb89b72d9c0,
0226f1866bc8cf34186f06af55463ee1f4f9642f,"Relative sea level in coastal regions of Newfoundland fell from late-glacial maximum levels to postglacial minima in several phases: (i) an early period of high relative sea level, when Late Wisconsinan ice was at the coast and discharging meltwater plumes into the ocean; (ii) a period of rapidly falling relative sea level, during which glaciers retreated inland; and (iii) a period without glacier ice, during which relative sea level continued to fall, but at decreasing rates. Falling relative sea level caused fluvial incision of glacial deposits in some coastal embayments, and culminated with the construction of lowstand marine deltas. These deltas were submerged during the subsequent Holocene transgression. Seismic reflection data from selected deltas show that they comprise wedges of sediment with prograded, seaward-dipping, foreset-style internal reflections. The depth of the relative sea-level lowstand varies spatially, and it was diachronous. It occurred relatively early and deep in peripheral regio..."
34e17530191babcf01295b700796550705be4531,"The evolution of nearshore bars at three sites is examined using a mathematical model to simulate wave-bed interactions induced by progressive wind-generated surface waves. The wave dynamics are described by nonlinear, dispersive, shallow-water theory; the wave-induced flux of sediment is calculated using an associated mass-transport velocity; and the bed topography is described using a continuity equation. The resulting coupled system of nonlinear partial differential equations is simplified using a modal decomposition of the surface wave and is approximated numerically. The model is used to simulate bar development from an initial plane shoreface slope (equivalent to the mean slope at each site) using peak wave periods for moderate and severe storms observed in the region. Predictions of bar number and spacing show good correlation with measured bathymetric profiles. The model provides a physical explanation for the observed correlation of bar number and spacing to basin dimensions and nearshore slope i..."
3c2ec92d8ca4dd7a811d3bdab5968d749cac282a,"Relative sea level in coastal regions of Newfoundland fell from late-glacial maximum levels to postglacial minima in several phases: (i) an early period of high relative sea level, when Late Wisconsinan ice was at the coast and discharging meltwater plumes into the ocean; (ii) a period of rapidly falling relative sea level, during which glaciers retreated inland; and (iii) a period without glacier ice, during which relative sea level continued to fall, but at decreasing rates. Falling relative sea level caused fluvial incision of glacial deposits in some coastal embayments, and culminated with the construction of lowstand marine deltas. These deltas were submerged during the subsequent Holocene transgression. Seismic reflection data from selected deltas show that they comprise wedges of sediment with prograded, seaward-dipping, foreset-style internal reflections. The depth of the relative sea-level lowstand varies spatially, and it was diachronous. It occurred relatively early and deep in peripheral regions (i.e., farther from the centre of the island), but was later and shallower landward, and close to its northern limits. Approximate ages of the lowstand are 9.5 + 1 ka in the St. George's Bay - Port au Port region, just over 8.6 ka in Hamilton Sound, before 7.0 ka at Swift Current, 8.7 ka at Connoire Bay, just over 8.2 ka in Bay d'Espoir, and ca. 6.5 ka on the Great Northern Peninsula. The relative sea-level minima range down to at least -30 m, and form a concentric pattern around central Newfoundland, similar to the pattern of raised marine limits."
41aed55ceadce8cdee81091c16ca769f3f7d1188,
5220c713833749787359ca2498939e149da3fc2b,
54c0d915e3229161d6f7d25cfb12d676cc1f12fc,
8abf4d0012673a6fc10756efe70d0913b58b1872,
eb15b5b7547ba2eb051c454d163621eee01cfbf9,
f2a35776cc459439202e85c9f1176ca7b5e4b1bb,
fa009603c24ae34eede4cd699b5e7f01f9cf6144,
2796404c85f92c4cde519d75f6e29afff814105c,
347ea5da1d221baffa07ce95ffca1e8b6ae684cb,"Grounded glacial ice in Notre Dame Bay formed deposits of acoustically incoherent glacial diamicton up to several tens of metres thick. By 13,000 B.P. the ice had retreated beyond the present coastline, and meltwater plumes were depositing acoustically stratified, glacio-marine, gravelly sandy mud. On the inner shelf the mud was deposited in a draped style and averages 5 m thick. In fjords the mud occurs as a thick, ponded, basin-fill deposit which contain acoustically transparent intervals interpreted as debris flows. The tops of these fjord sediments date to about 11,500 B.P. In the earlyto mid-Holocene, relative sea level dropped to a minimum of about -17 m in the east, but probably remained above the present level in White Bay. During and after the low stand, glacial and early postglacial deposits were reworked by waves, currents, and icebergs. Ponded mud was deposited in deep basins, but in the shallow, wave-dominated zone which fringes the outer coast the seabed has a veneer of mobile sand and gravel. This zone is most extensive off the Straight Shore, in the east of the study area, where sheets of gravel ripples occur on the seabed. A large prograded foreland which dates to earlier than 3000 B.P., and a transgressive barrier which stabilised just after 2000 B.P., occur on the Straight Shore. The age and structure of these coastal landforms is related to both inner shelf topography and the pattern of relative sea-level change."
4401082b2ffc41d0f67565e9aa751f4b996b259e,
759c8d1d06d3c63dddb19baba5aa6639ea564318,"Approximately 90% of Canada's ocean coastline is affected by seasonal or multiyear sea ice and winter ice develops on most lakes. Recent studies of ice effects in the shore zone have included investigations of ice-congested and protected shores in the north-west Canadian Arctic Archipelago, processes involved in the construction by ice of large shore ridges in the same region, direct ice scour and enhanced hydrodynamic scour in the presence of ice (strudel scour and ice wallow), particularly as potential hazards to buried pipelines in the Beaufort Sea, and the dynamics of boulder-strewn tidal flats and boulder barricades in eastern Canada. The extent and frequency of shore nourishment by ice and details of the processes involved, including the relative importance of ride-up versus pile-up, remain important research questions. Reports emphasizing the contribution of ice rafting to shoreface retreat along the Alaskan coast of the Beaufort Sea suggest the need for quantitative studies of this phenomenon in Canada, in particular with respect to prodelta sedimentation at the mouth of the Mackenzie River. The coastal zone in the Beaufort Sea is particularly sensitive to climate change through effects on thermokarst processes, rising sea level, the relation between ice cover and wave energy through fetch limitation, and potential changes involving ice dynamics and freeze-up processes."
cf0a2e5391bb4aa0f4dcac134177a082d807f2d9,
20e0d2c38acc00ae714767c7f5f089b5bc66c46c,"Marine geophysical surveys in Port au Port Bay, west Newfoundland, have revealed more than 50 m of Quaternary fill in East Bay and lesser amounts in other basins. Six seismostratigraphic units have been identified and interpreted as follows: (1) an acoustically unstratified unit, including till and other ice-contact deposits, representing the products of deposition or loading by grounded glacial ice or of other ice-contact processes; (2) a crudely stratified unit, believed to be mainly ice-contact or ice-proximal sand and gravel; (3) a conformably stratified unit, interpreted as glacimarine and early paraglacial sandy silt and clay; (4) a weakly-stratified to acoustically transparent unit consisting of postglacial mud; (S) a wedge-shaped unit dominated by clinoform reflections, representing postglacial deltaic sand and gravel in submerged terraces off Fox Island River; and (6) thin wedges of acoustically stratified deposits, considered to represent transgressive shoreface, late-delta, and tidal units of sand and gravel. Radiocarbon determinations on paired bivalves from unit 3 indicate a range of ages from 13.3 to10.8ka, implying initally rapid late- and post- glacial sedimentation. Paraglacial sediment supply from small glaciated river basins is known to be maximized during and shortly after deglaciation and to decrease rapidly thereafter. This is consistent with an interpretation involving rapid early development of the subaerial fan and submerged delta terraces of Fox Island River. Sediment derived from the river and from coastal erosion along the front of the fan has been transported southward under net longshore drift to sinks in the vicinity of Two Guts Pond. The submerged delta ten-aces and erosional shore platforms in Port au Port Bay and St. George's Bay indicate that the postglacial sea-level minimum in this area was approximately 25 m below present. These features are undated but their age is estimated to be about 9.5 ± 1 ka. 
 
 RESUME 
 Des leves geophysiques marins dans la baie de Port au Port, dans l'ouest de Terre-Neuve, ont montre la presence de plus de 50 m de dep ots quaternaires dans la baie East et des quantites moindres dans les autres bassins. Six unites sismostratigraphiques ont ete identifiers et interpreters com me suit: (1) une unite acoustiquement non stratififie, incluant du till et d'autres depots de contact de glace, qui representent les produits du depot ou du chargement par de la glaceechouee ou par d'autres processus de contact de glace; (2) une unite grossierement stratifice, consideree comme etant principalement des sables et des graviers de contact de glace ou a proximite de la glace; (3) une unite conformdment stratified, interpretee comme de l'argile et du silt sablonneux glacio-marins et paraglaciaires precoces; (4) une unite faiblement stratified a acoustiquement transparente consistant en boue postglaciaire; (5) une unite en biseau dominee par des reflections inclines, representant du sable et du gravier de delta postglaciaires dans des terrasses submergees au large de la riviere de Fox Island; et (6) des biseaux minees de depots acoustiquement stratifies, considers comme representant des unit es de sable et gravier transgressifs d'avant-plage, de delta tardives et tidales. Des datations au carbone sur des bivalves entiers de l'unite 3 indiquent un intervalle d'âge allant de 13.3 a 10.8 ka, impliquant une sedimentation tardi- et post-glaciaire initialement rapide. L'afflux de sediments paraglaciaires provenant de petits bassins de rivieres affectes par les glaciers est reconnuetre maximal pendant et peu apr es la d eglaciation et decroitre rapidement par la suite. Ceci est en accord avec une interpretation impliquant un developpement precoce rapide du cone subadrien et des terrasses de delta submergees de la riviere de Fox Island. Les sediments derives de la riviere et de l'erosion coitere le long de la partie frontale du cone ont ete transport es vers le sud sous l'effet de courants c otiers a des trappes dans les alentours de l'etang de Two Guts. Les terrasses de delta submergees et les plates-formes coti ereserosionnelles dans la baie de Port au Port et la Baie St.-George indiquent que le niveau postglaciaire minimal de la mer dans cette region etait approximativement 25 m sous le niveau actuel. Ces modeies ne sont pas dates mais leurâge est estime a environ 9.5 ± 1 ka. 
 
[Traduit par la redaction]"
ca86e0f636205c4c034408c56dba66fb71fdaf70,
d5bb3a45b94d24acbb571ff0d505d91e306d1ccf,
1fe3c86c01aed7b4704e97634c09ee767a252903,
95cce0390bda368979915160da84f7f490584295,
2d695904d84e9bf914092e4b3aa6d24b438f64c0,
48d6804e76b784972c95c515f73709e352861cd9,
5768d5fb08f42524ca1c335056c6b6c6e360a459,
73d8a20b6ad8a0a650d78086be34923617c8eee4,
874933ee2c7d6d4beb7803f18f65082160837946,
9b900d54782f563bfcb1871c9f5b77ffb9df39be,
b2840fada11dd7418f1aa22c8fe2d135f8d0edeb,"The migratory response of swash-aligned gravel-dominated barriers to sea-level rise is a relatively little-studied process. Story Head barrier, on the Atlantic coast of Nova Scotia, is swash-aligned and experiencing contemporary landward migration (6m a -1 ) via storm-generated crest overtopping and overwashing. Barrier migration rates are presented for the period 1945 to 1982. Landward migration of the seaward barrier shoreline is linearly proportional to both the 5-year smoothed rate of sea-level change (r = +0.91) and the annual sea-level change rate (r = +0.69), although the back-barrier migration rate is not related significantly to these rates of sea-level change. This difference between front and back-barrier migration response to sea-level rise reflects the intervening role of storm intensity (frequency and magnitude) superimposed on sea level rise. Story Head barrier fluctuates between dominance of barrier crest build-up by overtopping run-up and crest breakdown by overwashing flow. The balance between these two mechanisms, which controls the rate of onshore barrier migration, depends on both storm intensity and the rate of sea-level rise."
d3ac74a407b58156515b23ab59a68bc95d6441c8,
3da1e35787e4d2e5978e8f34f502cdb155f73845,"Shallow seismic reflection data collected in St. George's Bay, southwest Newfoundland, reveal a complex pattern of subsurface topography and acoustic facies. Two basins in the inner bay are underlain by glacially overdeepened valleys that extend to depths in excess of 180 m. Within the thick Quaternary sequence in the inner bay we recognize eight acoustic units. Units 1 (ice contact), 2 (subaqueous outwash), and 3 (draped glaciomarine) record the presence and retreat of a major Late Wisconsinan ice margin. Unit 4 (postglacial mud) has resulted from reworking of glaciogenic sediments in response to changes in relative sea level. Unit 5 (postglacial sand) is a shoreface deposit on the seaward front of the former moraine. Unit 6 (postglacial delta) was formed by fluvial reworking of glaciogenic sediments during the postglacial lowstand of relative sea level. Unit 7 (postglacial barrier–platform) comprises seaward-fining clinoform prisms that have prograded into the basins, and underlie gravel beach-ridge pla..."
9a471a06bed2c5adef6fd44b8c2e78ee3e5d79e7,"Sand beaches and coastal dunes are prominent features on the northeast coast of Newfoundland, between Bonavista Bay and Hamilton Sound. This is in contrast to most other parts of the island, where coastal progradation typically takes the form of gravel beach ridges. At Man Point, near Musgrave Harbour, an extensive beach- and dune-ridge foreland is mantled by freshwater peat up to 2.9 m thick. Radiocarbon dates on basal peat are 3,150 ± 90 BP, 3,060 ± 90 BP and 2,740±60 BP (unadjusted), The base of salt-marsh peat landward of the harrier is dated at 2,980 ± 90 BP. At Deadman's Bay, where moribund flood tidal deltas, flood channels and washover fans at the north end of the barrier system provide evidence of an arrested transgressive phase, organic deposits at two sites are dated at 1,780 ± 80 BP and 1,260 ± 70 BP. An extensive, partly buried, peat unit overlying old dune ridges at Cape Freels is dated at l,630 ± 50 BP. We also report on a core from a backbarrier marsh at Eastport, in western Bonavista Bay, where freshwater organic material at - 3.3 m is dated at 5,490 ± 120 BP. The evidence from these and other locations suggests that, between 12 and 10 ka BP, relative sea level fell below present datum to an undetermined minimum. During the subsequent transgression it was still below - 4 m at 5.5 ka BP, but had reached approximately - 0.7 m by 3 ka BP. With relatively early termination of the Holocene transgression, some coastal deposits in the area were stabilized, becoming mantled by woodland, marsh and bog. We note the apparent absence of major glacigenic sources, such as those which supply material to prograded coastal systems elsewhere on the island, and suggest a likely course for the episodic postglacial evolution of coastal sediment bodies in the region."
be9412b1890f9a171e04c427065b7c4456f9aff7,
e24f0d1a69f08ee519efc6ae65cec03211e715d6,
4d351ee50951b785c163e1deb3ab7d0b79fcbbac,
6465e77c7fcc938fd5e4cf5488a19fe3a7ba681d,
66be62e6509f7ed7cbd9cc18c64b54f4f1a0b584,
c6a65b0e239f8a797bc5419f37c5b564a70e42dd,"Abstract As part of the Canadian Atlantic Storms Program (CASP), near‐bottom current velocity, pressure, light transmission (as a measure of suspended sediment concentration) and water temperature were recorded using a variety of instruments deployed in water depths of 20 to 37 m on the inner Scotian Shelf, during February and March 1986. Detailed mapping of a 12‐km2 area encompassing the instrument mooring sites revealed a variety of bottom types. These include sand and gravel (both forming ripples at various scales), cobble‐boulder lags, and bedrock, resulting in bottom roughness estimates that vary widely (10−4 m < k < 10° m) over short horizontal distances (of the order 102 m). The velocity data provided information on the near‐bottom current response to winter storms anda basis for computations of sediment load and transport rates. The near‐bottom mean flow showed distinct storm‐driven circulation patterns, with velocities roughly parallel to alongshore wind stress but opposed to shore‐normal wind. W..."
d7d8639bff99291e9cbaddbf25cc538d3a5d1610,
ec764d9b13af19335591eae12c59317eaf527894,
33c1a5215ca310edd951fc5d800aab0d2c88519f,"Differential postglacial rebound has produced a wide range of Holocene relative sea-level {RSL) responses across Newfoundland, with profound effects on coastal evolution. Raised postglacial marine features are found throughout the island, except in the southeast, where submergence is though to have prevailed since the early Holocene, as suggested by published models invoking marginal forebulgecollapse. The models imply a zonation from southeast to northwest, where isostatic uplift has resulted in ongoing emergence, at least until very recently. Evidence from the Strait of Belle Isle, including a raised beachridge sequence that terminates seaward in an active transgressive storm ridge, suggests that even in this area RSL may now be stationary or rising. The pattern of Holocene RSL on the northeast coast is not well known. Recent evidence suggests that the wide beach-ridge plain at Doting Cove may be of considerable age: indurated and stained beach-ridge sand between -0.1 m and +2.3 m above present MWL is mantled by freshwater peat up to 2.8 m thick. No large onshore sediment supply is identifiable in this area, in contrast to the situation at many other large beach-ridge deposits on the island. In St George's Bay where RSL has been rising from a midHolocene minimum, largeamounts of glacigenic sand and gravel have been removed from 40 km of coastal exposures and transported northward to form a 15-km long spit complex at Flat Island. The oldest freshwater peats on this barrier date to 1.36 ka BP. In the southeast, at Placentia on the Avalon Peninsula, a large glacigenic source has provided the sediment source for the extensive gravel beach-ridge plain on which the town is constructed. The oldest basal peat here dates to 2.11 ka BP. In contrast, small barriers at a number of other sites on the Avalon, where glacigenic sources are more limited, show a pattern of earlier beach-ridge progradation followed by transgressive stormridge development involving reworking of the earlier barrier sediment Both regressive and transgressive phases have occurred within the context of rising RSL and the change in the pattern of barrier development is believed to have been triggered by a decrease in sediment supply as erodible material along the shore was progressively depleted."
7f719a76066caf0c8516f8e9ec2bcf5de78a838b,"Analysis of an 18-day time-lapse film record of shoreface ripple development, with concurrent measurements of near-bottom flow and surface waves, provides new insight on equilibrium bedform conditions, adjustment of ripple planform to variable hydrodynamics, and ripple migration behaviour. The study was conducted in approximately 10 m water depth, 1 km off Martinique Beach on the Atlantic coast of Nova Scotia (Canada), under low-energy summer wave conditions. Significant wave-height and peak period during the study averaged 0–7 m and 8 s, respectively, with extremes up to 1–7 m and 11 s during passage of three weak weather disturbances. Six mutually exclusive ripple types have been defined: (1) short-wavelength regular ripples; (2) variable bifurcated ripples; (3) variable terminated ripples; (4) short-crested ripples; (5) long-wavelength regular ripples; and (6) chaotic ripples. Ripple wavelength ranged from 0–07 m to 0–24 m and displayed a strong Reynolds number dependence. Together with other published field data, the results suggest a lower limit of γ=0–06 m for the wavelength of wave ripples in ocean shoreface environments. Ripple orientation ranged through 38° and responded rapidly to changes in wave approach direction, but did not conform to the orientation of the adjacent shoreline. Ripples were observed to migrate both on- and off-shore (with and against the wave advance direction) at rates up to ±0–1 m h-1, associated with net flows other than wave-induced onshore asymmetry and mass transport. Migration (mainly of ripple types 1 and 2) occurred during the peak of storm events, but showed no obvious correlation with measured near-bottom flow magnitude or direction. Ripple behaviour demonstrates equilibrium with prevailing dynamic conditions when straight-crested rippie types 1 and 5 are present. Disequilibrium in orientation or dimensions is expressed by increasing sinuosity, bifurcation and crest termination in types 2,3,4 and 6."
a775f4000c0d713d57155a2edb6880e66d00a589,
bd66bbeea5c4df7314f25edb38fd35b9b74823cd,"The reduction in sediment volume of an ebb-tide delta, as a consequence of lagoon outlet closure on a coarse clastic barrier in southeast Ireland, initiated a sequence of beachface and barrier changes downdrift. Elimination of sediment supply to the ebb-delta caused a cessation of downdrift longshore swash bar welding, and led to beach volume reductions which in turn allowed a temporal sequence of beach and nearshore morphodynamic domains to develop. These domains then controlled the sedimentation regime of the barrier adjacent to the old outlet. A temporal sequence of (a) dissipative barrier; (b) reflective barrier; and (c) inner reflective (barrier face)/outer dissipative (subtidal) wave regimes match respectively periods of (a) barrier crest build up by crestal dune development; (b) barrier crest instability (barrier width increasing, barrier height decreasing) due to rhythmic overwash; and (c) a return to barrier crest stability with limited aeolian accretion. Two barriers at different stages in this sequence are discussed."
c33c4b5b4d90c9767c3d2c6ee7cbbb29895f69e0,"Abstract A suitable computer system for coastal information can support management of data collected over several years, easing the problems of organizing and indexing many forms of observational records and helping to improve the standards for coastal classification. The simple topological character of the coast makes it possible to use a one‐dimensional model for longitudinal (alongshore) subdivision into quasi‐homogeneous segments of any convenient length. This is illustrated with reference to a case‐study locality on the Canadian Atlantic coast. Information related to specified sections of the coast is organized as a generalized data base. Using commercial data base management software gives great flexibility and power of application. The chief concession to the geographic and cartographic requirements is the use of a detailed digitized coastline file, which is linked to the divisions in the main file in order to produce maps as one of several forms of reporting."
055e8e73effb90ca8fb52eea3c138c0dd0b21211,
0f4acf2707268b713308f401cdbefc1b65bf147d,"ABSTRACT Sidescan sonar and photo/video surveys have revealed the presence of symmetrical gravel bedforms, interpreted as wave-generated ripples, with wavelengths of 1.3 to 3.0 m, occurring in patches and narrow ribbons in depths of 15 to 65 m on the inner Scotian Shelf. Direct observations and sediment sampling from the Pisces IV submersible have been used to ground-truth surface-based survey dam and to quantify relationships between the bedforms and the processes responsible for their formation. The ripples are formed in polymodal sandy gravels with modal sizes ranging from 0.1 to 45 mm. Evaluation of threshold criteria for coarse sediments together with an 11-year record from a nearby wave-rider buoy suggests that existing models for wave entrainment of gravel on rippled surfaces ove estimate the critical velocity by a factor of approximately 2 and that conditions suitable for transport of the coarsest (45-mm) size mode sampled in the ripple crests occur, on average, 43 hr/yr. Analysis of gravel ripples on the inner Scotian Shelf and of other examples noted in the recent literature extends the range of empirical data for oscillation ripple wavelengths by a factor of 3 and for ripple grain size by a factor of 5. The gravel ripples display a linear scaling to wave-orbital diameter similar to that reported for orbital ripples in sand. Use of such relations for paleoenvironmental interpretation of gravel ripples preserved in the rock record is complicated by the difficulty of selecting appropriate grain size and wave parameters. Conditions suitable for potential formatio of large-scale, symmetrical ripples in coarse sediments extend from the outer continental shelf to shallow shore face settings with possible fetch lengths as short as 20 km under intense storms."
16134b3ff25f1acdd085d2d8cbac20f5d80054d2,
79dd8a9b40b86f32fb790b120a8b49109c11e939,
775a648c3fe18bad0fb954c912fad8b3d7b18abd,
8cc23c02cccb3a483b345ea0bddc84942880b478,
a60c8cf5efce64667204e0c9d178637b83ba526f,
c2e3d39c5a7018b6c14c9010b162c1d13361d684,
c9214d46d437d3dc26cd8282b6ae0ae81780d066,
96032ae73a05d8d30a6749651f6fd810b200f525,
fbf7607fc40a42a2499470dbb831df5ca4c55ecf,
fcddb3cc9a0cd28581f73b0772a44e2e42b26651,
598a0f2171796ea89b66dbafb9664363ed1593ae,
56c1b47aa047a9ebbfa63a4afaa19b5073b56ba1,
cfcb8f4f97c935a9a057c72176007b355e877f1c,
3c90c3f9197fe78856e5c562e0bbc263637cd354,
2cbaea385d5575691975557720a8f27de70060c6,"A small recirculating laboratory flume was used to study the transportation and deposition of coarse silt (Do0 = 0.045 mm) under conditions of approximate transport equilibrium. Nineteen runs were made with a range of Froude numbers between 0.07 and 1.23 and with total sediment con- centrations as high as 393,000 mg/l. Depths of flow (as much as 10 cm or one half of channel width) were adjusted by means of a sidewall correction for subsequent calculations of Reynolds number, Froude number, and stream power. Ripple bed forms in the lower regime of flow developed at Froude numbers less than 0.75 approximately. Plane bed and standing wave or antidune bed forms developed at Froude numbers in excess of 1.0. Flat symmetrical ripples, irregular low ripples, and plane bed forms characterized the transition between the lower and upper regimes of flow. The transition occurred at a stream power value of approximately 2 g.cm.-1.s-1. There was, however, some overlap in the stability fields of the several types of bed forms. Lenticular, tabular, and irregular units of crosslamination were deposited, in addition to parallel (plane) lamination, in the lower regime of flow. Parallel (plane) lamination, undulose lamination, and thin units of cross-lamination were deposited in the transitional regime. The upper regime of flow was cha- racterized primarily by parallel and undulose lamination. The cross-laminated units deposited by ripple migration in the low- er regime of flow had an average length of about 0.7 x the ripple wave length and an average maximum thickness of 0.8 x ripple height. The silt behaved essentially as a cohesionless material."
f5360bea8d149510c16b894bf43fbe70a392c495,
3f36a98ce20e86d6ec1e658d05f78b9945e76f90,
3613de750731e0a88a6c25ad5b6a93ec326d59b7,
e2d92b860892d527e978f5aeabc0e7e1b5a7b6aa,
eb0fca90614c2d891852111f59cf4a3208c128e2,
62a7c7f50401fde6fa435eadd47b01e0f4bd9487,
71266b8fcca832e5ca8420dde464fdb95fb33717,"Examining urbanisation and tourism from the perspective of global Sustainable Development Goals is essential for achievinga balance between environmental protection and economic growth in the world's most polluted nations. Moreover, most polluted countries pay more attention to the nature of foreign direct investment (FDI) inflows to achieve a sustainableenvironment. This study intends to explore the impacts of FDI, tourism, urbanization, and economic growth on carbon dioxide emissions using panel data for the top ten most polluted nations for the period from 2000 to 2019. To guide empirical testing,the panel unit root tests LLC and IPS are used. The outcomes of LLC and IPS advise FM-OLS application on how to accomplish the goals. The findings provide proof of how FDI and other factors affect carbon emissions (CE). Particularly, renewable energy consumption (REC) has a detrimental but minor effect on CE. For the panel of developing nations, FDI had a favourable and significant effect on CE along with economic growth, tourism, and urbanization. The expansion of cities isalso harming nature and ecological footprints. These findings are alarming as all factors cause CE under consideration thatleads to the deterioration of the environment. Therefore, more environmental rules should be put into place to reduce CE, draw in clean FDI, and encourage quality-oriented investment in selected nations. Second, it is important to ensure the deployment of green technology and the upgrading of urbanized structures. The government can take several actions against the use of polluting goods and vehicles in urban areas, and any polluting industries should also be outlawed in such residential areas."
8ebbac98a2832f7a96a6ba21636675dcbd2fb745,
e93c312fe518cb9fabc51fe0e3d5dccf8ac3b40e,
111e796860afd7feb6fff6bd37f49c7e821f1001,"This paper explores the relationship between foreign direct investment (FDI) settlement, the contribution of clean energy and CO2 emission in United Arab Emirates (UAE). Using the time-series estimation methods, this study opts cointegration test to find the long-run association between FDI and CO2. Furthermore, energy source of UAE is based on fossil fuel, we employed fixed-effect regression model to determine the effect of FDI after controlling several other factors that causes CO2 emission to find the magnitude. After obtaining the data from 1971-2019, we find that FDI plays a significant role in CO2 emission in the economy. Another key contribution of this paper is regarding clean energy. The results confirm that clean energy helps to reduce the CO2 emissions. The findings of this research endorses the initiative of the UAE launched ‘Energy Strategy 2050’. This implies that UAE is moving in a right direction to curb the CO2 emission by increasing the contribution of clean energy in the total energy mix and would achieve this target through an effective implementation of the ‘Energy Strategy 2050’."
4161e3dff640a7febfc1a18e438cace63a8557ad,"The present-day financial system is being influenced by the rapid development of Fintech (financial technology), which comprises technologies created to improve and automate traditional forms of finance for businesses and consumers. The topic of Fintech as a financial disruptor is gaining popularity in line with the swift spread of digitalization across the banking industry, whereby this paper contributes to the field by presenting a novel bibliometric analysis of the academic literature related to Fintech as a financial disruptor. The analysis is based on metadata extracted from the Scopus database through the VOSviewer and Biblioshiny software. The bibliometric analysis of 363 documents identifies the most impactful sources of publication, keywords, authors, and most cited documents on the topic of Fintech as a financial disruptor. As our analysis demonstrates, the number of publications on the given topic is increasing, indicating both interest among academia and potential for future research."
4e489b6d4106f74ba4f1b8a0405889c0de8eb8ec,
57208507a542cc76f680d9b701969ecdaf1d3aae,"In recent years, geopolitical risk (GPR) has been a crucial factor in investment decisions and stock markets. Therefore, we explore the research on the GPR by employing bibliometric and scientometric analytical techniques. We find 366 scientific contributions in December 2021 from the Scopus database by searching “Geopolitical risk” in abstracts, keywords, and titles. Our findings show that GPR research has gained momentum in the last three years. Specifically, the journal Defence and Peace Economics has one of the highest numbers of research and citation on GPR. Authors in Asia also dominate the GPR literature. Overall, this study contributes to the literature by presenting the existing research that may give new insights for prospective studies in GPR."
71d675e93fc54dc043357df323057d6bab682419,"This paper introduces the need for blockchain technology integration for Islamic financial institutions. The paper presents three main applications of blockchain technology. It explains how such technology can be used in the banking and financial sectors by providing examples for each application. Given its relevancy, the paper expands on Central Bank Digital Currencies (CBDCs) as one of the blockchain applications. The paper then discusses salient points on how the banking sector would be affected by what is described as the future of money. Subsequently, an analysis of the use of blockchain in financial services and, in particular, the use for Islamic financial services is provided by examining examples of past successful implementations. The paper then introduces the Internet of Things (IoT) and illustrates the possible technology implementation in financial institutions. The inherent security weakness of IoT is summarized with the potential elimination of that weakness if combined with blockchain (BIoT). The paper concludes by providing a handful of suggestions and recommendations on the urgency of considering CBDCs for future daily operations, integrating Distributed Ledger technology, and using BIoT to safeguard the financial and clients' transaction records."
7d9c56b1632bcf50eb6053a1b300238a6c8e435b,"This paper analyzes the fiscal performance of Turkey and Argentina during the period 2000–2021, when both countries faced rapid economic growth with the consequent impact on social welfare. This work explored two different systems: Centralization in Turkey and Federalism in Argentina and, in general, studied the decentralization impact of both the systems on social welfare. This study intended to create new social welfare indexes in other regions to analyze the resource allocation in different regions of these countries. As a first step, we built a regional human development index (HDI) for each region. This attempt is considered a new contribution to the literature and intended to fill the gap in this field. Afterward, this index was compared with the fiscal resources allocation (FRA), used as a proxy of fiscal decentralization in an econometric panel data model. By using this method, we concluded that the social welfare indexes have a positive relationship with the fiscal resource allocation in the Federal system, such as in Argentina, but not in the centralized system such as in Turkey during the period analyzed from 2000 to 2020."
a00b48822d943a2abc383c14be18aa0451b18761,"This paper evaluates the performance of eight tests with null hypothesis of cointegration on basis of probabilities of type I and II errors using Monte Carlo simulations. This study uses a variety of 132 different data generations covering three cases of deterministic part and four sample sizes. The three cases of deterministic part considered are: absence of both intercept and linear time trend, presence of only the intercept and presence of both the intercept and linear time trend. It is found that all of tests have either larger or smaller probabilities of type I error and concluded that tests face either problems of over rejection or under rejection, when asymptotic critical values are used. It is also concluded that use of simulated critical values leads to controlled probability of type I error. So, the use of asymptotic critical values may be avoided, and the use of simulated critical values is highly recommended. It is found and concluded that the simple LM test based on KPSS statistic performs better than rest for all specifications of deterministic part and sample sizes."
cedc0140d5a360dfcfbc071bfcbb67dfecac7e7a,
f74e86b8946f060d207f5f6557455a9a7c63e3b2,
f763e79fef594fcd7c3586829b818e681d6ac553,"The increasing interest in Fintech, Blockchain, and Digitalization in Islamic Finance created a new area in the literature, requiring a systematic review of these academic publications. The scope of the analysis is limited to journal articles to understand the trends in the indexed journals. Results are categorized into three sections, Islamic banks’ digitalization, Blockchain and Crypto Assets research, and Islamic non-bank financial institutions’ digitalization. Islamic fintech has great potential mainly because of the overlapping norms of Shariah and fintech, making it easier to implement technological disruption into Islamic finance. Moreover, the trust shift to Islamic finance could be merged with the opportunities of fintech and increase the potential of Islamic fintech even more."
0109d831e5f173b00fe4cf2e4876fc04901ae9f6,
083f54a38f7045027961617ea3676b65dc4f71a6,"Abstract This study examines the effect of GDP per capita on the Gini index, which measures income concentration, in Colombia. The methodology used is an econometric analysis of time series with data extracted from the Inter-American Development Bank and the World Bank. The econometric results suggest that, at least during the period studied here, there is no evidence that GDP per capita has been an explanatory variable of the behaviour of income distribution in Colombia. The results also align with the understanding that the problem of inequality in the distribution of income is not merely economic but concerns persistent matters such as political and historical issues."
0cdf1f6c8eb4f070868f1f4bc108b95349a7bf39,"This study analyses the innovative performance of 5,273 companies across 64 different economic sectors and 32 different regions in Colombia. We assess the effects of education and open economy variables on the innovative performance of firms by analyzing firm, sectoral, and regional level determinants. The study takes the multilevel approach of the innovation process considering the structure and behavior of innovation systems in developing countries. We furthermore focus on technology transfer from foreign trade and the role of education in the process of innovation. We find that education and open economy variables have a significant relationship with innovation performance at the firm and regional levels. We finally conclude that Colombia has a fragmented innovation system with a weak institutional structure, and low interaction between policymakers, industry, universities, research centers."
210021cdaa0b65d08e636e98440983e267597e15,
211b8ffe7d4f88f16706166c047a6da100b3d441,"This paper explores the applicability of universal cryptocurrency exchange by analyzing crypto exchanges of Binance, Latoken, Kucoin and Qash, which also have their own cryptocurrencies in the cryp..."
22e177abedc7ed6ab4e4fe99f64b91eba542a392,"This paper investigates the effects of COVID-19 pandemic-related uncertainty focusing on the US tourism subsectors, including airlines, hotels, restaurants, and travel companies. Using daily stock price data, we compute connectedness indices that quantify the financial distress in the tourism and hospitality industry and link these indices with a measure of COVID-19-induced uncertainty. Our empirical results show that some subsectors of tourism are affected more than others. The connectedness of tourism companies has severely increased after March 2020. Restaurants are the most heavily influenced subsectors of tourism, while airline companies come the next. Besides, our quantile regression suggests that higher quantile COVID-19 uncertainty index has more effect on the connectedness of tourism companies. Our results guide the policymakers and investors to detect the stress accumulated in each subsectors of tourism and to take more informed and timely decisions."
32b865d6a3be86eab098a162b1d1ecea61503669,"ABSTRACT Citizenship by investment (CBI) programs have recently garnered significant academic and media attention. Turkey introduced such a program in 2017 that offers citizenship in exchange for investment in residential property. Through the program, thousands of foreigners, mainly from the Middle East and Asia, have purchased houses, particularly in Istanbul. Foreigners’ share of total houses sold in Istanbul almost sextupled and exceeded 10% of total sales. This study estimates the short-run impact of relatively wealthy foreigners on the residential property prices in Istanbul investing to buy a Turkish passport. It finds that the Turkish CBI program positively impacts house prices by 2% in the districts, which are likely to be favored most by immigrant investors."
3b4a5810bb53daa448a86f4de6839d5ce8ec36e2,"Blockchain is a path-breaking paradigm, and cryptocurrencies are one of the main application areas of Blockchain technology. Bitcoin leads the cryptocurrency markets, both in terms of market capitalization and in scientific interest. In this paper, we performed a comprehensive bibliometric study of the Bitcoin-related literature. Using the Scopus database, we created a sample that comprises 4495 documents written in the 2011–2020 period. Furthermore, we provided insights about dimensions such as the change in the number of publications over the course of years, the main research areas, types of published documents, most important platforms and sources of Bitcoin publications, highly cited studies, productive authors, author’s countries, and finally main funders of Bitcoin-related research. Lastly, our bibliometric study manifests the current state and future path of Bitcoin literature from distinct perspectives."
446932078383e8486ec1e323815dc4cd0d3ee178,"The main aim of this article is to examine the inter-relationships among the top cryptocurrencies on the crypto stock market in the presence and absence of the COVID-19 pandemic. The nine chosen cryptocurrencies are Bitcoin, Ethereum, Ripple, Litecoin, Eos, BitcoinCash, Binance, Stellar, and Tron and their daily closing price data are captured from coinmarketcap over the period from 13 September 2017 to 21 September 2020. All of the cryptocurrencies are integrated of order 1 i.e., I(1). There is strong evidence of a long-run relationship between Bitcoin and altcoins irrespective of whether it is pre-pandemic or pandemic period. It has also been found that these cryptocurrencies’ prices and their inter-relationship are resilient to the pandemic. It is recommended that when the investors create investment plans and strategies they may highly consider Bitcoin and altcoins jointly as they give sustainability and resilience in the long run against the geopolitical risks and even in the tough time of the COVID-19 pandemic."
4d3f8806ae2578275d49ae4f7b4a875f4b86913f,"This study aims to investigate the effect of fear sentiment with a novel data set on Bitcoin’s return, volatility and transaction volume. We divide the sample into two subperiods in order to capture the changing dynamics during the COVID-19 pandemic. We retrieve the novel fear sentiment data from Thomson Reuters MarketPsych Indices (TRMI). We denote the subperiods as pre- and post-COVID-19 considering January 13th, 2020, when first COVID-19 confirmed case was reported outside China. We employ bivariate vector autoregressive (VAR) models given below with lag-length k, to investigate the dynamics between Bitcoin variables and fear sentiment.Bitcoin market measures have dissimilar dynamics before and after the Coronavirus outbreak. The results reveal that due to the excessive uncertainty led by the outbreak, an increase in fear sentiment negatively affects the Bitcoin returns more persistently and significantly. For the post-COVID-19 period, an increase in fear also results in more fluctuations in transaction volume while its initial and cumulative effects are both negative. Due to extreme uncertainty caused by the COVID-19 pandemic, investors may trade more aggressively in the initial phases of the shock."
4deff3c3f19f058c41cea844b46233ca15d05483,"
 In this paper, we attempt to explore the extent to which the hard won development gains over the last several years could be reversed due to the unfolding COVID-19 global pandemic, how we can reboot the global response to accelerate the SDGs in times of uncertainties, and most importantly how to turn the recovery into an opportunity to build back better and more resilient economies. To do so, we examine the case of blockchain as one of the emerging innovative work-streams in development practices that could lead the way forward and pave the path for new developmental narratives as we all navigate the uncharted territories of the new digital age. This paper provides useful insights about the underlying dynamics underpinning the adoption of blockchain backed-solutions for sustainable development, and it showcases some of the promising use-cases being developed through trial-and-error experiments by its early adopters. The paper offers a deep dive into a burgeoning development practice in search of disrupting business-as-usual to solve increasingly complex development challenges by mainstreaming innovations such as blockchain-enabled solutions to rethink the ways in which development solutions are being delivered across the SDG spectrum. This work points to the significant potential of blockchain technology as a game changer in solving some of the most pressing issues hindering the global recovery post Covid-19 to transition towards greener and more inclusive economies. Nevertheless, we also stress that the hype-cycle behind the “let’s blockchain it” trend does not mean that blockchain-backed solutions are necessarily superior to other alternatives which might be less costly and less technical in nature. Development practitioners prototyping and implementing blockchain-based solutions for sustainable development can utilize these insights and discussions to make informed decisions in their journey to harness the disruptive potential of blockchain alone or in tandem with other emerging technologies in the new world of business as unusual.
"
65fcbf31f72aa62a99b97f7452017c4eb552dbbf,"
Purpose
This study aims to investigate the effect of fear sentiment with a novel data set on Bitcoin’s (BTC) return, volatility and transaction volume. The authors divide the sample into two subperiods to capture the changing dynamics during the COVID-19 pandemic.


Design/methodology/approach
The authors retrieve the novel fear sentiment data from Thomson Reuters MarketPsych Indices (TRMI). The authors denote the subperiods as pre- and post-COVID-19 considering January 13, 2020, when the first COVID-19 confirmed case was reported outside China. The authors use bivariate vector autoregressive models given below with lag-length k, to investigate the dynamics between BTC variables and fear sentiment.


Findings
BTC market measures have dissimilar dynamics before and after the Coronavirus outbreak. The results reveal that due to the excessive uncertainty led by the outbreak, an increase in fear sentiment negatively affects the BTC returns more persistently and significantly. For the post-COVID-19 period, an increase in fear also results in more fluctuations in transaction volume while its initial and cumulative effects are both negative. Due to extreme uncertainty caused by the COVID-19 pandemic, investors may trade more aggressively in the initial phases of the shock.


Practical implications
The authors are convinced that the results in this paper have more far-reaching implications for other markets regulated by the states. BTC provides a natural benchmark to understand how fear sentiment drives and impacts the markets isolated from any interventions. Hence, the results show that in the absence of regulatory frameworks, market dynamics are likely to be more volatile and the fear sentiment has more persistent impacts. The authors also highlight the importance of using micro, asset-specific sentiment measures to capture market dynamics better.


Originality/value
BTC is not associated with any regulatory authority and is not produced by the governments and central banks. COVID-19 as a natural experiment provides an opportunity to explore the pure effects of market sentiment on BTC considering its decentralized and unregulated features. The paper has two main contributions. First, the authors use BTC-specific fear sentiment novel data set of TRMI instead of more general market sentiments used in the existing studies. Next, this is the first study to examine the association between fear and BTC before and after COVID-19.
"
66e47c186062ff6a5c8c965b1c1a17bb828097e7,
9714aa32ce620921544965b40d763ae24e5423eb,
9b6b98c19fcc6b9f6108334b535e7e685b75c0d8,"We investigate how governance and the Global Financial Crisis (GFC) affect cash management. Assessing 169,916 firm-years in 26 developing Asian countries, our empirical results show that firms in *good (poor)* governance countries tend to hold more cash *before (after)* the GFC. In particular, the outcome effect of governance on cash holdings in the pre-crisis and crisis periods shifts to a substitution effect for governance in the post-crisis period in developing Asia."
a5390f3f706ea2130d6285626fc1619582088c31,"This research analyses the innovative performance of 5273 companies across 64 different economic sectors and 32 different regions in Colombia. We assess the different effects on the innovative performance of firms by analyzing firm, sector, and regional level determinants. The study involves the multilevel approach of the innovation process considering the structure and behavior of innovation systems in developing countries. We furthermore focus on technology transfer from foreign trade and the role of education in the process of innovation. We find that education and open economy variables have a significant relationship with innovation performance at the firm and regional levels."
c8d42df856bac54e24684e61d767f8bee0841905,
d75b462d2dfea7ede8b1698c3bfd714278889fa8,
dde8f01d472bdca2d38ed526fcc660b2594fe715,
e31545e19cdd26d2f521448273d4cf3c89a998f1,
ecf5e91b2ce7951cccd5ffa4b6a4869c623c010c,"This paper is a bibliometric study of the literature in Islamic social finance. The study analyses 595 articles, conference papers, and book chapters in Islamic social finance from 1991 to 2020 published in 262 Scopus indexed journals. The authors sourced the bibliographic data using the keywords “Islam and social finance,” “waqf,” “zakat,” “microfinance,” and variations thereof. This study is essential, especially in the wake of COVID-19 pandemic and the pandemic-induced economic disruption leading to increased global income and social inequalities, putting even more pressure on the SDGs funding gap. Novel solutions to plug the funding Gap are being sought, and recent literature has shown Islamic social finance’s potential as a solution to the SDG’s funding gap. The study finds that researchers in the field closely link Islamic social finance with sustainability and sustainable development concepts, as evidenced in keywords used by authors. We also find that Malaysia and Indonesia are leading the research in ISF. The study aims to map the field of Islamic social finance and provide a reference point for future researchers to identify the gaps in the literature and their role in enriching academic discourse in ISF to position Islamic finance appropriately in the sphere of development economics."
f0ff1dea33358e0e82d18347081779ad79562c02,"This paper conducts a bibliometric research in the literature on Fintech and Islamic finance. The data of this study consists of relevant articles obtained from the Scopus database as of February 2021. A keywords bundle related to Islamic finance and keyword has been used for the search, resulting in 89 publishments included in this research. Results show the stunning increase in the Islamic Fintech publishments after 2017, mainly in the fields of cryptocurrencies, micro-finance, impact investing, and SRI investing, and so on. The two main centers of Islamic Fintech research are Malaysia-Indonesia Region and the GCC area. The increasing number of Islamic Fintech publishments show the potential of the field for the industry's future."
f9cc517a3a4de6c823ec0af98909875b5ea23c54,"As the world is striving to recover from the shockwaves triggered by the COVID-19 crisis, all hands are needed on deck to transition towards green recovery and make peace with nature as prerequisites of a global sustainable development pathway. In this paper, we examine the blockchain hype, the gaps in the knowledge, and the tools needed to build promising use cases for blockchain technology to accelerate global efforts in this decade of action towards achieving the SDGs. We attempt to break the “hype cycle” portraying blockchain’s superiority by navigating a rational blockchain use case development approach. By prototyping an SDG Acceleration Scorecard to use blockchain-enabled solutions as SDG accelerators, we aim to provide useful insights towards developing an integrated approach that is fit-for-purpose to guide organizations and practitioners in their quest to make informed decisions to design and implement blockchain-backed solutions as SDG accelerators. Acknowledging the limitations in prototyping such tools, we believe these are minimally viable products and should be considered as living tools that can further evolve as the blockchain technology matures, its pace of adoption increases, lessons are learned, and good practices and standards are widely shared and internalized by teams and organizations working on innovation for development."
ff6e4c115e363cd7670c4e57a47bd2cec0e6614d,
7d318c8c4d338fafc2bbca2a8c08a53527f1b057,"The study explores whether blockchain technology can change the paradigm of the current financial structure and the balance of power in the international financial system. Accordingly, this study reviews the development of blockchain technology by analyzing China and Venezuela, both of which struggle to harness their technological advancement and to enhance their power in the international realm. We found that Venezuela invests in blockchain technology to create an alternative payment structure for survivability, while China’s desire is to become a global leader in global blockchain technology."
9b0bf1e33db9d051d822ed3edab7cdf75022a33f,
d26abd8e90d48884db8b18ea25c387815bb5a508,"As we embrace the new normal in the aftermath of Covid-19, the year 2020 also marks the decade of action as we start the 10-year countdown to achieving the Sustainable Development Goals (SDGs) by 2030. In this paper, we attempt to explore the extent to which the hardly won development gains over the last years could be reversed due to the unfolding COVID-19 global pandemic, how do we reboot the global response to accelerate the SDGs in times of uncertainties, and most importantly how to turn the recovery into an opportunity to build back better and more resilient economies. To do so, we examine the case of blockchain as one of the emerging innovative work-streams in development practices that could lead the way forward and pave the path for new developmental narratives as we all navigate the uncharted territories of the new digital age."
6909116f717854551d7eab0751722f18faffca27,"This paper attempts to assess whether the driving factor behind the rising credit card indebtedness of consumers in Turkey is financial illiteracy. Using the results of a nationwide survey, the authors conclude that even though credit card borrowing frequency and debt amount are affected by components of financial literacy, being credit-constrained has a very pronounced impact. An exploratory analysis finds that the probability of irrational credit card borrowing is increased by being credit-constrained but not affected by financial literacy. These findings suggest that credit card debt is at least as much a result of necessity as nescience."
7350eea15362ee4ce31327cdec414d6fb4a701f6,
feb48f1e445006a241c244733c7cf0dc96bf5d05,
57456fb870e433c8e54b8d218e09ee8d1d080ef8,
69957ce7dae094b3e734d891078d3006b949bc8b,"Using a discrete choice random utility model and unique data from a nationwide consumer survey, we show that consumers view credit cards as highly differentiated products with both bank-level and card-level nonprice features. They select their credit cards by predominantly considering these nonprice features. Although they charge higher prices, the majority of consumers choose private banks as issuers due to their bank-level and card-level nonprice benefits. Consumers who prioritize prices tend to choose participation or public banks. Product differentiation and bundling seem to underlie banks’ market power in the Turkish credit card market. Large private banks and public banks reap the benefits of bundling more than the other banks. Of card-level nonprice features, installments, bonuses/rewards/miles, and the prestige of the card seem to be particularly effective in consumers’ decisions. We argue that this highly differentiated nature of credit cards can be an alternative explanation for the credit card pricing puzzles."
d2d215e1a766e4ac260e9b8f6ee34137319c925d,
d92570739ce4f62a8e6cc018378f9829f8d153f7,"We examine the interest rate sensitivity of both deposits and credits at Islamic and con- ventional banks in Turkey. We find that the bank lending channel is especially operative for Islamic banks. Impulse responses for conventional and Islamic banks reveal that Islamic bank depositors’ sensitivity to policy rate changes are substantially larger than that of con- ventional bank depositors. Next to heavily dependence on deposit funding, we consider that inertia in Islamic bank deposit rates impedes these banks to keep those depositors who con- sider the opportunity cost of monetary policy rates is unbearable. At the lending side, we obtain similar results, implying that tight monetary policy leads to a larger contraction in Islamic bank credits. This finding is a reflection of the favorable attitude of Islamic banks towards SME financing. When similar relationships are analysed for currency and inflation shocks, we again find larger responses for Islamic banks showing the cyclical nature of SME credits."
66ea7c8999d4fb85e00a5789f4924662437f2217,
7a2ae95fc1b05f08d136afa25bda06d43c7738f4,"In a period of increasing foreign bank entry, the popular question of “What does foreign bank entry bring to the Turkish banking sector?” can partly be answered with respect to the productivity effects. This paper aims to find the productivity change in the banking sector between 1990 and 2007 just before the global crisis. We are especially interested in the period beginning with 2001 after which the Turkish banking system has almost been flooded with foreign banks. Using a sample of 20 commercial banks, we attempt to find the Data Envelopment Analysis (DEA) type Malmquist Total Factor Productivity Change Index over the specified period. We also look at the source of this change decomposing this index into its mutually exclusive and exhaustive components of efficiency change and technological change. Additionally, we further decompose the technical efficiency change into pure technical efficiency change and scale efficiency change. The DEA results guide us in comparing the performances of banks of different ownership status (state, private and foreign banks) and of different size."
8db3c28a21da89287dff17218d35ad06ac7c61e2,"Using unique data from a nationwide consumer survey and a discrete choice random utility model, we identify price and nonprice factors that affect consumers’ credit card choices. We observe that consumers differ in their preferences for credit cards, and that they value both bank-level and card-level nonprice benefits offered by banks. Large private banks, which are the market leaders, are avoided on the basis of prices, but are preferred on the basis of both bank-level and card-level nonprice benefits. Medium and small private banks are favored on the basis of installment conditions, and are preferred by risky revolvers. Public banks have loyal customer bases, benefit from their bank-level characteristics and are preferred on the basis of low interest rates. Participation banks are preferred on the basis low prices and are avoided on the basis of both bank-level and card-level nonprice benefits. These results suggest that nonprice competition prevails in the Turkish credit card market. We conclude that banks obtain market power through product differentiation and bundling."
9de0cd00dd40d64501a242d07117031020e496fe,
cd7e028e633b84bb07e1145bba7b9e19e8b809ed,"This paper examines to what extent macroprudential policies in the Turkish banking sector affected the functioning of depositor discipline. Our results suggest that depositors’ responses for poor bank performance get stronger after the 2008 crisis, when various macroprudential measures were implemented to preserve financial stability. In the aftermath of the crisis, bank behavior toward depositors also alters. Ahead of the crisis, banks did not significantly respond to the discipline exerted by depositors, however, banks begin offering higher rates to curb deposit withdrawals afterwards. Our findings suggest that the implementation of macroprudential tools seem to have a positive impact on financial stability, since, in the post-2008 period, regulatory supervision have been more firmly assisted by the market."
dff0b5ae1ce921cf45b124ba019afe07fad5bc81,"Events such as the ‘credit crunch’, ‘bank run’, ‘financial contagion’, ‘flight to quality’ and ‘systemic risks’ have widely transpired in recent times. One important dimension permeating these events is the dynamic link between macroeconomic shocks and banks’ behaviour. Economic crises experienced by five East Asian countries in the late 1990s were accompanied by financial sector problems. The Great Recession of the late 2000s also corresponded to heightened solvency risks affecting over-leveraged banks and financial institutions in many developed countries. In a world of imperfect information, adverse macroeconomic shocks could weaken firms’ balance sheets, diminish bank capital and trigger financial disintermediation. Positive shocks, on the contrary, could increase firms’ net worth and prompt additional bank lending. Understanding the nature of this interaction offers regulators, supervisors, firms and households valuable insights into the process of policymaking, financial intermediation and responding to boom and bust cycles in the economy."
3839726fd6d28e671b411121d988ed923866a75f,
9efcc15de1f0dee63176bdace5b030847de23aa0,
e1b91825979bf4568cd89df86b359a21c4e52324,"We examine the degree of sensitivity to interest rate changes among creditors and depositors in Islamic and conventional banks. We estimate bank lending channel proposed by Bernanke and Blinder (1992). We find that bank lending channel works for banks in Turkey on aggregate terms. An interest rate increase lead depositors to withdraw their money and in turn decrease the volume of lending in respective banks. Impulse responses for conventional and Islamic banks reveal that Islamic bank depositors’ sensitivity to interest rate changes is larger than conventional bank depositors. As a response to a one standard deviation interest rate increase, Islamic bank deposits withdrawal exceeds 5% at the end of six quarters. This figure is merely 2% percent in conventional banks. From lending side, we find similar results implying that tighter monetary policy leads to similar contraction in credits."
f87e847cdcb4f0af47867ad6484ad805d989df59,"ABSTRACT We analyze the drivers of nonperforming loans in the Turkish banking system after the 2000–01 Turkish banking crisis. By constructing a vector autoregression model, we perform dynamic out-of-sample forecasts, which yield quite accurate results compared to the actual data. Since forecasting is a very crucial tool for both policy makers and market players, these results are some of the main strengths and contributions of this study. This article shows various patterns between the economic and financial indicators and the nonperforming loans. One important message obtained from the results is that policy makers should be concerned about the status of the economy and the market expectations to maintain stability in the banking system."
23b095cfabafc040cafc5842e64e351499261a1d,
3b8988fd1bef22c6d193d5fb94d0e3c8311a8a0a,
a10d9d6adce87896f11804d2d5107dba4941f701,"This paper investigates the effectiveness of macroprudential policies introduced by Turkey in late 2010. The unprecedented quantitative easing policies of advanced countries after the global financial crisis have presented serious financial stability concerns for most emerging countries including Turkey. To cope with these challenges, Turkey has devised new policy tools such as asymmetric interest rate corridor and reserve option mechanism. From the perspective of capital flows, the interest rate corridor works mainly through stabilizing supply of foreign funds, and the reserve option mechanism through decreasing the sensitivity of equilibrium exchange rate to shifts in the demand for foreign funds. Using a large panel of 46 countries and employing [Bruno and Shin (2013a). Capital flows, cross-border banking and Global liquidity. Working paper, Princeton university; Bruno and Shin (2013b). Assessing macroprudential policies: Case of Korea. Working paper, Princeton university] methodology, we investigate whether the new policy framework in Turkey has been successful in cushioning the economy from volatile cross-border capital flows from a comparative perspective. The results show that, after controlling for a set of domestic and external variables and relative to a group of advanced and emerging countries, cross-border capital flows to Turkey have been less sensitive to global factors after the implementation of macroprudential policies."
3e80bda4e16663937403a8f2987acb154a766c37,
67c4ac845d158ebcab26b768e91208ee7ef84a34,
6df56064bf7360ac2980b369d58d65eea3f41af7,"The Turkish credit card market has recently undergone two important formal regulations: the interchange fee regulation in 2005 and the interest rate regulation in 2006. Banks started to charge annual fees to cardholders after the interest rate regulation, before which credit card ownership was costless. This practice sparked a widespread public outcry and legislative activity to control annual fees. Consumer unions vigorously called for regulations. They argued that in response to the fall in their interest revenues, banks started to exercise excessive market power in the payment services market. By employing the Panzar and Rosse (1982, 1987) method and a unique data set for all non-participation banks between 2002 and 2008, we investigate the effects of the formal and informal regulations on competition in the payment services market. We find that despite the increase in prices, the credit card payment services market actually became more competitive after the interest rate regulation. We attribute the rise in banks’ prices to a rise in their costs. Because of the prevailing informal regulations banks could pass along only part of the increase in their costs to the prices of their payment services. The resulting fall in their price-cost margins reduced banks’ market power in the payment services market. Thus, our results do not justify further price regulations."
17d2d4fbb46cdd5694982da827d5d30d21e4c421,"In light of the importance of banking sector outreach and given concerns that competition may adversely affect it, this study explores the empirical linkage between banking structure and outreach in Turkey for the period 1988-2010. Bank-, province-, and bank-province-level estimation results indicate that competition is in general conducive to the outreach of banks. We do not find evidence for collusive behavior among banks when they have multimarket contact. At the province level, the presence of foreign-owned banks is associated with higher outreach, while at the bank-province level, we observe that outreach of domestic banks exceeds that of foreign banks. Together, these results suggest that there are procompetitive spillover effects from foreign banks to their domestic counterparts."
578a646eb42038ebb3f16db9d0519697586b1201,"The last decade witnessed an unprecedented economic growth in Emerging Market Economies (EMEs). EMEs have also been the main drivers of growth in the recovery following the global financial crisis. Nevertheless, EMEs continue to face a number of institutional and structural challenges that may pose risks to the sustainability of their recent growth performance, with potentially significant repercussions for the world economy. In this paper, we present a detailed account of Turkey’s experience in dealing with various institutional and structural challenges during the last decade and provide evidence that taking the right steps can enable EMEs materialize their full growth potential going forward. Successful institutional and structural reforms can also provide room for monetary policymakers to effectively navigate their economies through turbulent times such as the recent global financial crisis."
68bfe2173928c6ec66c4da2c8429ab2dd5277d35,
7a15987d443fe95f57bdefc15ba07324cb927bf2,"The last decade witnessed an unprecedented economic growth in Emerging Market Economies (EMEs). EMEs have also been the main drivers of growth in the recovery following the global financial crisis. Nevertheless, EMEs continue to face a number of institutional and structural challenges that may pose risks to the sustainability of their recent growth performance, with potentially significant repercussions for the world economy. In this paper, we present a detailed account of Turkey’s experience in dealing with various institutional and structural challenges during the last decade and provide evidence that taking the right steps can enable EMEs materialize their full growth potential going forward. Successful institutional and structural reforms can also provide room for monetary policymakers to effectively navigate their economies through turbulent times such as the recent global financial crisis."
845248f81c0d2d2b2a9819cf3a1b42a6400ade85,"After the global crisis, one of the most important lessons learned for the Central Banks has appeared to be the vital importance of financial stability along with the price stability. Hence, finding solutions to how to incorporate the financial stability objective in the implementation of the monetary policy without diluting the price-stability objective has started to be heavily discussed by the academics and policy makers. Accordingly, it has started to be debated that using only short-term interest rates as the main policy tool may not be enough to maintain the price stability and the financial stability at the same time. Interest rates that provide price stability and financial stability can be different and this necessitates the central banks to use multiple policy tools. In view of this, the Central Bank of the Republic of Turkey adopted a new monetary policy framework called the new policy mix in which multiple tools are employed to achieve multiple objectives. In this framework, required reserves ratios, weekly repo rates, interest rate corridor, funding strategy and other macro prudential tools are jointly used as complementary tools for the credit, interest rate and liquidity policies to achieve the price and the financial stability objectives concurrently. This new monetary policy adopted in Turkey also provides an interesting case study to assess how a country came up with novel policies to account for its country specific characteristics."
86bee4ef27d28ff773d37fed83501a4b18568599,"We look at market discipline in the Islamic deposit market of Turkey for the period after the 2000 crisis. We find support for quantity based disciplining of Islamic banks through the capital ratio. The evidence for price disciplining is, however, less convincing. In addition, we also look at the effect of the deposit insurance reform in which the dual deposit insurance was revised and all banks were put under the same deposit insurance company in December 2005. We observe that the reform increased quantity based disciplining in the Turkish Islamic deposit market."
a4e33de42dd34c83604898806ef241dfb852c5d4,"The Eurozone debt crisis is an ongoing financial crisis that has made it difficult or impossible for some countries in the euro area to rollover their government debt without the help of third parties. The reasons for the Eurozone debt crisis can be summarized in three groups: (i) Errors on the structure of the Monetary Union, (ii) Macroeconomic instabilities among the monetary union, (iii) Problems on the financial system. In this paper, after discussing the origins of the crisis, we summarize the measures taken against the crisis and then describe the current stage of the crisis. Finally, we focus on the effects of the crisis on the Turkish economy. Turkey is affected by the crisis via three channels: trade, finance and expectations channel. We focus especially on the trade and finance channel for the scope of this paper. We show that these effects have remained rather limited for Turkey with the help of the strong macroeconomic and financial structure."
b925e197e312f0854e0cca53b9d4ea538f8ecc78,"The global crisis of 2008-09 resulted in major reverberations in virtually every financial market around the world. The Turkish financial sector has been among the very few global financial markets which passed global financial crisis turmoil without major damage and succeeded to recover quickly after the crisis. This paper aims to explore the underlying reasons for this success of the Turkish financial sector, which is predominantly composed of the banking sector. The improved macroeconomic conditions in the country, the increased fiscal discipline of the government, the soundness and the strong growth potential of the banking sector were among the most important causes of this development."
eb34cafd132c6927af429e58babf0f28f450b94f,"We evaluate the performance of participation banks (PBs) and analytically discuss the participation banking industry in Turkey. First, we examine establishment and deregulation of PBs. We also evaluate the performance and governance structure of the four full-fledged participation banks currently operating in the Turkish banking system. Participation banks expand the scope for financial inclusion for those who stay away from conventional banking due to religious sensitivity. The PBs play a pivotal role in channeling the idle capital into more productive sectors. In this sense, new sovereign sukuk issuances and their importance for the PBs' liquidity management are also discussed. We also examine the changing approaches for the supervision and regulation of PBs in different periods in Turkey. Finally, we discuss some critical views of and challenges for the PBs while providing a critical perspective for the development of the sector in the future."
ebed8d503a76f6e829600f120873cadecc6a829f,"By estimating discrete choice multinomial logit demand models, we unveil consumer preferences in the Turkish deposit and credit markets in the 2002-9 period. We find that consumers prefer banks with larger networks and more efficient technologies in both markets. Borrowers are very responsive to interest rates, but depositors are not. We conclude that monopolistic competition prevails in both markets. However, banks' market power in the credit market is much lower than in the deposit market. Moreover, the comparison of demand elasticities in these two markets shows that credits will respond more than deposits to the taxes imposed on them, suggesting that loan provisions can be more effective than reserve requirements as a macroprudential policy tool to restrict credit growth."
eceb6d1dca7c2df8cf8e3d1cf453523545d15ccc,"In some countries such as Turkey, Islamic banks and conventional banks coexist in banking industry. However, due to dissimilarities in their business models, these institutions may be subject to different statutory and regulatory arrangements. The arrangements in a dual banking system might repel potential customers from Islamic banks if privileged arrangements are solely open to conventional banks. Up until late 2005, Islamic banks in Turkey were recognized as “special finance houses” and were exempt from the rights that covered conventional banks, like deposit coverage. As an interesting case, the legislative changes in late 2005 have eliminated the deprivations and provided more constructive environment for Islamic banks. Yet, what these legislative changes have brought about for Islamic banks is still unexplored. In this paper, we study the effects of the legislative changes in Turkish Islamic banking on market structure. The results reveal that Islamic banks gained more market power after the enactment of various legislations and integration of these legislations to the Banking Law."
f8a81895bd43e043ec8fc73745e41fe498650067,"This topical book addresses the need for emerging economies in Central, Eastern and South-Eastern Europe to find a new, sustainable growth model that fosters continued convergence with the EU without leading to the build-up of new vulnerabilities."
fb8a12bbe1d990d37adf1dd87b827edfdd983ae0,
44cdc59b202b7fd76aacd6e243511336cde9b3c8,"Default problems and complaints about credit cards do not seem to diminish with declining credit card rates. Using a nationwide credit card user survey, we try to identify the determinants of customer satisfaction in the Turkish credit card market. Controlling for customer and card characteristics, we find that financial literacy is a major determinant of satisfaction. When people know more about financial matters and use their knowledge in their financial activities, they make more efficient decisions and have fewer financial problems, which in turn leads to higher satisfaction. We also find that people who tend to use their credit cards for unnecessary shopping and who have a history of credit card delinquency are less satisfied."
584b2f3e34502f885a3ccf74da58151c1424abb8,"By using data from 8 depository institutions in Turkey we evaluate the drivers of securitization between 2004 and 2009. Our analysis shows that previous period securitization as well as bank equity, level of profits and asset size are important factors in a bank’s decision to securitize its loan portfolio. Banks’ on-balance sheet liquidity on the other hand is not a significant factor. We also use a binary probit model and predict with good certainty the timing of a bank’s securitization in capital markets. Again, bank size, profitability and equity are also explanatory variables in making these accurate predictions."
8517ba9c369173d5f07d6014b9a6c0a87266ccfb,
ba0312d11afb1f8ba9a5cea0705378e71a65d0b6,
c27193a32de70f8fb537c05bcf3e2598af8b8596,
1349eebad99f6a91558a618e6989c2b0031ce97a,"Attempts to explain high and sticky credit card rates have given rise to a vast literature on credit card markets. This article endeavors to explain the rates in the Turkish market using measures of nonprice competition. In this market, issuers compete monopolistically by differentiating their credit card products. The fact that consumers perceive credit cards and all other banking services as a bundle allows banks to also employ bank level characteristics to differentiate their credit cards. Thus, the features and service quality of banks are expected to affect credit card rates. Panel data estimations also control for various costs associated with credit card lending. The results show that nonprice competition variables have significant and robust effects on credit card rates."
4ea29037d878c4f518255b50933568912d50a7b8,"The paper investigates the competition among 21 credit card issuers in Turkey, covering the time period between 2002 and 2008. Analysis is conducted by using an estimation methodology designed by Panzar and Rosse (1982, 1987), where the degree of competition is measured by the sum of elasticities of total revenue with respect to input prices. Accounting for the total revenue rather than interest revenue fills the gap in previous studies, which look only at one side of the credit card market. Liquidity management cost, which was first shown by Shaffer and Thomas (2007) to be important for evaluating the degree of competition in the credit card industry, emerges as an important variable. The estimated Panzar-Rosse statistics are consistent with product differentiation, implying that Turkish credit card issuers are involved in monopolistic competition."
8ddf9fe71790de78b43d86985f56e785b459742f,"This paper examines the efficiency and its relation to profitability in Turkish banking sector by employing Panel Stochastic Frontier Approach. In the post crises period, extensive structural changes have taken place and a great number of new developments have occurred, affecting the efficiency of banking sector. This is the first study that employs panel stochastic frontier approach for banking efficiency in Turkey. In this research, both cost and profit efficiency measures are estimated for the panel data consisting of 32 banks between 2002--2007. Results suggest that there is cost efficiency gain and convergence in the efficiency levels of banks. As another interesting result, foreign banks are less efficient and state banks are more efficient. This paper also analyzes the relation between efficiency and profitability and finds no robust relation between them. However, the bank size matters more for profitability."
97d120e4b62a495cd767d7f58a18d0d6a1300d0c,"This paper investigates consumer demand, competition and welfare in the Turkish deposit and credit markets in 2002-2009 period by using banks’market shares in these markets. A discrete choice structural demand model developed by Berry (1994) is employed in the estimations. As market shares reflect consumers’ final choices, the methodology starts with constructing the utility function of consumers who purchase deposit (loan) services from a bank. This method allows us to elaborate on demand, competition and welfare in the same analysis. In the model, a market share equation is derived from the utility function and elasticities which allow to comment on competiton are calculated. Lastly, the paper concludes with a welfare analysis based on consumers’ last choices. Results of the study show that price elasticities of credit customers are much higher than those of depositors. It may yield more effective results to make price competition in credit market, whereas banks may increase their market share in deposit side by differentiating their products for depositors. There is welfare loss for depositors within 2002-2009 period, but credit customers experience an increase in their welfare in the same period."
9c112880abf49928876f7714aa2cd07cb84a448c,"During the 1980s and the 1990s, private investment in the Middle East and North Africa (MENA) has on average shown a decreasing or stagnant trend. This contrasts with the situation of the Asian economies, where private investment has always been more dynamic. In this paper, it is empirically shown for a panel of 39 developing economies--among which four MENA countries-- that in addition to the traditional determinants of investment--such as the growth anticipations and the real interest rate--government policies explain MENA's low investment rate. Insufficient structural reforms--which have most of the time led to poor financial development and deficient trade openness¬¬--have been a crucial factor for the deficit in private capital formation. The economic uncertainties of the region have represented another factor of the firm's decisions not to invest. These uncertainties have consisted of the external debt burden and various measures of volatility."
ec7525c90d59c4a2a7ce72e991d228fba0822f3d,"This paper aims to ascertain the effects of convergence in governance on investment decisions among a sample of 43 developing countries, using dynamic system GMM estimations. In an increasingly interdependent economic world, regions with good governance are considered to be areas of higher investment, as a result of further integration and collaborative action among member states. Since its foundation, in 1992, Black Sea Economic Cooperation (BSEC) countries have gone through a transition process and, to a large extent, this is about institutional transformation. Good governance institutions are an assurance to guarantee property rights and minimize transaction costs, thus creating an environment conducive to investment and growth. In this paper, we investigate the impact of BSEC on its member countries regarding convergence of governance institutions. We show that convergence has occurred within the region with respect to bureaucratic quality, control over corruption, law and order, internal conflict, ethnic tensions, but not to government stability and democratic accountability. The paper also calculates how much capital accumulation the region would gain by reaching the average institutional standards of the EU-12. This study is the first attempt in the BSEC region to investigate the link between regionalization and institutional convergence, at the same time as to quantify its economic impact through investment."
1c162cf1e0e3d8d415caa4e08f511b4c1da7aceb,"The failure of competition and the consequent high and sticky interest rates in credit card markets have recently been the subject of considerable debate and research. This paper presents the first regression testing for the existence of price competition in a credit card market to be estimated free of dynamic panel bias using recent quarterly data from Turkey. The estimation reveals that even though the effect of the cost of funds on credit card rates is statistically significant, it is very weak. The paper thus provides empirical evidence for the failure of price competition in the Turkish credit card market."
24782044fe3557884144d2c8432800b769485360,"Credit card rates have been shown to be very high and non-responsive to the changes in the costs of funds. The failure of price competition led to a shift of interests from price to non-price competition in these markets. Credit card issuers create switching costs for their customers by providing non-price benefits. These benefits are either direct benefits that depend on credit card usage or indirect benefits that arise from the convenience and quality of the general services of the issuer bank. This paper empirically investigates the nature of non-price competition in the Turkish credit card market using panel data, introducing the first study of this kind for an emerging market. As the main result, a significant and robust positive relationship between switching costs and credit card interest rates is obtained, confirming that as non-price benefits increase, banks charge higher credit card interest rates to their customers."
9a85fb35d325693aacf0697848bccca560e6a960,"This paper investigates the relationship between interbank funds and efficiencies is for the commercial banks operating in Turkey between 2001-2006. Data Envelopment Analysis (DEA) is executed to find the efficiency scores of the banks for each year, and fixed effects panel data regression is carried out, with the efficiency scores being the response variable. It is observed that interbank 
funds (ratio) has negative effects on bank efficiency, while bank capitalization and loan ratio have positive, and profitability has insignificant effects. Our study serves as an illustrative evidence that interbank funds can have adverse effects in an emerging market."
e04a65e234809e894e0b63d1934c2c448b83d095,"This paper investigates the relationship between interbank funds and efficiencies for the commercial banks operating in Turkey between 2001 and 2006. Data Envelopment Analysis (DEA) is executed to find the efficiency scores of the banks for each year, and fixed effects panel data regression is carried out, with the efficiency scores being the response variable. It is observed that interbank funds (ratio) has negative effects on bank efficiency, while bank capitalization and loan ratio have positive, and profitability has insignificant effects. Our study serves as novel evidence that interbank funds can have adverse effects in an emerging market."
037d70c6798504c750235de2377e05ea41bff6d8,"This paper investigates the link between private investment decisions and various governance institutions in the form of corruption, quality of bureaucracy, judiciary, security of property rights, regulations and taxation, political stability, as well as political rights and civil liberties. This link is empirically tested for a panel of 32 countries by estimating a simultaneous model of private investment and governance institutions, where economic policy and other variables explain concurrently both variables. This empirical model also illustrates that economic reforms — in the form of financial development and trade openness — and human capital affect private investment decisions in two ways; directly, as well as through their positive impact on the quality of governance institutions. In MENA, deficient administration quality, political instability and low public accountability contributed significantly to the low investment decisions of the 1980s and the 1990s. Our empirical analysis also confirms that structural reforms constitute another challenge, if the region wants to catch up with more successful developing economies."
1c277b15ff4c29d0e61b2072ad3f02a8a1ee06c5,"Despite recent gains, the development challenges of the Middle East and North Africa (MENA) region remain formidable. Job creation remains the most important one given the magnitude of its demographic transition and the seventy million people that are expected to seek employment opportunities in the next twenty years. Four reports published by the MENA region of the World Bank in 2003-04 spelled out the state of reforms and the fundamental transitions needed in the MENA economies to move to higher and more sustainable sources of growth and job creation. The reports identified three critical realignments or transitions: (i) from public sector-dominated to private sector-led economies; (ii) from closed to more open economies; and (iii) from oil-dominated and volatile economies to more stable and diversified economies. The reform agenda for MENA remains large. In this paper, we assess and attempt to explain the progress or lack thereof achieved in economic reforms in the MENA region. In order to be more specific and limit the scope of the paper we focus on the dimensions of economic reforms which are related to the first transition mentioned above: the development of a dynamic private sector. The importance of the employment challenges makes the development of a competitive private sector to become the lead engine for more productive growth and employment creation a central concern. In a sense, private sector development can be seen as symptomatic of the process of reforms taking place in MENA. Hence, from this angle, we analyze reforms in the region and their prospects."
594e849b75e94438365bdc49ec8d2d2babe06b87,"This paper shows for a panel of 32 developing countries that political and governance institutions matter for private investment decision. This linkage is empirically verified for a broad number of institutions. This is the case for corruption, quality of bureaucracy, judiciary, security of property rights, regulations and taxation, political stability, as well as political rights and civil liberties. This result is obtained by estimating a simultaneous model of private investment and political and governance quality, where economic policy and other variables explain concurrently both variables. In MENA, the deficiencies in the administration quality, the political instability and the low public accountability contributed significantly to the low investment decisions of the 1980s and the 1990s. This paper shows as well that, although political and governance institutions constitute first order importance for private investment, economic reforms in the form of financial development and trade openness, and human capital affect private investment decision directly, as well as by enhancing the quality of political and governance institutions."
8867252a83a2eee069ef1d3c265baa958351b9bd,"This paper attempts to delineate the evolution of the Turkish banking sector in the post-crisis era after 2001. The paper summarizes the events in the Turkish banking sector until the 2001 crisis. After that, a section focuses on the major regulatory changes. A detailed account of the consolidation and transformation of Turkish banks following the crisis is presented with reference to various structural indicators of the sector. Efficiency and foreign bank entry are examined in for the post-crisis period as well."
904cdf0565dab15a0666e200c5de647130823b4d,"The analysis in this paper helps understand the progress in reforms and development of the private sector in the Middle East and North Africa. It shows the critical role played by the State-private sector relations in determining the progress of reforms and their impact on private sector development. The authoritarian nature of the political regimes and the existence of large oil and other rents, and conflict to a much lesser extent, have been the major factors which shaped the nature, extent and speed of reforms. JEL Classification: D78, N45, O43, O53."
d115deb6cfdecef57d2370b68c3e423cf08f5a8c,
de754ff9e40d8d908bac9a6d2843d9032376986c,"In a period of increasing foreign bank entry, the popular question of “what does foreign bank entry bring to the Turkish banking sector?” can partly be answered with respect to the productivity effects. This paper aims to find the productivity change in the banking sector between 1990 and 2006. We are especially interested in the period beginning with 2001 after which the Turkish banking system has almost been flooded with foreign banks. Using a sample of 20 commercial banks, we attempt to find the DEA type Malmquist Total Factor Productivity Change Index over the specified period. We also look at the source of this change decomposing this index into its mutually exclusive and exhaustive components of efficiency change and technological change. Additionally, we further decompose the technical efficiency change into pure technical efficiency change and scale efficiency change. The figures that we find guide us in comparing the performances of banks of different ownership status (state, private and foreign banks) and of different size"
f1727e96395520527c862a6726384a1c6641f5fd,"The attempts to explain the high and sticky credit card rates have given rise to a vast literature on credit card markets. This paper endeavors to explain the rates in the Turkish market using measures of non-price competition. In this market, issuers compete monopolistically by differentiating their credit card products. The fact that credit cards and all other banking services are perceived as a bundle by consumers allows banks to deploy also bank level characteristics to differentiate their credit cards. Thus, credit card rates are expected to be affected by the features and service quality of banks. Panel data estimations also control for various costs associated with credit card lending. The results show significant and robust effects of the non-price competition variables on credit card rates."
fb76141c1860a8b2f553bc20d4244743dd5f5f87,"This paper empirically shows that the perceived quality of governance is an essential determinant of the private investment decisions in the developing countries by stressing the existence of different types of possible measures of governance. We use three different indicators to measure the perceived quality of governance, “Quality of Administration” (QA), “Political Accountability” (PA) and “Political Stability” (PS). All of the three indicators were proved to be significantly –although at different levels of significance and magnitudes of influencecontributing for private investment decisions. We also confirm that Middle East and North Africa (MENA) region could have achieved a better private investment performance if it had an enhanced level of perceived institutions. In particular the low level of political accountability has been an influential factor which has been holding back the region from reaching its private investment potential."
0eb3146dbc3f851eb58c78dc53d628e1b1bbadbe,"By using a simultaneous equations model, this paper establishes that the perceived quality of governance, which is measured by three different indicators “Quality of Administration”, “Public Accountability” and “Political Stability”, has a positive effect on the private investment decisions in the developing countries. Our model allows us to point out the fact that the mechanisms through which each type of indicator affects private investment are different. In addition to our primary result we also show that Middle East and North Africa (MENA) region could have attained a better private investment performance if it had reached a more advanced level of perceived institutions in last two decades. The low level of public accountability, among other governance deficiencies, was predominantly responsible for the deficiency in private investment in MENA."
3d11443c8414cc0fe96161c6155054a3f548b4a5,"This paper shows for a panel of 32 developing countries that political and governance institutions matter for private investment decision. This linkage is empirically verified for a broad number of institutions. This is the case for corruption, quality of bureaucracy, judiciary, security of property rights, regulations and taxation, political stability, as well as political rights and civil liberties. This result is obtained by estimating a simultaneous model of private investment and political and governance quality, where economic policy and other variables explain concurrently both variables. In MENA, the deficiencies in the administration quality, the political instability and the low public accountability contributed significantly to the low investment decisions of the 1980s and the 1990s. This paper shows as well that, although political and governance institutions constitute first order importance for private investment, economic reforms in the form of financial development and trade openness, and human capital affect private investment decision directly, as well as by enhancing the quality of political and governance institutions."
4155ebbb1b3de603c1ee5487128ec90dd99bc8c5,"This paper examines Turkey’s international cost competitiveness in manufacturing with respect to the Slovak Republic, and quantitatively investigates the relationship between Turkish cost competitiveness and the exports of manufactured goods at an industry level. The Relative Unit Labor Cost (RULC) measure and dynamic panel data techniques are employed for this analysis. We find that Turkey is not competitive with respect to Slovakia for the 1995-1999 period. The Competitiveness of Slovakia mainly depends on its relatively higher level of labor productivity."
472f27e77cb37bb2d00f24368175a9f150718f66,
4f0c4297e77b4824e1afcbfd4b9ac17fc775b3d6,"This paper first points out the lack of consensus between empirical and theoretical studies of income inequality and redistribution. While theoretical papers show that income inequality increases redistribution, empirical studies fail to confirm the same result. The paper later shows that even an exogenously given efficiency of redistributive institutions (ERI) affects the relationship between income inequality and redistribution. This paper also introduces three specifications to endogenize ERI. In these various specifications, increasing inequality reduces the ERI when (1) ERI is an increasing function of average income or (2) political influence on ERI is positively associated with income or (3) the median voter has some prospect of upward mobility. There is one common element in these various specifications. While income inequality increases the pressure for redistribution, it also increases the incentive to reduce the efficiency of redistribution in order to constrain aggregate redistribution. Thus, the main conclusion is that one needs to consider these conflicting effects in order to account for the lack of strong empirical evidence of a positive relationship between income inequality and redistribution."
549ad59dbfd94e4515c9cec3e344c24df855d579,"This paper shows that exchange rate alignments are also used for redistribution of income among different groups. The heterogeneous impacts of stabilisation policies lead to formation of various coalitions throughout the evolution of stabilisation programmes. These coalitions can produce unsustainable economic policies at the expense of other groups. The model categorises these various groups with respect to their shares in total production of tradables and non-tradables. An increase in the relative prices of non-tradables benefits the poor more than the rich and middle classes. In addition to the poor, the rich benefit from unsustainable macroeconomic polices by lending to the government and eventually escaping the cost of stabilisation in the long run."
5b42500211917c9d24be5704ef02b122fb3608e1,"During the 1980s and 1990s, private investment in the Middle East and North Africa (MENA) has on average shown a decreasing or stagnant trend. This contrasts with the situation of the Asian economies, where private investment has always been more dynamic. In this paper, it is empirically shown for a panel of 39 developing economies--among which four MENA countries-- that in addition to the traditional determinants of investment--such as the growth anticipations and the real interest rate--government policies explain MENA's low investment rate. Insufficient structural reforms--which have most of the time led to poor financial development and deficient trade openness¬¬--have been a crucial factor for the deficit in private capital formation. The economic uncertainties of the region have represented another factor of the firm's decisions not to invest. These uncertainties have consisted of high external debt burden and the various measures of volatility"
722a070be88c8d0cb9f756e6e3e4d962c9c408e3,"This paper sheds light on the rising and declining manufacturing sectors in Turkey compared with other Eastern and European Counties considering the recent export and import trend of various manufacturing sectors. Eastern European Countries differ with respect to rising and declining sectors. However, similarities are noticed as well. Except two countries, all countries began to pass more skilled-labor intensive sectors which need more advanced technology. Even though the trade volume of conventional sectors increases in absolute terms, their share in total trade declines in favor of new rising sectors."
72c4921aa59a35b426340eb8d9474649d13a675a,"This paper compares the competitiveness of Turkey in manufacturing in terms of unit labor cost with the transition countries including Poland, Hungary, Czech Republic and Slovakia. Unlike the pure wage rate comparison, Turkey performs better than other countries in the sample with respect to unit labor cost. Turkey has lower unit labor cost almost in all the sectors. In addition, unit labor costs and their growth rates are examined in detail specifically for the manufacturing sectors with rising and declining export shares in recent years."
79847e6eed188292f908b042bc50731f31a76ddc,"This paper aims to find the productivity change in the banking sector between 1990 and 2006, with an emphasis to the period after 2001 crisis during which the Turkish banking system experienced a structural change. Using DEA, we find the Malmquist TFP Change Index and its mutually exclusive and exhaustive components of efficiency and technological changes over time. Additionally, we further decompose the technical efficiency change into pure technical and scale efficiency changes. The productivity of the banking sector is found out to have increased, the main reason being technological improvement rather than efficiency increase. For the cases of productivity decline, however, the changes come from the efficiency side rather than technology. An analysis with respect to the ownership status revealed that foreign banks were the most efficient group until 2001 after which state banks captured the first place. We attribute this change to the inflation accounting practice as well as better management of state banks with less political intrusion. The analysis with respect to bank size reveals that before 2000, the most efficient bank group was the medium-scale banks (the banks mainly purchased by foreign banks) followed by small banks while the efficiency scores converged after 2001."
7ab04ece939535af7b06ab27f0066e1decf88817,"This paper investigates the causes of Turkish export-boom after 2000 in the manufacturing sector. We mainly concentrate on cost and productivity aspects of the production in the manufacturing sector. Effects of productivity, wage and exchange rate are analyzed in the framework of the augmented unit labor cost model. Following the Edwards and Golub (2004) paper we use the dynamic panel data techniques for the analysis. In addition, the importance of the above mentioned factors is examined for the rising and declining sectors. We find that manufacturing export is negatively related to the unit labor cost (ULC). Decomposition of ULC into its two components also shows that an improvement in productivity increases export while an increase in nominal wages decreases it. We also find that nominal wage is an important factor in the declining sectors while productivity is the stimulus in rising sectors."
7cd3beda4b30fe6305854f3fbe84bee2ae4a7f4d,
915e6a26f4df16c45c2b3d9458839241ceeda2f8,"The high credit card interest rates in Turkey attracted considerable attention in recent years to regulate the Turkish credit card industry. Before any regulation decision taken, there needs to be better conceptualization and analysis of the Turkish credit card market. First, we highlight the most striking aspects of the Turkish credit card market. After exposing the problem, we benefit from the existing theoretical and empirical studies on the structure of competition in the credit card industry. Potential reasons for the lack of competitions are denoted. Having the existing studies in mind, we finally, construct an empirical model to estimate the market structure in the Turkish credit card industry. Newly disseminated data on the Turkish credit card industry is first introduced in this paper. Our empirical results are based on the panel data set of 22 banks from the second quarter of 2001 to the third quarter of 2005. In addition to random and fixed effects regressions, instrumental variable fixed effect regressions are run on this sample. Our results robustly conclude that the credit cards interest rates in Turkey are economically insensitive to the changes in the cost of fund. This result shows lack of strong competition Turkish credit card market."
ab771faa3a8fa1c1623ed1a5db28b0c65572d97f,"This paper addresses the issue of the low level of private investment in the MENA region, with special emphasis on the role of governance. Based on the existing literature, we categorize what types of governance institutions are more detrimental to entrepreneurial investments. We then estimate a simultaneous model of private investment and governance quality where economic policies concurrently explain both variables. Our empirical results show that governance plays a significant role in private investment decisions. This result is particularly true in the case of “Administrative Quality” in the form of control of corruption, bureaucratic quality, investment-friendly profile of administration, law and order, as well as for “Political Stability”. Evidence in favor of “Public Accountability” is also found. Our estimations also stress that structural reforms like financial development and trade openness, and human development affect private investment decisions directly, and/or through their positive impact on governance."
bce3493df3a4b920030e6411c2948c0c9983fc86,"After 2001 crisis, the macroeconomic environment led to important changes in Turkish banking sector which has experienced a process of concentration by involving in merger and acquisition activities and liquidation of some insolvent banks. Using the data from the detailed balance sheets of the banks that operated in the years from 2001 to 2005, we examine the degree of concentration and degree of competition in the market by applying Panzar and Rosse’s approach. We also explore the existence of relationship between efficiency and profitability of the banks taking into account the internationalization of banking. Our results do not suggest the existence of relationship between concentration and competition. There is also no robust relationship between efficiency and profitability."
c1809affddf3299f9684debf23640e92370b491a,
240f926c103d9e3d6adfb21269b08098bf5f9e53,"The high credit card interest rates in Turkey attracted considerable attention in recent years to regulate the Turkish credit card industry. Before any regulation decision taken, there needs to be better conceptualization and analysis of the Turkish credit card market. This paper sheds some light in this direction. First, we highlight the most striking aspects of the Turkish credit card market. After exposing the problem, we benefit from the existing theoretical and empirical studies on the structure of competition in the credit card industry. Potential reasons for the lack of competitions are denoted. Having the existing studies in mind, we finally, construct an empirical model to estimate the market structure in the Turkish credit card industry. Newly disseminated data on the Turkish credit card industry is first introduced in this paper. Our empirical results are based on the panel data set of 22 banks from the second quarter of 2001 to the third quarter of 2005. In addition to random and fixed effects regressions, instrumental variable fixed effect regressions are run on this sample. Our results robustly conclude that the credit cards interest rates in Turkey are economically insensitive to the changes in the cost of fund. This result indicates that Turkish credit card market is characterized with lack of strong competition and hence suggests some regulatory measures."
45f9031029733257f1531df52d00f0c0a6d2f6c6,"Sound macroeconomic policies, increasing global liquidity and higher real returns in developing countries played an important role in canalizing capital towards developing markets. Recent improvement in the developing Turkish economy brought the issue of foreign entry to the foreground. High growth potential backed by an increasing population, falling inflation rates and the birth of the mortgage sector made Turkey an ideal place to expand into. This article is not concerned about whether foreign entry is good nor does it discuss the subsequent effects. Rather, it attempts exclusively to shed light on the motivations behind entry to Turkey utilizing recent entry cases."
505cf6c80e2ec84bd6f63c3262787351e60b8706,"This paper addresses the issue of the low level of private investment in the Middle East and North Africa (MENA) region, with special emphasis on the role of governance. Based on the existing literature, we have categorized what types of governance institutions are more detrimental to entrepreneurial investments. We have then estimated a simultaneous model of private investment and governance quality where economic policies concurrently explain both variables. Our empirical results show that governance plays a significant role in private investment decisions. This result is particularly true in the case of “Administrative Quality” in the form of control of corruption, bureaucratic quality, investment-friendly profile of administration, and law and order, as well as for “Political Stability”. Evidence in favor of “Public Accountability” seems, however, less robust. Our estimations also stress that structural reforms -- such as financial development and trade openness – and human development affect private investment decisions directly, and/or through their positive impact on governance. These findings bring new empirical evidence on the subject of private investment in the developing world and in MENA countries in particular."
52cd4318a711f441d9fc211887ecd4415e646c7a,"During the 1980s and the 1990s, private investment in the Middle East and North Africa (MENA) has on average shown a decreasing or stagnant trend. This contrasts with the situation of the Asian economies, where private investment has always been more dynamic. In this article, it is empirically shown for a panel of 39 developing economies among which four MENA countries – that in addition to the traditional determinants of investment such as the growth anticipations and the real interest rate – government policies explain MENA's low investment rate. Insufficient structural reforms, which have most of the time led to poor financial development and deficient trade openness have been a crucial factor for the deficit in private capital formation. The economic uncertainties of the region have represented another factor of the firm's decisions not to invest. These uncertainties consisted of the external debt burden and various measures of volatility."
53ab60a676cff35663d7d93ec6fc7042074c7ecf,"This paper empirically shows, for a panel of 31 developing countries studied during the 1980s and the 1990s, that governance institutions constitute an important part of the investment climate of the developing economies. This result strongly holds for the “Quality of Administration” and confirms that a low level of corruption, a good quality of bureaucracy, a reliable judiciary, a strong security of property rights, a reasonable risk to operations, as well as a sound taxation and regulation contribute significantly to the firms' decision to invest. In Middle East and North Africa, improved governance institutions would greatly stimulate private investment decisions."
5cf4eacb2fbd97e14c1bd484f7eba245ba7273a0,"The rapid growth in Turkish credit card market brought together new issues. Card holders and consumer unions complain about the high interest rates, economists complain about the default rates and banks complain about the amnesties. After all of these complaints coinciding with the accelerating suicide incidences due to credit card debts, regulation has been enacted in the credit card market in Turkey. In 2003, credit cards had been taken into the scope of the Consumer Protection Law. This was the first legal arrangement on the credit cards. However it was not satisfying. It was criticized for bringing out temporary solutions. In 2005, a more comprehensive credit card law came into effect. With this regulation, Central Bank of Turkey has put a ceiling on the credit card interest rates and clarified some issues that were left untouched. In this paper; reasons, advantages, disadvantages of this regulation are discussed along with a quick glance on the development of credit card market in Turkey. The regulation and amnesties in 2003 and 2005 are examined and their effects are exposed from the point of view of parties involved: banks, customers and government."
60aa2ad724ac6266deedba0be6cf6a23e852915f,"Based on the economic developments, Turkish accounting profession has been in progress since the establishment of Turkish Republic (1923). As a result of industrialization, the need for accounting profession emerged. For this reason, the business managers and management accountants needed in private companies were mostly transferred from State Economic Enterprises. The legalization, which was a basic factor for the development of accounting profession, unfortunately was delayed until 1989. Thus, organization of the profession (TÜRMOB) could be achieved in a late time. On the other hand, the former accounting organizations have organized several accounting activities since 1957 including 17 accounting congresses and 25 accounting education symposiums."
9a8d4ab5a48d715d697622318c9ebe042248151f,"Recent financial crises highlight weaknesses in financial markets and the need for regulatory and supervisory bodies (RSB) to improve the stability of financial markets. Currently, international institutions like the IMF and the World Bank place the independent RSB among their principle policy recommendations to developing countries. This paper acknowledges the importance of independent RSB for the proper functioning of financial markets. However, this paper also points out the preconditions to establish independent RSB. Unless certain prerequisites are satisfied, policy recommendations to construct an independent RSB are doomed to fail. The recent Turkish experience is provided as a case study to elucidate this conclusion. This paper first presents the arguments for independent RSB and the policy recommendations in institution building for stronger financial system. Then, the background of Turkish experience for independent RSB is provided. Finally, we analyze the primary reasons for the deficient performance of Turkish RSB over the last five years in an attempt to provide actual lessons for the future institutional reforms."
9b6f98af4a064a8b84576de233f611d88eb4a2aa,"This paper shows that exchange rate alignments are also used for the redistribution of income among different groups. The heterogeneous impacts of stabilization policies lead to formation of various coalitions throughout the evolution of stabilization programs. These coalitions can produce unsustainable economic policies at the expense of other groups. The model categorizes these various groups with respect to their shares in total production of tradables and nontradables. An increase in the relative prices of nontradables benefits the poor more than the rich and middle classes. In addition to the poor, the rich benefit from unsustainable macroeconomic polices by lending to the government and eventually escaping the cost of stabilization in the long run."
9f62ada1046b78c9569ada39fba4675572ccccb4,"This paper sheds light on the distributional implications of the exchange rate based stabilizations with financial imperfections when a country is populated by heterogeneous agents with respect to their source of income. This paper shows that boom-bust cycles in developing countries lead to income redistribution from tradable to nontradable sectors. Since the share of tradable sectors in aggregate GDP increases above its usual share with the devaluation of the currency, the individuals in tradable sectors pay more tax than what they receive as capital inflow in the expansion phase of the economy. The opposite holds for the individuals in nontradable sectors who gain more from the capital inflow as compared to what they lose from taxation"
c43d1ce4c18557d12995683afff2762181d7d8c6,"This paper addresses the issue of the low level of private investment in the Middle East and North Africa (MENA) region, with special emphasis on the role of governance. Based on the existing literature, we have categorized what types of governance institutions are more detrimental to entrepreneurial investments. We have then estimated a simultaneous model of private investment and governance quality where economic policies concurrently explain both variables. Our empirical results show that governance plays a significant role in private investment decisions. This result is particularly true in the case of “Administrative Quality” in the form of control of corruption, bureaucratic quality, investment-friendly profile of administration, and law and order, as well as for “Political Stability”. Evidence in favor of “Public Accountability” seems, however, less robust. Our estimations also stress that structural reforms -such as financial development and trade openness – and human development affect private investment decisions directly, and/or through their positive impact on governance. These findings bring new empirical evidence on the subject of private investment in the developing world and in MENA countries in particular."
cff52fad2608273f82c39576c13ac1a7716eac63,"This paper analyzes the current state of research in macroeconomics in the light of first year macroeconomics courses offered in the 16 leading Ph.D programs. Our investigation confirms that methodological and ideological differences of 1970’s and 80’s are about to disappear as methodological gap between macroeconomics and microeconomics is narrowing. Moreover, while use of mathematics in macroeconomics is increasing, applied mathematics gains more importance than theoretical mathematics. This paper also shows that, in the first year of Ph.D. programs, lectures in macroeconomics are designed to teach basic techniques and methods while the topics like the imperfection based models, monetary and fiscal policies, welfare implications of these policies and the role of institutions in economy are left to the following years."
e4eb0691af0afe968adafc963c1522498f4d76f8,"The high credit card interest rates in Turkey attracted considerable attention in recent years to regulate the Turkish credit card industry. Before any regulation decision taken, there needs to be better conceptualization and analysis of the Turkish credit card market. This paper sheds some light in this direction. First, we highlight the most striking aspects of the Turkish credit card market. After exposing the problem, we benefit from the existing theoretical and empirical studies on the structure of competition in the credit card industry. Potential reasons for the lack of competitions are denoted. Having the existing studies in mind, we finally, construct an empirical model to estimate the market structure in the Turkish credit card industry. Newly disseminated data on the Turkish credit card industry is first introduced in this paper. Our empirical results are based on the panel data set of 22 banks from the second quarter of 2001 to the third quarter of 2005. In addition to random and fixed effects regressions, instrumental variable fixed effect regressions are run on this sample. Our results robustly conclude that the credit cards interest rates in Turkey are economically insensitive to the changes in the cost of fund. This result indicates that Turkish credit card market is characterized with lack of strong competition and hence suggests some regulatory measures."
e6f676b037f00924de6d7930412f0e7a7b00a317,"The high credit card interest rates in Turkey attracted considerable attention in recent years to regulate the Turkish credit card industry. Before any regulation decision taken, there needs to be better conceptualization and analysis of the Turkish credit card market. First, we highlight the most striking aspects of the Turkish credit card market. After exposing the problem, we benefit from the existing theoretical and empirical studies on the structure of competition in the credit card industry. Potential reasons for the lack of competitions are denoted. Having the existing studies in mind, we finally, construct an empirical model to estimate the market structure in the Turkish credit card industry. Newly disseminated data on the Turkish credit card industry is first introduced in this paper. Our empirical results are based on the panel data set of 22 banks from the second quarter of 2001 to the third quarter of 2005. In addition to random and fixed effects regressions, instrumental variable fixed effect regressions are run on this sample. Our results robustly conclude that the credit cards interest rates in Turkey are economically insensitive to the changes in the cost of fund. This result shows lack of strong competition Turkish credit card market."
e9ecf40576eb236d7868dbfb04dcfdb881857db1,"Motivated by the increased importance of foreign bank entry, this paper takes a look at the issue from the perspective of both foreign entrants and the host country. What are the conditions that make the host country market attractive to foreign entrants? What changes in the home country motivate foreign banks to expand abroad? Attracted by the “pull” and “push” factors, foreign banks enter into the banking sector of the host country resulting in both benefits and costs to the domestic sector. Having given the reasons and the effects of foreign entry in a theoretical framework, this study attempts to find out any match of the theory with the evidence."
eef8db3be75f5ecda5a572c8422d7c4dab8b6a1b,"This paper provides a model to account for the empirical evidence that volatility reduces growth. In the model, greater volatility increases the cost associated with capital market imperfections and induces the financial intermediaries to charge higher interest rates. The model is based on one of overlapping generations with two types of technologies. The more productive technology requires fixed investment in the first period. Individual with income less than the amount of fixed investment may borrow in financial markets to obtain more productive technology. Increase in volatility raises the cost of borrowing and makes it less attractive to invest in more productive technology for individuals whose first period income is below certain income. Hence, volatility reduces growth by deterring people from taking advantage of more productive technology. This model also explains the empirical findings of Ramey and Ramey (1995) that investment is not the channel between volatility and growth by suggesting that total factor productivity rather than total factor accumulation is the key for growth."
0a4b50222a844462c622e6842afd727dcfe877ae,"This paper first points out the lack of consensus between empirical and theoretical studies of income inequality and redistribution. While theoretical papers show that income inequality increases redistribution, empirical studies fail to confirm the same result. The paper later shows that even an exogenously given efficiency of redistributive institutions (ERI) affects the relationship between income inequality and redistribution. This paper also introduces three specifications to endogenize ERI. In these various specifications, increasing inequality reduces the ERI when (1) ERI is an increasing function of average income or (2) political influence on ERI is positively associated with income or (3) the median voter has some prospect of upward mobility. There is one common element in these various specifications. While income inequality increases the pressure for redistribution, it also increases the incentive to reduce the efficiency of redistribution in order to constrain aggregate redistribution. Thus, the main conclusion is that one needs to consider these conflicting effects in order to account for the lack of strong empirical evidence of a positive relationship between income inequality and redistribution."
d40e74117cd542d1d7d1d255e03f8f0f7de6a97f,
db3526046ee7e9cfbda9f05b36f621235c413093,"This paper analyzes the determinants of unsatisfying private investment growth in the Middle East and North Africa (MENA) throughout the 1980s and 1990. In this period, private investment in MENA has on average shown a decreasing or stagnant trend in contrast to the rest of the world. This paper show empirically for a panel of 40 developing economies -among which five MENA countries -that in addition to the traditional determinants of investment -such as the growth anticipations and the real interest rate -government policies explain MENA's low investment rate. Insufficient structural reforms represented as poor financial development and deficient trade openness has been a crucial factor for the deficit in private capital formation. Economic uncertainties of the region have constituted major deterrent for firms to invest. High external debt burden and economic volatility arise as primary reasons for high uncertainty in the region. These findings provide new empirical evidences on the determinants of private investment in the developing world and in MENA countries in particular."
dd33f91b1fd9696c7e74bb408b5cd7da7fda8b86,"In spite of both theoretical and empirical contributions to investigate the determinants of redistribution, an important gap remains in the literature, which is the effect of efficiency of redistributive institutions on redistribution. This paper is an attempt to show that the state apparatus with its redistributive institutions plays a major role in determining the size of redistribution. Redistribution is mainly approximated with social security and welfare expenditures by the governments. We utilize the indices of ‘Quality of Bureaucracy’ and ‘Control of Corruption’ from the International Country Risk Guide to quantify efficiency of redistributive institutions. When measures of ERI are incorporated into the existing empirical specifications of income inequality and redistribution, cross-sectional and panel data regressions show that ERI significantly increases redistribution. This result is robust to alternative specifications of the empirical model as well as to alternative data sets. However, we find weaker evidence for the role of income inequality on redistribution. Income inequality does not appear to be strongly significant in various specifications of the redistribution equation. Based on this evidence, this paper concludes that efficiency of redistributive institutions plays an important role in redistribution but this effect does not resolve the fiscal policy puzzle."
418d7d6d120e161d89da16033905eaccb2ef94ab,"The process of neoliberal globalisation has been associated with successive financial crises in the context of the 1990s, raising serious doubts concerning the sustainability of rapid growth in an environment of uncontrolled movements of short-term capital. The article probes into the origins of the financial crises in the semi-periphery through a structured comparison of three key recent crises in the world economy, namely the Mexican and Turkish crises of 1994 and the Asian crises of 1997. Whilst the magnitude of the capital flows and the dimensions of the subsequent crises are strikingly different, there are nonetheless important elements common to all three cases studied. One such common element involves the overdependence of the countries concerned on the short-term financial flows, in a setting characterised by premature capital account liberalisation in the absence of adequate regulation. It is striking that, contrary to the conventional IMF wisdom, financial crises have occurred in spite of 'sound fundamental', namely fiscal equilibrium and low inflation. The recent financial crises highlights a paradoxical situation: the need for effective regulation at a time when the capacity of the nation state to undertake the type of regulation needed severely circumscribed. Hence, the establishment of an effective regulatory framework at the global level emerges as a major requirement if successive financial crises, with significant economic and social cost, are to be avoided in the future."
290d29d93a6bb5316e69ddfa912d935e1c5a5509,
9c79369d0290afc0a978bdefe5c4a6b464fb8d36,"Cryo-electron tomography (cryo-ET), which produces three dimensional images at molecular resolution, is one of many applications that requires image reconstruction from projection measurements acquired with irregular measurement geometry. Although Fourier transform based reconstruction methods have been widely and successfully used in medical imaging for over 25 years, assumptions of regular measurement geometry and a band limited source cause direction sensitive artifacts when applied to cryo-ET. Iterative space domain methods such as compressed sensing could be applied to this severely underdetermined system with a limited range of projection angles and projection length, but progress has been hindered by the computational and storage requirements of the very large projection matrix of observation partials. In this paper we derive a method of dynamically computing the elements of the projection matrix accurately for continuous basis functions of limited extent with arbitrary beam width. Storage requirements are reduced by a factor of order 107 and there is no access overhead. This approach for limited angle and limited view measurement geometries is posed to enable dramatically improved reconstruction performance and is easily adapted to parallel computing architectures."
9d7957646f5359f79a82645e677d74b616534285,"Improved accurate measurement models and improved iterative reconstruction algorithms would benefit cryo-electron tomography (cryo-ET) performance. Filtered back- projection and related algorithms, successful in CT and MRI, assume a measurement model which is not well matched to the limited range of projection angles, large angular increments, and incomplete projections in cryo-ET. Iterative methods, such as compressed sensing (CS) can include irregular measurement models and spatial extent constraints, and have great potential for solution of severely under-determined systems. This paper uses source models with square and pyramidal basis functions and variable finite width aperture measurement to compare space domain and frequency domain CS reconstruction approaches in the cryo-ET context."
2d339b4473c549ba9e9fe4f5718fa470df158635,"A model is proposed for incorporating the effects of organ motion into the calculation of dose in a statistical fashion based on serial imaging measurements of organ motion. These measurements can either come from a previously studied population of patients, or they can be specific to the particular patient undergoing therapy. The statistical distribution underlying the measurements of organ motion, including the changes in organ shape, is reconstructed non-parametrically without requiring any assumptions about its functional form. The model is thus capable of simulating organ motions that are not present in the original measurements, yet nonetheless come from the same underlying statistical distribution. The present model overcomes two particular limitations of many organ motion models: (a) the fact that they do not account for changes in organ shape, and (b) the fact that they make physically unrealistic assumptions about the functional form of the statistical distribution of organ motion, such as assuming that it is Gaussian. The present model can form the foundation of methods for the more accurate and clinically relevant calculation of the dose to the target volume and normal tissues."
a5c8a203b97a52c44926458a0e8eb8339e940740,"Predicting late-term normal-tissue complication probability (NTCP) after radiotherapy is an important factor in the optimization of conformal radiotherapy. We propose a new NTCP model, based on the properties of the high dose region. The principal assumption of the new model is that a whole-organ complication will occur when the radiation damage to a normal organ volume (a portion of the total organ) exceeds a threshold value. The dose threshold for complications varies with the size of the volume (percent of the total organ). We hypothesize that a complication occurs if the complication threshold is exceeded for any organ volume. We used the average dose to a volume as a measure of radiation damage to that volume. Also, we used the power law to scale the average dose to various organ volumes to a whole-organ equivalent dose, and to identify the volume with the most harmful dose-size combination-the critical volume. We used a logistic distribution to calculate the probability that the patient will develop a complication, given the dose delivered to the critical volume. We used a maximum likelihood fit to estimate the model parameters for late-term rectal complications in a set of patients treated for prostate carcinoma with external photon beam radiotherapy (EBRT). Good correspondence was found between the experimental data and the model predictions."
ca3928e2283db534334dd08e6155120ca18251df,"We previously proposed a model for incorporating the effects of organ motion, including the changes in organ shape, into the calculation of dose in a statistical fashion based on serial imaging measurements of organ motion. In the present paper, numerical studies were used to investigate how the accuracy of the statistical calculation of dose depends on the number of organ motion measurements provided as input into the model. The dose calculated statistically with the model was consistently more accurate than the one obtained by directly resampling the serial measurements of organ motion. It was also more robust relative to the random variabilities present in the input organ motion measurements. The results confirm that the model can reproduce the statistical distribution of the organ motions measured in a serial imaging study, including the changes in organ shape, without making any assumptions about the functional form of this distribution. The model allows a more accurate calculation of dose to be performed from a given number of measurements of organ motion than would otherwise be obtained by directly resampling the measured data. It thus maximizes the information that is extracted from serial imaging measurements."
79ea19cde3a9cbae31ad1993a4220d08280afde0,"The present study is one part of an investigation of the hypothesis that a previously formulated statistical model of organ motion can predict a more accurate distribution of dose to the target volume and normal tissues than would otherwise be calculated based only on the static anatomical information available in the planning CT scan. This study concerns utilizing affine transformations to model the day-to-day variability in the shape and size of the prostate, bladder, and rectum. Using data from a CT serial imaging study, the accuracy of this affine approximation was quantified for each organ by calculating the distance between corresponding points on the surface of the organ as delineated in the initial and subsequent CT scans, after the initial surface was transformed onto the subsequent scans using the affine transformation calculated for that organ motion. It was found that the distance between corresponding points on the two surfaces was less than 7.45 mm for 95% of the prostate points analyzed (average 2.7 mm), less than 10.6 mm for 95% of the bladder points analyzed (average 3.4 mm), and less than 14.5 mm for 95% of those rectum points for which this distance could be quantified (average 5.5 mm). However, on certain CT planes the rectum surfaces exhibited deviations that could not be properly quantified with the method utilized, and consequently the distance values for the rectum are not an accurate representation of the true accuracy of the affine transformation."
8fe107e0c9581104fc5a9448153276a1f3d4d978,
60960de0266287fa475951b2a494a8e8eca17848,"To determine the shape of a radiation beam aperture a margin is typically applied to the clinical target volume (CTV) to yield the planning target volume (PTV), and the aperture is then determined from the projection of the PTV onto the aperture plane. This margin accounts for setup variability and organ motion originating from respiration or other physiologic processes. The use of either a uniform margin, or alternatively one which takes into account only the expected magnitude and direction of target motion, fails to account for the three-dimensional nature of the target; such a method neglects the volumetric effect of target shape on the fractional target volume irradiated when the target shifts partially out of the aperture. A mathematical framework is developed to analyze and illustrate the consequences of irradiating an irregular target shape in the presence of target motion. The effect of target shape on volume coverage is demonstrated for selected cases involving conventional BEV aperture design techniques. The volumetric implications of target shape are considered from two complementary points of view. The first involves transformation into a ""displacement space,"" which isolates the volumetric effect of the shape of the target allowing it to be studied independently of the probability distribution of target motion. The second point of view combines the effects of the 3D target shape and the probability distribution of motion in a manner independent of beam direction to yield a 3D ""target distribution."" The two points of view represent distinct starting points for computation of the expected value of fractional target volume coverage in the presence of target motion. In certain cases it may be beneficial to (1) employ ""target distributions"" for the target and normal tissues in place of the conventional static PTV and, (2) include the aperture shape, on equal footing with parameters such as beam weights and energies, into a quantitative optimization process explicitly accounting for uncertainties in the position of the target volume and critical structures."
c23de341127a8fb9448503af3043fe2ca07d0e39,
22b862fd00e97cf644b202eb4e338f580eaf0175,
7b426154274d971e8aa834edfbbd42524d5cb7b0,"Objective: The primary endpoint of the study was to determine the proportion of patients with HIV RNA < 50 copies/mL at 48 weeks. Design: Phase IV, multicentric, open-label, single-arm clinical trial of participants recruited in 2018–2019 to evaluate the efficacy and safety of tenofovir alafenamide/emtricitabine/elvitegravir-cobicistat (TAF/FTC/EVG-c) as first-line treatment in HIV-1 infected naïve participants with advanced disease. Methods: Adverse events were graded according to the Division of AIDS scale version 2.0. Quantitative variables were recorded as median and interquartile range, and qualitative variables as absolute number and percentage. T-Student or Wilcoxon tests were used to analyze intragroup differences of the continuous variables. Results: Fifty participants were recruited with a baseline median CD4 lymphocyte count of 116 cells/µL and a viral load of 218,938 copies/mL. The proportion of patients with viral load <50 copies/mL at week 48 was 94% in the per-protocol analysis, with a median time of 1.9 months to achieve it. Three adverse events attributed to the study drug caused trial discontinuation. Conclusions: the use of TAF/FTC/EVG-c in patients with advanced HIV disease in our study demonstrated efficacy comparable to data from pivotal clinical trials with a good safety profile."
f405ced13cdb5291cd461616057f91e8aca63ef9,
a2b6fad6b95ba57d1daf5835c99896051321e03e,"Background Cobicistat is used in clinical practice as a pharmacokinetic enhancer of protease and/or integrase inhibitors. Nevertheless, the mechanism by which this occurs (metabolism inhibition) makes cobicistat-containing HIV regimens very prone to interact with chronic treatments, which triggers toxicity. Purpose To reconcile HIV treatments containing cobicistat and to analyse the interactions with the chronic treatment. Material and methods Patients attending the outpatient pharmacy clinic between January and September 2018 with a regimen containing cobicistat were included. During the dispensation of their HIV medication, patients’ treatment was reconciliated by two methods: pharmacy interview and consultation of the prescribed medication in the primary records. The interaction between the cobicistat and the patients’ chronic treatment was checked in drugs.com. In this website interactions are classified as major, moderate, minor and non-interaction. Results Eight-hundred and forty-two treatments were reconciliated (patients: 47.9±11.5 years old; 82.4% male). Twenty-eight different HIV regimens were identified, the most frequent being the one containing Genvoya (cobicistat, elvitegravir, emtricitabine, tenofovir alafenamide) (68.4%).Two-hundred and forty different chronic drugs were prescribed (2.2±2.4 drugs per patient). Twenty-one drugs were classified to have a major interaction with cobicistat, 40 a moderate interaction, five minor, 147 did not have any interaction registered in drugs.com and 27 drugs did not appear in this web. Pharmacists made 87 interventions with 35 different drugs. The most frequent were inhaled budesonide (12) and nasal fluticasone (11). Forty-four (51%) of the pharmaceutical interventions did not need the physician’s approval (17 to interrupt chronic treatments, 13 to change treatments, 12 to monitor and one to change dose). The rest (43) required physician approval and these consisted of more varied actions, highlighting six changes in the HIV regimen to eliminate cobicistat. We registered possible/probable toxicities related to the inhibition of metabolism due to cobicistat in eight patients. Conclusion Pharmacist reconciliation detects numerous potential interactions. Pharmacist intervention helped to modify several treatments and make treatments safer. References and/or acknowledgements None. No conflict of interest."
da81536fc85f3cc3b4f1b9062ea827009eac97b6,"Background Tenofovir alafenamide (TAF) is associated with less renal and bone toxicity compared with tenofovir disoproxil (TDF) but with elevation of cholesterol levels. In our hospital, patients were automatically changed from a regimen with Eviplera (rilpivirine (RPV) +emtricitabine (FTC)+TDF) to a regimen with Odefsey (rilpivirine (RPV) +emtricitabine (FTC)+TAF). Patients were informed of the switch by the pharmacist. Patient views on the process of these medication switches have been rarely explored. Purpose To assess the patient satisfaction and knowledge of the switch from RPV/FTC/TDF to RPV/FTC/TAF. Material and methods Patients attending the outpatient pharmacy clinic in the months of August and September 2018 who had been previously treated with RPV/FTC/TDF and who came for the second dispensation to take RPV/FTC/TAF were included. In a face-to-face meeting with the pharmacist or by telephone, patients were asked to complete a survey. Demographic domains included gender, age, nationality of birth, education level and work status. Satisfaction and knowledge questions regarding the medication switch were assessed using a five-point Likert scale of agreement/disagreement. Patients were also asked if the treatment switch had been informed by the physician or the pharmacist. Basic descriptive statistics (frequencies and percentages) were calculated for all survey questions. Results A total of 48 patients underwent the medication switch from RPV/FTC/TDF to RPV/FTC/TAF (43±9 years’ old; 71% males; 75% born in Spain). Most patients (73%) reported understanding why the switch was made, 90% correctly identified that TAF was associated with reduced bone adverse effects and 83% correctly identified that TAF was associated with reduced renal adverse effects. Only 44% of the patients knew that their cholesterol levels might increase. In regard to the brief handout that was given to all patients, only 17% respondents reported receiving written information about the new medication. Ninety-eight per cent of the patients knew RPV/FTC/TAF must be taken with food and 90% knew that proton pump inhibitors were contraindicated. Conclusion Patient education from an ambulatory clinic-based HIV specialist pharmacist resulted in high rates of patient satisfaction and understanding of the switch from TDF to TAF-containing ART. References and/or acknowledgements None. No conflict of interest."
133315020512e9ee9a4bb638bef92b9534cd1bbe,
25cc22d53169fac0fadf3c87a32c01b99790964c,"Chronic low‐grade inflammation and immune activation may persist in HIV patients despite effective antiretroviral therapy (ART). These abnormalities are associated with increased oxidative stress (OS). Bilirubin (BR) may have a beneficial role in counteracting OS. Atazanavir (ATV) inhibits UGT1A1, thus increasing unconjugated BR levels, a distinctive feature of this drug. We compared changes in OS markers in HIV patients on ATV/r versus efavirenz (EFV)‐based first‐line therapies."
c406d743f00edab27d333f11038f43da546d4ba4,"Chronic hepatitis C patients may require steroids due to other comorbidities. However, there is not enough information to consider steroids as beneficial or harmful drugs on natural history of chronic hepatitis C. The aim of the present study was to examine the effect of low‐dose prolonged therapy with corticosteroids with or without azathioprine on these study patients. A retrospective–prospective observational study was established. Twenty‐eight patients with chronic hepatitis C and treated with corticosteroids at low‐dose (≤30 mg/day) with or without azathioprine for more than 6 months were included. AST, ALT, HCV RNA, and liver fibrosis were determined, and results were compared with a control group of non‐treated chronic hepatitis C patients. The mean age was 47 ± 10 years. The male proportion was 43%. The mean dose of prednisone was 9 ± 5 mg/day (range: 2.5–30 mg/day). The mean treatment time was 76 ± 80 months (range: 7–349 months). Thirty six percent received concomitant azathioprine. Transaminases decreased significantly only within the first 3 months of treatment, with non‐significant changes thereafter. Corticosteroids led to a non‐significant increase in HCV RNA. Knodell Histology Activity Index decreased (from 8.5 ± 3.7 to 4.7 ± 1.7; P = 0.1). Fibrosis progression per year (final fibrosis stage—initial fibrosis stage/time between explorations, in years), was lower in treated cases than in control group (0.054 ± 0.25 units vs. 0.196 ± 0.6 units, P = 0.26). In conclusion, corticosteroid treatment caused a significant initial decrease in transaminases, non‐significant changes in HCV RNA, and a trend to a slower fibrosis progression in comparison to a control group. Therefore, corticosteroids did not accelerate progression of chronic hepatitis C. J. Med. Virol. 86:758–764, 2014. © 2014 Wiley Periodicals, Inc."
fd6923ebc384d6b85335dcbb25714fcc49f35ff2,"HIV/AIDS continues to place a devastating toll on individuals, families and communities globally, and western industrialized countries are by no means exempt. Today, there are more than 1 million Americans and 100,000 Britons living with HIV, with a disproportionate burden of new and prevalent HIV infections borne by gay, bisexual and other men who have sex with men (MSM), racial/ethnic minorities, migrants and persons who use drugs. Epidemic concentration in urban areas, especially among: population sub‐groups with high prevalence of risk behaviours; the socio‐economically marginalized; or those with poor access to services, has been well documented. Recent increases in HIV incidence in the rural south US, and in MSM in both countries, reflect the dynamic and evolving nature of these epidemics. New national HIV prevention strategies in both countries have refocused attention on these domestic epidemics, prioritizing HIV testing scaling up, linkage to quality care and tackling long‐standing health inequalities. There are also significant differences between the two countries – in part a reflection of the different health and social care systems; historical approaches to the funding and coordination of HIV prevention; and underlying patterns of health inequalities and their social and structural determinants. In addition, the social–political acceptability of using the sexual health frame to guide more holistic and integrated approaches to HIV prevention efforts remains a key difference. This presentation will compare and contrast HIV prevention responses in the US and UK over the past decade, identifying opportunities for enhancing the prevention response in these and other western industrialized countries in the 21st century."
9f6da7e36fc3a943a8d23577f8cc6e284cb4d00f,"Irregular FUP/ADH were associated with virologic failure [ 1 ] leading to an increase in mortality [ 2 ]. SEAD was a multidimensional intervention project, designed from the patient's perspective, to specifically attend patients with poor FUP/ ADH in an HIV/AIDS outpatient clinic."
c7aabaefac53da16022e863796a8575d0d775728,"OBJECTIVES
To evaluate the clinical significance of Streptomyces isolates in different clinical samples.


MATERIAL AND METHODS
Review of the records of all cases of Streptomyces isolated from any clinical sample at a tertiary Hospital, during a seven-year period.


RESULTS
Streptomyces was isolated from 13 patients. All of them had underlying diseases. Only in one patient Streptomyces was considered to have a pathogenic role in the clinical picture. We report the third case of catheter-related infection caused by this microorganism.


CONCLUSIONS
Streptomyces is usually isolated from patients with underlying diseases. Before considering them significative, Streptomyces isolates must be interpreted in the clinical context."
c467fd8ef868f0b480feffce41161d81c07997b9,"BACKGROUND AND OBJECTIVE
To know the durability of consecutive regimens of antiretroviral treatment is important to design a long-term therapy, but there is not much information about this subject.


PATIENTS AND METHOD
Retrospective epidemiological study of a sample of 401 patients who began antiretroviral treatment between January 1997 and April 2000 at ten Spanish hospitals. The duration of each consecutive antiretroviral regimen was calculated and the reasons for modification and discontinuation were described.


RESULTS
In the 3 years and 3 months covered by the study, 48.6% of the patients received more than one regimen of therapy. Seventy five of the initial prescribed combinations included protease inhibitors. Median duration of consecutive lines of therapy was decreasing: 560, 360, 330 and 202 days for the first, second, third and fourth regimens, respectively. The main reason to modification was intolerance or toxicity (46.2, 49.1 and 47.1% for the first, second and third modification). A fifth of changes was originated by difficulties to follow the therapy. Virological failure was the reason for modification in 21.8, 24.5 and 26.5% of first, second and third changes.


CONCLUSIONS
Duration of consecutive antiretroviral regimens progressively decreases. Intolerance or drug toxicity were the main reasons conditioning the change of treatment."
7969b20c49446f30338c11a2ca4038236e6fa69e,"For more than 100 years, the fruit fly Drosophila melanogaster has been one of the most studied model organisms. Here, we present a single-cell atlas of the adult fly, Tabula Drosophilae, that includes 580,000 nuclei from 15 individually dissected sexed tissues as well as the entire head and body, annotated to >250 distinct cell types. We provide an in-depth analysis of cell type–related gene signatures and transcription factor markers, as well as sexual dimorphism, across the whole animal. Analysis of common cell types between tissues, such as blood and muscle cells, reveals rare cell types and tissue-specific subtypes. This atlas provides a valuable resource for the Drosophila community and serves as a reference to study genetic perturbations and disease models at single-cell resolution. Description Cell type diversity in a whole fly The fruit fly Drosophila melanogaster has served as a premier model organism for discovering fundamental and evolutionarily conserved biological mechanisms. Combining recent advances in single-cell sequencing with powerful fly genetic tools holds great promise for making further discoveries. Li et al. present a single-cell atlas of the entire adult fly that includes 580,000 cells and more than 250 annotated cell types. Cells from the head and body recapitulated cell types from 15 dissected tissues. In-depth analyses revealed rare cell types, cell-type-specific gene signatures, and sexual dimorphism. This atlas provides a resource for the Drosophila community to study genetic perturbations and diseases at single-cell resolution. —BAP A single-nucleus transcriptomic map reveals more than 250 distinct cell types in the entire adult Drosophila melanogaster. INTRODUCTION Drosophila melanogaster has had a fruitful history in biological research because it has contributed to many key discoveries in genetics, development, and neurobiology. The fruit fly genome contains ~14,000 protein-coding genes, ~63% of which have human orthologs. Single-cell RNA-sequencing has recently been applied to multiple Drosophila tissues and developmental stages. However, these data have been generated by different laboratories on different genetic backgrounds with different dissociation protocols and sequencing platforms, which has hindered the systematic comparison of gene expression across cells and tissues. RATIONALE We aimed to establish a cell atlas for the entire adult Drosophila with the same genetic background, dissociation protocol, and sequencing platform to (i) obtain a comprehensive categorization of cell types, (ii) integrate single-cell transcriptome data with existing knowledge about gene expression and cell types, (iii) systematically compare gene expression across the entire organism and between males and females, and (iv) identify cell type–specific markers across the entire organism. We chose single-nucleus RNA-sequencing (snRNA-seq) to circumvent the difficulties of dissociating cells that are embedded in the cuticle (e.g., sensory neurons) or that are multinucleated (e.g., muscle cells). We took two complementary strategies: sequencing nuclei from dissected tissues to know the identity of the tissue source and sequencing nuclei from the entire head and body to ensure that all cells are sampled. Experts from 40 laboratories participated in crowd annotation to assign transcriptomic cell types with the best knowledge available. RESULTS We sequenced 570,000 cells using droplet-based 10x Genomics from 15 dissected tissues as well as whole heads and bodies, separately in females and males. We also sequenced 10,000 cells from dissected tissues using the plate-based Smart-seq2 platform, providing deeper coverage per cell. We developed reproducible analysis pipelines using NextFlow and implemented a distributed cell-type annotation system with controlled vocabularies in SCope. Crowd-based annotations of transcriptomes from dissected tissues identified 17 main cell categories and 251 detailed cell types linked to FlyBase ontologies. Many of these cell types are characterized for the first time, either because they emerged only after increasing cell coverage or because they reside in tissues that had not been previously subjected to scRNA-seq. The excellent correspondence of transcriptomic clusters from whole body and dissected tissues allowed us to transfer annotations and identify a few cuticular cell types not detected in individual tissues. Cross-tissue analysis revealed location-specific subdivisions of muscle cells and heterogeneity within blood cells. We then determined cell type–specific marker genes and transcription factors with different specificity levels, enabling the construction of gene regulatory networks. Finally, we explored sexual dimorphism, finding a link between sex-biased expression and the presence of doublesex, and investigated tissue dynamics through trajectory analyses. CONCLUSION Our Fly Cell Atlas (FCA) constitutes a valuable resource for the Drosophila community as a reference for studies of gene function at single-cell resolution. All the FCA data are freely available for further analysis through multiple portals and can be downloaded for custom analyses using other single-cell tools. The ability to annotate cell types by sequencing the entire head and body will facilitate the use of Drosophila in the study of biological processes and in modeling human diseases at a whole-organism level with cell-type resolution. All data with annotations can be accessed from www.flycellatlas.org, which provides links to SCope, ASAP, and cellxgene portals. Tabula Drosophilae. In this single-cell atlas of the adult fruit fly, 580,000 cells were sequenced and >250 cell types were annotated. They are from 15 individually dissected sexed tissues as well as the entire head and body. All data are freely available for visualization and download, with featured analyses shown at the bottom right."
4a81204a5f292ebc9d9e317e9d806bb0ba4c394d,
1798d5c05daf06fbf799dec7806d0c7abf56fc53,1. Remedies for Anti-Social Behaviour 2. An Overview of the Anti-Social Behaviour Act 2003 3. Part I of the Act: Premises Where Drugs are Used Unlawfully 4. Part 2 of the Act: Housing 5. Part 3 of the Act: Parental Responsibilities 6. Part 4 of the Act: Dispersal of Groups 7. Part 5 of the Act: Firearms 8. Part 6 of the Act: The Environment 9. Part 7 of the Act: Public Order and Trespass 10. Part 8 of the Act: High Hedges 11. Part 9 of the Act: Miscellaneous Powers APPENDICES Appendix 1: Anti-Social Behaviour Act 2003 Appendix 2: Regulations Appendix 3: Guidance
832d8cdae9247f9285ca73c9f7655073d92956b5,"What has been the effect of three hundred and fifty years of colonialism and apartheid on South African architecture? What role has architecture played in reinforcing and perpetuating racist ideology? Can a building or a space be racist? Racism in Three Dimensions explores these questions and others pertaining to the relationship between race, racism and architecture in post‐apartheid South Africa and seeks to define a role for the architectural profession in the struggle to heal the wounds of apartheid"
a9dac981e4bd34cefd429b9545ead09e50bb5395,"Local government constitution. Principles of local government administrative law. Decisions. Members. Officers. Finance. Performance, property and service provision. Liability and accountability in law. Appendix."
e529cdb6480c4a9dad7dab8346d7d549fd8fc299,
0bd4f913788def67e8f6d36a03048e124841bdd4,
a6c5c23b900ba17a8859e08cbe81f9c898b028fb,Homelessness Disrepair - Civil Remedies Statutory Nuisance Unlawful Eviction/Harassment Possession Proceedings Break-up of Relationship Appeals: County Court and High Court.
835b277f3e30e4741a29b7ec0fc34fd607562353,"ABSTRACT Dengue (DENV) is a mosquito-borne virus with four serotypes causing substantial morbidity in tropical and subtropical areas worldwide. V181 is an investigational, live, attenuated, quadrivalent dengue vaccine. In this phase 1 double-blind, placebo-controlled study, the safety, tolerability, and immunogenicity of V181 in baseline flavivirus-naïve (BFN) and flavivirus-experienced (BFE) healthy adults were evaluated in two formulations: TV003 and TV005. TV005 contains a 10-fold higher DENV2 level than TV003. Two-hundred adults were randomized 2:2:1 to receive TV003, TV005, or placebo on Days 1 and 180. Immunogenicity against the 4 DENV serotypes was measured using a Virus Reduction Neutralization Test (VRNT60) after each vaccination and out to 1 year after the second dose. There were no discontinuations due to adverse events (AE) or serious vaccine-related AEs in the study. Most common AEs after TV003 or TV005 were headache, rash, fatigue, and myalgia. Tri- or tetravalent vaccine-viremia was detected in 63.9% and 25.6% of BFN TV003 and TV005 participants, respectively, post-dose 1 (PD1). Tri- or tetravalent dengue VRNT60 seropositivity was demonstrated in 92.6% of BFN TV003, 74.2% of BFN TV005, and 100% of BFE TV003 and TV005 participants PD1. Increases in VRNT60 GMTs were observed after the first vaccination with TV003 and TV005 in both flavivirus subgroups for all dengue serotypes, and minimal increases were measured PD2. GMTs in the TV003 and TV005 BFE and BFN groups remained above the respective baselines and placebo through 1-year PD2. These data support further development of V181 as a single-dose vaccine for the prevention of dengue disease."
de621a2f5a721f5cc62b121d0e505314fb1f8865,"Abstract Background Dengue (DENV) is a mosquito-borne virus with four serotypes causing substantial morbidity in tropical and subtropical areas worldwide. A dengue vaccine that can be given to both seronegative and seropositive populations remains an important unmet medical need. V181 is an investigational live, attenuated, quadrivalent dengue vaccine. Methods In this phase 1 double-blind, placebo-controlled study, the safety, tolerability, and immunogenicity of V181 in healthy adults were evaluated in two formulations: TV003 and TV005. TV005 has a 10-fold higher DENV2 component as compared to TV003. Two-hundred participants [~ 50% baseline flavivirus-experienced (BFE) and 50% baseline flavivirus-naive (BFN)] were randomized 2:2:1 to receive TV003, TV005, or placebo on Days 1 and 180. Immunogenicity against each of the four DENV serotypes was measured using a Virus Reduction Neutralization Test (VRNT60) after each vaccination and out to 1 year after the second dose. Results There were no discontinuations due to adverse events (AEs) or vaccine-related serious AEs. The most common AEs Days 1-28 after any TV003 or TV005 vaccination were rash, headache, fatigue, and myalgia. DENV VRNT60 seropositivity to 3 or 4 serotypes (i.e. tri-or tetravalent) was demonstrated in 92.6% of BFN TV003 participants, 74.2% of BFN TV005 participants, and 100% of the BFE participants at 6 months postdose 1 (PD1). Vaccine viremia, a measure of vaccine infectivity, was transiently detected from all four DENV types after the first dose of TV003 and TV005. Tri- or tetravalent vaccine-viremia was detected in 63.9 % and 25.6 % of BFN TV003 and TV005 participants, respectively, PD1. Compared to baseline, robust increases in VRNT60 GMTs were observed after the first dose of TV003 and TV005 in both flavivirus subgroups for all DENV serotypes and minimal increases were observed PD2. GMTs in the TV003 and TV005 BFE and BFN subgroups remained above the respective baselines and placebo at 1-year PD2. Conclusion Both formulations of V181 were generally well tolerated in healthy adults. Overall, viremia and immunogenicity were higher after TV003 as compared to TV005. These data support the continued development of the V181 TV003 formulation as a single-dose vaccine for the prevention of DENV disease. Disclosures Kevin Russell, MD, MTM&H, Merck & Co., Inc. (Employee, Shareholder) Richard E. Rupp, MD, Merck & Co., Inc. (Research Grant or Support) Clemente Diaz-Perez, MD, Merck & Co., Inc. (Research Grant or Support) Charles P. Andrews, MD, Merck & Co., Inc. (Research Grant or Support) Andrew W. Lee, MD, Merck & Co., Inc. (Employee, Shareholder) Tyler S. Finn, BA, Merck & Co., Inc. (Employee, Shareholder) Kara Cox, MS, Merck & Co., Inc. (Employee, Shareholder) Amy Falk Russell, MS, Merck & Co., Inc. (Employee, Shareholder) Margaret M. Schaller, BS, Merck & Co., Inc. (Employee, Shareholder) Jason C. Martin, PhD, Merck & Co., Inc. (Employee, Shareholder) Donna M. Hyatt, BA, Merck & Co., Inc. (Employee, Shareholder) Sabrina Gozlan-Kelner, MS, Merck & Co., Inc. (Employee, Shareholder) Androniki Bili, MD, MPH, Merck & Co., Inc. (Employee, Shareholder) Beth-Ann Coller, PhD, Merck & Co., Inc. (Employee, Shareholder)"
38cc93570ad94173fc404b925f691f81379a9572,"Background The safety and efficacy of doravirine were compared with that of efavirenz as initial treatment of adults living with HIV-1 infection (NCT01632345). Methods A Phase IIb double-blind trial with participants stratified by screening HIV-1 RNA (≤ or >100,000 copies/ml) and randomized 1:1:1:1:1 to receive once-daily doravirine (25, 50, 100 or 200 mg) or efavirenz 600 mg (Part I) for up to 96 weeks, with open-label tenofovir disoproxil fumarate 300 mg/emtricitabine 200 mg (TDF/FTC). After dose selection at week 24, doravirine 100 mg was provided to participants receiving the other doses of doravirine and additional participants were randomized 1:1 to receive once-daily doravirine 100 mg or efavirenz 600 mg for 96 weeks with TDF/FTC (Part II). Primary outcomes were the proportion of participants with HIV-1 RNA <40 copies/ml at week 24, and central nervous system (CNS) adverse events (AEs) by weeks 8 and 24 (Parts I+II combined). Results 210 and 132 participants were randomized in Parts I and II, respectively, and 216 (108 on doravirine 100 mg, 108 on efavirenz) were evaluable for Parts I+II combined. At week 24, the proportion of participants with HIV-1 RNA <40 copies/ml was 72.9% for doravirine 100 mg and 73.1% for efavirenz (difference −0.5 [95% CI −12.3, 11.2]). In addition, CNS AEs were reported by 26.9% and 47.2% of doravirine and efavirenz recipients, respectively (difference −20.4 [95% CI −32.6, −7.5]; P=0.002). Conclusions Doravirine 100 mg with TDF/FTC demonstrated similar antiretroviral activity and superior CNS safety compared with efavirenz 600 mg with TDF/FTC."
7510a21216e52430151e4aa2df53ea760bb6cdcf,"GSK3532795 (formerly known as BMS-955176) is a second-generation maturation inhibitor targeting a specific Gag cleavage site between capsid p24 and spacer peptide 1 of HIV-1. Study 205891 (previously AI468038) investigated the efficacy, safety, and dose response of GSK3532795 in treatment-naive, HIV-1-infected participants. Study 205891 (NCT02415595) was a Phase IIb, randomized, active-controlled, double-blind, international trial. Participants were randomized 1:1:1:1 to one of three GSK3532795 arms at doses 60 mg, 120 mg or 180 mg once daily (QD), or to efavirenz (EFV) at 600 mg QD, each in combination with tenofovir disoproxil fumarate and emtricitabine (TDF/FTC) (300/200 mg QD). Primary endpoint was proportion of participants with plasma HIV-1 RNA <40 copies/mL at Week 24. Between May 2015 and May 2016, 206 participants received treatment. At Week 24, 76–83% participants receiving GSK3532795 and 77% receiving EFV achieved HIV-1 RNA <40 copies/mL. Fifteen participants receiving GSK3532795 and one receiving EFV met resistance testing criteria; 10/15 receiving GSK3532795 had emergent substitutions at reverse transcriptase positions M184, and one at position K65, while the participant receiving EFV did not have any nucleoside reverse transcriptase inhibitor (NRTI)/non-NRTI mutations. EFV, relative to GSK3532795, had more serious adverse events (9% versus 5%) and adverse events leading to discontinuation (17% versus 5%). However, 3–4-fold higher rates of gastrointestinal adverse events were observed with GSK3532795 relative to EFV. GSK3532795 combined with TDF/FTC is efficacious with 24 weeks of therapy. However, GSK3532795 showed a higher rate of gastrointestinal intolerability and treatment-emergent resistance to the NRTI backbone relative to EFV. Trial registration: ClinicalTrials.gov NCT02415595."
ab600f6b34eb99d7af8be0b9a77e5347f5415a65,
8dc5beca0e3e3ff50b40171573ad5e9d2a154bf9,
1763a3fcde97f491fe742e6da44fd166530e5a99,"BACKGROUND
Effective treatment for hepatitis C virus (HCV) in patients coinfected with human immunodeficiency virus type 1 (HIV-1) remains an unmet medical need.


METHODS
We conducted a multicenter, single-group, open-label study involving patients coinfected with HIV-1 and genotype 1 or 4 HCV receiving an antiretroviral regimen of tenofovir and emtricitabine with efavirenz, rilpivirine, or raltegravir. All patients received ledipasvir, an NS5A inhibitor, and sofosbuvir, a nucleotide polymerase inhibitor, as a single fixed-dose combination for 12 weeks. The primary end point was a sustained virologic response at 12 weeks after the end of therapy.


RESULTS
Of the 335 patients enrolled, 34% were black, 55% had been previously treated for HCV, and 20% had cirrhosis. Overall, 322 patients (96%) had a sustained virologic response at 12 weeks after the end of therapy (95% confidence interval [CI], 93 to 98), including rates of 96% (95% CI, 93 to 98) in patients with HCV genotype 1a, 96% (95% CI, 89 to 99) in those with HCV genotype 1b, and 100% (95% CI, 63 to 100) in those with HCV genotype 4. Rates of sustained virologic response were similar regardless of previous treatment or the presence of cirrhosis. Of the 13 patients who did not have a sustained virologic response, 10 had a relapse after the end of treatment. No patient had confirmed HIV-1 virologic rebound. The most common adverse events were headache (25%), fatigue (21%), and diarrhea (11%). No patient discontinued treatment because of adverse events.


CONCLUSIONS
Ledipasvir and sofosbuvir for 12 weeks provided high rates of sustained virologic response in patients coinfected with HIV-1 and HCV genotype 1 or 4. (Funded by Gilead Sciences; ION-4 ClinicalTrials.gov number, NCT02073656.)."
4dbe46fb7931dfbf580344397cb8677c1994d1ff,
9ea1805e496749072a24972c34646fa6f6811f0d,"Sensitive assays are needed for detection of residual HIV in patients with undetectable plasma viral loads to determine if eradication strategies are effective. The gold standard quantitative viral outgrowth assay (QVOA) underestimates the magnitude of the viral reservoir, while sensitive PCR‐based assays lack the ability to distinguish replication competent from defective virus. We sought to determine whether xenograft of leukocytes from HIV‐1 infected patients with undetectable plasma viral loads into severely immunocompromised mice would result in viral amplification and measurable viral loads within the aberrant murine host."
b34b9e13f43013063a2072106abe87e0138838bd,"BACKGROUND
In light of the increasing rate of dengue infections throughout the world despite vector-control measures, several dengue vaccine candidates are in development.


METHODS
In a phase 3 efficacy trial of a tetravalent dengue vaccine in five Latin American countries where dengue is endemic, we randomly assigned healthy children between the ages of 9 and 16 years in a 2:1 ratio to receive three injections of recombinant, live, attenuated, tetravalent dengue vaccine (CYD-TDV) or placebo at months 0, 6, and 12 under blinded conditions. The children were then followed for 25 months. The primary outcome was vaccine efficacy against symptomatic, virologically confirmed dengue (VCD), regardless of disease severity or serotype, occurring more than 28 days after the third injection.


RESULTS
A total of 20,869 healthy children received either vaccine or placebo. At baseline, 79.4% of an immunogenicity subgroup of 1944 children had seropositive status for one or more dengue serotypes. In the per-protocol population, there were 176 VCD cases (with 11,793 person-years at risk) in the vaccine group and 221 VCD cases (with 5809 person-years at risk) in the control group, for a vaccine efficacy of 60.8% (95% confidence interval [CI], 52.0 to 68.0). In the intention-to-treat population (those who received at least one injection), vaccine efficacy was 64.7% (95% CI, 58.7 to 69.8). Serotype-specific vaccine efficacy was 50.3% for serotype 1, 42.3% for serotype 2, 74.0% for serotype 3, and 77.7% for serotype 4. Among the severe VCD cases, 1 of 12 was in the vaccine group, for an intention-to-treat vaccine efficacy of 95.5%. Vaccine efficacy against hospitalization for dengue was 80.3%. The safety profile for the CYD-TDV vaccine was similar to that for placebo, with no marked difference in rates of adverse events.


CONCLUSIONS
The CYD-TDV dengue vaccine was efficacious against VCD and severe VCD and led to fewer hospitalizations for VCD in five Latin American countries where dengue is endemic. (Funded by Sanofi Pasteur; ClinicalTrials.gov number, NCT01374516.)."
5d90132f995393ffb50eb64a94ce64dce7bcad33,"Doravirine (DOR) is an investigational NNRTI (aka MK‐1439) that retains activity against common NNRTI‐resistant mutants. We have previously reported the Part 1 results from a two‐part, randomized, double‐blind, Phase IIb study in ART‐naïve HIV‐1‐positive patients [ 1 ]. At doses of 25, 50, 100 and 200 mg qd, DOR plus open‐label tenofovir/emtricitabine (TDF/FTC) demonstrated potent antiretroviral activity comparable to EFV 600 mg qhs plus TDF/FTC and was generally well tolerated at week 24. DOR 100 mg was selected for use in patients continuing in Part 1 and those newly enrolled in Part 2."
fd6923ebc384d6b85335dcbb25714fcc49f35ff2,"HIV/AIDS continues to place a devastating toll on individuals, families and communities globally, and western industrialized countries are by no means exempt. Today, there are more than 1 million Americans and 100,000 Britons living with HIV, with a disproportionate burden of new and prevalent HIV infections borne by gay, bisexual and other men who have sex with men (MSM), racial/ethnic minorities, migrants and persons who use drugs. Epidemic concentration in urban areas, especially among: population sub‐groups with high prevalence of risk behaviours; the socio‐economically marginalized; or those with poor access to services, has been well documented. Recent increases in HIV incidence in the rural south US, and in MSM in both countries, reflect the dynamic and evolving nature of these epidemics. New national HIV prevention strategies in both countries have refocused attention on these domestic epidemics, prioritizing HIV testing scaling up, linkage to quality care and tackling long‐standing health inequalities. There are also significant differences between the two countries – in part a reflection of the different health and social care systems; historical approaches to the funding and coordination of HIV prevention; and underlying patterns of health inequalities and their social and structural determinants. In addition, the social–political acceptability of using the sexual health frame to guide more holistic and integrated approaches to HIV prevention efforts remains a key difference. This presentation will compare and contrast HIV prevention responses in the US and UK over the past decade, identifying opportunities for enhancing the prevention response in these and other western industrialized countries in the 21st century."
4b262c8facf97db176c24d024efe4ef84b1b82a7,"TMC278-C204 (NCT00110305), a 96-week trial of the nonnucleoside reverse transcription inhibitor (NNRTI) rilpivirine (RPV, TMC278) in 368 HIV-1-infected, treatment-naive patients, was extended to investigate long-term safety and efficacy. Week 192 analysis results are presented. This was a long-term follow-up of a Phase IIb, randomized trial. No significant RPV dose-response relationships with respect to the primary endpoint (composite ITT-TLOVR algorithm) were observed at week 48 or 96. All RPV-treated patients were switched to open-label 75 mg qd at week 96 and then to 25 mg qd, the Phase III dose, at approximately week 144 as it gave the best benefit-risk balance. All control patients continued receiving open-label efavirenz (EFV) 600 mg qd. At week 192, 59% of RPV- and 61% of EFV-treated patients maintained confirmed viral load <50 copies/ml (ITT-TLOVR algorithm). The mean changes from baseline in CD4 cell count were similar in both groups (RPV: 210 cells/mm(3) vs. EFV: 225 cells/mm(3)). No new safety concerns were noted between week 48 and 192. In the week 192 analysis, RPV compared with EFV was associated with a lower overall incidence of grade 2-4 adverse events (AEs) at least possibly related to treatment, including rash (p<0.001) and neurologic AEs (p<0.05 Fisher's exact test, post hoc analyses) Incidences of serious AEs, grade 3 or 4 AEs, and discontinuations due to AEs were similar across groups. Increases in total cholesterol, LDL-cholesterol, HDL-cholesterol, and triglycerides were significantly lower with RPV than with EFV. RPV continued to show sustained efficacy similar to EFV at week 192 with a generally more favorable safety profile."
f9e6a792b042d6df2f58d6d1c3b398048ba7b968,"Abstract:Raltegravir as initial HIV therapy was examined in a double-blind study; 160 patients were randomized to raltegravir (400 mg bid after dose-ranging), 38 to efavirenz, both with tenofovir/lamivudine. At week 240, HIV-RNA remained <50 copies per milliliter in 68.8% (raltegravir) versus 63.2% (efavirenz), and CD4 increases were 302 versus 276 cells per microliter, respectively. Early HIV-RNA decline predicted later CD4 increases in both groups. Raltegravir resistance was observed in 3 of 10 raltegravir recipients with virologic failure. Few drug-related adverse events were reported after week 48. Raltegravir had minimal effect on laboratory values, including lipids. Raltegravir with tenofovir/lamivudine showed durable efficacy and good tolerability over 5 years."
d87bfc41fa0f885470720f2f869b23cdc39f4c2e,"Objective:TMC278 is a next-generation nonnucleoside reverse transcriptase inhibitor highly active against wild-type and nonnucleoside reverse transcriptase inhibitor-resistant HIV-1 in vitro. The week 96 analysis of TMC278-C204, a large dose-ranging study of TMC278 in treatment-naive HIV-1-infected patients, is presented. Design:Phase IIb randomized trial. Methods:Three hundred sixty-eight patients were randomized and treated with three blinded once-daily TMC278 doses 25, 75 or 150 mg, or an open-label, active control, efavirenz 600 mg once daily, all with two nucleoside reverse transcriptase inhibitors. The primary analysis was at week 48. Results:No TMC278 dose–response relationship for efficacy and safety was observed. TMC278 demonstrated potent antiviral efficacy comparable with efavirenz over 48 weeks that was sustained to week 96 (76.9–80.0% and 71.4–76.3% of TMC278-treated patients with confirmed viral load <50 copies/ml, respectively; time-to-loss of virological-response algorithm). Median increases from baseline in CD4 cell count with TMC278 at week 96 (138.0–149.0 cells/μl) were higher than at week 48 (108.0–123.0 cells/μl). All TMC278 doses were well tolerated. The incidences of the most commonly reported grade 2–4 adverse events at least possibly related to study medication, including nausea, dizziness, abnormal dreams/nightmare, dyspepsia, asthenia, rash, somnolence and vertigo, were low and lower with TMC278 than with efavirenz. Incidences of serious adverse events, grade 3 or 4 adverse events and discontinuations due to adverse events were similar among groups. Conclusion:All TMC278 doses demonstrated potent and sustained efficacy comparable with efavirenz in treatment-naive patients over 96 weeks. TMC278 was well tolerated with lower incidences of neurological and psychiatric adverse events, rash and lower lipid elevations than those with efavirenz. TMC278 25 mg once daily was selected for further clinical development."
461d8b6419dff96157354f456b4ea0d4014af582,"Objectives:The purpose of this study was to evaluate the safety and efficacy of raltegravir vs efavirenz-based antiretroviral therapy after 96 weeks in treatment-naive patients with HIV-1 infection. Methods:Multicenter, double-blind, randomized study of raltegravir (100, 200, 400, or 600 mg twice a day) vs efavirenz (600 mg every day), both with tenofovir/lamivudine (TDF/3TC), for 48 weeks, after which raltegravir arms were combined and all dosed at 400 mg twice a day. Eligible patients had HIV-1 RNA ≥5000 copies per milliliter and CD4+ T cells ≥100 cells per microliter. Results:One hundred ninety-eight patients were randomized and treated; 160 received raltegravir and 38 received efavirenz. At week 96, 84% of patients in both groups achieved HIV-1 RNA <400 copies per milliliter; 83% in the raltegravir group and 84% in the efavirenz group achieved <50 copies per milliliter (noncompleter = failure). Both groups showed similar increases in CD4+ T cells (221 vs 232 cells/uL, respectively). An additional 2 patients (1 in each group) met the protocol definition of virologic failure between weeks 48 and 96; no known resistance mutations were observed in the raltegravir recipient; the efavirenz recipient had nucleoside reverse transcriptase inhibitor and nonnucleoside reverse transcriptase inhibitor resistance mutations. Investigator reported drug-related clinical adverse events (AEs) were less frequent with raltegravir (51%) than efavirenz (74%). Drug-related AEs occurring in >10% of patients in either group were nausea in both groups and dizziness and headache in the efavirenz group. Laboratory AEs remained infrequent. Raltegravir had no adverse effect on total or low-density lipoprotein cholesterol or on triglycerides. Neuropsychiatric AEs remained less frequent with raltegravir (34%) than efavirenz (58%). There were no drug-related serious AEs in patients receiving raltegravir. Conclusions:In antiretroviral therapy-naive patients, raltegravir with TDF/3TC had potent antiretroviral activity, which was similar to efavirenz/TDF/3TC and was sustained to week 96. Raltegravir was generally well tolerated; drug-related AEs were less frequent in patients treated with raltegravir compared with efavirenz."
8c4d8b16ee841cd9d8a9e4881427473a2610fa71,
f3a12dfe440ffcc103fb1f37cfaa46e79b6df942,"Objective:To evaluate a simplification strategy for HIV-1-infected patients virologically suppressed on antiretroviral therapy (ART) by switching to a single-tablet regimen consisting of efavirenz/emtricitabine/tenofovir disoproxil fumarate (EFV/FTC/TDF). Design:Prospective, randomized, controlled, open-label, multicenter study. Methods:Patients on stable ART with HIV-1 RNA <200 copies per milliliter for ≥3 months were stratified by prior nonnucleoside reverse transcriptase inhibitor-based or protease inhibitor-based therapy and randomized (2:1) to simplify treatment to EFV/FTC/TDF or to stay on their baseline regimen (SBR). Efficacy and safety assessments were performed at baseline and at weeks 4, 12, 24, 36, and 48. Additional patient-reported outcomes included the following: adherence by visual analog scale, quality of life by SF-36 (v2) survey, HIV Symptom Index, and the Preference of Medication and Perceived Ease of the Regimen for Condition questionnaires. Results:Three hundred patients (EFV/FTC/TDF 203, SBR 97) were evaluated (prior protease inhibitor-based ART, 53%; nonnucleoside reverse transcriptase inhibitor-based ART, 47%). The arms were well balanced at baseline with 88% males, 29% blacks, and a mean age of 43 years; CD4 was 540 cells per cubic millimeter, 96% had HIV-1 RNA <50 copies per milliliter, and 88% were on their first ART regimen. Through 48 weeks, 89% vs. 88% in the EFV/FTC/TDF vs. SBR arms, respectively, maintained HIV-1 RNA <200 copies per milliliter by time to loss of virologic response algorithm (intent to treat, noncompleters = failures) with the difference (95% confidence interval) between arms of 1.1% (−6.7% to 8.8%), indicating noninferiority of EFV/FTC/TDF vs. SBR. Similarly, maintenance of HIV-1 RNA <50 copies per milliliter by time to loss of virologic response algorithm was 87% vs. 85% for EFV/FTC/TDF vs. SBR, respectively [difference (95% confidence interval) 2.6% (−5.9% to 11.1%)]. Discontinuation rates were similar (EFV/FTC/TDF 11%, SBR 12%); more discontinuations for adverse events occurred in the EFV/FTC/TDF arm vs. SBR (5% vs. 1%), most commonly for nervous system symptoms. More patients withdrew consent in the SBR arm vs. EFV/FTC/TDF (7% vs. 2%). Estimated glomerular filtration rate (by Modification of Diet in Renal Disease) remained unchanged over 48 weeks in both arms (median change <1 mL·min−1·1.73 m−2). A decrease in fasting triglycerides was observed at 48 weeks in the EFV/FTC/TDF vs. SBR arm (−20 vs. −3.0 mg/dL; P = 0.035). Adherence of ≥96% was reported by visual analog scale in both arms at baseline and at all study visits. Conclusion:Simplification to EFV/FTC/TDF maintained high and comparable rates of virologic suppression vs. SBR through 48 weeks."
14f4b51dbf0da24dfea57914052cd15af74facb4,"Background:Raltegravir is an HIV-1 integrase strand-transfer inhibitor with potent in vitro activity. This study explored the antiretroviral activity and safety of raltegravir in treatment-naive patients with plasma HIV-1 RNA levels ≥5000 copies/mL and CD4+ T-cell counts ≥100 cells/mm3. Methods:Multicenter, double-blind, randomized, controlled study of raltegravir at doses of 100, 200, 400, and 600 mg twice daily versus efavirenz at a dose of 600 mg/d, all in combination with tenofovir at a dose of 300 mg/d and lamivudine at a dose of 300 mg/d (clinicaltrials.gov identifier: NCT00100048). Results:In the 198 patients treated (160 on raltegravir and 38 on efavirenz), the mean HIV-1 RNA level ranged from 4.6 to 4.8 log10 copies/mL at baseline. At weeks 2, 4, and 8, the proportion of patients achieving an HIV-1 RNA level <50 copies/mL was greater in each of the raltegravir treatment groups than in the efavirenz group. By week 24, all treatment groups appeared similar, with plasma HIV-1 RNA levels <400 copies/mL in 85% to 98% of patients and <50 copies/mL in 85% to 95% of patients. These reductions were maintained through week 48 in 85% to 98% of patients and in 83% to 88% of patients, respectively. Five (3%) patients on raltegravir and 1 (3%) on efavirenz experienced virologic failure before week 48. Drug-related clinical adverse events were less common with raltegravir than with efavirenz. After 24 and 48 weeks of treatment, raltegravir did not result in increased serum levels of total cholesterol, low-density lipoprotein cholesterol, or triglycerides. Conclusions:Raltegravir at all doses studied was generally well tolerated in combination with tenofovir and lamivudine. Raltegravir exhibited potent and durable antiretroviral activity similar to that of efavirenz at 24 and 48 weeks but achieved HIV-1 RNA levels below detection at a more rapid rate."
cd43f656ba5206d177acd88508e02c4f24e7791b,"Background:MK-0518 is a novel HIV-1 integrase strand transfer inhibitor with potent in vitro activity against HIV-1 (95% inhibitory concentration [IC95] = 33 nM in 50% human serum) and good bioavailability in uninfected subjects. This study explored the antiretroviral activity and safety of MK-0518 versus placebo for 10 days as monotherapy in antiretroviral therapy-naive HIV-1-infected patients with plasma HIV-1 RNA levels of at least 5000 copies/mL and CD4+ T-cell counts of at least 100 cells/mm3. Methods:This was a multicenter, double-blind, randomized, placebo-controlled 2-part study, with the first part using MK-0518 in 1 of 4 doses (100, 200, 400, and 600 mg) versus placebo (randomized 1:1:1:1:1) given twice daily for 10 days of monotherapy. Patients were monitored for safety, pharmacokinetic parameters, and antiretroviral effect. Results:Thirty-five patients were enrolled (6-8 patients per treatment group) and completed 10 days of therapy; the mean baseline log10 HIV RNA level ranged from 4.5 to 5.0 copies/mL in each group. On day 10, the mean decrease from baseline in the log10 HIV RNA level was −0.2 copies/mL for the placebo group and −1.9, −2.0, −1.7 and −2.2 log10 copies/mL for the MK-0518 100-, 200-, 400-, and 600-mg treatment groups, respectively. All dose groups had superior antiretroviral activity compared with placebo (P < 0.001 for comparison of each dose with placebo). At least 50% of patients in each MK-0518 dose group achieved an HIV RNA level <400 copies/mL by day 10. Mean trough MK-0518 concentrations at each dose exceeded the IC95 of 33 nM. Study therapy was generally well tolerated. The most common adverse experiences were headache and dizziness; these were similar between active and control groups. There were no discontinuations because of adverse experiences and no serious adverse experiences. Conclusions:MK-0518 showed potent antiretroviral activity as short-term monotherapy and was generally well tolerated at all doses. Based on these results, part 2 of the study, a dose-ranging 48-week trial of MK-0518 versus efavirenz in a combination regimen, has been initiated."
9d93aff66775a7a1fb01c532fedb9befbf363728,"BACKGROUND
Efavirenz is a nonnucleoside reverse-transcriptase inhibitor of human immunodeficiency virus type 1 (HIV-1). We compared two regimens containing efavirenz, one with a protease inhibitor and the other with two nucleoside reverse-transcriptase inhibitors, with a standard three-drug regimen.


METHODS
The study subjects were 450 patients who had not previously been treated with lamivudine or any nonnucleoside reverse-transcriptase inhibitor or protease inhibitor. In this open-label study, patients were randomly assigned to one of three regimens: efavirenz (600 mg daily) plus zidovudine (300 mg twice daily) and lamivudine (150 mg twice daily); the protease inhibitor indinavir (800 mg every eight hours) plus zidovudine and lamivudine; or efavirenz plus indinavir (1000 mg every eight hours).


RESULTS
Suppression of plasma HIV-1 RNA to undetectable levels was achieved in more patients in the group given efavirenz plus nucleoside reverse-transcriptase inhibitors than in the group given indinavir plus nucleoside reverse-transcriptase inhibitors (70 percent vs. 48 percent, P<0.001). The efficacy of the regimen of efavirenz plus indinavir was similar (53 percent) to that of the regimen of indinavir, zidovudine, and lamivudine. CD4 cell counts increased significantly with all combinations (range of increases, 180 to 201 cells per cubic millimeter). More patients discontinued treatment because of adverse events in the group given indinavir and two nucleoside reverse-transcriptase inhibitors than in the group given efavirenz and two nucleoside reverse-transcriptase inhibitors (43 percent vs. 27 percent, P=0.005).


CONCLUSIONS
As antiretroviral therapy in HIV-1-infected adults, the combination of efavirenz, zidovudine, and lamivudine has greater antiviral activity and is better tolerated than the combination of indinavir, zidovudine, and lamivudine."
200f2e1b258f4e8139e4642aca891f7c084ae5d4,"Quasi-isotropic antennas are promising candidates due to their applications in modern communication systems, where full spatial coverage and/or uniform signal reception is required. In this work, an in-depth review of quasi-isotropic antennas is presented, with the aim of understanding the working principles of such antennas and presenting the recent advancements, challenges, and solutions offered by various researchers. First, different design techniques adopted to achieve quasi-isotropic patterns, such as the use of complementary dipoles, multiple monopoles or dipoles, and an array of discrete elements are discussed. Then, different types of quasi-isotropic antennas—for example, planar, electrically small, 3-D printed, dual-band/wideband, circularly polarized, metamaterial-inspired, and dielectric resonator-based quasi-isotropic antennas—are revisited. Their applications in various technologies, such as RFID, energy harvesting, wireless sensor networks, and the IoT, are briefly explained. Lastly, different key performance parameters, such as complexity of configuration, design approach, physical profile, far field and radiation characteristics, reflection coefficients, operating frequency and bandwidth, gain deviation, and fabrication process, are discussed and tabulated. This review not only provides a guideline but will also help antenna engineers in designing a quasi-isotropic antenna with desirable performance."
6b6616db65b4d784ce755afaf87b79bcb38363ed,"In this paper, a bio-inspired origami quasi-Yagi helical antenna having beam direction and beamwidth switching capability is presented. The structure of the antenna is based on the transformable origami DNA which is inspired from DNA of living cell. The length of a living cell DNA is few microns in the folded state and can be transformed to 2 m by unfolding. In this study, three origami DNA geometries named as DNA #1, DNA #2 and DNA #3, are applied to design quasi-Yagi helical antenna. DNA #2 serves as a driven element and DNA #1 and DNA #3 serve as parasitic elements. The beam direction and beamwidth of the antenna can be controlled by folding and unfolding the parasitic elements. The presented antenna provides four beam steering states having first three states with narrow beamwidth and fourth state with wide beamwidth at the fixed resonant frequency of 1.9 GHz. For example, the beam direction of the antenna for the states 1, 2, 3, and 4 can be steered to −30°, 0°, +30°, and −40° respectively. The presented antenna provides wider 3 dB beamwidth of 104° in the fourth state, while beamwidth of the first three states are narrower than 64°."
c03965b24bdb1ea15c16c448ea0952f0e48342e4,"In this paper, a novel origami helical antenna has been presented based on DNA structure. A DNA consists of double helical chain, whose length can be varied from a few microns up to 2 meter during unfolding. These interesting features are utilized in our proposed antenna design to make it reconfigurable, with respect to the mode as well as the frequency. The operating mode can be switched between normal and axial, whereas the resonance can be shifted from 360 MHz to 2.5 GHz, for different values of spacing between the turns of the antenna. The proposed design was also fabricated by using a copper tape on DNA shaped folded PET substrate, and the sample was measured. Flexible design procedure, efficient folding, compact packaging, easy deployment, as well as low cost have confirmed the potential use of the proposed origami helical antenna for the CubeSat and other space technologies."
eb212a25bd8bdfe8b0e85a3522e37c5f916d982e,"In this paper, a reconfigurable frequency selective surface has been presented for bistate (transmittance/reflectance) operations. The proposed structure is designed based on microfluidic technology, where liquid metal alloy (eutectic gallium indium, EGaIn) is encased within a flexible dielectric substrate. The novelty of the design lies in its switching characteristic from dual-polarized allpass to polarization-selective bandpass/ bandstop behavior, based on whether EGaIn is injected in the microchannels. The structure, being asymmetric in nature, exhibits bandpass behavior under vertical polarization, whereas it gives rise to bandstop response during orthogonal polarization. In addition, the design performs well under various angles of incidence for both the polarizations. The performances of the proposed structure is also validated by the measured responses of the fabricated prototype."
047b05d54c4804fc6bab3a9615cb8901faec78e9,"1. W.C. Chew, Waves and fields in inhomogeneous media, WileyIEEE Press, New York, 1999. 2. J.M. Jin, Theory and computation of electromagnetic fields, WileyIEEE Press, Hoboken, 2010. 3. W.C. Chew, M.S. Tong, and B. Hu, Integral equation methods for electromagnetic and elastic waves, Morgan & Claypool, San Rafael, 2008. 4. F.P. Andriulli, A. Tabacco, and G. Vecchi, Solving the EFIE at low frequencies with a conditioning that grows only logarithmically with the number of unknowns, IEEE Trans Antennas Propog 58 (2010), 1614– 1624. 5. M.K. Li and W.C. Chew, Wave-field interaction with complex structures using equivalence principle algorithm, IEEE Trans Antennas Propag 55 (2007), 130–138. 6. Z. Chen and M.M. Ney, Method of moments: A general framework for frequencyand time-domain numerical methods, In: Workshop on Computational Electromagnetics in Time-Domain, CEM-TD, Perugia, 2007, pp. 1–4 7. S.M. Rao, D.R. Wilton, and A.W. Glisson, Electromagnetic scattering by surface of arbitrary shape, IEEE Trans Antennas Propag 30 (1982), 409–418. 8. M.S. Tong, Efficient electromagnetic analysis for interconnect and packaging structures using dual basis function in the method of moments, IEEE Trans Compon Packaging Manuf Technol 1 (2011), 1089–1097. 9. W.T. Sheng, Z.Y. Zhu, K. Yang, and M.S. Tong, Efficient evaluation of weakly singular integral arising from electromagnetic surface integral equations, IEEE Trans Antennas Propog 61 (2013), 3377–3381."
c5dcb57db8b122466284273d4cae2bab2ebb859c,"In this paper, a frequency switchable antenna is proposed by using an origami magic cube. The proposed antenna consists of a microstrip-fed monopole designed on an origami magic cube. The proposed origami magic cube antenna can be folded and unfolded, switching its operating frequency in consequence. The operating frequency of the antenna in the unfolded state is 950 MHz, while in the folded state its operating frequency is 1.57 GHz. The origami magic cube is composed of paper. The antenna's performance is theoretically and experimentally demonstrated by measuring the S-parameters and radiation patterns. The measured 10 dB impedance bandwidth of the proposed origami antenna is 15% (850–1000 MHz) and 20% (1.3–1.6 GHz) for the unfolded and folded states respectively. The measured peak gain of the antenna is 0.51 dBi and 1.2 dBi at the unfolded state and the folded state, respectively."
47a1ca2063e87af84d183dd1dd99a8411b344024,"In this article, a novel hybrid printing technology that realizes a microstrip patch antenna is introduced. The proposed hybrid printing technology uses both two‐ and three‐dimensional (2D and 3D) printing technologies. The antenna's dielectric materials are built using a 3D printer. The metallic pattern is printed using a commercial inkjet printer. Polylactic acid (PLA) is used for the filament of the 3D printer and silver nanoparticle ink is printed on paper using the inkjet printer. For a highly efficient antenna design with lossy PLA material, the mesh PLA is printed and its electromagnetic parameters are characterized. Finally, the printed patch antenna shows a return loss of 24 dB and a peak gain of 7 dBi at 1.9 GHz. © 2016 Wiley Periodicals, Inc. Microwave Opt Technol Lett 58:2602–2606, 2016"
4b47deca59efb396117573ea2c3d9c3a43664a6e,"In this paper a novel design for miniaturization of patch antenna is presented. The proposed design for the miniaturized wide band patch antenna consists of hybrid techniques of H-shaped slot on the patch and defected ground structure. Defected ground structure is formed by cutting a combination of U and L-shaped slots. Patch and the ground plane are shorted for the further size reduction. These practices increase the current distribution path accompanied by increase in electrical length of the antenna. To validate the design a reference patch antenna is designed and its performance is compared with the proposed antenna. The designed antenna is 86% smaller than the conventional patch antenna. The miniaturized antenna has gain in the range of 4dBi to 5dBi while impedance bandwidth is in the range of 105MHz to 1700MHz. This form of compact wide band antenna is appropriate for numerous wireless communication systems like cell phone, Laptop and Hyperlane."
64738940e0cd85f9d1d69c4a5949aa1fa76cd33c,"A novel design for miniaturization of microstrip patch antenna is presented for electronic multifunctional wireless communication systems. The proposed design consists of four horizontal slots of various lengths on the patch. Combination of U and L-Shaped slots is designed on the ground plane. Patch and the ground plane are shorted through a shorting pin. These practices increase electrical length of the antenna and reduce size of antenna up to 86%. The proposed antenna is operating on multiple bands and able to cover numerous applications. The reflection co-efficient is <; -10dB for all the desired bands. The proposed miniaturized antenna has acceptable gain for all the bands which is in the range of 3.5dBi to 6.6dBi. The impedance bandwidth of the miniaturized antenna is in the range of 50MHz to 650MHz. This type of reduced size antenna is very appropriate for use in compact devices like Cell Phone, WLAN and Laptop."
461fe5cf3890f7ec9cf847f53221c3630c84c9e2,"not consider it necessary to offer a defense. After all, any challenge to Hume’s endorsement of (good) lively reason would itself be an instance of lively reason. And while one has no option but to engage in lively reason to get by, it is a choice whether to employ lively reason to level a skeptical argument against lively reason. Finally, the question of whether (good) lively reason tracks truth is complicated by the fact that Hume does not offer a theory of truth in book 1. This account comes later in book 2, and nothing he says there suggests a tension between assenting to lively reason and following the truth. So much remains in Qu’s book worth of discussing at length. The book is a mine of literature on and superb analysis of central questions in Hume, and students as well as scholars will find it deeply engaging and rewarding. M i r e n B o e h m University of Wisconsin-Milwaukee"
4c52c16acd85d36e06718bd54f36a78194fe1172,"Diaspora Jewish solidarity with Palestine has broken out of the man-bites-dog category of quirky story, and into being a significant element in both global Palestine solidarity as well as among US Jews. Israel’s repetitive assaults on Gaza, the Trump presidency, the effect of BLM in conscientizing younger Jews, and the latest chapter of Palestinian struggle in May 2021 have all been contributing factors. The trend is reflected in growing US Jewish distance from Israel: a recent Pew Poll showed that only 50% of 18–49 year-olds feel attachment to Israel compared to 61% in 2013 (overall it was 57% in 2020 compared to 69% in 2013) (Pew Forum 2021). This poll created no ripples of surprise; Israel has been a cause of contention and dissension among US Jews for decades now. It is the reason that Ron Dermer, the former Israeli Ambassador to the US, recently advised that Israel should concentrate its hasbara efforts on Christian evangelicals rather than diaspora Jews, who are “disproportionately among our critics” (Magid 2021). Thus, a book which promises in-depth examination of these critics is very timely, although it only examines one aspect of their movement. Its main argument is that “Jewish Palestine solidarity activists and other critics of the occupation and Zionism constitute a social movement operating to transform the meaning of Jewishness” (Omer 2019: 9). As such, the book is more interested in how these activists relate to Jewishness and Judaism, than in Palestine solidarity, which is largely sidelined. This is a pity, as the author has interviewed seventy Jewish and thirty non-Jewish activists, and thus has the material to discuss the movement in round. The opening chapters begin well by outlining the terrain, or at least the Jewish part of it, that this movement operates in – revealing how Jewish communal institutions channel younger Jews towards Zionism, and how they silence dissent through deploying a mixture of Islamophobia, accusations of antisemitism, and lawfare. There is also an excellent exploration of how Jews come to Palestinian solidarity, and here the book adds to previous literature which discusses the moral shocks experienced when encountering Israel, the cognitive dissonance between their liberal values and Zionism, and the importance of prior politicization and encounters with Palestinians (Abarbanal 2012; IJV 2008). Of particular interest"
38b8e1ff82c68663bf447dda3fb9d3237433bd9b,"Shepherd’s argument against Hume’s thesis that an object can begin its existence uncaused has received short shrift in the secondary literature. I argue that the key to understanding that argument’s success is understanding its dialectical context. Shepherd sees the dialectical situation as follows. Hume presents an argument against Locke and Clarke the conclusion of which is that an object can come into existence uncaused. An essential premise of that argument is Hume’s theory of mental representation. Hume’s theory of mental representation, however, is itself implausible and unsupported. Therefore, one need not accept this premise or this conclusion. Thus, Shepherd proceeds to her discussion of the relation of cause and effect free to help herself to the thesis that every beginning of existence must have a cause. Additionally, she elsewhere pays down the debt she incurs in that argument by presenting her own alternative theory of mental representation, which is both plausible in its own right, and can account for the error that she takes Hume to make."
3df4d94bb7f7f868f6ee6b3e2912aa3a11e17647,
67850d073c948d105266591d1b3cded089d18b4f,
6c72d127ad75806bf6f6d8c4e6d8af0e7558aa66,"Abstract Scholars working on Kant’s Anticipations of Perception generally attribute to him an argument that invalidly infers that objects have degrees of intensive magnitude from the premise that sensations do. I argue that this rests on an incorrect disambiguation of Kant’s use of Empfindung (sensation) as referring to the mental states that are our sensings, rather than the objects that are thereby sensed. Kant’s real argument runs as follows. The difference between a representation of an empty region of space and/or time and a representation of that same region as occupied by an object entails that, in addition to their extensive magnitude, objects must be represented as having a matter variable in intensive magnitude. Since it is the presence of sensation (sensing) in a cognition that marks the difference between representing only the extensive magnitude of the object and the object as a whole, it is sensation that represents its intensive magnitude."
711d12dde992e0add3e5825e3077186260041441,
a8d44d642ec27d1fac2a86a9a3d8c00dda8e6cf7,"Lady Mary Shepherd holds that the relation of cause and effect consists of the combination of two objects (the causes) to create a third object (the effect). She also holds that this account implies that causes are synchronous with their effects. There is a single instant in which the objects that are causes combine to create the object which is their effect. Hume argues that cause and effect cannot be synchronous because if they were then the entire chain of successive causes and effects would all collapse into a single moment, and succession would not be possible. I argue that Shepherd has a ready, although implicit response, to Hume’s argument. Since causation is combination on Shepherd’s view, she is free to hold that there are times in between those instants in which combinations occur, during which times other, non-combinatory changes (such as changes in the location of objects) occur, which changes account for succession."
0f5fdfd07f4f51bcf2599fada62bbe35d2f9417b,"Abstract This article investigates how development education campaigns affect schoolchildren’s understanding of both global and domestic poverty, through examining the 2012 Lenten campaign by the Irish agency, Trócaire. It analyses Trócaire campaign material as well as its reception by schoolchildren, aged 10–12. We found the material contained neo-colonial stereotypes about Africa as a primitive place dependent on Western aid, displaying continuity with the prior ‘Black Baby Phenomenon’. This refers to the collection boxes Irish children were given with a picture of a black baby on them, which fostered an understanding of Africans as helpless childlike victims, Children in our focus groups understood the Africans portrayed by the Trócaire campaign as hopeless, primitive, intellectually inferior Others. Furthermore, this there was strong indication that this imagery fostered social distance towards ethnic minorities in Ireland and encouraged children to minimise domestic poverty, through considering this to be only an issue in ‘developing countries’."
863368939da9ac165a1fe8febb404f2e81a30889,"Scholars have rejected Wilfrid Sellars’s argument for an ontology of absolute processes on the grounds that it relies on a dubious and dogmatic appeal to the homogeneity of color. Borrowing from Rosenthal’s recent defense, but ultimate rejection of homogeneity, I defend this claim on Sellarsian/Kantian transcendental grounds, and reconstruct the remainder of his argument. I argue that Sellars has good reason to suppose that homogeneity is a necessary condition of any possible experience, including indirect experience of theoretical-explanatory posits, and therefore good reason to hold that Reductive Materialism, as he conceives it, is an untenable account of color. The remainder of his argument aims to answer the question of what the metaphysical relation is between the state of an experiencing subject that we take color to be and the colorless microphysical particles that we take to constitute that subject. After rejecting Substance Dualism, Epiphenomenalism, and Wholistic or Emergent Materialism as explanatorily inadequate, Sellars proposes that both color-states and micro-physical particles should be understood as manifestations of an underlying ontology on absolute processes."
b8b30b20bce8ffdba8a20a03f8429f8696287e1b,"Pessoptimism is the best approach. During the recession years the Irish left moved from a position of near complete irrelevance into being a powerful force in Irish politics.1 Huge victories were won on the economic and social front and electoral gains were made both locally and nationally. But now the highwater of these years of militancy is over, the question is how substantial these gains were and whether the left-wing parties simply occupied the comfortable subaltern place in Irish politics temporarily vacated by the collapse of the old centre-left."
2e2b9c003459a94503278e40ba5489c508fdaf62,"The fiscal crisis of 2008 led to severe recession and hardship in Ireland, yet there was relatively little civic unrest and public protest until the autumn of 2014 when, paradoxically, economic conditions had improved significantly. Sociologists often explain such patterns by invoking a social mechanism based on perceived ‘relative deprivation’ among a population sub-group. We show that these processes cannot explain the temporal pattern of protest in Ireland and argue instead that events should be understood through the interaction of two different processes: first, the development of an ‘incidental’ grievance which framed popular discontent about the ‘structural’ grievances brought about by the wider fiscal crisis and recession. Second, the early absence of, and later emergence of coordinated political opposition with effective ‘strategies of contention’. We use a mixed methods approach, drawing on seven waves of the European Social Survey combined with qualitative interviews."
6e060b83905fb753dca5147fc42b166df73acafd,
9890c7fbf07abc3cf70dac73644054365179563c,"In a recent paper, Karl Schafer argues that Hume's theory of mental representation has two distinct components, unified by their shared feature of having accuracy conditions. As Schafer sees it, simple and complex ideas represent the intrinsic imagistic features of their objects whereas abstract ideas represent the relations or structures in which multiple objects stand. This distinction, however, is untenable for at least two related reasons. Firstly, complex ideas represent the relations or structures in which the impressions that are the objects of their simple components stand. Secondly, abstract ideas are themselves instances of complex ideas. I draw two important conclusions from these facts. Firstly, contra Schafer and Garrett (to whom Schafer responds), the Copy Principle, properly emended, constitutes the entirety of Hume's theory of mental representation. Secondly, whereas paradigm examples of complex ideas, e.g. ideas of spatial and temporal complexes, are structured by relations of contiguity, abstract ideas are those complex ideas instead structured by relations of resemblance. As such, they represent their objects not as spatially or temporally contiguous but rather as resembling."
0f3709637f13de64946f0cbff193b80ef5a1523f,
135aaea7ffa3bd00e2c7d60368fb3ed88dff13f9,
1a737b2ba614a3bdb5e6642f79127ae96b7d14bc,
278e22021047b9cf8d1660adc5cde6027588cc6a,"read before the Society, 20 April 2017; Symposium 2016-2017: Globalisation, Inequality and the Rise of Populism"
5cd52c1564784b28db200b89fb1fcdfc66e7e84b,
6020f14d32c5815669cd5c040203002f8ad06399,
c13066800e9eba780d0598a66bfc53750e069422,"ABSTRACT The ‘City of David’ in Silwan is on the original site of Jerusalem. Located in Israeli-occupied East Jerusalem, it is both an illegal Israeli settlement in a Palestinian neighbourhood and a popular international tourist destination. This article examines how the site is narrated by tour operators and tourists through fieldwork, interviews and analysis of tourist comments on the TripAdvisor site. It argues that Israeli settlers have successfully harnessed tourist discourse in order to present their vision of a Jewish Jerusalem in which Palestinian existence is ignored or treated as a threat. The site allows tourists to connect to and experience a mythical biblical past, something which answers to tourist desires to have an authentic encounter with the destination culture. In the site’s narratives, the presence of Palestinians in the area is elided over through spatial and linguistic separation and by denying their legitimate presence. This indicates how the congruence between Zionist and tourist discourses discursively legitimises Israeli colonisation of Silwan."
cb3f5ca56ec8c9a6a47355b1f6c347516dfd3f3b,
e253cebb5cbd2d69da654fa58be508f16800b20e,
fc812713f63117ae2dc9e0858a688cdf76fad3bc,
7963b2807e963f2bea64c5aa3dfbc7620194e558,"According to Hume’s theory of general representation, we represent generalities by associating certain ideas with certain words. On one understanding of this theory, calling things by a name does not represent any real qualities of those things or any real relations between them. This interpretation runs into difﬁculty when applied to Hume’s own use of such general terms throughout the Treatise. Because these distinctions do theoretical-explanatory work in Hume’s philosophical system, they require that the items so distinguished really are different. This reveals that Hume employs a more sophisticated understanding of the science of human nature than has previously been understood. While Hume is a thoroughgoing nominalist about terms in the language of the vulgar, he is a realist about the theoretical-explanatory terms of science."
b99f5510f77051b925488475f484ba3792387df8,"In Kant, Science, and Human Nature, Robert Hanna argues against a version of scientific realism founded on the Kripke/Putnam theory of reference, and defends a Kant-inspired manifest realism in its place. I reject Kriple/Putnam for different reasons than Hanna does, and argue that what should replace it is not manifest realism, but Kant's own scientific realism, which rests on a radically different theory of reference. Kant holds that we picture manifest objects by uniting manifolds of sensation using concepts-qua-inferential-rules. When these rules are demonstrated to be invalid, we replace the picture of the macroscopic world with a picture of the microscopic entities of theoretical science that unites the very same manifolds using different rules of inference. Thus, we refer to ""unobservable"" theoretical entities in the same way that we do manifest ones: by specifying both their determinate location in space and time and the concepts by which they are understood."
fb2e6f693bfca5feb3f3c0632619dca932074d06,"The downside of this interpretation is simply that it is difficult to reconcile with some of the textual evidence provided in the early phases of the transition project. Perhaps scholars should simply view the Opus postumum as a work in progress and conclude that Kant often changed his views on the nature and purpose of the project. Hall agrees that the Opus postumum is unfinished, but nevertheless attempts to provide one unified interpretation of the work. My remark shows, not surprisingly, that it is difficult to achieve consensus on the goal or purpose of Kant’s transition project. This is not in any way meant to detract from the quality of Hall’s work. The Post-Critical Kant provides a significant advance in the study of Kant’s final drafts and all future scholars working on the Opus postumum will have to take account of Hall’s book."
0cfbe170943d8741e5621b68e0e306f8d66b45e2,"This article examines the dynamics through which the migrant field in Ireland is governed, and, through analysing NGO contention over the Immigration, Residence and Protection (IRP) Bill, explores the role they play in the field. The article argues that NGOs both influence and are institutionally channelled by state discourse surrounding migrants. The IRP Bill was designed to regulate non-EU citizens in Ireland, and codified the discretionary powers of the Irish state over migrants. While NGOs offered an influential critique of this process, in doing so they reproduced the dominant representation of migrants in Ireland as problems to be separated out and managed by elite experts. In explaining why this was so, I draw on Bourdieu's argument that those within a field of practice articulate their identities and goals through the dominant language of that field."
5f1cfc7089230e731561fe154bf2dff6d3eb6d0a,Introduction 1. Hume's Theory of Mental Representation 2. Two Objections to Hume's Theory of Mental Representation 3. The A-Deduction and the Nature of Intuitions 4. The Object of Representation 5. Self and World in the Analogies of Experience 6. The Inferential Self Postscript of Transcendental Idealism
d98b740f26cfcbf7272264d083f3e2b87ca2518c,"This article applies field theory in order to build an understanding of aspects of social movements practice. It argues that the way social movements are positioned within their various fields of practice and the way these fields inter-relate with each other can help explain how movements arrive at their strategies and ideologies. The relationship between the fields within which social movements operate also provides a means to explain how movement participants can become agents for change. The article discusses the case of British Jewish Israel-critical groups, an example of a movement suspended between several different fields of practice – both local and distant. The internal movement debate around boycotting Israel illustrates how movement activities are channelled by the local fields within which they contend. Their relationship with the distant Palestinian field demonstrates the importance of the influence of external fields in forming social movement ideology. This model views social movement actors – especially those within distant issue movements – as translators between various fields of practice. This provides a mechanism to explain how challengers within a field can overcome the limitations of internal field habitus and become agents for field transformation."
e493219225b0d5a2efb9554d0e636b0f6df230f8,"De Pierris has argued that Hume is what she calls an inductivist about the proper method of scientific inquiry: science proceeds by formulating inductively-established empirical generalizations that subsume an increasing number of observable phenomena in their scope. De Pierris thus limits Hume's understanding of scientific inquiry, including his own science of human nature, to observable phenomena. By contrast, I argue that Hume's conception of science allows for the positing of, and belief in, unobservable theoretical entities on purely explanatory grounds. I present the details of De Pierris's interpretation of Hume, and the reasons and means for rejecting it. I then consider Hume's explicit statements on his science of human nature to show that all of these are compatible with Hume's accepting a more expansive understanding of scientific explanation. Finally, I briefly consider some examples from the Treatise of Hume's employing just such a methodology."
9a9c8f6b984d3486f2da2ce6982123fefa942ca6,
c324575ad02c32b43642e84f2663183d9af410b6,"On 6 December 2013, the inaugural conference of the International Solidarity Research Network (ISRN) was held in Trinity College Dublin. The event, which was organised by ISRN committee members David Landy (Trinity College Dublin), Hilary Darcy (Maynooth University) and José Gutiérrez (University College Dublin) and sponsored by the Department of Sociology (Trinity College Dublin) in association with the Institute for International Integration Studies, addressed themes and questions arising out of the praxis of international solidarity movements. The practices, problems and possibilities of international solidarity were explored from both an academic and an activist perspective resulting in lively debate and discussion. While there is an established school of research on the development and aid sector, solidarity movements remain under-researched. This is despite the growing political significance of these movements in an age of globalisation and growing transnationalism of social movement activity. A key feature of international solidarity activism has been changes in the focus of its practitioners from national liberation and third worldism in the 1960s, to the more humanitarian and ‘global justice’ approach of present day solidarity groups. With this in mind, the conference aimed to explore some of these issues, particularly from the perspective of the day-to-day practice of international solidarity movements."
f0aed10a570687978bf0386ef430b2da3e96c7aa,"This article explores the tensions between international solidarity groups and those they are in solidarity with. Taking the case of the Palestine Solidarity Movement and in particular the Ireland Palestine Solidarity campaign, the article argues that solidarity groups take the position of ‘not intervening in the internal affairs’ of the Palestinian people for a variety of reasons. The stance is adopted because of activist awareness of the possibilities of Northern groups causing harm to the people they are in solidarity with (of particular importance for Palestine), as well as serving as a means of declaring a belief in the political autonomy of the Palestinian people. As such, this position is used to differentiate solidarity groups from humanitarian/aid organisations. The article looks at the difficulties of putting this stance of ‘non-involvement’ into practice through examining solidarity groups’ response to Palestinian infighting in 2007, and to the Palestinian statehood bid of 2011. The article concludes by examining the problems associated with a policy of noninvolvement in internal affairs, arguing that such a policy may lead to a superficial understanding of solidarity and a lack of communication, something which in turn can block the transformative potential of solidarity movements."
3fb149ab490f40a28ce3b8c1a32ecfd440432eb4,"The fragmentation of Palestinian lives into exile, under occupation and within Israel has led to a complex interweaving of collective memory and individual memories in the attempt to come to terms with and represent this existence. Central to Palestinian self-understanding is the key interruptive event of the Nakba, the ethnic cleansing of Palestine in 1948 which disrupted the people’s links to the land of Palestine – not only for Palestinians in exile but also for those within present-day Israel. Memorialization practices, such as those undertaken in village memorial books which record in detail the Palestinian villages destroyed in 1948, work to foster a collectivity linked across generations and borders. However these practices also repress marginalized voices, especially the voices, experiences and perspectives of women. By highlighting these voices, by engaging in collecting memories and by critically assessing the process of collective memorialization, the authors reviewed present a decentred, complex and kaleidoscopic version of Palestinian self-understanding and identity."
546f0f8d24ce7d2bfe835eafabe0920cac180f6c,": In a recent paper, Robert Hanna argues that Kant's incongruent counterparts example can be mobilized to show that some mental representations, which represent complex states of affairs as complex, do so entirely non-conceptually. I will argue that Hanna is right to see that Kant uses incongruent counterparts to show that there must be a non-conceptual component to cognition, but goes too far in concluding that there must be entirely non-conceptual representations that represent objects as existing in space and time. Kant is deeply committed to the thesis that no representation of a complex state of affairs as complex can be entirely non-conceptual. For Kant, all representations of complex states of affairs as complex (including those of incongruent counterparts) are conceptually structured. I present an interpretation of the Transcendental Aesthetic according to which Kant not only aims at Leibnizian and Newtonian accounts of space and time, but also Hume's. Hume's account fails to make representations of complex states of affairs sufficiently determinate. Kant offers an account later in the Critique that is meant to correct this failing by requiring that all representations of complex states of affairs as complex be conceptually (inferentially) structured."
867edeb06deec353fecb0984a2c704e3a27337a8,"the manner in which we seek to implement that action: its timing, its mode of execution and its prioritization in relation to other ethical goals should all be coloured by our knowledge of human propensities. There is a skill to acting ethically which can always be improved by better knowledge of human circumstances and behaviour. The fact that this knowledge can always be modified and extended provides no justification for refraining from acting ethically. It is simply that we should try to act on the most informed basis possible. As Louden aptly puts it, ‘morality is not easy for human beings. But anthropology also teaches us that there are things we can do, given human nature, to promote the development of moral character’ (71). Acting morally arises against a background of struggle within us between moral and non-moral motives. Self-knowledge is an important dimension of practising morality. In seeking to oppose our non-moral and anti-moral motives and strengthen those that are compatible with morality we need to know about our character. ‘The opportunity to practice goodness in small matters through civilized behaviour is a mundane feature of daily life, but it has the cumulative effect on character’ (72). No action which affects others is too mundane for us to ignore its moral dimensions. So doing the right thing always requires some anthropological reflection. We need always to be assured that our natural incentives for action are not determining what we do. Thus, instead of regarding our incomplete knowledge of the human individual as a barrier to developing our ethical conduct Kant regards it as an incentive. We know from pure moral philosophy the quality of an action that makes it good; what we learn from reflection on ourselves and others are the best possible steps (given our present information) in overcoming our reluctance to carry it out. Howard Williams Aberystwyth University email: hlw@aber.ac.uk"
8a85aa2f6420c978ce7df693567678146a96523e,"The fragmentation of Palestinian lives into exile, under occupation and within Israel has led to a complex interweaving of collective memory and individual memories in the attempt to come to terms with and represent this existence. Central to Palestinian self-understanding is the key interruptive event of the Nakba, the ethnic cleansing of Palestine in 1948 which disrupted the people’s links to the land of Palestine – not only for Palestinians in exile but also for those within present-day Israel. Memorialization practices, such as those undertaken in village memorial books which record in detail the Palestinian villages destroyed in 1948, work to foster a collectivity linked across generations and borders. However these practices also repress marginalized voices, especially the voices, experiences and perspectives of women. By highlighting these voices, by engaging in collecting memories and by critically assessing the process of collective memorialization, the authors reviewed present a decentred, complex and kaleidoscopic version of Palestinian self-understanding and identity."
f49760aa4dd0b7192088c3f1be813b50f5308f7b,"Human rights discourse is central for the work of international social movements. Viewing human rights as a context-dependent and socially constructed discourse, this article investigates how it is used by a specific social movement – Israel-critical diaspora Jewish activists – and argues that it can simultaneously challenge and reproduce existing practices of domination. The article applies contemporary critiques of human rights to the case of Palestine, where this discourse has arguably been used to undermine Palestinians’ political subjectivity and collective struggle, and legitimise outside intervention. Nevertheless, transnational groups critical of Israel, particularly diaspora Jewish organisations, rely on a human rights frame. There are several reasons for this: it offers activists a means to achieve ‘cognitive liberation’, to speak about the issue and to frame their activities so as to attract recruits. The article investigates this paradoxical role of human rights, and recommends understanding it as a language which both constrains and enables the practice of transnational solidarity."
17d9531765c69034f4a325bc0c283da4f2672f94,"Hume's arguments in the Treatise require him to employ not only the copy principle, which explains the intrinsic properties of perceptions, but also a thesis that explains the representational content of a perception. I propose that Hume holds the semantic copy principle, which states that a perception represents that of which it is a copy. Hume employs this thesis in a number of his most important arguments, and his doing so enables him to answer an important objection concerning the status of the copy principle. I further argue that the semantic copy principle is necessary, a priori, and discovered through an analysis of our general idea of representational content."
79b53b44f1aaf49da28c15457092d88fc11e75f5,"The Communal Gadfly: Jews, British Jews and the Jewish State-Asking the Subversive Questions, by Geoffrey Alderman. Brighton: Academic Studies Press, 2009. 282 pp. $35.00. There is something in Geoffrey Alderman's book, The Communal Gadfly to irritate everybody, which is quite as Geoffrey Alderman would like it. This book, comprising selected columns written for the British Jewish Chronicle, provides an overview of both Alderman's and British Jewry's main interests from about 2002 to 2008. Although some of the columns are dated, they are a pleasure to read, educational and opinionated, when dealing with the subject of British Jewry and of British politics. It is when he raises his eyes from these local affairs to the Middle East that his critique deserts him and his articles become a drumbeat of orthodox Zionist cliches, which not even the author's inimitable style can rescue. The distinction between the two is sharp and perhaps reveals the limitations of what can and cannot be said in good Anglo-Jewish company. In discussing British Jewry, Professor Alderman, who wrote its history over a decade ago, comes from a conservative but never dull vantage point. He certainly has much material to work on and does it with unseemly glee at times-analyzing, dissecting and ruthlessly disposing of many of the guiding myths of the Jewish ""community."" One of his main theses, running like an arrow through the analysis of British Jewish foibles, is that there is no such thing as a unified British Jewish community and the sooner that Jews realize this, accept it, have their institutions reflect it, and move on, the better for all. It is hard to disagree with Professor Alderman in this. The main trend in British Jewry over the last fifty years has been the decline/collapse of Central Orthodoxy as the main guiding point of British Jews and the fragmentation of the Jewish community into diverging movements. Jews in Britain, as elsewhere, are divided by religion and politics, and misrepresented by those communal leaders who pretend otherwise. One can see the best part of this book as a contemporary chronicle of a fragmenting Anglo-Jewish field, be it Masorti (Conservative) Jews opposed to Orthodox ""outreach,"" liberal Jews abandoning their attempts to be accepted by the Orthodox, or the splintering of Orthodox Jews. Many of the situations he analyzes have their analogies in the U.S., and readers will perhaps go through these sections with a sinking sense of deja-vu. Alderman is scathing of those with pretensions to speak for an increasingly mythical Jewish community. Thus, in the first section of the book, the Board of Deputies of British Jews who purportedly represent the community are treated as a bunch of bumbling idiots and the Jewish Leadership Council as a group of secretive undemocratic plutocrats-the ""funding fathers"" in Alderman's memorable put-down. In this criticism, Alderman is probably voicing the opinions of most British Jews. There is an entire section devoted to the hapless Orthodox Chief Rabbi of England, Jonathan Sacks, although ""devoted"" may be the wrong word to use here. Amidst the often very funny criticism, Alderman is making a serious point-as mainstream Orthodox Jews become less and less interested in religion, the organs of Orthodox Judaism are increasingly taken over by hardline religious leaders, and have become subservient to the diktats of Ultra-Orthodoxy. …"
4411c6e805321f3dd4dfb40ef5bd2f1b052f3474,
49af6428d351aa7ead5622f6969789ecadd5ee61,"Diaspora Jews are increasingly likely to criticise Israel and support Palestinian rights. In the USA, Europe and elsewhere, Jewish organisations have sprung up to oppose Israel’s treatment of Palestinians, facing harsh criticism from fellow Jews for their actions.

Why and how has this movement come about? What does it mean for Palestinians and for diaspora Jews? Jewish Identity and Palestinian Rights is a groundbreaking study of this vital and growing worldwide social movement, examining in depth how it challenges traditional diasporic Jewish representations of itself. It looks at why people join this movement and how they relate to the Palestinians and their struggle, asking searching questions about transnational solidarity movements."
f81857d60f83e935e38ac2ac2902a3ea085c051b,"In his, ‘Descartes' Ontology of Thought’, Alan Nelson presents, on Descartes' behalf, a compositional theory of mental representation according to which the content of any mental representation is either simple or is entirely constituted by a combination of innate simples. Here the simples are our ideas of God, thought, extension, and union. My objection will be that it is simply ludicrous to think that any four simples are adequate to the task of combining to constitute all of human thought, and that the simples God, thought, extension, and union are particularly ill suited to it."
0e115a44901c3fc348476aece3f0175f9f18fd06,"In his graduate-seminar lectures on Kant—published as Kant and Pre-Kantian Themes (Sellars 2002)—Wilfrid Sellars argues that because Hume cannot distinguish between a vivacious idea and an idea of something vivacious he cannot account for the human ability to represent temporally complex states of affairs. The first section of this paper aims to show that this argument is not properly aimed at the historical Hume who can, on a proper reading, distinguish these kinds of representations. This is not, however, Sellars’ only argument for this conclusion. The next section of this paper continues with a discussion of an argument that Sellars presents on Kant’s behalf in Science and Metaphysics, and its key Kantian premise that, pace Hume, only conceptual representations can represent any complex states of affairs as such. The conclusion of this discussion is that Sellars does indeed present compelling reasons for rejecting a Humean account, reasons centred on the ambiguity of the associative structure of mental representation (which Sellars subsequently replaces with inferential structure). The paper concludes with an examination of the sense in which non-conceptual representations—such as those that Hume considers—represent and the role that these play in conceptual representation for both Sellars and Kant."
609f80bf95b984ca380b0b231dfcf651879dc64d,
698efe6e1a999260208f45f9b695c38646b62597,"One recent trend in Kant scholarship has been to read Kant as undertaking a project in philosophical semantics, as opposed to, say, epistemology, or transcendental metaphysics. This trend has evolved almost concurrently with a debate in contemporary philosophy of mind about the nature of concepts and their content. Inferentialism is the view that the content of our concepts is essentially inferentially articulated, that is, that the content of a concept consists entirely, or in essential part, in the role that that concept plays in a system of inferences. By contrast, relationalism is the view that this content is fixed by a mental or linguistic item's standing in a certain relation to its object. The historical picture of Kant and the contemporary debate about concepts intersect in so far as contemporary inferentialists about conceptual content often cite Immanuel Kant not only as one of the founding fathers of a tradition that leads more or less straightforwardly to contemporary inferentialism, but also as the philosopher who first saw the fatal flaws in any attempt to articulate the content of our concepts relationally."
dc565f0fe82d6ff5d842f9c64c0b4a4fcab03370,"This article explores the strengths and limitations of movement intellectuals’ theorisation of their movement and its terrain of activism. It looks at four published collections of Jewish writers critical of Israel and Zionism and asks how these books represent and defend a developing diaspora Jewish Israelcritical movement, and whether they manage to effectively theorise its terrain of activism. I argue that although these books offer some important purchase on the issues surrounding Israel/Palestine, through promoting the subjectivity of Jewish activists, and by being constrained by what is acceptable among mainstream Jewish thought, they efface the voice and presence of Palestinians, producing a partial understanding of the issue and the movement. I suggest that this may be due to the particular phase of this movement getting to know itself and its terrain of activism, which I characterise as its ‘mirror stage’. Interface: a journal for and about social movements Volume 1 (1): 188 215 (January 2009) Landy, The mirror-stage... 189 Books reviewed in this article: Anne Karpf, Brian Klug, Jacqueline Rose, and Barbara Rosenbaum. 2008. A Time to Speak Out: Independent Jewish Voices on Israel, Zionism and Jewish identity. London: Verso. 224 pp ISBN-13: 978-1844672295 (pb). Paperback £7.49 Seth Farber. Radical, Rabbis and Peacemakers. Conversations with Jewish Critics of Israel. Maine: Common Courage Press. 2005. 400 pp ISBN-13: 9781567513264 (pb). Paperback $19.95 Mike Marquesee. 2008. If I Am Not For Myself: Journey of an anti-Zionist Jew. London: Verso.: 256 pp ISBN-13: 978-1844672141(hb). Hardback £15.49 Murray Polner and Stefan Merken. Peace, Justice, and Jews: Reclaiming our Tradition. New York: Bunim and Bannigan. 2007. 338 pp. ISBN-13: 9781933480152 (hb). Hardback $18.25 In the last few years there has been an explosion of diaspora Jewish writing critical of Israel and Zionism. This has coincided, though it is hardly coincidental, with the formation of a specifically diaspora Jewish movement that criticises Israel. There have always been Jews critical of Israel and supportive of Palestinians. Recently though, this has been transformed from a ‘Not in my name’ individual opposition to and withdrawal of support from Israel to ‘Not in our name’ a collective attempt to withdraw legitimacy from Israel’s claim to represent Jews, and to forge a specifically Jewish collectivity whose aim is to oppose Israel’s policies. We can speak for the first time, not of disaggregated people criticising Israel, but a social movement whose aim is to oppose its treatment of Palestinians. In this article I seek to explain the relationship between diaspora Jewish writings critical of Israel and this movement. Many of the authors I examine are active in Israel-critical groups (both Jewish and wider) and can be seen, using Gramsci’s term, as ‘organic intellectuals’ for this movement. This is not simply because of any activist involvement, but also because their books are explicitly Interface: a journal for and about social movements Volume 1 (1): 188 215 (January 2009) Landy, The mirror-stage... 190 designed to build this developing movement – to serve as guides that provide reasons, recruits and routes for its journey. As such these productions offer an insight into the relationship that movement intellectuals have with a recently established social movement, and how well they can describe and analyse their movement and its concerns. The question of how movement intellectuals understand and present the movement they are associated with has wider application than the specific issue of Israel/Palestine; it links in with the debate comparing academic and activist forms of theorising. There is no need to reiterate criticism of academic research on social movements, the main points being that these academic descriptions are neither useful for the movements themselves, nor relatedly, are they very good analyses of these movements (Bevington and Dixon 2005; Johnston and Goodman 2006). Such critiques in some cases explicitly contrast academic shortcomings with the output of movement intellectuals (Cox and Barker 2002). In viewing the type of knowledge produced by movements, Cox and Barker (2002) maintain that it derives from their character as movements in action, rather than static debating fora. The knowledge produced is above all practical. It may be practical in providing ideological and moral justifications of the movement or in providing strategic and practical proposals – it is always, however, directed towards what Cox and Barker see as the essential feature of movements – their dialogical and developmental nature – a fact which ensures that movement knowledge is ‘attempts to find answers to the question “what is to be done?” in situations which they do not fully control.’ (2002, 45) Yet this activist theorising needs to be critically analysed in terms of what forms of knowledge it produces, how it produces this knowledge, and what are the effects of knowledge being produced in conditions of contention. I ask these questions about this movement and with respect to a certain type of activist theorising – books on the movement that have been produced by activists and academics with some degree of movement involvement. In choosing to analyse published books, I do not claim that this form encompasses all forms of activist theorising. Far from it, such material is overly Interface: a journal for and about social movements Volume 1 (1): 188 215 (January 2009) Landy, The mirror-stage... 191 representative of movement elites, is produced under market conditions and in response to other force than movement dynamics, including some of the forces that produce academic works. Nevertheless they represent an important public face of the movement and seriously wrestle with issues affecting it. They serve as exercises in movement justification and strategic thinking, and undertake crucial work in identity building that all movements but particularly this one needs to undertake. In the article, I firstly introduce this movement – its aims, origins and its terrain of activism – does it try to change diaspora Jewish discourse on Israel/Palestine, does it try change wider opinions, does it try to do both? Then, I ask what relationship sympathetic academic/activist theorising has with this heterogeneous movement. I examine four books from the last couple of years – from America there is Seth Farber’s interviews with anti-Zionists, which forms a useful contrast with a collection of articles from ‘moderate’ critics of Israel (Farber 2005; Polner and Merken 2007). There’s Mike Marquesee’s transcontinental memoirs and finally another anthology from Jewish academics and activists, this time from Britain (Karpf, Klug, Rose, and Rosenbaum 2008; Marquesee 2008a). These books offer in their different forms memoir, interviews and multiple voices – various ways to understand both movement and terrain of activism. In doing so I also ask whether the ‘manifesto’ aspect of this writing detracts from understanding of the issue or whether their engagement contributes to it. I argue that it does offer powerful theoretical engagement with the issues around Israel/Palestine, but that it comes with a downside. Put simply, through focusing on Jews, these books fail to consider Palestinians adequately. By contesting the terrain of Jewish identity some of these books find themselves in an identity trap whereby the subjects of activism are simply Jews, not Palestinians, something which serves to offer a very partial vantage point on the Israel/Palestine issue. I further argue that this is not a fault of the books being insufficiently objective and academic. On the contrary, it is partly the failure to fully concern themselves with the many way that Jews engage in Interface: a journal for and about social movements Volume 1 (1): 188 215 (January 2009) Landy, The mirror-stage... 192 activism on Israel/Palestine, and to adequately represent the movement that has led to this effacement of Palestinians. This article is based on research into this movement – specifically on research into English Jewish Israel-critical activism, as well as readings of these books. As a Jewish activist in the Ireland Palestine Solidarity Campaign, I research this activism in the spirit of critical solidarity – seeing it as a ‘partial, imperfect, yet significant praxis’ (Johnston and Goodman 2006: 17). It is in the same spirit I approach these books. Description of movement Firstly are we talking about a movement? There’s certainly something happening in the Jewish world. Since 2002, Jewish groups that oppose Israel have sprung up throughout the diaspora in Canada, Australia, France, Scotland, even in Germany – and of course many organisations in the US. Taking England, there’s an alphabet soup of groups Jews for Justice for Palestinians (JfJfP), Independent Jewish Voices (IJV), Jews Against Zionism (JAZ), Jews for Boycotting Israeli Goods (J-BIG, Slogan “It’s kosher to boycott Israel”), not to mention the older Jewish Socialist Group (JSG). This initially confusing profusion should not obscure the networked and decentred nature of this activism. Groups undertake joint activities with Jewish and non-Jewish groups; activists are sometimes linked to no groups, sometimes to many, Jewish and non-Jewish; there are links with radical Jewish cultural groups; and many activists are involved in mainstream political parties. Thus, contrary to the claims of detractors (Atzmon 2005), the English experience shows that what is being created is no new Jewish ghetto, but a situated response to the Israel/Palestine conflict and Zionist support of it, networked both to other Jews and non-Jews. There are some linkages with Israeli and (to a smaller extent) Palestinian organisations and there are efforts to form coalitions – European Jews for Just Peace (EJJP) in Europe, and the more radical International Jewish Anti-Zioni"
1d98c96e843513c58bcd73fedb548ad7a3b09a95,"I here discuss Hegel's rule‐following considerations as they are found in the first four chapters of his Phenomenology of Spirit. I begin by outlining a number of key premises in Hegel's argument that he adopts fairly straightforwardly from Kant's Transcendental Deduction. The most important of these is that the correctness or incorrectness of one's application of a rule must be recognizable as such to the rule‐follower. Supplementing Hegel's text as needed, I then argue that it is possible for an experiencing subject to follow a rule only where there is a community of individuals whose agreement can provide a standard for the correctness and incorrectness of his use. I further argue that a community must consist of members that are compresent, and thus that a collection of time‐slices of an individual will not serve this purpose. I conclude by raising a potential problem for Hegel's account of rule‐following concerning the correctness and incorrectness of the judgments of a community, and pointing to a possible line of response to this problem."
76b87d54c6c9e9f88bcb5acbbd8ea14d4168873d,
1b9e37301e7d0a4cd6a95f92968d1e81fd3fe8de,"This paper examines the construction of Irish-Jewish identity, through the prism of the Ireland–Israel soccer match in 2005. While, under the terms of ‘celebratory multiculturalism’ Irish Jews were able to use joke-work to bat away the implied loyalty test of ‘which side are you on’, the pro-Palestinian political mobilisation on the day of the match was more problematic. Within the narrative of Irish Zionism, these pro-Palestinian activities were linked to antisemitism, an interpretation which alienates Jews from those left-liberal elements in Irish society most open to a reading of Jewishness as part of a multicultural Ireland and re-inscribes Jews as ‘a people apart’."
597efdf7467e886e807e67e8ccdaddf53c526010,":  In A Treatise of Human Nature, Hume attempts to explain all human cognition in terms of impressions, ideas, and their qualities, behaviors, and relations. This explanation includes a complicated attempted reduction of beliefs, or judgments, to single ideas. This paper attempts to demonstrate one of the inadequacies of this approach, and any of its kind (any attempted reduction of judgments to their constituent parts, single or multiple) via an argument concerning the logical forms of judgment found implicitly in Kant's Critique of Pure Reason, and more explicitly in the works of Wilfrid Sellars."
cd27c6cf368ea6dc2275f11def3508ba7b807263,
f5fb023ab37587f239f2c2ff860a496ea40d434d,"Understanding the distinction between impressions and ideas that Hume draws in the opening paragraphs of his A Treatise on Human Nature is essential for understanding much of Hume's philosophy. This, however, is a task that has been the cause of a good deal of controversy in the literature on Hume. I here argue that the significant philosophical and exegetical issues previous treatments of this distinction (such as the force and vivacity reading and the external-world reading) encounter are extremely problematic. I propose an alternative reading of this distinction as being between original mental entities and copied mental entities. I argue that Hume takes himself to discover this distinction as that which underlies our pre-theoretical sorting of mental entities. Thus, while the Copy Principle is initially treated by Hume as a mere empirical fact, it later comes to play a more substantial explanatory role in his account of human nature. This reading makes Hume's distinction a more philosophically robust one, and avoids many of the exegetical difficulties of previous interpretations."
47406da0c34e923bb0ffccfc2e841b3300e558fc,
3fd0b1d796734243d8ebd250d29bfdac62f6b53d,
7b8166f167c3fa7287828668e36120c6414729ff,
a09d2d4fc2c0eb3b5c9620a644e2b1cf52742dcd,
7213fa7a88ae86fa88b130f690b5b3414110de83,"To study cultures as ideational systems without mapping the complex cybernetic circuits that link them to social systems, to ecosystems, and to the psychology and biology of individuals would turn cultural analysis into an arcane pursuit isolated from surrounding disciplines at a stage when a fantastic burst of scientific knowledge—with human survival as the stakes—is being launched: a burst that should relegate to the realm of Ptolemaic astronomy (or at least preWatson-Crick genetics) previous theories in ecology, the neurosciences, psychology, and related fields."
1005593977cefa4b1fa530681c5798d5d21482e8,
f07966e7398c385080171162212ffd76362ab297,
0995d0c4a82df760665e78e53bae8ae608ccbcc7,
a0aa49769dc2abd7e879c6bc10aff91489f5db84,
42839dce8f5854c706c8b78be732f14f5284a099,
943c05bb558e4634fc1c1bbce0efb2886217d286,
7829a4fa0636afee2e76471842da62acdbad75ad,
59de68ce22b5e4bebd4b054058a8376dab68366b,
a02f0dd6e2faee43fea125b525c80714043e6337,
c8b98d49befc1d418935a5f545bf518e1d1c78d3,"While considerable progress has been made in relation to women’s labour market participation over the past decades, deeply entrenched inequalities persist. They are the result of discriminatory norms and attitudes, the unequal distribution of care responsibilities in the household and the way institutional structures consider and integrate gender. While women’s activity rate has increased during the past years, men’s activity rate has remained largely stable. The slow tempo of changes in the sharing of unpaid work represents a serious constraint for women’s equal access to the labour market and their equal control over economic resources."
4a2822697ba5fa20aba44260b12740434bd6ae28,"A number of investigators are agreed that the popular medical systems of tribal, peasant, and allied peoples are ""effective."" Most of the literature closely examining that effectiveness focuses on the ethnopharmacological dimensions of the healing systems and generally ignores psychosocial factors. Recent developments in psychophysiology may offer insights into these neglected areas. The specific idea to be examined here is that successful ""general medical treatment,"" or ""symbolic healing,"" by either the shaman or physician, is based in part on a psychosocial mobilization of the patient's biochemical response system. Moreover, it is argued that to account fully for these processes we must reconceptualize the character of the human organism; a unitary alternative to standard Western Cartesian dualism (mind vs. body) is proposed, based on a model derived from recent research in neuroendocrinology. This model can be the basis for a nonreductionist theory of medical effectiveness needed to account for a series of observations (derived from both anthropological and medical contexts) which seem to transcend the explanatory powers of the traditional reductionist biomedical model."
0cd42123aa5fde6ce0877e48508454fcad74c485,
d99b8dd0c8f6f4b043a14701f84fe28b6a9b419b,
46da6f70c9de70093bfa2fc5ec3cf8ca325025d9,
b19213a2a7a4ddd7ef196c1a28d076f599e8fcb6,
b3bf4217ebdd158d72a6f6936afb74672689b63a,
726a81892491aa200196269a831f1ff8af8b285e,
26a44fd19070a93a4734a841caf36e0f6d29a9be,"To examine effects on the curers role of the contest between indigenous and Western medical systems the concept of role adaptation is proposed and considered as a conceptual tool and as it may relate to such associated concepts as cultural broker role analogue and role ambiguity. Data were taken from a variety of studies by anthropologists and others in an effort to place the changing role of the curer into substantive and theoretical perspective and to contribute to a model for the study of role adaptation under conditions of culture change. The instance of the traditional curers role under potential stress from the demands and temptations of the compeing mdical system represents andextension of Goodes theory of role strain. The following assumptions are made: the curers place in his society originally was relatively secure until threatened by the pressure of culture change and his status was traditional and prescribed though not invariably ascribed regardless of whether his pesonality was in phase with the behavioral norms of his own sociocultural system; prior to contact in addition to ameliorating the effects of illness and disease the curers activities were oriented toward enhancing and/or reinforcing his social position; a measure of security was present in that role performance expectations were shared with other members of the society and competition originated primarily from within the group and presented a relatively known range of possibilities; and the curing role could be a full or part time one but prestige in 1 role tended to be linked to prestige in others so that status in any tended to reinforce the power of all. Analysis of role adaptation of curers in selected societies undergoing acculturation suggests a model of adaptation possibilities in which the data may be grouped into 3 categories--adaptive attenuated and emergent curing roles--and the data are examined in these terms. Primarily it is the local curere who borrows elements from Western medicine rather than vice versa although many ""primitive"" botanicals have been incorporated or synthesized. In his role adaptations to culture change the traditional healer not only incorporates but elaborates Western elements. Resysthesis flows in the other direction when customary ceremonies and fetishes become used for new functions especially to relieve some of the tensions and anxieties of acculturative pressures as in West Africa for example the Fanti Anang Ibibio and Ashanti. The curers status becomes attenuated when the expectations of his community are such tath the technology if not the values of scientific medicine is perceived by them as so clearly superior that they distinctly prefer to their own. As for the new emergent curing or quasi-medical roles it may be predicted that they are likely to be thrust into competitive stress and strain with their analogues within the Western medical system especially such paramedical roles as those of nurse attendant and medical technician."
4ec18d0dbf48a5de0d96a525e24b840775a8f747,
f0cc06bf17a56cd36dadbc14321370b3b0de0641,
8273b85a2229151c0e91164f31f89a50d648aca0,
be6026a1b33fea60355a18542c6e7d641e86c050,
e70f495b37ec7369554a6fe0fb1be00564a9b5fb,
d97d3a9cbf06a60c77ec35e537ac04f2dd43a918,
148b92e3c2c96568dd6877e2e861f16dbffcbfc2,
3ced3775d62338df5ba0da9140c91bfc1d86b0b6,
71e4cc51c1ec15582bfaf94152c7e7b4d3279d82,
bb6939208d88bf5bb0bd4a9c4b359a6df629beb1,
1fe47a1315b109093895ae2ccbd6edd79558fdab,
e7218aa89f5432510ae6584c0af6cc98e1a79d74,
7ea8f0bde990b1bf40032c11902f6d52593951b6,
6eef40c32459a4c0308ba9b875352dbfe156bf9e,
c3c98a2e750dcc89183c019744b60a1631ef6a06,
2620aa2ab0fca16eabd2bf057e5b962430d4c936,
712a267b5add50149524e52c1f73ed674729a74a,"This paper discusses the general concept of the basic culture-bearing unit and proposes a new definition-the cultunit. This proposal is a response to the need for units of cross-cultural surveys to be comparable and to be rigorously defined if these studies are to be validated statistically. Such anthropologists as Schapera, Berndt, Whiting, Evans-Pritchard, Reichard, Radcliffe-Brown, Fortes, Ember, Murdock, Nadel, Leach and Driver have given much thought to the problem of defining basic culture-bearing units. From their writings, we can see at least six possible criteria for defining societal units: (1) language (which nearly everyone thinks is important), (2) political organization, (3) territorial contiguity, (4) distribution of particuar traits being studied, (5) ecological adjustment, and (6) local community structure. The cultunit concept uses the first three of these: a cultunit is defined as a group of people who are domestic speakers of mutually intelligible dialects and who also belong to the same state or contiguous contact group. Four types of cultunits can be distinguished: the Hopi type, a contiguous linguistic group who belong to no state; the Flathead type, a state whose members all speak a single language; the Aztec type, domestic speakers of the lingua franca of a linguistically diverse state. Three periods of time of any of these four types can also be usefully distinguished: palaeoethnographic periods, aboriginal periods, and colonial periods. Although the cultunit has some theoretical justification, it is offered as an arbitrary definition whose justification is its convenience in cross-cultural surveys. Through its use, sampling biases from inconsistent sampling units can be statistically controlled, and the importance of diversity of societal type can be statistically assessed."
a9d0fcb2dcaf01b1cda4e5b8d24c0dec44ad2276,
2dd3f93f503803c5e3002e95e57eaddfa7301065,
5be50991e0d89113885528364c48eb264861ed20,
98a92e1ad35c3a78abbca57f2730eeda6e93d2ba,
b48b57c20aff2d4f5bd62a1890f585076c1cab62,
20e18c9360a7b809c2d96b5529cd364dd4106c3b,
3ed196bdbf8d0701d84e10e7df14fc4e1ea1ed8b,
7c1451053a755a8afdf8432c62ead7cfc5a000d0,
bd630fd097786a11c2c1dde154d6303a2ee66a47,"at psychological reconstitution and social acceptance. These stages and possible dislocations occur when the patient (1) leaves his community of orientation for the hospital; (2) moves from the acute or disturbed admission ward to the convalescent ward; (3) moves, in an increasing number of institutions, into an agency which serves as a transitional buffer for the often traumatic change from hospital to community culture (day hospital, &dquo;halfway house&dquo;, foster home, etc.); (4) moves from the hospital or transitional residence into his former or a different community and residence. (8, 9, 10) An aim of this paper is to describe and analyse an experiment by a hospital in transitional residential after-care of the discharged male psychiatric patients, such as that indicated in stage (3) above. Unfortunately, psychiatry has not yet reached the stage where all patients can be acculturated successfully into, or away from, the hospital and therefore often patients are &dquo;ready for discharge&dquo; without being prepared fully to leave the hospital culture or face the task of resocialization and reacculturation to the outside world. Such individuals may have many personal strengths. Rather than retain them indefinitely, or transfer them to a custodial ward or institution, or send them into the community without proper building up of their defences, it is often considered feasible to use intervening measures to enable them to bridge the cultural chasm. One device is to place the patient into a new independent residence (6) where he may have the protective and permissive atmosphere of a convalescent ward combined with the greater freedom and acculturative opportunities that are part of community living."
d74fa632ce705daff9a044e46e7acf59a72ee57c,
ff677d20bd0a2186afb495c3896f579f61f532f6,
fcd032e00030a63b9b346439095edc489076ee5a,"Introduction Hospitalization for mental illness is a serious, and at times traumatic, event for both patients and their families. Once the family or the patient’s physician has decided that the patient should be hospitalized, waiting for hospitalization may be disruptive of family and individual emotional health. A survey of studies1,5of hospital waiting lists reveals most of them to be unsatisfactory in terms of asking such questions as these: How does the family react to waiting-list placement? How does the potential patient react? What kinds of behaviors led to waiting-list placement? What factors contribute to the family’s perception of the pre-waitinglist situation as being or not being a critical one? The present study is an attempt to explore these questions.* This paper has as its main objective to describe the prospective patients, and to try to clarify the psychiatric and familial nature of persons placed"
31e7feae9ea6b04d6a2cbd1c4c025d32415569eb,"Pressures of acculturation continue to weaken the cement of Tuscarora tribalism, which so far has held together the retaining wall of social and cultural survival around their reservation community. This is not to presume impending ""disorganization,"" but to suggest that the society continues to undergo incisive changes which cut to the quick of traditional ways of life. Nevertheless, it is apparent from the anthropologist Anthony F. C. Wallace's studies1 and those of the present author,z that the Tuscarora, despite what may appear at first glance to be an almost completely acculturated Indian community, retain a good deal of their original nonmaterial culture, and have managed to resist to some degree the tides of culture change. The Tuscarora have always been a deviant group among the Iroquois, and despite their acceptance into the Confederacy, have never been emotionally assimilated as blood-brothers by the Five Nations. It would seem inevitable that they would still be viewed by the other members as something of a stranger within the gates of the League"
51a6a0209c7157d2eabb0bd54fb16be63a61ba84,"Every society, no matter how small and homogeneous, includes a mass-core of persons who behave normally, a more or less sizable number who behave in variant, but not necessarily punishable, ways, and some whose ways of adaptation have proceeded beyond acceptable variance to unacceptable and punishable deviance. These alternative behavior possibilities are separated by blurred boundaries that tend to shift with stresses and strains which"
5449ad7166307a460377d681425528a4efe1ffe8,
548169ccf1827c0c3067a5a5baec9ccf328b7436,
a68d3d25aa6b14fea330b83a3f95c4834eab2289,"W h a t role does the anthropologist play in the research program of the mental hospital? I n recent years, there has been a rapidly growing interest in the relations between anthropology and psychiatry,1 and anthropologists have become particularly concerned with research in the mental hospital? Enough experience has been gained so that it should now be worth-while to analyze the role of the anthropologist in such hospital research and to note the problems he must solve if he is to be effective in his work. This paper3 is based primarily upon my own experience as director of research on several aspects of the rehabilitation of the psychiatric patient, which we view as a process of acculturation.* T h e hospital is a 130-bed teaching and training institution operated by the state of Massachusetts. Here a"
b17b628ff25e993c54d4eee83199c345b38c881f,"The Rehabilitation Project of the Massachusetts Mental Health Center has been based on the conceptions that mentally ill patients have untapped potentialities for successful adjustment, and that optimal work in the field would require mobilization of patients, hospital and community resources in a comprehensive effort. The introduction of a new program requiring close collaboration of many services and disciplines has had a noticeable impact on hospital structure. Some role groups, especially psychiatrists and social workers, appear to be threatened by the new emphasis. Other role groups such as nurse, occupational and industrial therapists, have enjoyed a reinforcement of their status, and increased value seems to be attached to their functions. The experiences thus far with day hospital, halfway houses and social therapeutic clubs indicates that these methods have specific value in selected cases. Intensive investigation is underway to determine optimum relationship between the special facilities and ho..."
b8427eb8d9a0e4fe2672fb276052df94a51a0ec4,
16ec58626e6bce88a59717205ef188de344bf16c,
2507e58b6800f7ba5127eb3b12c340c443d17333,
b4b6de013a3fad04c5e2883eb2bb0b861276e8f9,
dcbb08ce33ebf6391f6aa421097d8598822ec571,
dea5a84672cfd1c2518512585c1ecd1842c2b21c,
0197a22c74b61daef8657fc9a737a6f280db325c,
07e5c37395ac54faab92b2cadc63412f4f9ae6e1,"
 
 
 People living with HIV (PLWH) have a higher risk of CAD. Whether coronary artery calcification (CAC) score could better stratify the CV risk remains debated in this middle age population.
 
 
 
 We conducted the French CAC score study to compare the CAC score between PLHIV and HIV− subjects at intermediate risk.
 
 
 
 689 subjects (257 PLHIV and 432 HIV−) were consecutively addressed for CV risk evaluation including CAC score assessment in 2 centers between 2013 and 2019. Subjects were included if they were 18 years'old or older, had no known cardiovascular disease and had a CAC score along with cardiometabolic assessment.
 
 
 
 The mean age of the cohort was 59.3 years ± 10.7 and predominantly male (54%). PLHIV were younger (55.8 years ± 9.1 vs 61.3±11.3, p<0.004), with a lower rate of diabetics (18% vs 26%, p=0.009) and lower levels of total cholesterol, LDLc, and HDLc (2.0 mmol/l vs, 2.2 mmol/l; 1.2 mmol/L vs 1.3 mmol/L; and 1.2 mmol/L vs 1.4 mol/L respectively, all p<0.001). The prevalence of hypertension, dyslipidemia and obesity was similar between PLHIV and the HIV− group (40%, 43% and 21%, respectively). No differences were observed in CV therapies prescribed including antihypertensive drugs and statins between PLHIV and the HIV− group (39% vs 37%, p=0.54 and 46% vs 39%, p=0.065). In contrast, PLHIV had higher rate of triglycerides (1.4 mmol/L vs 1.2 mmol/L, p=0.01), higher rate of active smoking (28% vs 14%, p<0.001), CRP level (4.0 mg/L vs 2.9 mg/L, p<0.001) along with higher black ethnicity representation (17% vs 5%;, p<0.001). 96% of PLHIV were under antiretroviral therapy at the time of enrollment with 88% who had been exposed to protease inhibitors. In subjects who had carotid and/or limb ultrasound evaluation (67% and 45% respectively in PLHIV and HIV), no increased rate of arterial stenosis >50% was found (13% vs 8%, p=0.16). Concerning, CV risk estimation using 10-year ASCVD and HEART scores, we observed that the median ASCVD score was similar between PLHIV and HIV− group (10.7% vs 9.8, p=0.15) but that PLHIV had a higher HEART score (3.0% vs 2.0%, p<0.001). The repartition between the ASCVD or HEART scores classes between the 2 groups was similar with 80% of the cohort classified as low or intermediate CV risk with the ASCVD score and 88% at intermediate or borderline CV risk with the HEART score. However, no statistically significant differences was found between the 2 groups regarding the prevalence of CAC = 0 (41% vs 44%, p=0.52) and the median CAC score (7.7 vs 8.2, p=0.81).
 
 
 
 In this cohort of subjects at intermediate CV risk according to CV risk scores estimation (ASCVD or HEART scores), PLHIV had a similar prevalence of CAC = 0 and median CAC scores as compared to HIV− subjects despite a younger age (6 years). Longitudinal follow up will explore whether this similar CAC score evaluation will be translated into higher CV events.
 
 
 
 Type of funding sources: None.
"
15f747cbb608fc661c648289641694841d34d3b2,"Objectives: People with HIV (PWH) are at an increased risk of atherosclerotic cardiovascular disease. Suboptimal responses to statin therapy in PWH may result from antiretroviral therapies (ARTs). This open-label extension study aimed to evaluate the long-term safety and efficacy of evolocumab up to 52 weeks in PWH. Design: This final analysis of a multinational, placebo-controlled, double-blind, randomized phase 3 trial evaluated the effect of monthly subcutaneous evolocumab 420 mg on low-density lipoprotein cholesterol (LDL-C) during the open-label period (OLP) following 24 weeks of double-blind period in PWH with hypercholesterolemia/mixed dyslipidemia. All participants enrolled had elevated LDL-C or nonhigh-density lipoprotein cholesterol (non-HDL-C) and were on stable maximally tolerated statin and stable ART. Methods: Efficacy was assessed by percentage change from baseline in LDL–C, triglycerides, and atherogenic lipoproteins. Treatment-emergent adverse events (TEAEs) were examined. Results: Of the 467 participants randomized in the double-blind period, 451 (96.6%) received at least one dose of evolocumab during the OLP (mean age of 56.4 years, 82.5% male, mean duration with HIV of 17.4 years). By the end of the 52-week OLP, the overall mean (SD) percentage change in LDL-C from baseline was −57.8% (22.8%). Evolocumab also reduced triglycerides, atherogenic lipid parameters (non-HDL-C, apolipoprotein B, total cholesterol, very-low-density lipoprotein cholesterol, and lipoprotein[a]), and increased HDL-C. TEAEs were similar between placebo and evolocumab during the OLP. Conclusion: Long-term administration of evolocumab lowered LDL-C and non-HDL-C, allowing more PWH to achieve recommended lipid goals with no serious adverse events. Trail Registration: NCT02833844 Video abstract: http://links.lww.com/QAD/C441"
20b6d3d19bc65d24cf85df2ccd492fddb0b41123,"Background: Animal studies have demonstrated that fetal exposure to high maternal cholesterol levels during pregnancy predisposes to aortic atheroma in the offspring. In humans, little is known about the consequences of this exposure on the development of atherosclerotic cardiovascular disease later in life. We wanted to assess whether maternal/paternal inheritance of familial hypercholesterolemia (FH) gene mutation could be associated with subclinical coronary atherosclerosis. Methods: We retrospectively included 1350 patients, followed in the French registry of FH, with a documented genetic diagnosis. We selected 556 age- and sex-matched pair of patients based on the sex of the parents who transmitted the FH gene mutation, free of coronary cardiovascular event, and with a subclinical coronary atherosclerosis evaluation assessed using coronary artery calcium (CAC) score. We performed univariate and multivariate analysis to assess the individual effect of parental inheritance of the FH gene mutation on the CAC score. Results: In the whole population, patients with maternal inheritance of FH gene mutation (n=639) less frequently had a family history of premature cardiovascular events (27.7% versus 45%, P<0.0001) and were 2 years older (46.9±16.8 versus 44.7±15.9 years old, P=0.02) than those with paternal inheritance (n=711). There was no difference in the prevalence of cardiovascular events between the two groups. In the matched subgroup, maternal inheritance was significantly associated with an increase in CAC score value by 86% (95% CI, 23%–170%; P=0.003), a 1.81-fold risk of having a CAC score ≥100 Agatston units (95% CI, 1.06–3.11; P=0.03), and a 2.72-fold risk of having a CAC score ≥400 Agatston units (95% CI, 1.39–5.51; P=0.004) when compared with paternal inheritance in multivariate analysis. Conclusions: Maternal inheritance of FH gene mutation was associated with more severe subclinical coronary atherosclerosis assessed by CAC score and may be considered as a potential cardiovascular risk factor."
2ed3d12cfb0299e157d2ef390955a33774da0910,
3862b8ec491b8768bd7f13149b5523934e3b5627,
55d2eb146ba7846c5ee4c3da4ae87130e9f18112,
5c6fe3c42009f6f1592a725e0e82fdba77edc264,"
 
 
 Increased Epicardial Adipose Tissue (EAT) volume has been associated with increased risk of CAD in people living with HIV (PLWH). However, the underlying mechanisms remain unknown.
 
 
 
 We conducted the PIECVIH study to compare EAT properties in relation with CAD between PLWH and HIV-negative patients, all undergoing coronary artery bypass graft (CABG).
 
 
 
 The PIECVIH study is a cross sectional prospective study performed in a single center enrolling 11 ART-controlled PLWH and 11 matched (age ± 3 years and sex) HIV-negative patients requiring CABG. During surgery, EAT and thoracic subcutaneous fat samples were taken. Gene expression was analyzed in samples with sufficient mRNA quality (7 PLWH and 7 HIV− for subcutaneous fat, 9 PLWH and 8 HIV− samples for EAT). The expression of 30 genes, mainly related to inflammation, immune activation, fibrosis and adipokines, was evaluated and related to the expression of the reference gene 18S.
 
 
 
 The mean age of the cohort was 59.8 years (100% male). The cardiovascular risk profile was quite similar between both groups including 66% smokers, 64% hypercholesterolemia, 36% hypertriglyceridemia and 56% hypertension. However, HIV− subjects had a higher prevalence of diabetes (73% vs 18%, p=0.002) and a higher body mass index than HIV− (23,2 vs 27.5 kg/m2, p=0.017). The level of gene expression of all tested genes was not different between PLWH and HIV− subjects in subcutaneous fat. Conversely, in EAT, the relative expression of IL-6 and CCL2 was 3–5-fold higher in samples issued from PLWH than from HIV−: respectively 0.46 vs 0.13 (p=0.03) and 1.13 vs 0.24 (p=0.03). Moreover, only in EAT, and only in PLWH, the expression of the chemokines CCL2 and CCL5 and of the macrophage immune activation markers (CD68, CD163, CD206), was globally related to the expression of genes involved into fibrosis: collagen genes (COL1A1, COL3A1, COL3A1, COL6A2, COL6A3), TGFB, LOX (lysyl-oxidase) and ASAH1 (acid ceraminidase). Only in EAT and only in PLWH, the expression of IGF1 and CES (carboxylesterase 1), two genes associated with increased cardiovascular risk, was related to the expression of genes associated with immune activation, fibrosis and vascularization (VEGFA). Only in PLWH, the Gensini score, evaluating the severity of CAD, was associated with EAT expression of collagen 6 and of the CV risk factors IGF1 and CES.
 
 
 
 In very high CV risk subjects undergoing CABG, inflammation/immune activation of EAT was higher in PLWH as compared to controls. In EAT of PLWH, immune activation was strongly associated with fibrosis stressing for a dysfunctional EAT. Moreover, the severity of CAD, as addressed by the Gensini score, was associated with collagen 6 expression, a deleterious collagen in the context of EAT fibrosis. We propose that in PLWH, altered EAT immune profile and fibrosis could be responsible for reported accelerated CAD.
 
 
 
 Type of funding sources: Public grant(s) – National budget only. Main funding source(s): French Agency for Research on AIDS and Viral Hepatitis
"
610fc0018f97fb81f3cc81eb1369a73717632247,"Objective: The aim of this study was to compare clinical characteristics and adipose/liver tissue histology analysis in HIV-infected and HIV-uninfected subjects undergoing bariatric surgery. Design: This was a cross-sectional study of HIV-infected subjects undergoing single-port sleeve gastrectomy with prospective enrolment and frequency age (±5 years), sex, and body mass index (BMI, ± 5 kg/m2) matched on HIV-uninfected subjects. Methods: This study was conducted at a single clinical site at Pitié-Salpêtrière hospital-Paris-France comprising 19 HIV-uninfected and 21 HIV-infected subjects with plasma VL < 20 copies/mL, all with a BMI > 40 kg/m2 or >35 kg/m2 with comorbidities. Histology of subcutaneous and visceral abdominal adipose tissue (SCAT/VAT) and liver biopsies was collected during single-port sleeve gastrectomy. Outcomes included anthropometric characteristics, comorbidities, cardiovascular parameters, adipose tissue, and liver histology. Results: The age of HIV-infected participants was (median, interquartile range IQR) 48 y (42–51), with 76.2% females, a BMI of 41.4 kg/m2 (37.3–44.4), an antiretroviral duration of 16 y (8–21), current integrase strand transfer inhibitor (INSTI)-based regimen in 15 participants and non-INSTI regimen in 6 participants, and a CD4 count of 864/mm3 (560–1066). The age of controls was 43 y (37–51), with 78.9% females and a BMI of 39.2 kg/m2 (36.3–42.6). Anthropometric characteristics, comorbidities, and cardiovascular parameters did not differ according to HIV status and INSTI treatment. The number of macrophage crown-like structures in SCAT was lower in INSTI-treated participants than in HIV-uninfected participants (P = 0.02) and non–INSTI-treated HIV-infected subjects (P = 0.07). Hepatic steatosis and liver disease severity global score were lower in INSTI-treated participants than in non–INSTI-treated HIV-infected participants (P = 0.05 and P = 0.04, respectively). Conclusions: HIV-infected and HIV-uninfected subjects undergoing bariatric surgery presented a similar profile regarding anthropometric measures, cardiovascular parameters, and comorbidities. However, INSTI-treated participants presented milder SCAT and liver alterations than non–INSTI-treated participants."
bf6187ceb62a015666774857ffec053d6e2e2ec5,
c0038af6384e8db5ba256e0f45c94f385657eac6,
f0287e846ea09dcdd519f870d5301ca90a9b8db9,
0673c6c09317a543f4787ac38bfb31f372fd6e1e,"Introduction: Patients with established coronary artery disease (CAD) are at very high risk for cardiovascular events. Methods: The DAUSSET study is a national, multicenter, non-interventional study that included very high-risk CAD patients followed by French cardiologists. It aimed to describe real-life clinical practices for low-density lipoprotein (LDL) cholesterol control in the secondary prevention of CAD. Results: A total of 912 patients (mean age, 65.4 years; men, 76.1%; myocardial infarction, 69.4%; first episode, 80.1%) were analyzed. The LDL cholesterol goal was 70 mg/dL in most cases (84.9%). The LDL cholesterol goal <70 mg/dL was achieved in 41.7% of patients. Of the 894 (98.0%) patients who received lipid-lowering therapy, 81.2% had been treated more intensively after the cardiac event, 27.0% had been treated less intensively and 13.1% had been maintained. Participating cardiologists were very satisfied or satisfied with treatment response in 72.6% of patients. Moderate satisfaction or dissatisfaction with lipid-lowering therapy was related to not achieving objectives (100%), treatment inefficacy (53.7%), treatment intolerance (23.4%) and poor adherence (12.3%). Conclusion: These real-world results show that lipid control in very high-risk patients remains insufficient. More than half of the patients did not achieve the LDL cholesterol goal. Prevention of cardiovascular events in these very high-risk patients could be further improved by better education and more intensive lipid-lowering therapy."
0740443b1601cfe193b6626fa7b91710178fced8,
0d20db93e011c89f5c9685032a35f7ac48578983,
2ccd91c607f391112d0299e377c548370bc4fb59,
834235e8233e270fca6d4e5745fa9a3b99c4a936,
97c6c26ff424c94ddeb4ff459cd6f755b5f1f8c8,
afb92db09063645319da891d2838ebe7591bb4d2,
b561522abf90b5d7e69ba4e4297200c5020781e1,
cce073c52508af04361fa986ff4b7ba8271f8293,
00feaadbf14981271bee5bcaa4168def5e65564b,
08ad68ae4fdb2eca4dd5987ce9c511a4fbe380cc,"
 
 
 Evaluation of right atrial cavities and right atrial (RA) remodelling in atrial fibrillation (AF) has been poorly studied.
 
 
 
 The aim of this study was to evaluate in AF, the role of three-dimensional (3D) transthoracic echocardiography (TTE) and strain to refine the evaluation of the RA according to evolution of cardiac rhythm ad mid-term follow-up.
 
 
 
 A complete 2D and 3D TTE was performed in patient hospitalized for AF. In addition to the usual parameters, RA parameters were specifically assessed: 3D RA end-systolic (ES) and end-diastolic (ED) volume, RA ejection fraction (EF) and the global longitudinal strain (GLS) of the RA. A complete clinical evaluation and electrocardiogram were performed at admission (M0) and 6 months after inclusion (M6) to determine the cardiac rhythm at follow-up.
 
 
 
 34 consecutive patients hospitalized for AF were analysed. At M0, there was no significant difference between 2D RA ES volume and the 3D RA ES volume. Three groups of patients were individualized according to cardiac rhythm at M0 and M6: AF at M0 and AF at M6 (AF-AF), AF at M0 and sinus rhythm (SR) at M6 (AF-SR), SR at M0 (spontaneous reduction before the admission ECG) and SR at M6 (SR-SR). At M0 echocardiography, in the AF-AF group and AF-SR in comparison with SR-SR group were as follow: the 3D RA ED volume was significantly higher (respectively 21.6 (18.2-26.6) and 26.9 (19.4-36.8) versus 11.3 (6.9-16.6) mL/m2 ; p = 0.0025), the 3D RAEF was significantly lower (42.2 (35.1-44.7) and 34.5 (24.1-38.6) versus 57.6 (53.4-62.2)%; p = 0.0105) and the RA GLS was also significantly lower (7.6 (6.5-10.5) and 9.1 (5.8-11.2) versus 26.6 (22.0-35.0)%; p = 0.0001) (Figure 1).
 
 
 
 3D ultrasound and strain could be useful tools for evaluating the anatomical and functional RA remodelling in AF, to be further validated in a larger ongoing study.
 Abstract P350 Figure 1
"
0bb97547d331d1538e507678a467e68794b8a8e0,
0ecb21026b1dfb6a44ec371626572003c5f0a9bb,
158b09ed5336a0cd7521dd1bba69580fbbaa7c3c,
3e0b3f9caa7a86b5cc79eaca7271d633b024c736,
43bd10f30c8055216e58914ddb4daf6322e6409b,
47d9d1e02bc7a765c24d422a08886ca7b946e583,
4fc05a8e79b1622917d5990274803a9eb4621c7f,
547079a1b3a1b42f9fcc8ddb6ad81cfd5393d25e,
5d44e8e58f0cc5424aeb351dc2a39baa49b86a1f,"
 
 
 Acute kidney injury (AKI) occurring after diuretic treatment initiation for acute heart failure (AHF) is a common phenomenon, with an incidence estimated between 20 and 50% of AHF hospitalizations. Previous studies found that persistent AKI is associated with poor prognosis. Treatment-induced hemoconcentration is associated with improved prognosis, but several definitions previously used are not suited for clinical practice. Transient AKI, with or without hemoconcentration, is of unsettled prognosis. We aim to determine the independent prognostic value of transient AKI, persistent AKI and hemoconcentration in the context of AHF hospitalization, using practical definitions.
 
 
 
 Data were obtained from the Greater Paris University Hospitals (GPUH) Clinical Data Warehouse. Patients hospitalized for AHF in various GPUH units were included. AHF hospitalization was defined as hospitalization with at least one AHF ICD-10 code and at least one recorded furosemide administration. Bumetanide is rarely used in GPUH hospitals hence it was not considered. AKI in a period of 14 days following first furosemide administration was defined based on KDIGO guidelines. Hemoconcentration was defined as an increase in serum proteins ≥ 5 g/l during the same period. Multivariate logistic regression was performed to determine which characteristics were predictive of AKI. Cox regression of 100 days all-cause mortality using multiple confounders was performed to determine the prognostic value of transient AKI (< 14 days), persistent AKI (≥ 14 days) and hemoconcentration. Patients with AKI upon hospital entry were excluded from regression analyses. AKI and hemoconcentration were treated as time-dependent covariates to adjust for immortality bias.
 
 
 
 Five hundred seventy nine patients were included. Among them, 529 had no AKI upon hospital entry and 513 had at least one recorded serum proteins and creatinine value following furosemide initiation. Median follow-up was 114 days. AKI in a period of 14 days following furosemide initiation occurred in 234 patients (40.4%). At baseline, patients in the AKI group more frequently suffered from chronic kidney disease or presented with clinical and echocardiographic signs of right heart failure. Independent predictors of AKI were arterial hypertension upon furosemide initiation (adjusted OR 1.86 [1.08 – 3.22]), elevated serum creatinine upon furosemide initiation (adjusted OR 1.07 [1.01 – 1.14] per 10 µmol/l increase) and initial intravenous administration of furosemide (adjusted OR 2.42 [1.39 – 4.29]). Death during follow-up occurred in 35% of patients in the AKI group compared to 21% in the non-AKI group (p < 0.001). In multivariate analysis, persistent AKI was independently associated with increased mortality in a period of 100 days following furosemide initiation (adjusted HR 2.31 [1.07 – 4.99]). Transient AKI was not significantly associated with mortality (adjusted HR 0.64 [0.34 – 1.19]). Hemoconcentration was independently associated with decreased mortality (adjusted HR 0.46 [0.27 – 0.79]).
 
 
 
 After furosemide initiation during hospitalization for AHF, persistent AKI (≥ 14 days) was independently associated with increased 100 days mortality. Hemoconcentration, using a definition suited for clinical practice (≥ 5 g/l increase in serum proteins), was independently associated with decreased 100 days mortality. No significant association was found between mortality and transient AKI (< 14 days). Those findings show that laboratory tests at a limited cost – serum proteins and creatinine – are helpful to evaluate treatment response and mortality risk during AHF. Prospective randomized controlled trials are needed to establish diuretic strategies based on both AKI and hemoconcentration.
"
6a466a6f313d6bced52844d0a512e975f84031f1,
6aa5da6f80f59db7610e46fed5a59958a489f6d8,"
 
 
 The severity of tricuspid regurgitation (TR) in patients with restored normal sinus rhythm (SR) after atrial fibrillation (AF) has been poorly assessed.
 
 
 
 Our study aimed to assess (1) right chamber remodelling and (2) TR severity in patients with AF who have had their rhythms restored to normal sinus.
 
 
 
 We prospectively evaluated 94 consecutive patients hospitalized for AF who received either ablation, direct current cardioversion, or pharmacological therapy. Patients were divided into two groups according to their cardiac rhythm at 6 months follow up (6M): restoration to SR (SR group, n=54), persistence of AF (AF group, n=40). TR vena contracta (VC), TR grade severity was divided into 4 grades using an integrated approach (0: none or trace; 1: mild; 2: moderate; 3: severe TR). Two dimensional (2D) end diastolic (ED) tricuspid annulus (TA) diameter in the apical 4 chambers view, three-dimensional (3D) indexed volumes (3D Vi) of the right atrium (RA) and right ventricle (RV) in end systole (ES) and ED were acquired using transthoracic echocardiography at admission and at 6M.
 
 
 
 At 6M, in the SR group a significant improvement in TR VC (Figure A) and TR grade (Figure B) were noted, whereas there was no differences in the AF group (0.41 vs. 0.42cm, p=0.24 for TR VC; 1.70 vs. 1.76, p=0.16 for mean TR grade). In the SR group a significant reduction in 3D ES RV Vi, 2D ED TA diameter, 3D ES and ED Vi of the RA (Table) were observed. Regression of TR VC was correlated with regression of right cavities parameters (ρ=0.47, p<0.001 for 2D ED TA diameter; ρ=0.34, p<0.005 for 3D ES RA Vi; ρ=0.33, p<0.005 for 3D ED RV Vi; ρ=0.29, p<0.005 for 3D ES RV Vi).
 
 
 
 Restoration of normal SR in patients with AF results in beneficial remodelling of right cavities at 6M of follow-up which were associated with a significant decrease in TR severity. Strategies for normal SR restoration in patients with AF and TR should be vigorously attempted.
 TR Evolution
 
 
 
 Type of funding source: None
"
6b0508468140d9fe2362d0b07c4a82abf584955f,
6fc0052a1034b301c8f24c65d9db33ee416a5ebb,"Variants in LMNA, encoding A-type lamins, are responsible for laminopathies including muscular dystrophies, lipodystrophies, and progeroid syndromes. Cardiovascular laminopathic involvement is classically described as cardiomyopathy in striated muscle laminopathies, and arterial wall dysfunction and/or valvulopathy in lipodystrophic and/or progeroid laminopathies. We report unexpected cardiovascular phenotypes in patients with LMNA-associated lipodystrophies, illustrating the complex multitissular pathophysiology of the disease and the need for specific cardiovascular investigations in affected patients. A 33-year-old woman was diagnosed with generalized lipodystrophy and atypical progeroid syndrome due to the newly identified heterozygous LMNA p.(Asp136Val) variant. Her complex cardiovascular phenotype was associated with atherosclerosis, aortic valvular disease and left ventricular hypertrophy with rhythm and conduction defects. A 29-year-old woman presented with a partial lipodystrophy syndrome and a severe coronary atherosclerosis which required a triple coronary artery bypass grafting. She carried the novel heterozygous p.(Arg60Pro) LMNA variant inherited from her mother, affected with partial lipodystrophy and dilated cardiomyopathy. Different lipodystrophy-associated LMNA pathogenic variants could target cardiac vasculature and/or muscle, leading to complex overlapping phenotypes. Unifying pathophysiological hypotheses should be explored in several cell models including adipocytes, cardiomyocytes and vascular cells. Patients with LMNA-associated lipodystrophy should be systematically investigated with 24-h ECG monitoring, echocardiography and non-invasive coronary function testing."
78b10b02363621b50e51889555ddf92eadf14f82,
78b10b02363621b50e51889555ddf92eadf14f82,
79d2fcee47306eae49531113730f985c4886e039,
970bb8d28f2b9fb007519d57d9981bb205dfc34b,
9d6cd4d07f768bf0f6f763895c7501db7f8da512,"BACKGROUND
An increased risk of cardiovascular disease (CVD) was reported in patients coinfected with human immunodeficiency virus (HIV) and hepatitis C virus (HCV), without identifying factors associated with atherosclerotic CVD (ASCVD) events.


METHODS
HIV-HCV coinfected patients were enrolled in the ANRS CO13 HEPAVIH nationwide cohort. Primary outcome was total ASCVD events. Secondary outcomes were coronary and/or cerebral ASCVD events, and peripheral artery disease (PAD) ASCVD events. Incidences were estimated using the Aalen-Johansen method. Factors associated with ASCVD were identified using cause-specific Cox proportional hazards models.


RESULTS
At baseline, median age of the study population (n=1213) was 45.4 (interquartile range [IQR] 42.1-49.0) years and 70.3% were men. After a median follow-up of 5.1 (IQR 3.9-7.0) years, the incidence was 6.98 (95% confidence interval [CI] 5.19-9.38) per 1000 person-years for total ASCVD events, 4.01 (2.78-6.00) for coronary and/or cerebral events, and 3.17 (2.05-4.92) for PAD ASCVD events. Aging (hazard ratio [HR] 1.06, 95% CI 1.01-1.12), prior CVD (HR 8.48, 95% CI 3.14-22.91), high total cholesterol (HR 1.43, 95% CI 1.11-1.83), high-density lipoprotein cholesterol (HR 0.22, 95% CI 0.08-0.63), statin use (HR 3.31, 95% CI 1.31-8.38), and high alcohol intake (HR 3.18, 95% CI 1.35-7.52) were independently associated with total ASCVD events, while undetectable baseline viral load (HR 0.41, 95%CI 0.18-0.96) with coronary and/or cerebral events.


CONCLUSION
HIV-HCV coinfected patients experienced a high incidence of ASCVD events. Some traditional cardiovascular risk factors were the main determinants of ASCVD. Controlling cholesterol abnormalities and maintaining undetectable HIV viral load are essential to control cardiovascular risk."
ba3b64717bb4a11b56895dbe8395ee7398f3e8b1,
c5940235c2812c4165d98365282a7b7e8380ac9d,"Background It is unclear whether HIV infection affects the long‐term prognosis after an acute coronary syndrome (ACS). The objective of the current study was to compare rates of major adverse cardiac and cerebrovascular events after a first ACS between people living with HIV (PLHIV) and HIV‐uninfected (HIV−) patients, and to identify determinants of cardiovascular prognosis. Methods and Results Consecutive PLHIV and matched HIV− patients with a first episode of ACS were enrolled in 23 coronary intensive care units in France. Patients were matched for age, sex, and ACS type. The primary end point was major adverse cardiac and cerebrovascular events (cardiac death, recurrent ACS, recurrent coronary revascularization, and stroke) at 36‐month follow‐up. A total of 103 PLHIV and 195 HIV− patients (mean age, 49 years [SD, 9 years]; 94.0% men) were included. After a mean of 36.6 months (SD, 6.1 months) of follow‐up, the risk of major adverse cardiac and cerebrovascular events was not statistically significant between PLHIV and HIV− patients (17.8% and 15.1%, P=0.22; multivariable hazard ratio [HR], 1.60; 95% CI, 0.67–3.82 [P=0.29]). Recurrence of ACS was more frequent among PLHIV (multivariable HR, 6.31; 95% CI, 1.32–30.21 [P=0.02]). Stratified multivariable Cox models showed that HIV infection was the only independent predictor for ACS recurrence. PLHIV were less likely to stop smoking (47% versus 75%; P=0.01) and had smaller total cholesterol decreases (–22.3 versus –35.0 mg/dL; P=0.04). Conclusions Although the overall risk of major adverse cardiac and cerebrovascular events was not statistically significant between PLHIV and HIV− individuals, PLHIV had a higher rate of recurrent ACS. Registration URL: https://www.clinicaltrials.gov; Unique identifier: NCT00139958."
c5c1db13261e1968661fdeaadfdcf49572247671,"
 Introduction:
 The natural history of tricuspid regurgitation (TR) and right heart chambers remodeling in patients with atrial fibrillation (AF) according to the cardiac rhythm at mid-term follow-up has been poorly assessed.
 
 
 Hypothesis:
 Restoration of sinus rhythm in AF patients is beneficial to the remodeling of right heart chambers and decrease in TR severity.
 
 
 Methods:
 We prospectively and serially evaluated 24 consecutive patients hospitalized for AF using three dimensional (3D) transthoracic echocardiography (TTE) at admission (M0) and every 6 months during a 2 years-follow-up (FU, M6, M12, M18, M24) (120 TTE exams). Patients were divided into two groups according to their cardiac rhythm at M24: restoration to SR (SR group, n=14) and persistence of AF (AF group, n=10). TR grade severity was divided into 4 grades using an integrated approach (0: none or trace; 1: mild; 2: moderate; 3: severe TR). 3D indexed volumes (3D Vi) of the right atrium (RA) and right ventricle (RV) as well as 3D tricuspid annulus (TA) area were analyzed in end systole (ES) and end diastole (ED).
 
 
 Results:
 Beyond 6 months of FU, the SR group had overall significantly lower 3D ES RA Vi, 3D ED RA Vi, 3D TA ES area and TR severity in comparison with AF group (Figure). There were no significant differences between the 2 groups with regard to 3D RV Volumes and 3D ED TA area.
 
 
 Conclusions:
 According to our results on both TR evolution and right heart cavities reverse remodeling, strategies aiming at SR restoration in patients with AF and TR should be broadly discussed.
 
 
 
"
dd7cfd82af16bda5e3c6849cde6d66fb56c3385b,"
 
 
 Acute kidney injury (AKI) frequently occurs after diuretic treatment initiation during acute heart failure (AHF). Treatment-induced hemoconcentration seems associated with improved prognosis. Transient AKI, with or without hemoconcentration, is of unsettled prognosis.
 
 
 
 We aimed to determine the independent prognostic values of transient AKI, persistent AKI and hemoconcentration in the context of hospitalized AHF.
 
 
 
 Data were obtained from our institution's Clinical Data Warehouse. Patients that visited our unit at least once were screened. All hospitalizations in our institution were examined (>30 hospitals). Inclusion criteria were: ≥1 hospitalization with ≥1 recorded furosemide administration and ≥1 AHF ICD-10 code. Only the first hospitalization fulfilling these criteria was considered. AKI during 1–13 days following first furosemide administration was defined based on Kidney Disease Improving Global Outcome guidelines. Hemoconcentration was defined as an increase in serum proteins ≥5 g/l during the same period. We performed multivariate logistic regression to determine which characteristics were predictive of AKI. We used Cox regression of 100-days all-cause mortality using several confounders to determine the prognostic values of transient AKI (lasting <14 days), persistent AKI (lasting ≥14 days) and hemoconcentration. To account for immortality bias, AKI and hemoconcentration were treated as time-dependent covariates.
 
 
 
 We included 579 patients in the study. Median follow-up was 114 days. AKI following furosemide initiation occurred in 234 patients (40.4%). Patients that experienced AKI more frequently suffered from chronic kidney disease (43.6% vs. 33%, p=0.01) or presented with right ventricular dilatation (12% vs. 6.7%, p=0.04). Independent predictors of AKI were arterial hypertension (adjusted OR: 1.86 [1.08–3.22]), elevated serum creatinine at baseline (adjusted OR: 1.07 [1.01–1.14] per 10 μmol/l increase) and initial intravenous furosemide (adjusted OR: 2.42 [1.39–4.29]). Death during follow-up occurred in 35% of patients in the AKI group compared to 21% in the non-AKI group (p<0.001). In Cox regression, persistent AKI was independently associated with increased mortality in a period of 100 days following furosemide initiation (adjusted HR: 2.31 [1.07–4.99]). Transient AKI was not significantly associated with mortality (adjusted HR: 0.64 [0.34–1.19]). Hemoconcentration was independently associated with decreased mortality (adjusted HR: 0.46 [0.27–0.79]).
 
 
 
 In the context of hospitalized AHF, AKI that developed 1–13 days after furosemide initiation and that lasted ≥14 days was independently associated with decreased 100 days survival. Hemoconcentration, using a clinically relevant definition, was independently associated with improved survival. These findings show that serum creatinine and proteins, routinely used and with limited cost, accurately stratify mortality risk during AHF.
 Kaplan-Meier curves
 
 
 
 Type of funding source: None
"
e9e9f728469246b31592e7da8e82f72f7262be75,
fa3d576181a5b6afddd141735d85fe20ad4c28a0,"
 
 
 Few studies have assessed the evolution of cardiac chambers deformation imaging in patients with atrial fibrillation (AF) according to cardiac rhythm outcome.
 
 
 
 To evaluate cardiac chamber deformation imaging in patients admitted for AF and the evolution at 6-month follow-up (M6).
 
 
 
 In forty-one consecutive patients hospitalised for AF two-dimensional transthoracic echocardiography was performed at admission (M0) and after six months (M6) of follow up. In addition to the usual parameters of chamber size and function, chamber deformation imaging was obtained including global left atrium (LA) and right atrium (RA) reservoir strain, global left ventricular (LV) and right ventricular (RV) free wall longitudinal strain. Patients were divided into three groups according to their cardiac rhythm at M0 and M6: AF at M0 and sinus rhythm (SR) at M6 (AF-SR) (n=23), AF at M0 and AF at M6 (AF-AF) (n=11), SR at M0 (spontaneous conversion before the first echocardiography exam) and SR in M6 (SR-SR) (n=7)
 
 
 
 In comparison with SR patients (n=7), at M0, AF patients (n=34)) had lower global LA reservoir strain (+5.2 (+0.4 to 12.8) versus +33.2 (+27.0 to +51.5)%; p<0.001), lower global RA reservoir strain (+8.6 (−5.4 to 11.6) versus +24.3 (+12.3 to +44.9)%; p<0.001), lower global LV longitudinal strain (respectively −12.8 (−15.2 to −10.4) versus −19.1 (−21.8 to −18.3)%; p<0.001) and lower global RV longitudinal strain (respectively −14.2 (−17.3 to −10.7) versus −23.8 (−31.1 to −16.2)%; p=0.001). When compared with the AF-SR group at M0 the AF-AF group had no significant differences with regard to global LA and RA reservoir strain, global LV and RV longitudinal strain (Table). Between M0 and M6 there was a significant improvement in global longitudinal strain of the four chambers in the AF-SR group whereas no improvements were noted in the AF-AF and SR-SR group (Figure).
 
 
 
 Initial atrial and ventricular deformations were not associated with rhythm outcome at six-month follow up in AF. The improvement in strain in all four chambers strain suggests global reverse remodelling all cardiac cavities with the restoration of sinus rhythm.
 Evolution of strain between M0 and M6
 
 
 
 Type of funding source: None
"
4b33ceb88184f073642d0850c8139c41fd9412e7,Introduction: Patients living with human immunodeficiency virus (PLHIV) who undergo percutaneous coronary intervention (PCI) have a substantial risk of reccurent ischemic events after ACS. Hypothes...
63589665f72811b4d7ab0d637dbc194578abf0d3,"BACKGROUND
An accurate estimation of the risk of life-threatening (LT) ventricular tachyarrhythmia (VTA) in patients with LMNA mutations is crucial to select candidates for implantable cardioverter defibrillator (ICD) implantation.


METHODS
We included 839 adult patients with LMNA mutations, including 660 from a French nationwide registry in the development sample, and 179 from other countries, referred to 5 tertiary centers for cardiomyopathies, in the validation sample. LTVTA was defined as a) sudden cardiac death or b) ICD-treated or hemodynamically unstable VTA. The prognostic model was derived using Fine-Gray's regression model. The net reclassification was compared with current clinical practice guidelines. The results are presented as means (standard deviation) or medians [interquartile range].


RESULTS
We included 444 patients 40.6 (14.1) years of age in the derivation sample and 145 patients 38.2 (15.0) years in the validation sample, of whom 86 (19.3%) and 34 (23.4%) suffered LTVTA over 3.6 [1.0-7.2] and 5.1 [2.0-9.3] years of follow-up, respectively. Predictors of LTVTA in the derivation sample were: male sex, non-missense LMNA mutation, 1st degree and higher atrioventricular block, non-sustained ventricular tachycardia, and left ventricular ejection fraction. In the derivation sample, C-index (95% CI) of the model was 0.776 (0.711-0.842) and calibration slope 0.827. In the external validation sample, the C-index was 0.800 (0.642-0.959) and calibration slope 1.082 (95% CI, 0.643-1.522). A 5-year estimated risk threshold ≥7% predicted 96.2% of LTVTA and net reclassified 28.8% of patients with LTVTA compared with the guidelines-based approach.


CONCLUSIONS
Compared to the current standard of care, this risk prediction model for LTVTA in laminopathies facilitated significantly the choice of ICD candidates.


CLINICAL TRIAL REGISTRATION
URL: https://www.clinicaltrials.gov. Unique Identifier: NCT03058185."
747bfc072a1f6e536e4c8a4b05c3dda96ea26951,
8d2828e760b2d2521dd7269a77e33742660a76e4,"We examined trends in the MI incidence and age at MI diagnosis among adults living with HIV-1 between 2000 and 2009, by comparison with the French MI registries, by gender. Age standardized incidence rates and standardized incidence-ratios (SIRs) were estimated for individuals included in the French hospital database on HIV (n = 71 204, MI = 663) during three periods: 2000–2002, 2003–2005 and 2006–2009. Median ages at MI diagnosis were compared using the Brown-Mood test. Over the study periods, the absolute rate difference and relative risks were higher in women than in men in 2000–2002 and 2006–2009, with respective SIRs 1.99 (1.39–2.75) and 1.12 (0.99–1.27) in 2006–2009. The trends were different for men and women with a decreasing trend in SIRs in men and no change in women. In both sexes, among individuals with CD4 ≥500/μL and controlled viral-load on cART, the risk was no longer elevated. Age at MI diagnosis was significantly younger than in the general population, especially among women (-6.2 years, p<0.001; men: -2.1 years, p = 0.02). In HIV-1-positive adults, absolute rate difference and relative risks and trends of MI were different between men and women and there was no additional risk among individuals on effective cART."
978dbc4c676d59a06b055cf1a3f61f8da951cffb,"
 
 
 People living with Human Immunodeficiency Virus (PLWHIV) under antiretrovirals have an increased risk of atherosclerotic cardiovascular disease (ASCVD) events. The risk factors associated with ASCVD events in this high risk population are various including traditional vascular risk factors and specific HIV-related factors. However their respective influence is questionable.
 
 
 
 Our aim was to determine the incidence of ASCVD events in a large cohort of PLWHIV and to identify the risk factors associated.
 
 
 
 We conducted a longitudinal observational cohort study of asymptomatic PLWHIV at high risk of ASCVD addressed to our preventive cardiovascular unit for non-invasive cardiovascular evaluation. The first ASCVD event was censored and included CV death, acute coronary syndromes, coronary and peripheral revascularizations (PCI or CABG or endarterectomy or limb procedures) and ischemic strokes.
 
 
 
 From January 2003 to December 2014, 763 consecutive asymptomatic PLWHIV were enrolled (mean age of 51.3±8.3 years, 87% men, 90% were free of known coronary artery disease, mean Left ventricular ejection fraction 60%). At baseline, traditional CV risk factors were as follow: 54% had dyslipidemia, 43% hypertension, 35% were active smokers, 22% had family history of CAD and 11% were diabetics. Statins were prescribed in 38% of the cohort, aspirin in 14%, clopidogrel in 14% betablockers in 14%, RAS blockers in 32%, Calcium channel blockers in 8%. At baseline, median duration of HIV seropositivity was 19.8 years (14.0–23.6), 94% were under ARV predominantly protease inhibitors (68%). Median CD4 cell count was 545/mm3 (404–745) and 92% had undetectable HIV viral load. During a median follow up of 5.8 years (3.7–8.7), 58 (7.3%) subjects had a first ASCVD event (incidence of 12.70 [9.78–16.51] per 1000 persons-years) including 5 cardiovascular deaths, 14 ACS, 20 coronary revascularizations, 13 peripheral vascular procedures and 6 strokes) with a median time of occurrence of 3.1 years (1.5–5.1). CV death (first and second ASCVD events) occurred in 8 patients (22%) after CV death related to malignancies (33%) but before deaths related to unexplained causes (21%), infectious disease (13%), liver disease (8%) and suicides (3%). Coronary events including coronary death, MI, and coronary revascularization occurred in 39 patients (5.2%); Incidence of 8.28 [6.00–11.43] per 1000 persons-years. Conventional multivariate Cox model shows that age and tobacco were the independent risk factors associated with ACSVD events [Hazard ratio (HR) 1.04, 95% CI 0.99–1.09, p=0.05 and HR 2.17, 95% CI 1.07–4.38, p=0.03].
 
 
 
 Traditional vascular risk factors (age and active smoking) are associated with the occurrence of ASCVD events predominantly coronary artery disease in our observational cohort of asymptomatic PLWIHV at high risk for ASCVD. Cardiovascular prevention including tobacco cease action is mandatory in the aging HIV population.
"
9a5ed56c964fe077004d2bb4ec1658fccf92aff3,"
 
 
 Several studies highlighted an increased risk of cardiovascular disease (CVD) in HIV-HCV co-infected patients without clearly identifying specific virologic factors associated with atherosclerotic CVD (ASCVD) events.
 
 
 
 Hence, we analyzed data collection from the French nationwide ANRS CO13 HEPAVIH cohort to determine the incidence of ASCVD events in HIV-HCV co-infected patients and the predictive factors associated with its occurrence.
 
 
 
 The French multicenter nationwide ANRS CO13 HEPAVIH clinic-based cohort collected prospective clinical and biological data from HIV-HCV co-infected patients followed-up in 28 different university hospitals between December 2005 to November 2016. Participants with at least one year of follow-up were included. Primary outcome was the occurrence of major ASCVD events (cardiovascular death, acute coronary syndrome, coronary revascularization and stroke). Secondary outcomes were total ASCVD events including major ASCVD events and minor ASCVD events (peripheral arterial disease [PAD]). Incidence rates were estimated using Aalen-Johansen method and factors associated with ASCVD identified with Cox proportional hazards models.
 
 
 
 A total of 1213 patients were included: median age 45.4 years [42.1–49.0], 70.3% men, current smoking 70.2%, overweight 19.5%, liver cirrhosis 18.9%, chronic alcohol consumption 7.8%, diabetes mellitus (5.9%), personal history of CVD 2.7%, and statins use 4.1%. After a median follow-up of 5.1 years [3.9–7.0], 44 participants experienced at least one ASCVD event (26 major ASCVD event, and 20 a minor event). Incidences for total, major and minor ASCVD events were of 6.98 [5.19; 9.38], 4.01 [2.78; 6.00], and 3.17 [2.05; 4.92] per 1000 person-years, respectively. Personal history of CVD (Hazard Ratio (HR)=13.94 [4.25–45.66]), high total cholesterol (HR=1.63 [1.24–2.15]), low HDL cholesterol (HR=0.08 [0.02–0.34]) and undetectable HIV viral load (HR=0.41 [0.18–0.96]) were identified as independent factors associated with major ASCVD events while cirrhosis status, liver fibrosis and HCV sustained viral response were not.
 Cumulative incidence of CV events
 
 
 
 HIV-HCV co-infected patients experience a high incidence of ASCVD events both coronary and peripheral artery diseases. Traditional CV risk factors are the main determinants of ASCVD whereas undetectable HIV viral load seems to be protective. Management of cholesterol abnormalities and controlling viral load are essential to modify this high cardiovascular risk.
 
 
 
 Agence Natoinale de Recherche sur le SIDA et les Hépatites virales
"
daf927f04467f2636c573466fc4dbf43d1291145,"Abstract Background HIV-infected individuals undergoing effective antiretroviral therapy (ART) present an increased risk of atherosclerotic cardiovascular disease. We identified serum metabolites associated with carotid intima-media thickness (c-IMT) and its evolution. Methods One hundred forty-three hydrophilic serum metabolites were measured by ultraperformance liquid chromatography coupled with high-resolution mass spectrometry in 49 HIV+ ART+, 48 HIV+ ART-naïve and 50 HIV-negative, age-matched, never-smoking male triads. Metabolites differentially altered between groups (“features”) were defined as having a Benjamini-Hochberg-adjusted P value <.05 from a t test and >0.25 log2 absolute mean fold change in metabolite levels. c-IMT was measured across 12 sites at inclusion in all individuals and at the carotid artery (cca) after a median of 5.1 years in 32 HIV+ ART+ individuals. The difference in c-IMT (cross-sectional analysis) and slope of cca-IMT regression/progression per year (longitudinal analysis) for each log10 (area) increase in metabolite level were estimated with linear regression. Results Compared with HIV-, metabolite features of HIV+ ART+ were increased N6,N6,N6-trimethyl-L-lysine and decreased ferulate and 5-hydroxy-L-tryptophan, whereas features of HIV+ ART-naïve were increased malate, kynurenine, 2-oxoglutarate, and indole-3-acetate and decreased succinate and 5-hydroxy-L-tryptophan. In HIV+ ART+ individuals, quinolinate and/or indole-3-acetate were positively associated with c-IMT (P < .03), cca-IMT (P < .03), and cca-IMT progression (P < .008). These associations were not observed in HIV+ ART-naïve or HIV-negative individuals. In HIV+ ART+ individuals, the metabolites xanthosine and uridine, from nucleotide metabolism, and g-butyrobetaine, from lysine/dietary choline degradation, were also positively or negatively associated with c-IMT and/or cca-IMT (all P < .01), but not its evolution. Conclusions In these highly selected HIV-positive ART-controlled males, 2 novel metabolites derived from tryptophan catabolism, indole-3-acetate and quinolinate, were associated with c-IMT and its progression."
ddefc519ae4997a666ca108b9e0772f8ba610c4f,
e05f7add72b7ecaf89f59344a69ab54a0a1167d5,
e727b76b1141507624e029e0dd7abe7f00cf88de,
edddc59a6ccb8a2830e366e8e33ec3f999fff1f7,
f302ae904255e3b6d63d8b4eeceff211020bb9e8,"BACKGROUND
The Data Collection on Adverse Events of Anti-HIV Drugs (DAD) study has reported an increased risk of cardiovascular diseases in people with human immunodeficiency virus who were exposed to darunavir (DRV) but not to atazanavir (ATV). Our objective was to evaluate associations between ATV or DRV exposures and the risk of myocardial infarction (MI) in a nested case-control study within ANRS-CO4 French Hospital Database on HIV (FHDH).


METHODS
Cases were individuals who had a first validated MI between 2006 and 2012. Up to 5 controls were selected at random with replacement among individuals with no history of MI, followed at the time of MI diagnosis, and matched for age and sex. Conditional logistic regression models were used to adjust for potential confounders (MI risk factors and HIV-related parameters) and for cumulative exposure to each antiretroviral drug (ARV).


RESULTS
Overall, 408 MI cases and 1250 controls were included: 109 (27%) cases and 288 (23%) controls had been exposed to ATV, and 41 (10%) cases and 107 (9%) controls had been exposed to DRV. There was no significant association between exposure to ATV (adjusted odds ratio [OR] = 1.54; 95% confidence interval [CI], .87-2.73) or DRV (adjusted OR = 0.51; 95% CI, .11-2.32) and the risk of MI.


CONCLUSIONS
In FHDH, exposures to ATV or to DRV were not significantly associated with the risk of MI, adjusting for complete ARV history, contrary to the analysis in DAD."
f5b11ce17a2e790a2a110f40d4806ce2e08bf46a,
f67de92d71f004104a0dd431cb243d623cba9bd0,"ABSTRACT Introduction: Efficient antiretroviral-treatment (ART) generally allows control of HIV infection. However, persons-living-with-HIV (PLWH), when aging, present a high prevalence of metabolic diseases. Area covered: Altered adiposity, dyslipidemias, insulin resistance, diabetes, and their consequences are prevalent in PLWH and could be partly related to ART. Expert opinion: At first, personal and lifestyle factors are involved in the onset of these complications. The persistence of HIV in tissue reservoirs could synergize with some ART and enhance metabolic disorders. Altered fat repartition, diagnosed as lipodystrophy, has been related to first-generation nucleoside-reverse-transcriptase-inhibitors (NRTIs) (stavudine zidovudine) and some protease inhibitors (PIs). Recently, use of some integrase-inhibitors (INSTI) resulted in weight/fat gain, which represents a worrisome unresolved situation. Lipid parameters were affected by some first-generation NRTIs, non-NRTIs (efavirenz) but also PIs boosted by ritonavir, with increased total and LDL-cholesterol and triglycerides. Insulin resistance is common associated with abdominal obesity. Diabetes incidence, high with first-generation-ART (zidovudine, stavudine, didanosine, indinavir) has declined with contemporary ART close to that of the general population. Metabolic syndrome, a dysmetabolic situation with central obesity and insulin resistance, and liver steatosis are common in PLWH and could indirectly result from ART-associated fat gain and insulin resistance. All these dysmetabolic situations increase the atherogenic cardiovascular risk."
2b3347100de30f249d3383da41a6f2cf329ec0e3,
8e30df8ffb9dd4054f706a51d037e3839235a601,"Abstract Background Superficial venous thrombosis (SVT) is common, but often perceived to be a non-serious condition. This pathology should not be overlooked as it can lead to complications that may require anticoagulation. We present a case of SVT complicated by pulmonary embolism (PE) revealing an unexpected cause Case summary A 41-year-old woman was admitted to the emergency department for chest pain and intense sudden pain of the left groin, revealing an extended great saphenous SVT associated with a PE. Further investigation showed that the thrombosis was caused by a sewing needle located between the superficial femoral artery and the femoral vein. Successful extraction was performed in a vascular surgery unit. Discussion Superficial venous thrombosis can be associated with deep venous thrombosis and PE, and can be caused by local inflammation, direct compression, and foreign bodies. These aetiologies should be investigated if no evident cause to SVT is found."
90cb16cd4b37c13cf3a37ad749b4e8df4ca3465e,
90cb16cd4b37c13cf3a37ad749b4e8df4ca3465e,
cfba0109bd18b8baa002b23ca01b1ae9259163a7,"We here report the case of a 35-year old man with HIV-1 but with no previous medical-surgical history hospitalized in Abidjan, Côte d'Ivoire, due to fever, cough, dyspnea, chest pain and unfolding of the aortic arch observed on chest x-ray a week after having started antiretroviral therapy (ART). CT angiography of the thoracic aorta showed overall, extended aortic ectasia with mural thrombus. Transesophageal echocardiography objectified type A ascending aortic dissection (Stanford classification). The diagnosis of tuberculosis was confirmed based on Mycobacterium tuberculosis culture isolation. Eight years after, the patient was still alive without surgical treatment and complained of intermittent chest pain. Blood pressure was stable with moderate renal failure. We here report a rare case of aortic aneurism dissection in an adult patient with tuberculosis infected with HIV-1 during immune reconstitution inflammatory syndrome."
09c7d5e65e5ea08829d56447009d3cdea575de2c,"&NA; This series of review articles outlines the complex cause of HIV-related cardiovascular diseases (CVDs) particularly the interactions of viral factors, complications of antiviral therapy such as metabolic derangement, and chronic systemic inflammation. These factors, directly stemming from chronic HIV infection, are important in the pathogenesis of HIV-related CVD. Addressing each issue has likely underpinned the improved morbidity and increased life expectancy enjoyed by patients in the modern era of HIV management. The global management of HIV-related CVD may, however, be simpler than previously imagined, as the disease likely follows a pathway shared by multiple systemic diseases. Other chronic systemic diseases, including diabetes, rheumatoid arthritis, and autoimmune disease, share numerous pathophysiological mechanisms with HIV and provoke similar cardiac complications. CVD risk management in patients living with HIV (PLHIV) may be optimized by drawing upon existing knowledge of chronic systemic diseases which may open up new concepts in treatment and address the current shortfalls in cardiovascular management of PLHIV."
1eef628eb9d841b81f4d6a83e721c84251f8d0c0,
2d0cf459b34f4b9f7274c94486cfe2c837c431db,"Objective: The study aims to assess the association between proprotein convertase subtilisin/kexin type 9 (PCSK9), a major regulator of LDL cholesterol (LDL-C) homeostasis, and HIV-related dyslipidaemia in a cohort of HIV-positive (HIV+) patients under protease inhibitors. Methods: Plasma PCSK9 levels were measured in 103 HIV+ patients before and after initiating protease inhibitor-based antiretroviral therapy (ART), and in 90 HIV-negative controls matched for age and sex. PCSK9 was measured by ELISA. HIV+ patients who were not virologically suppressed at follow-up or were on lipid-lowering therapy were excluded. Results: In HIV+ (median age 36 years; 77.7% men), PCSK9 levels did not increase after protease inhibitor exposure (median 14 months) (279.5 ng/ml before, 289.6 ng/ml after; P = 0.49) and were significantly elevated versus controls at all timepoints (adjusted P value before and after: <0.05). After protease inhibitor initiation, total cholesterol, LDL-C and HDL cholesterol levels increased, but LDL-C remained lower versus controls. At baseline, PCSK9 levels were positively associated with immunodeficiency and the severity of HIV disease [HIV-1 viral load (P = 0.01), CD4+ T-cell count <200/&mgr;l, P = 0.002], stage C HIV disease (P = 0.0002). In protease inhibitor-treated patients, PCSK9 levels were no longer associated with HIV-related factors but with total cholesterol (P = 0.0006), LDL-C (P = 0.01), HDL cholesterol (P = 0.01), triglycerides (P = 0.05) and glycaemia (P = 0.006). Conclusion: PSCK9 levels are elevated in HIV+ patients. In ART-naive patients, the relationship between PCSK9 levels and infection severity suggests an effect of HIV disease. After initiating protease inhibitor-containing ART in virologically suppressed patients, PCSK9 levels were associated with dyslipidaemia similar to controls."
337e4af4534f88ef4e16853275f8ac33ac2d86fc,"Aim
To explore platelet reactivity on dual antiplatelet therapy (DAPT) of acute coronary syndrome (ACS) patients infected with HIV.


Methods and results
Acute coronary syndrome patients infected with HIV (n = 80) were matched to ACS patients without HIV (n = 160) on age, sex, diabetes, and DAPT (aspirin 100%, clopidogrel 68%, prasugrel 31%, ticagrelor 1%). Platelet reactivity was evaluated after ACS (>30 days) by measuring residual platelet aggregation (RPA) to aspirin and to P2Y12 inhibitors with light transmission aggregometry (LTA), VerifyNow aspirin assay (ARU), and P2Y12 assay (PRU) and with the VASP platelet reactivity index (VASP-PRI). Proportion of patients with high residual platelet reactivity (HPR) was evaluated. HIV-infected ACS patients had higher levels of platelet reactivity in response to P2Y12 inhibitors (RPA: 23.8 ± 2.7% vs. 15.3 ± 1.3%; P = 0.001; PRU: 132 ± 10 vs. 107.4 ± 6.6; P = 0.04; and VASP-PRI: 45.2 ± 2.6% vs. 32.0 ± 2.0%; P < 0.001) and to aspirin (RPA: 3.6 ± 1.5% vs. 0.4 ± 0.1%; P = 0.004 and ARU: 442 ± 11 vs. 407 ± 5; P = 0.002) compared with non-HIV. HIV-infection was independently associated with increased platelet reactivity regardless of the test used (RPA: P = 0.005; PRU: P < 0.001 and VASP-PRI: P < 0.001) and a higher proportion of HPR (OR = 7.6; P < 0.001; OR = 2.06; P = 0.06; OR = 2.91; P = 0.004, respectively) in response to P2Y12 inhibitors. Similar results were found with aspirin. Protease inhibitors use was associated with increased platelet reactivity and higher rate of HPR.


Conclusions
Acute coronary syndrome patients infected with HIV have increased levels of platelet reactivity and higher prevalence of HPR to P2Y12 inhibitors and aspirin than non-HIV patients. These results could provide potential explanations for the observed increase risk of recurrent ischemic events in the HIV-infected population."
48d3e0bf796c76caccc35826b478d65063376ffe,
5643d8dc84dadb738b042cd420dd5a1abde3c1b6,
570f1b23a7cb0370938d87af52d904c362f97f45,
74bd741f238590a7d344c1359531541e378f2c31,
828745ff81de83ba554507692f3b910ae37f6a61,
85d25720f5d979cd72da1c3b82afb81af1cd3fb7,
a7866e12b718bd05c884e04a28066c31583e78ad,"&NA; Populations living with HIV who access effective antiretroviral therapies are ageing and thus facing chronic disease-related comorbidities. Cardiovascular disease is now a leading cause of morbidity and mortality in the HIV population as in the general population. The increased incidence of cardiovascular complications experienced by the HIV population is due to physiological aging and consequently the increased risk of hypertension, diabetes, and renal failure. Whether HIV itself is an additive and independent risk factor for cardiovascular disease (CVD) remains a central question. If and how HIV impacts the ageing process is an important and related question. The purpose of the present review is to highlight the risk of CVD in the ageing HIV population, particularly concerning atherosclerotic CVD (ASCVD) and heart failure, and to address effective CVD prevention in an aging HIV population at risk of poly-pharmacy."
ad8f5c70d169a605649b430bd93838307ebd177f,
d3e62f80d38211c293973198a19ed947010af4ef,
0defcb9a31dcb569808605a7960b69fec0f52b0a,
262ce73ee72b1c4d454ccc0ba944c74ef84a78b1,"
        
      "
3233bc48e7b05a682ce33e436549510795ec7a7e,
3646d27384ca3eca9b3f0e3179e72442c3f9cf09,
3bfa7a9f1cb77f4c75a867cf98eb5a4a00ee1008,
4c2f47b22bb99403688d49cbb24c114611af7766,
56f85f5365eaf5aa18f6d155e42530306dd9fed7,"Objective: We compared aortic stiffness between HIV-infected and HIV-uninfected individuals and examined the determinants of vascular aging during HIV infection. Methods: Aortic stiffness using carotid–femoral pulse wave velocity (cf-PWV) was evaluated cross-sectionally between HIV-infected individuals and uninfected controls frequency-matched for age and sex, and longitudinally in a subgroup of HIV-infected individuals. Determinants of elevated cf-PWV levels were assessed using logistic regression. Changes in cf-PWV levels during follow-up (mixed-effect linear regression) and risk factors for achieving cf-PWV below (Group 1) or above the median (Group 2) at last follow-up visit were evaluated only in HIV-infected individuals. Results: A total of 133 HIV-infected and 135 HIV-uninfected individuals (mean age: 47.7 ± 8.9 years, 91% men) were enrolled. Median cf-PWV at baseline was similar between HIV-infected individuals and controls [7.5 m/s (interquartile range = 6.7–8.4) vs. 7.5 m/s (interquartile range = 6.6–8.4), respectively; P = 0.64]. In multivariable analysis, only mean arterial pressure showed significant association with elevated cf-PWV in the overall population (P = 0.036). In HIV-infected individuals, elevated cf-PWV was associated with current smoking (P = 0.042), and nadir CD4+ T-cell count less than 200 cells/&mgr;l (P = 0.048). Ninety-one HIV-infected individuals were followed for a mean 7.6 ± 2.0 years. cf-PWV progression was associated with age (P = 0.018), mean arterial pressure (P = 0.020), and nadir CD4+ T-cell count (P = 0.005). Patients from Group 2 had higher baseline waist circumference, pulse pressure, and nadir CD4+ T-cell count less than 200 cells/&mgr;l. Conclusion: We observed no difference in aortic stiffness between HIV-infected and controls. Moreover, aortic stiffness aging was independently associated with past severe immunodeficiency, along with other traditional risk factors. Our results call for early antiretroviral initiation."
5a5cc81fde758d1b09bc7728943823286efdad3d,
5cb692ad6c7aa59c1187e07a7f1c017012c0b74c,"Autosomal dominant hypercholesterolemia (ADH) is a human disorder characterized phenotypically by isolated high-cholesterol levels. Mutations in the low density lipoprotein receptor (LDLR), APOB, and proprotein convertase subtilisin/kexin type 9 (PCSK9) genes are well known to be associated with the disease. To characterize the genetic background associated with ADH in France, the three ADH-associated genes were sequenced in a cohort of 120 children and 109 adult patients. Fifty-one percent of the cohort had a possible deleterious variant in LDLR, 3.1% in APOB, and 1.7% in PCSK9. We identified 18 new variants in LDLR and 2 in PCSK9. Three LDLR variants, including two newly identified, were studied by minigene reporter assay confirming the predicted effects on splicing. Additionally, as recently an in-frame deletion in the APOE gene was found to be linked to ADH, the sequencing of this latter gene was performed in patients without a deleterious variant in the three former genes. An APOE variant was identified in three patients with isolated severe hypercholesterolemia giving a frequency of 1.3% in the cohort. Therefore, even though LDLR mutations are the major cause of ADH with a large mutation spectrum, APOE variants were found to be significantly associated with the disease. Furthermore, using structural analysis and modeling, the identified APOE sequence changes were predicted to impact protein function."
7b53bd26deff6bd4624524613aacfef6f575d8b2,"Objective: We aimed to evaluate the determinants of aortic stiffness progression in HIV-infected individuals. Design and method: A prospective study (2005–2015) was conducted in 91 HIV-infected individuals addressed in our cardiologic center. Aortic stiffness was assessed using carotid-femoral pulse wave velocity (cf-PWV) with the Complior® device. The change in cf-PWV during follow-up was modeled using simple and mixed-effect linear regression models. Differences in cf-PWV during follow-up were modeled between covariables (age, waist circumference, current smoking, diabetes, systolic blood pressure, heart rate, mean arterial pressure, hypertension, CD4+ T-cell nadir) in univariable and then in multivariable models using the forward-stewise procedure. The variables associated with cf-PWV in univariable analysis that had a P value < 0.20 were included in the final model. Results: Ninety-one HIV-infected individuals free of cardiovascular disease were included in this longitudinal study (mean follow up 7.6 years). At baseline, the mean age of the cohort was 47.8 ± 7.8 years (95% were male) and median duration of HIV infection was 14.1 (8.5–17.3) years and 88% of the patients were virologically suppressed. At follow up several of the clinical characteristics had changed. Specifically, there was an observed increase in body mass index 24.4 vs. 23.5 kg/m2 (<0.001), waist circumference (WC) 90 vs 86 cm (p < 0.001), and in the prevalence of hypertension (47% vs. 32%, p < 0.001) and dyslipidemia (63% vs. 45%, p < 0.001) and a decrease in active smoking (19% vs. 37%, p = 0.003). Mean first cf-PWV was 7.5 m/s (6.6–8.2) and the second 8.2 m/s (7.2–8.8) (p < 0.001), representing an annual absolute increase in cf-PWV of 0.10 m/s per year. Overall 46 patients (51%) had either an increase in cf-PWV or persisted with high cf-PWV after 7 years. In the final multivariable regression model the changes in cf-PWV over time were associated with age (p = 0.003), SBP (p = 0.002) and consistently with the CD4+ T-cell nadir at the first visit < 200/mm3 (p = 0.002). Conclusions: Our results suggest that vascular aging in HIV-infected individuals is associated with past profound immunodeficiency, in combination with traditional risk factors like age and blood pressure."
911e251f3c7fe8286953080cead40f35cfcede87,
a6ed451b14ea4c604440bab19dc525c1dd673f90,
a70d34e096bce41581b8002df8ad4b70cdf03e44,
d684d4ec9735c08e264958b2bf4f5c7e888d4536,"Atrial fibrillation is associated with left atrial (LA) and left atrial appendage (LAA) thrombus (LAT) formation which leads to thromboembolic events with significant morbidity and mortality. Wirchow's triad described three pathophysiological mechanisms required for thrombus formation associating: abnormal changes in flow by stasis in LA, abnormal changes in vessel walls and abnormal changes in blood contituents. Transesophageal echocardiography (TEE) is the main tool for LAT detection. Cardiac computed tomography could be a non invasive alternative to detect LAT. Predictive factors of LAA thrombus are: clinical: age, blood hypertension, diabetes mellitus, medical history of stroke, myocardial ischemia, heart failure, CHASD2, CHA2DS2-Vasc, overweight; biological: BNP, D-dimer, hyperuricemia; echocardiography: LA dilatation, left ventricle dysfunction, impaired LAA function assessed by TEE. LAA thrombus prevalence ranges from 1.5% to 2% in patients undergoing efficient anticoagulation. Rate of resolution of LAT after four weeks in patients treated with vitamin K antagonist is 80%. Direct oral anticoagulant demonstrated efficiency in treatment for LAT, however these data are extracted from observational retrospective studies and case report. Two prospective studies which are not published aimed to assess efficiency and tolerance of rivaroxaban and dabigatran for LAT resolution."
06c057de7bea797cc6160b362e053c2674da575c,"Objective: This study aimed to assess the 24-hour blood pressure pattern of hypertensive (HTN) individuals living with HIV and quantify the major contributive risk factors of non-dipping. Although hypertensive cardiovascular complications are closely related to 24-hour blood pressure pattern, major gaps in current knowledge exist regarding this subject in HIV+ populations. Design and method: Data analysed was from the Register of cardiovascular Complications among people living with HIV (RECOVIH) which included 174 known hypertensive HIV+ individuals with 1 or more CV risk factors who underwent 24-hour Ambulatory Blood Pressure Monitoring (ABPM) in our cardiac centre. Multiple logistic regression with non-dipping as outcome quantified risk factors and their impact. Diagnostic criteria: Hypertension on ABPM: at or above 130/80 mmHg, systolic and/or diastolic Dipping: day to night systolic blood pressure decline at or above 10% Non-dipping: day to night systolic blood pressure decline less than 10% Results: The cohort had a mean age of 52 ± 7.5 years, was predominantly male (90%), and had a long duration of HIV and antiretroviral (ARV) exposure (14 ± 6 and 10 ± 4.5 years respectively). The cohort had a mean BMI of 24.5 ± 4, 27% were current smokers, 24% had clinical lipodystrophy (60% atrophic), 13% had diabetes, and 71% were under antihypertensive therapy (72% under renin angiotensin antagonists, 35% under beta-blockers, 24% under diuretics and 16% under calcium channel blockers). Non-dipping hypertension prevalence was 48% in RECOVIH. No statistically significant differences between dippers and non-dippers were observed for demographic, clinical and biologic parameters. No statistically significant differences between the two groups were detected for nocturnal heart rate decrease or duration of known HIV status, ART exposure, and HIV-related biologic parameters. Non-dipping was associated with age (p = 0.049; OR 1.04; 95%CI 1.0002–1.08), use of diuretics (p = 0.03; OR 0.36; 95%CI 0.14–0.88), known hepatitis B virus (p = 0.049; OR 4.09; 95%CI 1.005–16.64), and HTN on the ABPM (p = 0.04; OR 2.02; 95%CI 1.05–3.88). Conclusions: Non-dipping hypertension prevalence is high among RECOVIH's middle age subjects at moderate to high risk of cardiovascular disease. If and how this high frequency of non-dipping pattern is associated with worse cardiovascular prognosis needs further study."
180ca14160c7c2ef5471cdaba402ffcc2a3a54b2,
184d52e25f96901aa28b0483538bfe95a30a13e2,
29eeec88ffe89e8a055a008f597626670470d21a,
447e917bd69ff410052a6a877a878af44d082dae,
4aa250410c2c723cbfaf71e7661afaccea1c24e7,"Objective: This study aimed to determine the utility of 24-hour ambulatory blood pressure monitoring (ABPM) in a priori normotensive and known hypertensive people living with HIV by quantifying new hypertension (HTN), masked hypertension, uncontrolled BP, and white coat effect. Design and method: Data analysed was from the Register of cardiovascular Complications among people living with HIV (RECOVIH), including 263 HIV+ individuals with 1 or more CV risk factors who underwent 24-h ABPM in our cardiac centre. Diagnostic criteria: Elevated clinic BP: at or above 140/90 mmHg Elevated mean 24-h ABPM: at or above 130/80 mmHg, systolic and/or diastolic New hypertension: elevated clinic BP and/or elevated mean 24-h ABPM Masked hypertension: normal clinic BP and elevated mean 24-h ABPM Uncontrolled BP: elevated clinic BP and/or elevated mean 24 h ABPM, in known HTN White coat effect: elevated clinic BP and normal mean 24-h ABPM, in a priori normotensives. Results: The cohort had a mean age of 50.3 ± 7.7 years, was predominantly male (91%), had a long median HIV duration (15.3 years), and included 150 (57%) known HTN. In RECOVIH the prevalence of new HTN was 22% (n = 25), of which 50% masked hypertension diagnosed by 24-h ABPM solely. Uncontrolled HTN prevalence was 45% using clinic BP alone and 32% using 24-h ABPM alone. 24-h ABPM revealed that this masked uncontrolled HTN was frequently due to poor nocturnal BP control. White coat effect prevalence was not significantly different between the 2 groups (6.3% a priori normotensives vs. 9.3% known HTN, p = 0.37). HTN subjects were older, had higher BMI, and more frequently had a history of diabetes, coronary heart disease, and heart failure as compared to normotensives. Conclusions: Masked hypertension prevalence is high in RECOVIH, particularly among a priori normotensives. Suboptimal BP control is frequent among patients with treated and well-controlled clinic BP. Clinic BP monitoring alone is inadequate to diagnose HTN and assess true BP control because elevated nocturnal BP was frequent. These findings suggest ABPM should be more routinely used to diagnose HTN and confirm BP control in people living with HIV."
67742919e59e0681e313deb532de0eb0464f7a57,
7a2961f5dabb87cb69d7727e984081e87776e438,
95367cfd739dd35881db22cfe2b5de60133cc9f7,"Background The effect of statins on all-cause mortality in the general population has been estimated as 0.86 (95%CI 0.79-0.94) for primary prevention. Reported values in HIV-infected individuals have been discordant. We assessed the impact of statin-based primary prevention on all-cause mortality among HIV-infected individuals. Methods Patients were selected among controls from a multicentre nested case-control study on the risk of myocardial infarction. Patients with prior cardiovascular or cerebrovascular disorders were not eligible. Potential confounders, including variables that were associated either with statin use and/or death occurrence and statin use were evaluated within the last 3 months prior to inclusion in the case-control study. Using an intention to continue approach, multiple imputation of missing data, Cox’s proportional hazard models or propensity based weighting, the impact of statins on the 7-year all-cause mortality was evaluated. Results Among 1,776 HIV-infected individuals, 138 (8%) were statins users. During a median follow-up of 53 months, 76 deaths occurred, including 6 in statin users. Statin users had more cardiovascular risk factors and a lower CD4 T cell nadir than statin non-users. In univariable analysis, the death rate was higher in statins users (11% vs 7%, HR 1.22, 95%CI 0.53-2.82). The confounders accounted for were age, HIV transmission group, current CD4 T cell count, haemoglobin level, body mass index, smoking status, anti-HCV antibodies positivity, HBs antigen positivity, diabetes and hypertension. In the Cox multivariable model the estimated hazard ratio of statin on all-cause mortality was estimated as 0.86 (95%CI 0.34-2.19) and it was 0.83 (95%CI 0.51-1.35) using inverse probability treatment weights. Conclusion The impact of statin for primary prevention appears similar in HIV-infected individuals and in the general population."
9c090db9ef171739f76ab30cabf470b95633407c,
a22e2f3b3928368f5444365f19ebbf8dee629d3a,"Objectives:Increased risk of cardiovascular disease in patients infected with HIV has been attributed to immune activation, inflammation, and immunosenescence, all of which are linked to chronic immune activation by viral infections, particularly cytomegalovirus (CMV). Our aim is to evaluate the impact of these atherogenic markers in HIV-infected patients who never smoked. Design:Exposure-matched, cross-sectional study. Methods:In 59 HIV-infected individuals [n = 30 undergoing ≥4 years of antiretroviral therapy (ART); n = 29 never treated with ART] and 30 age-matched HIV-negative controls, we measured the level of activation and senescence, as well as the frequency of CMV-specific T cells, on peripheral blood mononuclear cells, while examining their association with carotid intima–media thickness. Partial correlations were adjusted for age, systolic blood pressure, and nadir CD4+ cell count. Results:The previously described roles of T-cell activation, CMV, and immunosenescence in the atherosclerotic risk of HIV-infected patients, as assessed by carotid intima–media thickness, were not apparent in our cohort of particularly ‘healthy’ HIV-infected never-smokers. Conclusion:In HIV-infected individuals at low cardiovascular disease risk, our data show that the increased risk of carotid atherosclerosis is not associated with immunological markers described to be associated with HIV disease progression."
b169260c8980f2a470563bc0c722d948e48a03e4,
d9829550773b522b6d09030aef11df82f559df6a,"AIM
This open-label, randomized, and multicentre trial tested the hypothesis that, on a background of aspirin, continuing clopidogrel would be superior to stopping clopidogrel at 12 months following drug-eluting stent (DES) implantation.


METHODS AND RESULTS
Patients (N = 1799) who had undergone placement of ≥1 DES for stable coronary artery disease or acute coronary syndrome were included in 58 French sites (January 2009-January 2013). Patients (N = 1385) free of major cardiovascular/cerebrovascular events or major bleeding and on aspirin and clopidogrel 12 months after stenting were eligible for randomization (1:1) between continuing clopidogrel 75 mg daily (extended-dual antiplatelet therapy, DAPT, group) or discontinuing clopidogrel (aspirin group). The primary outcome was net adverse clinical events defined as the composite of death, myocardial infarction, stroke, or major bleeding. Follow-up was planned from a minimum of 6 to a maximum of 36 months after randomization. Owing to slow recruitment, the study was stopped after enrolment of 1385 of a planned 1966 patients. Median follow-up after stenting was 33.4 months. The primary outcome occurred in 40 patients (5.8%) in the extended-DAPT group and 52 in the aspirin group (7.5%; hazard ratio 0.75, 95% confidence interval 0.50-1.28; P = 0.17). Rates of death were 2.3% in the extended-DAPT group and 3.5% in the aspirin group (HR 0.65, 95% CI 0.34-1.22; P = 0.18). Rates of major bleeding were identical (2.0%, P = 0.95).


CONCLUSIONS
Extended DAPT did not achieve superiority in reducing net adverse clinical events compared to 12 months of DAPT after DES placement. The power of the OPTIDUAL trial was however low and reduced by premature termination of enrolment.


CLINICALTRIALSGOV NUMBER
NCT00822536."
f71fb2ebdee8e20189a6afe32ef2ea8c7d946a6c,"Objective:Some intestinal microbiota-generated metabolites of phosphatidylcholine are recognized to be proatherogenic. As the HIV population is vulnerable to cardiovascular disease and can develop intestinal dysbiosis associated with systemic inflammation, we investigated the novel relationship between microbiota-derived metabolites of phosphatidylcholine and coronary atherosclerosis in HIV. Design/Methods:One hundred and fifty-five HIV-infected and 67 non-HIV-infected individuals without known history of cardiovascular disease were previously recruited to assess coronary plaque by computed tomography angiography. In the current study, we evaluate whether serum choline, trimethylamine (TMA), or trimethylamine-N-oxide (TMAO) levels are associated with plaque features. Results:Young, asymptomatic HIV-infected patients (age 47 ± 7 years) demonstrated significantly higher prevalence of plaque (53 vs. 35%, P = 0.01) and number of total plaque segments (1.8 ± 2.5 vs. 1.2 ± 2.2, P = 0.03) when compared with well matched noninfected individuals with similar comorbidities. TMA was significantly associated with calcium score (r = 0.22, P = 0.006), number of total (r = 0.20, P = 0.02) and calcified (r = 0.18, P = 0.03) plaque segments, and calcium plaque volume (r = 0.19, P = 0.02) and mass (r = 0.22, P = 0.009) in the HIV cohort only. In multivariate modeling among HIV-infected patients, TMA remained significantly associated with calcium score (P = 0.008), number of total (P = 0.005) and calcified (P = 0.02) plaque segments, and calcium plaque volume (P = 0.01) and mass (P = 0.007), independent of Framingham risk score. In contrast, there was no association of TMAO to coronary plaque features in either cohort. Conclusion:A link between TMA and atherosclerosis has not previously been established. The current study suggests that TMA may be a nontraditional risk factor related to the number of plaque segments and severity of calcified plaque burden in HIV."
04bfa33d7f6ff78c8c9535f2f8229335da8b3795,"Background While residual plasma viremia is commonly observed in HIV-infected patients undergoing antiretroviral treatment (ART), little is known about its subclinical consequences. Methods This cross-sectional study included 47 male, never-smoking, non-diabetic patients with ≥4 years of ART and controlled HIV-replication (HIV-viral load, VL <20 copies/mL for ≥1 year). Residual HIV-VL was measured using an ultrasensitive assay (quantification limit: 1 copy/ml). Patients were categorized as having detectable (D; 1-20 copies/mL, n = 14) or undetectable (UD; <1 copies/mL, n = 33) HIV-VL. Linear regression was used to model the difference in total carotid intima-media thickness [c-IMT, measures averaged across common carotid artery (cca), bifurcation, and internal carotid artery] and cca-IMT alone across detection groups. Multivariable models were constructed for each endpoint in a forward-stepwise approach. Results No significant differences were observed between viremia groups with respect to median ART-duration (9.6 years, IQR = 6.8–10.9), nadir CD4+T-cell (208/mm3, IQR = 143–378), and CD4+T-cell count (555/mm3, IQR = 458–707). Median adjusted inflammatory markers tended to be higher in patients with D- than UD-viremia, with differences in IL-10 being significant (p = 0.03). After adjustment on age, systolic blood pressure, and insulin resistance, mean cca-IMT was significantly lower in patients with undetectable (0.668 mm±0.010) versus detectable viremia (0.727 mm±0.015, p = 0.002). Cca-IMT was also independently associated with age and insulin resistance. Mean adjusted total c-IMT was no different between viremia groups (p = 0.2), however there was large variability in bifurcation c-IMT measurements. Conclusions Higher cca-IMT was observed in patients with detectable, compared to undetectable, HIV-VL in never-smoking ART-controlled patients, suggesting that residual HIV viremia may be linked to atherosclerosis."
4d41c6ece36d81912538dc2ea8dfe8ddf238b200,"We examined whether impaired kidney function is an independent risk factor for myocardial infarction in HIV-infected individuals without pre-existing coronary artery disease. The odds ratio for impaired kidney function fell from 1.22 (95% confidence interval 0.90–1.66) to 0.99 (95% confidence interval 0.69–1.41) after adjustment for cardiovascular risk factors and HIV-related parameters, with hypertension, high-density lipoprotein cholesterol, smoking and the CD4+ T-cell nadir as most influential confounders. In this setting, no association was found between impaired kidney function and the risk of myocardial infarction."
56949b174c6b1a0fde6fd36e304352c9b509c4aa,"There is concern about coronary heart disease (CHD) in the HIV-infected population using antiretrovirals. Across Europe and North America, this population is at a higher risk of myocardial infarction (MI) than the general population [1–3], although the difference seems to diminish in recent years [4]. Consequently, identifying HIV-infected individuals at risk for CHD, and more broadly for all atherosclerotic cardiovascular disease (ASCVD) events including CHD and stroke, is desirable albeit extremely challenging."
70b0a9c66c99b20da503d0a0361b4ded50a6cdef,
d094bf578e8f0fb672d073262716efad5631e7ba,"Primary tumors are a rare phenomenon, while car- diac metastasis is rather more frequent. Cardiac masses and metastasis are usually first detected by echocardiography, which allows to describe and detail the size, the insertion, the mobility, the relation with cardiac and non-cardiac struc- ture, and local invasion. Myocardial contrast echocardiogra- phy is able to improve tissue characterization of the masses. Transthoracic echocardiography evaluation is generally completed by cardiac magnetic resonance and computed tomography scanner, which give some important and addi- tional information on tissue characteristics and tissue pro- perty such as vascularity and fibrosis."
0d016722cd32b3532dcd13815e2d82ae94e57264,
18473acac3591085d06587802cfe44fe315ae2c8,"La prevalence de la fibrillation atriale (FA) est particulierement elevee chez le sujet âge. Les patients en FA sont exposes a une augmentation du risque embolique arteriel, en particulier du risque ischemique cerebral. Chez ces patients un traitement anticoagulant est recommande car reduisant ce risque des 2/3. Cependant, de nombreux patients eligibles au traitement anticoagulant ne sont pas traites, du fait de limitation liee aux antivitamines K, delai d’action retardee, metabolisme aleatoire, risques de saignements et necessite d’une surveillance biologique. Les nouveaux anticoagulants NACO developpes depuis quelques annees ont ete proposes comme alternative au traitement par warfarine. Cet article resume les donnees des differents essais rapportes a ce jour, comparant le dabigatran (inhibiteur direct de la thrombine), le rivaroxaban et l’apixaban (inhibiteurs du facteur Xa). Le dabigatran (150 mg × 2/j) est d’une efficacite superieure dans la reduction du risque ischemique cerebral et arteriel peripherique, comparativement a la warfarine (1,53 % versus 1,69 %, P < 0,001). Le risque de saignement majeur est similaire (3,32 %/an versus 3,57 %/an, respectivement, P = 0,32). Le rivaroxaban (20 mg/j en 1 prise) n’est pas inferieur a la warfarine dans la reduction du risque d’ischemie cerebrale et d’embolie arterielle peripherique (2,1 % versus 2,4 %, P < 0,001). Il n’y a pas de difference significative entre le rivaroxaban et la warfarine dans le risque de saignement majeur ou de saignement cliniquement pertinent (14,9 %/an versus 14,5 %/an, P = 0,44). l’apixaban (5 mg × 2/j) est superieur a la warfarine dans la prevention du risque ischemique cerebral et des embolies systemiques (1,27 % versus 1,60 %, P = 0,01). L’apixaban reduit de facon significative les saignements majeurs, comparativement a la warfarine (2,13 %/an versus 3,09 %/an, P < 0,001). Comparativement a la warfarine, la mortalite totale est numeriquement plus faible avec le dabigatran (P = 0,051), similaire avec le rivaroxaban (P = 0,15). Le taux de mortalite est plus faible sous apixaban que sous warfarine (3,52 % versus 3,94 %, P = 0,047). Ces 3 NACO (en attendant les donnees de l’etude ENGAGE avec l’edoxaban) reduisent de facon significative les hemorragies intracrâniennes, comparativement a la warfarine. Les NACO constituent donc une alternative a la warfarine dans differentes populations de patients en fibrillation atriale, du fait des risques moindres d’interactions, de saignement et de l’absence de necessite de surveillance biologique du traitement anticoagulant, en dehors de la necessite d’une surveillance de la fonction renale."
7355090ce64dcd781e2d1351d5e54690f383ce83,"Objectives:To determine whether the reported increased atherosclerotic risk among HIV-infected individuals is related to antiretroviral therapy (ART) or HIV infection, whether this risk persists in never-smokers, and whether inflammatory profiles are associated with higher risk. Design:Matched cross-sectional study. Methods:A total of 100 HIV-infected patients (50 ART-treated >4 years, 50 ART-naive but HIV-infected >2 years) and 50 HIV-negative controls were recruited in age-matched never-smoking male triads (mean age 40.2 years). Carotid intima–media maximal thickness (c-IMT) was measured across 12 sites. Pro-inflammatory [highly sensitive C-reactive protein (hs-CRP), resistin, interleukin-6, interleukin-18, insulin, serum amyloid A, D-dimer) and anti-inflammatory (total and high molecular weight adiponectin, interleukin-27, interleukin-10) markers were dichotomized into high/low scores (based on median values). c-IMT was compared across HIV/treatment groups or inflammatory profiles using linear regression models adjusted for age, diabetes, hypertension, and, for HIV-infected patients, nadir CD4+ cell counts. Results:Although adjusted c-IMT initially tended to be thicker in ART-exposed patients (P = 0.2), in post-hoc analyses stratifying by median HIV duration we observed significantly higher adjusted c-IMT in patients with longer (>7.9 years: 0.760 ± 0.008 mm) versus shorter prevalent duration of known HIV infection (<7.9 years: 0.731 ± 0.008 mm, P = 0.02), which remained significant after additionally adjusting for ART (P = 0.04). Individuals with low anti-inflammatory profile (median score) had thicker c-IMT (0.754 ± 0.006 mm versus 0.722 ± 0.006 mm, P < 0.001), with anti-inflammatory markers declining as prevalent duration of HIV infection increased (P for linear trend <0.001). Conclusion:Known HIV duration is related to thicker c-IMT, irrespective of ART, in these carefully selected age-matched never-smoking HIV-treated and ART-naive male individuals. Higher levels of anti-inflammatory markers appeared protective for atherosclerosis."
7b4a43a214a2225f19716e8da1821bfc95b97808,"Background: The ARCTIC trial failed to demonstrate improvements in outcomes with platelet-function monitoring and treatment adjustment for PCI

Aim: To analyze clinical outcome according to clopidogrel pharmacodynamic (PD) response using the VerifyNow assay just before PCI.

Methods: Among patients randomized in the monitoring arm of the ARCTIC trial (n=1213), 419 (34.5%) had high on-treatment platelet reactivity (HPR patients) before PCI and underwent treatment adjustment. We compared the occurrence of the composite primary end point (PEP) of death, MI, stent thrombosis, stroke, or urgent revascularization 1 year after stent implantation in patients presenting HPR to clopidogrel before PCI (n=419), non-HPR patients (n=794) and patients randomized in the conventional arm without PD evaluation (n=1227). Safety bleeding endpoints were also analyzed.

Results: HPR to clopidogrel was associated with higher age, female gender, higher bodyweight, diabetes and PPI use (all p <0.05). HPR patients underwent more aggressive antiplatelet strategy than non-HPR patients due to the study protocol, with significant more frequent use of GPIIbIIIa inhibitors (79.2% vs. 5.2%, p<0.01), clopidogrel LD (80.2% vs 3.5%, p<0.001) or prasugrel LD before PCI (3.3% vs 0.1%, p<0.001) as compared with non-HPR patients. High clopidogrel MD (150mg or more) (82.6% vs. 32.1%, p<0.001) or prasugrel 10mg MD (18.6% vs. 4.4%%, p<0.001) was also significantly more frequent in HPR patients at discharge as compared with non-HPR patients.

The PEP occurred in 34.6% of the patients with HPR who underwent treatment adjustment, in 34.6% of the non-HPR patients (HR 0.97 [0.80; 1.19], p= 0.8) and in 31.1% of the patients randomized in the conventional arm (HR 1.11 [0.91; 1.34] p= 0.31 and 1.14 [0.97; 1.33], p=0.11) for both comparisons). Despite using a much stronger antiplatelet strategies in HPR patients, major bleeding did not differ between HPR patients as compared with non-HPR patients (2.6% vs. 2.1%, respectively, HR 1.19 [0.56; 2.54] p= 0.65) or from patients randomized in the conventional arm of the ARCTIC study (2.6% vs. 3.3%, HR 0.78 [0.40; 1.53] p= 0.47).

Conclusion: In ARCTIC, the prognosis of patients with HPR before PCI leading to antiplatelet therapy adjustment did not differ from patients without HPR and without adjustment of therapy. Although, this subgroup analysis comforts the main finding of the study, the hypothesis that tailoring might have corrected impaired prognosis associated with HPR in this high-risk group of patients cannot be eliminated by the ARCTIC study."
9bddbf1101f5b6179a7a98da8e51258bda396db0,
9c6462b3ac5cb58c33106d07e6aad23059885b4e,
c7404d347c54ff175541c003177569f07246ecc7,"Objective—Inactivating peroxisome proliferator-activated receptor-&ggr; (PPAR&ggr;) mutations lead to a syndrome of familial partial lipodystrophy (FPLD3) associated with early-onset severe hypertension. PPAR&ggr; can repress the vascular renin–angiotensin system (RAS) and angiotensin II receptor 1 expression. We evaluated the relationships between PPAR&ggr; inactivation and cellular RAS using FPLD3 patients’ cells and human vascular smooth muscle cells expressing mutant or wild-type PPAR&ggr;. Approach and Results—We identified 2 novel PPARG mutations, R165T and L339X, located in the DNA and ligand-binding domains of PPAR&ggr;, respectively in 4 patients from 2 FPLD3 families. In cultured skin fibroblasts and peripheral blood mononuclear cells from the 4 patients and healthy controls, we compared markers of RAS activation, oxidative stress, and inflammation, and tested the effect of modulators of PPAR&ggr; and angiotensin II receptor 1. We studied the impact of the 2 mutations on the transcriptional activity of PPAR&ggr; and on the vascular RAS in transfected human vascular smooth muscle cells. Systemic RAS was not altered in patients. However, RAS markers were overexpressed in patients’ fibroblasts and peripheral blood mononuclear cells, as in vascular cells expressing mutant PPAR&ggr;. Angiotensin II–mediated mitogen-activated protein kinase activity increased in patients’ fibroblasts, consistent with RAS constitutive activation. Patients’ cells also displayed oxidative stress and inflammation. PPAR&ggr; activation and angiotensin II receptor 1 mRNA silencing reversed RAS overactivation, oxidative stress, and inflammation, arguing for a role of angiotensin II receptor 1 in these processes. Conclusions—Two novel FPLD3-linked PPARG mutations are associated with a defective transrepression of cellular RAS leading to cellular dysfunction, which might contribute to the specific FPLD3-linked severe hypertension."
e7065aeca4f2d96976717ede8ab6f94dbbbb456b,
f05528389450842b6ce1903beaeee4eeff906b19,
f18f75b615e0ff5047e66325dc6eb3f5f39367ce,"Understanding the pathophysiology of coronary heart disease (CHD) in the human immunodeficiency virus (HIV)– infected population with access to antiretroviral therapy (ART) has become an important focus of recent research. CHD is, in fact, an emerging complication for individuals infected with HIV and presents a challenge for physicians involved in the care of HIV-infected patients. Cardiovascular disease is currently the third leading cause of death in HIV-infected individuals, after AIDS-related complications and malignancies. CHD is now the leading cardiovascular cause of death and morbidities, far more than heart failure, in HIV-infected patients on ART [1], whereas congestive heart failure related to immunocompromise is the leading cause of death in countries with poor access to ART [2].Many factors have been associated with myocardial infarction in the HIV population, ranging from traditional risk factors (eg, hypertension, tobacco use, dyslipidemia, diabetes, family history of premature CHD, microalbuminuria, and chronic renal failure) [3] to infection with hepatitis C virus [3], genetic factors [4], HIV itself [5], chronic inflammation [6], and immune activation [7]. Little is known about CHD in HIVinfected women. In several studies on acute coronary syndrome in HIV-infected patients, the proportion of women varied from 3% to 19% (mean, 10%) [1]. These figures are similar to those reported for HIV-uninfected women from the general population in a North American registry, where women aged <50 years accounted for <10% of cases of myocardial infarction [8]. The age distributions of HIV-infected and HIV-uninfected populations are, however, very different; those in the HIV-infected population are 10 to 15 years younger. Data from a North American epidemiological study have shown that the relative risk (RR) of myocardial infarction is higher in HIVinfected women than in uninfected female controls, with an RR of 2.98 [9]. Similarly, a French epidemiological study reported that the standardized morbidity ratio (SMR) was 2.7 for HIV-infected women vs uninfected female controls [10]. Of note, both the RR and the SMR were higher in HIV-infected women than in HIV-infected men (RR 1.4, SMR 1.4 [9, 10]). In the Women’s Interagency HIV Study (primary prevention study), 12% of HIV-infected women had a highly predicted 10-year risk of CHD, similar to that in the HIV-uninfected women; the mean age of the women was 40 years [11]. These data indicate that there are unmet needs in HIV-infected women at risk for CHD. How do we identify these women, how do we prevent an acute event, and what mechanisms are involved in their elevated risk of myocardial infarction? Fitch et al, in their article published in this issue of the Journal, focus on subclinical coronary atherosclerosis in HIVinfected women without symptoms or history of cardiovascular disease [12]. Using computed tomographic angiography (CTA), the authors report, for the first time, an increased prevalence of specific noncalcified coronary plaques and increased immune activation (monocyte activation) in asymptomatic HIV-infected women, when compared with female controls. These noncalcified plaques are potentially more prone to rupture than calcified plaques and, therefore, more likely to cause an acute coronary event. Fitch et al also found that age, HIV infection, and immune activation (particularly Received and accepted 1 August 2013; electronically published 16 September 2013. Correspondence: Franck Boccara, Department of Cardiology, Saint Antoine Hospital, Univ-Paris 6, AP-HP, 184 rue du Faubourg St Antoine, 75571 Paris Cedex 12, France (franck.boccara@ sat.aphp.fr). The Journal of Infectious Diseases 2013;208:1729–31 © The Author 2013. Published by Oxford University Press on behalf of the Infectious Diseases Society of America. All rights reserved. For Permissions, please e-mail: journals. permissions@oup.com. DOI: 10.1093/infdis/jit511"
52a606dd290df56c45abd140a6c9a6e8bf783311,"Background: The CHA2DS2VASc score has been proposed as an additional tool in thromboembolic (TE) risk stratification, beyond the CHADS2 score, in patients with non-valvular atrial fibrillation (NVA..."
56e131160045ab2b613340b4762877d43b0a5ecd,
5c0b4bbea00268e88ebc2f33ea9f6e119576122a,"Les complications coronaires sont devenues la premiere cause de complication cardiovasculaire chez les patients infectes par le VIH dans les pays ayant au traitement antiretroviral. Il s’agit de la 4 e cause de deces chez ces patients. Le plus souvent, l’infarctus du myocarde survient chez des sujets jeunes de moins de 50 ans fumeurs et dyslipidemiques. L’impact des facteurs de risque traditionnel est reconnu, en particulier le tabagisme important. D’autres facteurs sont associes a ce risque accru par rapport a la population generale du meme âge. L’inflammation chronique, l’activation immune, l’anomalie de repartition des graisses et le role des antiretroviraux ont ete mis en avant dans le processus d’atherosclerose de ces patients. La prise en charge de ces complications est identique a la population avec une attention importante aux interactions medicamenteuses entre antiretroviraux et drogues cardiologiques en particulier statine. La prevention primaire doit maintenant devenir un reflexe de prise en charge de la part des medecins prenant en charge ces patients en identifiant ceux a haut risque."
61e9d4135b5c12b6d9924fd6f368b2a41cecc16d,
62d8f50503a1de068087c27e74adccab7e22f37c,
651a8d5b052eca118e4176a4b33a971e1ad37839,
7f45d748a1009756859374896412dab6e7676428,"BACKGROUND
Individuals infected by human immunodeficiency virus (HIV) have a higher risk of cardiovascular disease than the general population. The specific effects of virological and immunological parameters on the risk of myocardial infarction (MI) in HIV-infected individuals are debated.


METHODS
We conducted a nested case-control study within the French Hospital Database on HIV. Case patients (n = 289) were patients who, between January 2000 and December 2006, had a prospectively recorded and validated first MI. Up to 5 HIV-infected controls (n = 884) matched for age, sex, and clinical center were selected, at random with replacement, among patients with no history of MI. Conditional logistic regression models were used to identify predictors of the risk of MI.


RESULTS
Plasma HIV-1 RNA levels >50 copies/mL, a low CD4 T-cell nadir, and a high CD8 T-cell count were independently associated with an increased risk of MI, with respective odds ratios of 1.51 (95% confidence interval, 1.09-2.10), 0.90 (.83-.97) per log(2) unit, and 1.48 (1.01-2.18) for the highest tertile of CD8 T-cell counts (>1150 cells/mm(3)) compared with the lowest (≤760 cells/mm(3)).


CONCLUSIONS
Independently of cardiovascular risk factors and antiretroviral therapy, HIV replication, a low CD4 T-cell nadir and a high current CD8 T-cell count are associated with an increased risk of MI in HIV-infected individuals. This suggests new paths for interventions to diminish the risk of MI in HIV-infected patients."
a0f8089d52a73edbe80af6e8495355b2bf3c55a9,"Background: Transesophageal echocardiography (TEE) parameters (left atrial appendage, LAA thrombus or spontaneous echocardiographic contrast (SEC), aortic atheroma (AAA) are powerful markers of thr..."
a89fdbff3e00b148c2237855fec4b623eb2b5091,
d2c8037a41a280f062f1cbfb177328b678d17e6c,
dbf3e51c9887d376420b963259b28619b2906b76,
f5080fedac771aec5ff868db93bde9becf693881,
1001c26ed82d9285dd09e6dab0a5f3bf051078a9,
1a8d74f101f76ef6faab60d075cc9bd5e3d9e4b7,
29229f952cd91f2dcde5e1d9a94ed611eb633d74,
2b94f9334c0575f69421c3ba102f9ecaf50be3ec,
40a72e0df772dba894cd813d749aa0cd8f0f6562,
4229186fb78dd39d3fd39dd6c830236c1eac3270,
475c39301e8da02bff7411c6dbb1eea9e5d78056,
49f6ac2d1ee10b69ef7db01f668fdd197eb46f1d,
50f2a3a0aae093e1e7ac157653472d883b97ffaf,
66d59164277b4518f69596dce5a57dd69510189d,
6f5e9ecafcbc63bfbfa8fec1c26b3589366692cd,
856ff90a01f78ed773d5611ab88757818fa03184,
8c302ff1bf924f897b404eb52128ef8d84d643f4,Purpose: The PACS-HIV study was designed to determine the 3-year prognosis of acute coronary syndrome (ACS) in HIV-infected patients (HIV+) as compared to HIV-uninfected patients (HIV-) in a prospe...
978b231fc6639779d6977c4a36f7d70a673e356a,
b1379f7b33761eba66206c9eb2e1d96240f1b889,"AIMS
Natural history and prognosis of acute coronary syndrome (ACS) in HIV-infected patients remain to be determined. We sought to compare coronary risk factors, angiographic features, acute results of percutaneous coronary intervention, in-hospital outcomes, and pre-specified 1 year prognosis of HIV-infected and HIV-uninfected patients with ACS.


METHODS AND RESULTS
HIV-infected and HIV-uninfected patients with a first episode of ACS were matched for age (±5 years), sex, and type of ACS. The primary endpoint was the rate of major adverse cardiac and cerebral events (MACCE), comprising cardiac death, recurrent ACS, recurrent coronary revascularization, and stroke. Overall, 103 HIV-infected and 195 HIV-uninfected patients were enrolled (mean age 49.0 ± 9.4 years, 94% men). Coronary risk factors were well balanced, but HIV-infected patients more frequently used illicit drugs (23 vs. 6%, P = 0.001) and had higher triglyceride concentrations (246 ± 189 vs. 170 ± 139 mg/dL, P = 0.002) compared with HIV-uninfected patients. Angiographic features of coronary artery disease were similar (multivessel disease 41 vs. 39%, P = 0.96; ACC/AHA type culprit lesion ≥B2, both 77%, P = 0.83). At 1 year, the rate of occurrence of first MACCE did not differ between groups [hazard ratio (HR) 1.4, 95% CI 0.6-3.0]. Recurrent ACS was more frequent in HIV-infected patients (HR 6.5, 95% CI 1.7-23.9) with no difference in the rate of clinical restenosis.


CONCLUSIONS
These results suggest that the acute management of ACS in HIV-infected patients can routinely be the same as that of HIV-uninfected patients, but that specific secondary prevention measures are needed to alleviate the increased risk of recurrent ACS."
f66b76cee8e4ebc158ca20516b3ba6329e6a98f5,Purpose: The PACS-HIV study was designed to determine the 3-year prognosis of acute coronary syndrome (ACS) in HIV-infected patients (HIV+) as compared to HIV- in a prospective observational study....
2004e1ee7bb47fd510b6e658e9adec22fe32a12f,Background: The prevalence of masked arterial hypertension seems to be high in patients with Obstructive Sleep Apnea Syndrome (OSAS). MH has been associated with a higher risk of cardiovascular eve...
24283cc51a8ae4ce99b1e4245b9146885778f5ed,"MOTS CLÉS Syndrome coronaire aigu ; Angioplastie coronaire ; Cardiovascular disease, and particularly coronary artery disease-related morbidity, has become the fourth most common cause of death in HIV-infected patients in France [1] and the third most common cause in the United States [2]. Since the advent of a potent antiretroviral therapy in the mid 1990s, HIV infection has become a chronic rather than a fatal disease. The life expectancy of a young HIV-infected patient without hepatitis co-infection is today more than 35 years [3]. In developed countries, the causes of death in HIV-infected patients besides AIDS-related morbidity (still the primary cause of death usually related to late diagnosis of HIV) are similar to those of uninfected patients (e.g., non-AIDS neoplastic disease, cardiovascular disease, hepatitis). However, these complications tend to emerge earlier (around 10 years) than in the general population, suggesting an acceleration of ageing in patients infected with HIV. Premature and acceleration of atherosclerosis are, therefore, an emerging complication in HIV. Recently, Lang et al. [4] estimated the incidence of myocardial infarction in HIV-infected patients in comparison with the general population. The sexand age-standardized morbidity ratio was estimated as 1.5 (95% confidence interval [CI] 1.3—1.7]) overall, 1.4 (95% CI 1.3—1.6) in men and 2.7 (95% CI 1.8—3.9) in women. Similar results were observed in the North American population [5]. Of note is the young age at which myocardial infarc-"
24501fe72b68f410298482a74fa0fdbce3b7574f,
27ec776a954ce3e01f437e6debf8d1d13c5562da,
3d31530c3f9859cae31b8b99660376afc389f3ab,
448df7c3b02eaed7e264b60ca4997d652d46a5e5,"Background:HIV infection and its treatment with protease inhibitors, especially when boosted with ritonavir, can cause lipid disorders. Statins, with the exception of fluvastatin, pravastatin and rosuvastatin, interact with protease inhibitor metabolism via CYP450. Pravastatin is recommended for patients with protease inhibitor-associated dyslipidemia. Rosuvastatin is the statin most effective on low-density lipoprotein cholesterol (LDL-c) in non-HIV patients. Methods:HIV-1-infected patients treated with boosted protease inhibitor were randomized to receive either rosuvastatin 10 mg/day or pravastatin 40 mg/day for dyslipidemia (LDL-c >4.1 mmol/l and triglycerides <8.8 mmol/l). The percentage change in LDL-c, triglyceride and high-density lipoprotein-cholesterol levels, measured in a central laboratory, was determined after 45 days of statin treatment. Results:Eighty-eight patients were randomized and 83 took the study drugs, 41 rosuvastatin and 42 pravastatin. The median duration of prior antiretroviral treatment was 9 years. At baseline, the median LDL-c level was 4.93 mmol/l, the triglyceride level 2.29 mmol/l, and the high-density lipoprotein-cholesterol level 1.27 mmol/l. The median percentage changes in the rosuvastatin and pravastatin arms were −37 and −19% for LDL-c (P < 0.001), respectively, and −19 and −7% for triglycerides (P = 0.035), respectively. The change in the high-density lipoprotein-cholesterol level was not significantly different between the two arms. None of the four severe adverse events was attributed to the statins; in particular, there were no renal, hepatic or muscular events. Conclusion:Rosuvastatin 10 mg/day was more effective than pravastatin 40 mg/day on LDL-c and triglyceride levels in HIV-1-infected patients receiving a boosted protease inhibitor."
47e6fab865b046df2e8b8e32e403fa6f299c9239,
63932eeaa19945df64c2108689ec10eea5574519,
74ed9d5fc0f18fab01950af2177f42e46a7fb81d,"Abstract Objectives. The emergence of non-AIDS-related events in the HIV-infected population experiencing a longer life expectancy implies the implementation of a comprehensive approach of HIV clinical management through better access to care, prevention, and early diagnosis of co-morbidities. Methods. The Orchestra program is a computer-assisted HIV care and support tool implemented since December 2004 in the outpatient clinic of a University Hospital set in Paris, France. The intervention aims at improving access to HIV information care and support specifically targeted five areas of actions: cardiovascular risk factors; gynecological follow-up; anti-hepatitis B virus (HBV) vaccine coverage; sexuality and prevention of sexually transmitted infections; and compliance to antiretrovirals. The impact of this program was examined prospectively on a “before–after” basis after a two-year implementation. Results. In the two-year period, 1717 patients were regularly followed. The level of the database information significantly increased in time (low density lipoprotein (LDL) cholesterol and glycemia were informed in 74% of patients at inclusion versus 95% at two years, and 83% versus 97%, p<0.001, respectively). The number of targeted interventions was also higher. For eligible women, papanicolaou smears and mammography were prescribed in 52% of cases after intervention, versus 44% at inclusion, p=0.04 and 83% versus 50%, p<0.001, respectively. Indicators of care eventually improved significantly. Initially 72% non-adherent patients declared to be adherent after the intervention (p<0.001) and 67% of patients with initial LDL-hypercholesterolemia normalized their LDL level within two years (p<0.001). Conclusion. The Orchestra program has provided a unique opportunity to assess and improve prevention and management of co-morbidities in HIV patients."
98fd2212940c3643a6609efe1849956d9b2cf96d,
a4c52e39969f058fbaba1e8fb60f92887aa8ebdd,
b0f124181daafbef5e7830af060f0d48bcc2e90b,"Objective—To determine whether and how protease inhibitors (PIs) could affect vascular aging. Methods and Results—HIV therapy with PIs is associated with an increased risk of premature cardiovascular disease. The effect of ritonavir and a combination of lopinavir and ritonavir (for 30 days) on senescence, oxidative stress, and inflammation was evaluated in human coronary artery endothelial cells (HCAECs). These HCAECs were either cotreated or not cotreated with pravastatin or farnesyl transferase inhibitor (FTI)-277 or with 2 antioxidants (manganese [III] tetrakis [4-benzoic acid] porphyrin [MnTBAP] and N-acetyl cysteine). Senescence markers were evaluated in peripheral blood mononuclear cells (PBMCs) from HIV-infected patients under PI treatment. PIs induced senescence markers, prelamin A accumulation, oxidative stress, and inflammation in HCAECs. Senescence markers and prelamin A were also observed in PBMCs from HIV-infected patients under ritonavir-boosted PIs. Pravastatin, FTI-277, and antioxidants improved PI adverse effects in HCAECs. Senescence markers were lower in PBMCs from PI-treated patients cotreated with statins. Conclusion—PIs triggered premature senescence in endothelial cells by a mechanism involving prelamin A accumulation. Accordingly, circulating cells from HIV-infected patients receiving PI therapy expressed senescence markers and prelamin A. Statin was associated with improved senescence in endothelial cells and patient PBMCs. Thus, PIs might promote vascular senescence in HIV-infected patients; and statins might exert beneficial effects in these patients."
b8ee9616c3e585b770504ebc3fc60d0bf3937249,
beb5757a792d5833d019c7ccee2513c0013bce8c,"The incidence of myocardial infarction (MI) is lower in France than in English-speaking and northern European countries. We estimated the incidence of MI in the HIV-infected population in France, on the basis of the data from the FHDH-ANRS CO4 cohort, by comparison with the general population. The sex- and age-standardized morbidity ratio was estimated as 1.5 [95% confidence interval (CI) 1.3–1.7] overall, 1.4 (95% CI 1.3–1.6) in men and 2.7 (95% CI 1.8–3.9) in women."
ce7f33ee3c10807549ca043e61e5674c08b24fb3,"Adipose tissue redistribution occurred at first in HIV-infected patients about 15 years ago after initiation of combination antiretroviral treatment (ART) and the responsibility of drugs was rapidly considered. This lipodystrophic syndrome can associate lipoatrophy, affecting subcutaneous adipose tissue in priority with fat hypertrophy, in particular in the upper part of the body, and metabolic alterations, dyslipidemia and altered glucose tolerance with insulin resistance. The primary role of thymidine analogue reverse transcriptase inhibitors (tNRTI) in peripheral lipoatrophy has been clearly shown in vitro and in vivo, these drugs inducing a severe mitochondrial dysfunction and an increased oxidative stress together with fat inflammation leading to fat loss. In vitro and in vivo studies suggest that some protease inhibitors (PI) or non-NRTIs also exert adverse effects on adipocytes and could act in synergy to amplify the effect of tNRTI. While severe lipoatrophy is now less prevalent in HIV-infected patients, fat hypertrophy is frequently observed: a role for drugs from the different classes acting in synergy to induce fat hyperplasia and hypertrophy is suggested, with milder mitochondrial dysfunction but increased inflammation and activation of the cortisol system. In addition, it is now considered that long-term viral infection, even if controlled, could induce low-grade inflammation and prepare fat to the deleterious effect of ART. Both lipoatrophy and lipohypertrophy are involved in metabolic disorders and increased cardio-metabolic risk that likely participate to early aging reported in these patients. ART can also be directly responsible for metabolic alterations. Strategies to revert or reduce lipodystrophy are important to consider in these patients in addition to the required control of the metabolic disorders."
dcea0d7ecd578f159b77b08f63d368207af85f68,"BACKGROUND
The role of exposure to specific antiretroviral drugs on risk of myocardial infarction in human immunodeficiency virus (HIV)-infected patients is debated in the literature.


METHODS
To assess whether we confirmed the association between exposure to abacavir and risk of myocardial infarction (MI) and to estimate the impact of exposure to other nucleoside reverse transcriptase inhibitors (NRTIs), protease inhibitors (PIs), and non-NRTIs on risk of MI, we conducted a case-control study nested within the French Hospital Database on HIV. Cases (n = 289) were patients who, between January 2000 and December 2006, had a prospectively recorded first definite or probable MI. Up to 5 controls (n = 884), matched for age, sex, and clinical center, were selected at random with replacement among patients with no history of MI already enrolled in the database when MI was diagnosed in the corresponding case. Conditional logistic regression models were used to adjust for potential confounders.


RESULTS
Short-term/recent exposure to abacavir was associated with an increased risk of MI in the overall sample (odds ratios [ORs], 2.01; 95% confidence interval [CI], 1.11-3.64) but not in the subset of matched cases and controls (81%) who did not use cocaine or intravenous drugs (1.27; 0.64-2.49). Cumulative exposure to all PIs except saquinavir was associated with an increased risk of MI significant for amprenavir/fosamprenavir with or without ritonavir (OR, 1.53; 95% CI, 1.21-1.94 per year) and lopinavir with ritonavir (1.33; 1.09-1.61 per year). Exposure to all non-NRTIs was not associated with risk of MI.


CONCLUSION
The risk of MI was increased by cumulative exposure to all the studied PIs except saquinavir and particularly to amprenavir/fosamprenavir with or without ritonavir and lopinavir with ritonavir, whereas the association with abacavir cannot be considered causal."
e294fa2d2e2020821c0e4a71a59cd99f6b76c17b,"Background HIV-infected patients under antiretroviral therapy that includes HIV protease inhibitors (PIs) are prone to develop a complex metabolic syndrome including insulin resistance, lipodystrophy and hypertension. Whether hypertension and cardiovascular events could result from the adipocyte renin angiotensin system (RAS) overactivation has never been investigated. Methods Primary human adipocytes and 3T3-F442A murine adipocytes were incubated with lopinavir or atazanavir boosted with ritonavir, with or without the angiotensin II type-1 receptor (AT1R) blockers (ARBs), irbesartan or telmisartan, and the peroxysome proliferator- activated receptor-γ (PPAR-γ) regulators, rosiglitazone and GW9662. Adipose RAS activation and adipocyte functions were evaluated. Results The ritonavir-boosted PIs activated the adipose RAS in human and murine adipocytes as shown by the overexpression of AT1R protein, angiotensinogen messenger RNA and the amplified effect of angiotensin II on extracellular signal-regulated kinase 1/2 activity. ARBs prevented the PI effect on RAS activation (AT1R overexpression and signalling) and adipocyte functions (dedifferentiation, insulin resistance, oxidative stress and inflammation). Consistent with a role of PPAR-γ signalling in PI-induced RAS activation, the PPAR-γ agonist (rosiglitazone) normalized PI-induced AT1R overexpression and adipocyte dysfunction. Conversely, the PPAR-γ antagonist (GW9662) induced AT1R overexpression and reduced the beneficial effect of telmisartan on PI toxicity. Conclusions We report that two frequently prescribed PI combinations could activate the adipose RAS in cultured cells, in part through a PPAR-γ-dependant signalling pathway. Our data suggest a role for the adipose RAS in the development of hypertension in HIV-infected patients under PI treatment, and point out the potential use of ARBs to decrease PI adverse effects."
ef82df477fd141a837672123fb370a92dd23b675,
39709e611c84a4ab11d0a823a34550aecb8e8531,
5e3f6915adac107fcce5642f7324dc65e164df54,
9010b098e6e6ddeaaf7271c691a4458d1a220044,
9e4b04b267e80f5bd49ddd2d99ca6b4a71f8e80c,"AIMS
QTc interval prolongation and torsades de pointes have been reported in HIV-infected patients. Protease inhibitors (PIs) are suspected to contribute to this adverse reaction. However, many factors can prolong QTc interval. We examined factors influencing QTc duration in HIV-infected patients.


METHODS
Unselected HIV-infected patients (n = 978) were enrolled in this prospective, single-centre cross-sectional study. Variables related to infection and treatments were collected. A digital electrocardiographic record was recorded in each patient and QT interval duration was measured and corrected using both Bazett's (QTcB) and Fridericia's (QTcF) formula. Results were analysed with a multivariable linear model.


RESULTS
After excluding arrhythmias and complete bundle branch blocks, QT interval was measured in 956 patients. The mean (SD) QTcB was 418 ms (23) and QTcF was 405 ms (20). QTc was found prolonged (>450 ms in women and >440 ms in men) in 129 [13.5%; 95% confidence interval (CI) 11.5, 15.8] and 38 (4%; 95% CI 2.9, 5.4) patients using Bazett and Fridericia corrections, respectively. On multivariable analysis, incomplete bundle branch block, ventricular hypertrophy, signs of ischaemic cardiopathy, female gender, White ethnic origin and age were significantly associated with QTc prolongation. The only HIV variable independently associated with QTc prolongation was the duration of infection (P = 0.023). After adjustment, anti-HIV treatment, in particular PI (P = 0.99), was not associated with QTc prolongation.


CONCLUSIONS
Although PIs block in vitro hERG current, they are not independently associated with QTc interval prolongation. Prolonged QTc interval in HIV-infected patients is primarily associated with factors commonly known to prolong QT and with the duration of HIV infection."
cfe9c6f0ad1287c8aeeb6faab4ee3a5ac6a49b32,
d214e15337d3e0f28942639a5c02ed9cff619fd2,"Background Atrial fibrillation (AF) is associated with an increased risk of death and stroke compared to patients in sinus rhythm. The influence of an inflammatory state, as characterized by CRP le..."
e8418a54eb3bb8935bd16cf9a06510c27ea1e643,
0ab1a2c964546f97851bd7b1e713729e087988f5,
26f0d27b9a1f1380ef69f65d4bf65f1331b9013c,"Resting and exercise cardiac function, skeletal muscle oxygenation and whole-body aerobic exercise capacities were evaluated prospectively in cardiac symptom-free HIV+ men receiving antiretroviral therapies and in healthy controls matched for age, physical activity, smoking and body surface area. HIV+ patients showed resting cardiac dysfunction, altered cardiac responses to exercise and depressed exercise tolerance. Exercise stroke volume kinetics and muscle oxygenation were impaired in HIV+ patients, especially in those with resting diastolic dysfunction."
33641435f4aae0a6b446c6270461362e4c2e4f0e,
3ca155c58b21d2319c996eec082a72c9ead7fc78,
4065fc58a84427c040dc44a1b9b9b77ef1d940ae,
6e69ca22fffc22460d5c380120ced775176204b8,"Before the introduction of successful antiretroviral therapy (ART), cardiovascular complications in HIV-infected patients were largely those resulting from immunosuppression (e.g. myocarditis, pericarditis, tamponade). With the advent of ART, there has been a spectacular decrease in morbidity and mortality in HIV-infected individuals. However, alongside metabolic complications caused by ART such as insulin resistance, dyslipidemia and lipodystrophy syndrome have been observed, which potentially increase the risk of cardiovascular complications, in particular coronary artery disease. Whether HIV infection and ART are independent and individual coronary risk factors is still controversial. More and more data are available demonstrating that increasing the duration of exposure to ART, and in particular protease inhibitors, increases the risk of myocardial infarction. At the same time, chronic infection, inflammation and the disruption of immune balance as a result of HIV infection itself may have the potential to alter vascular structure and function. In this article, we will review cardiovascular complications in HIV-infected patients before and after the advent of ART, focusing on coronary artery disease, its diagnosis, prognosis and therapy."
90c53e4f9a9eadf39f544cef8d17bedc4615cf87,
91f96f2d221ca449034473be1026625e9b525648,
c0f34e53f94c24d4450529c0527e147e0ebf8598,"Coronary artery disease (CAD) is an emerging complication in HIV-infected patients treated with highly active antiretroviral therapy. Immediate results and long-term outcome after coronary artery bypass graft (CABG) have not been yet evaluated in this population. Between January 1997 and December 2005, we compared baseline characteristics, immediate results and clinical outcome [Major Adverse Cardiac Events (MACE): death for cardiac cause, myocardial infarction (MI), coronary revascularization] at 41 months in 27 consecutive HIV-infected (HIV+) patients and 54 HIV-uninfected (HIV-) controls matched for age and gender (mean age of the cohort, 49+/-8 years; 96% male) who underwent CABG. Cardiovascular risk factors were well-balanced and nearly identical in both groups. In HIV+ group, mean preoperative CD4 was 502+/-192/mm(3) compared with 426.2+/-152.6/mm(3) postoperatively (p=0.004) without clinical manifestations at follow-up. At 30-day, the rate of post-operative death, MI, stroke, mediastinitis, re-intervention was identical in both groups. At follow-up [median: 41-months (range: 34-60)], rate of occurrence of 1(st) MACE was higher in HIV+ group compared with HIV- group (11, 42% versus 13, 25%, p=0.03), mostly due to the need of repeated revascularization using percutaneous coronary intervention of the native coronary arteries but not of the grafts in the HIV+ group [9 (35%) versus 6 (11%), p=0.02]. CABG is a feasible and safe revascularization procedure in HIV+ patients with multivessel CAD. Immediate postoperative outcome was similar compared to controls. However, long-term follow-up was significantly different, due to an increased rate of repeated revascularization procedure in the native coronary arteries of HIV+ patients."
c4245ff0a8fe7dce0f8a7dd15469c29581fdff47,
c68561f3344c1a260393c240eaa9f6b320ef5f0e,"Tienda online donde Comprar Cardiovascular Disease in AIDS al precio 151,19 € de Boccara, Franck | Barbaro, Giuseppe, tienda de Libros de Medicina, Libros de Microbiologia y Enfermedades infecciosas - SIDA/HIV"
f45836d3059286c1fd62ac660ee9088c9d5fa263,
fdf388afe6f6767af6211e24909c60bb3f7e7d8d,
11cbfcaa9411e117e2a6a1f01d5d2d604e4d7634,"Plasma fibrin d-dimers (hereafter d-dimers) are generated when the endogenous fibrinolytic system degrades fibrin, as in venous thrombo-embolism (VTE) and they consist of two identical subunits derived from two fibrin molecules. Unlike fibrinogen degradation products, which are derived from fibrinogen and fibrin, d-dimers are a specific cross-linked fibrin derivative.1 Because 2–3% of plasma fibrinogen is degraded to fibrin, small amounts are detectable in the plasma of healthy individuals.1 d-dimer levels may be increased in any condition involving the formation and degradation of fibrin, such as VTE, pulmonary embolism, infections, cancer, surgery, cardiac or renal failure, acute coronary syndromes, acute non-lacunar stroke, pregnancy, and sickle cell crises. d-dimer levels increase linearly with age.1 The classic microplate enzyme-linked immunosorbent assay (ELISA) technique is considered the gold standard but it is not used as a routine emergency test. Fully automated techniques have been developed, such as the VIDAS test (ELISA method with a final detection in fluorescence), but also immunofiltration (membrane ELISA) techniques and instantaneous methods which allow a diagnosis within a few minutes at the expense of a semiquantitative approach.1

Levels of coagulative markers are increased in patients with atrial fibrillation (AF), including d-dimers, pro-thrombin fragments 1 + 2, and thrombin–antithrombin complexes, indicating abnormal thrombogenesis.2 d-dimer levels have been shown to be increased in AF, especially in patients having multiple risk factors … 

*Corresponding author. Tel: +33 1 49 28 28 86; fax: + 33 1 49 28 28 84. E-mail address : ariel.cohen{at}sat.aphp.fr"
5c50e717e01de86d57a1809ce8f075f761660a78,
850756d35ad63cbcabfccb11904b4729757ad067,
ccb6954210ab5889d0247b98d7284be900f829fb,"INTRODUCTION
Several studies performed before the introduction of highly active antiretroviral therapy (HAART) have shown that HIV-1 infection is an important cause of dilated cardiomyopathy. However, factors associated with the development of HIV-associated cardiomyopathy in developing countries are still debated.


OBJECTIVES
To assess the prevalence of dilated cardiomyopathy, diagnosed by echocardiography, in HIV-infected Rwandese patients not receiving HAART and the risk factors associated with its development.


METHODS
A sample of 416 HIV-infected african patients, without a previous definite history of cardiovascular disease, attending University hospitals in Rwanda, from January to December 2005, were included in a multicenter, observational, prospective, cohort study, with the collaboration of two European Clinical Centers (in France and in Italy). Clinical and laboratory tests along with echocardiographic examination were performed in all patients included in the study.


RESULTS
Out of 416 patients included in the study, dilated cardiomyopathy was documented by echocardiography in 71 (17.7%). By both univariate and multivariate analysis, low socio-economic status, estimated duration of HIV-1 infection, CD4 count, HIV-1 viral load, CDC stage B and C of HIV disease and low plasmatic level of selenium were factors significantly associated with the development of cardiomyopathy. Alcohol consumption and smoking were factors associated with the development of cardiomyopathy only by univariate analysis.


CONCLUSIONS
HIV-associated cardiomyopathy is a significant clinical problem in HIV-infected patients not receiving HAART in Rwanda. Early tracking of cardiomyopathy in African HIV-infected patients is therefore recommended. Before administering HAART, clinicians should be aware of a possible existing cardiomyopathy to ensure appropriate, comprehensive, and rational patient care."
cd5f4b05e9bad83cf7427cff47eb69100f6e908e,
f541cd4dd078aeda3727a9b66b3c82b4a102f2d9,
40199d192d7214d2c201ce7aa704b5b2fc60f078,"Forty-two pravastatin-treated HIV-positive patients and 42 sex, age, and smoking status-matched hypercholesterolemic HIV-positive patients not under lipid-lowering treatment were compared for differences in intima-media thickness (IMT) of the common carotid artery (CCA) and aortic stiffness. Pravastatin had no influence on carotid artery structure and function, or aortic stiffness. Age and body mass index were independent determinants of IMT of the CCA. Mean arterial pressure, age, duration of HIV infection and protease inhibitor exposure determined aortic stiffness."
40c346a38a764df02289a9e2e8c16f84ec589bd9,"The advent of new antiretroviral agents in 1996 dramatically reduced human immunodeficiency virus (HIV)-associated morbidity and mortality. Prevention and treatment of cardiovascular complications in HIV infected (HIV+) patients are a new and emerging challenge for physicians caring for these patients because of the prolongation of survival and long term complications of highly active antiretroviral treatment. Few data are available regarding the results of conventional treatment—that is, coronary revascularisation and management of the cardiovascular risk factors in HIV+ patients.1,2 In acute coronary syndromes, previous studies reported a high rate of major adverse cardiac events (MACE) and target vessel revascularisation (TVR) in HIV+ patients.3,4

The objective of this study was to compare the immediate results and long term prognosis after percutaneous coronary intervention (PCI) in HIV+ and non-HIV infected (HIV−) patients.

We conducted an observational study and clinical outcome analysis of 50 consecutive HIV+ patients undergoing PCI (January 2001 to December 2003). All patients underwent PCI during the same admission for ST segment elevation myocardial infarction (MI), non-ST segment elevation MI, unstable angina, and stable angina pectoris. Patients with cardiogenic shock at the time of admission were excluded. A control group of 50 consecutive HIV− patients who underwent PCI, matched for age and sex, were enrolled during the same period. The study was conducted according to the guidelines of the Declaration of Helsinki and approved by the local ethics committee.

Baseline demographic data, cardiovascular risk …"
53ac83f0a69d04a5ae3dff90a9fa49450b96dd1d,
6c1d0bc9c1011bc8dc0fe3904eadc21e9147f494,
75f5407f14c7dbc4124d4d045b17bbe31b906796,
8d91e21501643e86c7fcf8a277e626766720a883,"HIV-related abnormal fat redistribution, associated or not with metabolic anomalies, was first reported in 1997. A lipoatrophic aspect was characterized at an early stage when the first large cohorts were described. Elevated levels of triglycerides and total cholesterol, associated with decreased high-density lipoprotein cholesterol (HDLc) and increased low-density lipoprotein cholesterol (LDLc) were subsequently described in HIV patients receiving the HAART combination of antiretroviral agents [1–3]. The etiology of fat redistribution, hyperlipidemia and insulin resistance among patients with HIV infection has not been fully elucidated, and the syndromic association of these manifestations is the subject of regular debate and research [4]. The first case of coronary artery disease (CAD), published in 1998, followed by several case reports and series, thus, raised concerns about an emerging cardiovascular risk for patients. The prevention of CAD and the treatment of metabolic disorders therefore constitute new challenges in HIV medicine [5]."
b4e78eb006c585bcd5ef5b88c9e00c0f99527b0b,"Low molecular weight heparin (LMWH) are obtained through chemical or enzyme depolymerisation of unfractioned heparins (UFH). LMWHs present several advantages over UFH: they exhibit a smaller interindividual variability of the anticoagulant effect, they have a greater bioavailability, a longer plasma half-life and do not require monitoring of the anticoagulant effect. LMWH have restrictive indications in AF patients, cardioversion (II level C and TEE for ACC/AHA/ESC and 2C for ACCP guidelines) or use as a bridge therapy (IIB, level C for ACC/AHA/ESC). The ACE study (Anticoagulation for cardioversion using enoxaparin), showed a reduction, though not statistically significant, of 42% of the composite end point (embolic event, major bleeding and death) 2.8% under enoxaparin vs. 4.8 % under conventional treatment, relative risk 0.58, CI 95% 0.23-1.46). Other studies, using dalteparin, confirmed that an anticoagulant treatment using LMWH followed by warfarin was at least as good as conventional management. ACUTE II (Assessment of cardioversion using transesophageal echochardiography), a randomized multicenter trial, compared the efficacy and tolerance of enoxaparin (1 mg/kg every 12 hours) and UFH in 155 patients eligible for a TEE-guided cardioversion. These patients were administered LMWH or UFH for 24 hours before TEE or cardioversion. There were no significative differences regarding the incidence of the study end points, in particular stroke and bleeding, and no death occurred. HAEST (Heparin in acute embolic stroke trial), a randomized, placebo-controlled, double blind trial failed to show the LMWH superiority over aspirin in patients with acute ischemic stroke and atrial fibrillation. Finally, LMWH have been proposed as a bridge therapy in patients under chronic VKA prior to surgery or invasive procedures. This strategy resulted in a low rate of thromboembolic events and major bleedings."
baa57b1221cec8208129e81183ab74d735100e79,"There is a clear need for a large multicentre trial comparing the efficacy of the two available drug eluting stents, sirolimus and paclitaxel, in diabetic patients with multivessel disease"
ce5b5d6c9ae2d725f972bdc682e554515eee9bdf,
db27b2f384f6f9c42c364b75948baf564af5b29f,"s S115 Eur J Echocardiography Abstracts Supplement, December 2006 Conclusions: The presence of significant CAD in perimenopausal women can be suspected in women with mean CIMT values exceeding 1.05 mm, irrespectively of age. Additionaly, in the present study smoking was the only predictor of CAD in perimenopausal women."
dcc34713f4920dfc7ca9c635e7398feffb1f210a,"Published Online First 31 January 2006 . . . . . . . . . . . . . . . . . . . . . . . C oronary artery disease is a major cause of morbidity and mortality in diabetic patients. Coronary revascularisation, using percutaneous coronary intervention (PCI) and/or coronary artery bypass graft (CABG), has lower prognosis in terms of repeated interventions and mortality in diabetic compared with the nondiabetic population. In the late 1980s, the BARI trial (1988–1991) demonstrated that diabetic patients with three-vessel disease or two-vessel disease involving the proximal left anterior descending artery had greater survival with CABG compared with PCI (without stenting) (76.4% v 55.7%, p = 0.0011) even though at seven years there was no difference in the nondiabetic population. In a more recent study including patients with multi-vessel disease (ARTS 1) the rate of event-free survival of the diabetic patients remains better in the CABG group despite a systematic use of stenting in the PCI group. The increased risk of cardiovascular events after CABG in diabetic patients may be partly due to the higher prevalence of comorbidities including renal failure, congestive heart failure, and peripheral vascular disease. Therefore, the benefits of interventional cardiology in this high risk subgroup of diabetic patients remain to be proven."
23f2e9c46cb2e8a2d88e58867a7a0f8e1863eacd,
28107470b6b8502f22a3eae2d3492ffeb90abf8e,
426e5f578c74e87a72a9a56ab60e210fbbbfd521,
64ade82cde2c5cc7100cd7aaa7c3a9144f3e9e72,
8e3db7a5eb9ec5efeba3c2604e30ea562c97441b,
9514532372b4e6ebc85f846e58804fbf58bd3267,
afdd9d08316cfb1692376ffb8c1ef4ac3bf0ca64,Acute coronary syndromes (ACSs) and coronary artery disease are emerging complications in HIV‐infected patients on highly active antiretroviral treatment. The aim of this study was to determine the mid‐term prognosis of ACS in HIV‐infected patients.
e6a424afbb5c17ad31f5c6c060560ebf7f9bf354,"Atrial fibrillation, the most commonly encountered arrhythmia in clinical practice, is associated with substantial morbidity and mortality. Its incidence and prevalence are increasing, and it represents a growing clinical and economic burden. Recent research has highlighted new approaches to both pharmacological and non-pharmacological management. Pooled data from trials comparing antithrombotic treatment with placebo show that warfarin reduces the risk of stroke by 62% and that aspirin alone reduces the risk by 22%. Overall, in high-risk patients, warfarin was better than aspirin in preventing strokes, with a relative risk reduction of 36%, but the risk of major hemorrhage with warfarin was twice that with aspirin. Anticoagulation treatment needs to be tailored individually for patients on the basis of age, comorbidities, and contraindications. However, warfarin remains under-prescribed in clinical practice, for reasons related to patients (comorbidities) and physicians. The limitations of warfarin treatment have prompted the development of new anticoagulants with predictable pharmacokinetics that do not require as frequent monitoring. Ximelagatran, an oral direct thrombin inhibitor, was compared with warfarin in the SPORTIF program, which found both agents to be broadly effective in the prevention of embolic events, but observed abnormal liver function tests in 6% of patients on ximelagatran. Liver function monitoring during treatment is thus needed. Idraparinux, a factor Xa inhibitor administered by once weekly subcutaneous injections, is being evaluated in patients with atrial fibrillation. The ACTIVE trial is currently assessing the role of aspirin plus clopidogrel, compared with adjusted dose warfarin, in the prevention of vascular events in high-risk patients with atrial fibrillation. Angiotensin-converting enzyme inhibitors and angiotensin II receptor-blocking drugs interfere with atrial remodeling and show promise in atrial fibrillation, as suggested in the LIFE trial. Preliminary studies suggest that statins may reduce the risk of recurrence after electrical cardioversion. Finally, percutaneous methods for occlusion of the left atrial appendage are currently under investigation in patients at high risk of thromboembolism but with contraindications for chronic warfarin."
15af029cb3f21cd5d1ee0fddb4ed36dbf58f10cd,Patients with both diabetes mellitus and prior myocardial infarction are at particularly high risk for cardivascular mortality
5d9fcc332af2d70718eb8db2463011e57baff888,
0642ca490f7c9090d54700fc5f98ace0e1db016d,"La fibrillation auriculaire (FA) est le trouble du rythme cardiaque le plus frequent associe a une augmentation de la mortalite d’un facteur 2 et du risque thromboembolique d’un facteur 2 a 7. Le risque thromboembolique est principalement medie par la formation d’un thrombus intra-auriculaire depiste au cours de cette pathologie dans 12 a 26 % des cas. Le risque absolu d’accident vasculaire cerebral (AVC) varie entre 1 et 12 % en fonction de la presence ou non de facteurs de risque cliniques. L’echographie transthoracique (ETT), qui comprend une approche bidimensionnelle (2D) et un examen Doppler complet, est d’indication systematique chez tous les patients en FA. L’ETT, technique sure et rapide, donne acces a une etude exhaustive des structures et fonctions cardiaques, permettant de rechercher une etiologie sous-jacente et d’evaluer le risque de complications, en particulier thromboemboliques. Les dimensions de l’oreillette gauche, l’epaisseur parietale et la masse ventriculaire gauche, ainsi que la presence d’une dysfonction systolique ventriculaire gauche, sont des predicteurs independants de la survenue d’une FA. L’etude du flux transmitral, du flux veineux pulmonaire et les modalites recentes, telles le Doppler couleur en mode M du flux mitral et le Doppler tissulaire, donnent acces a l’evaluation des pressions de remplissage ventriculaire gauche. L’echographie transœsophagienne (ETO), realisee avec des sondes multiplans sur des endoscopes modifies, est une technique sure, dont la haute resolution spatiale donne acces a une etude precise des structures cardiaques posterieures, difficilement accessibles en ETT, en particulier les massifs auriculaires et les auricules, la cloison interauriculaire, les veines pulmonaires, ainsi que les segments ascendant et horizontal de l’aorte thoracique. Les sources potentielles thromboemboliques peuvent etre identifiees, en particulier thrombus et contraste spontane dans les massifs auriculaires, vegetations valvulaires, anomalies de la cloison interauriculaire et atherome protrusif de l’aorte thoracique. L’ETO dans la FA est indiquee principalement pour rechercher le marqueur du risque thromboembolique, en particulier anomalies auriculaires gauches (thromboses, basses velocites, contraste spontane dans l’oreillette et l’auricule gauches) et la presence d’un atherome aortique protrusif, complexe, a haut risque thromboembolique. L’ETO avant cardioversion est indiquee chez les patients ayant une FA recente, avec une indication de cardioversion rapide du fait d’une mauvaise tolerance chez les patients ayant un traitement anticoagulant non efficace ou a haut risque hemorragique et chez ceux a haut risque de thrombose auriculaire gauche ou d’accident thromboembolique arteriel. La prediction du succes a court terme et long terme inclut, outre l’âge, la duree de la FA, la presence d’une etiologie et le traitement anti-arythmique, des parametres echographiques tels la dilatation de l’oreillette et/ou de l’auricule gauches, la presence d’une dysfonction systolique ventriculaire gauche et les basses velocites dans l’auricule gauche mesurees en ETO. Des applications plus recentes sont en cours de validation : l’echographie intracardiaque pourrait orienter l’ablation par catheter des FA focales et detecter les complications, en particulier la thrombose sur catheter. Enfin, plus recemment, l’ETO a ete utilisee pour guider le positionnement correct des dispositifs d’occlusion percutanee de l’auricule gauche, indiques chez les patients ayant une contre-indication formelle au traitement anticoagulant au long cours et un risque thromboembolique eleve."
40f509ace2e3b2f3f2634ff4b12852c352b2ed11,"The advent of new antiretroviral agents dramatically reduced mortality and HIV-associated morbidity. In the highly active antiretroviral therapy (HAART) era, long-term side effects such as severe metabolic disorders and related acute cardiovascular complications including myocardial infarction, peripheral vascular disease and stroke have been reported. Prevention and therapeutics for cardiovascular complications in HIV-infected patients are a new and emerging challenge for physicians involved in HIV infection care because of the prolongation of survival and the long-term complications of HAART. In the present overview, we will discuss the incidence, pathophysiology, prevention and treatment of coronary heart disease and stroke in HIV-infected patients."
411808b080011f0f7b65532ed9fee8dbc9654be2,"The authors report 4 cases of acute coronary syndromes with increased troponine levels during junctional tachycardia in patients with angiographically normal coronary arteries. ST segment changes during junctional tachycardia have no predictive value for the detection of coronary artery disease. Increased troponine, a marker of myocardial cellular necrosis, is not a sign of coronary lesions. A disequilibrium between the increased metabolic and energetic requirements of the myocardium and decreased perfusion due to the tachycardia could explain this observation. The recommended management of these patients is not to perform coronary angiography initially in the absence of cerebrovascular risk factors, but rather to document myocardial ischaemia by a non-invasive method such as echocardiography or scintigraphy."
7c3466a3b5ca3fda6bdaea2365c3d7a7ed41bbdb,
abfb49477c8bb6936965a451c95151010c6d3d14,
f01648aa0a223cc53ca90445a00a25532709332e,"Behçet's disease is associated with cardiac complications which may affect all three cardiac layers in 1 to 6% of cases. Although pericardial and coronary disease are the most common, the myocardium may also be affected. The clinical presentation may be left ventricular dysfunction with signs of dilated cardiomyopathy. The cause of the left ventricular dysfunction is usually coronary artery disease but it can also be inflammatory, resulting in a myocarditis with normal coronary arteries. The authors report two cases of Behçet's disease with symptomatic left ventricular dysfunction presenting as dilated cardiomyopathy with normal coronary arteries in one of the cases. Recent echocardiographic studies suggest that the incidence of myocardial disease is underestimated in this pathology: 20 to 35% of patients with Behçet's disease but no cardiac symptoms had left ventricular diastolic dysfunction. A more attentive investigation of left ventricular diastolic function in these patients should enable earlier diagnosis of this complication."
7db23c992be67c9e961cf1fc011b220322b2870e,
9c3cd3514def07b04c6ac3faf4c26fa04119b16c,
ab94c241eea5a94b4c7af1f3a61cb98825f56a0a,"A previously unreported complication, acute left main coronary artery occlusion with anterior myocardial infarction, in a patient at low coronary risk under HIV protease inhibitors, is described. Severe premature coronary artery disease has been reported in young men receiving HIV protease inhibitors, usually associated with hypertriglyceridemia, hypercholesterolemia, glucose intolerance and lipodystrophy syndrome. Percutaneous transluminal coronary angioplasty and stent implantation were successfully performed."
e8078f076dadd745464fa5d9e4441b9e1e7dcf5e,"Depuis 1998, de nombreux cas de syndrome coronarien aigu (infarctus du myocarde et angor instable) ont ete rapportes chez des patients jeunes seropositifs pour le virus de l'immunodeficience humaine. Le traitement antiretroviral efficace incluant un inhibiteur de protease initie depuis 1996 a transforme le pronostic de cette maladie en augmentant l'esperance de vie et diminuant la morbidite de ces patients. Parallelement, des anomalies de la repartition des graisses (syndrome de lipodystrophie) et des troubles metaboliques secondaires aux traitements antiretroviraux ont ete decrit : hypertriglyceridemie, hypercholesterolemie, intolerance au glucose (diabete sucre) et hyperinsulinisme. La relation de causalite entre le virus de l'immunodeficience humaine, le traitement antiretroviral, les troubles metaboliques et l'apparition d'evenements cardiovasculaires est encore meconnue."
ff49ac8d8f7a7b43297def96cb5cf6c0bd07f28d,"Depuis 1998, de nombreux cas de syndrome coronarien aigu (infarctus du myocarde et angor instable) ont ete rapportes chez des patients jeunes seropositifs pour le virus de l'immunodeficience humaine. Le traitement antiretroviral efficace incluant un inhibiteur de protease initie depuis 1996 a transforme le pronostic de cette maladie en augmentant l'esperance de vie et diminuant la morbidite de ces patients. Parallelement, des anomalies de la repartition des graisses (syndrome de lipodystrophie) et des troubles metaboliques secondaires aux traitements antiretroviraux ont ete decrit : hypertriglyceridemie, hypercholesterolemie, intolerance au glucose (diabete sucre) et hyperinsulinisme. La relation de causalite entre le virus de l'immunodeficience humaine, le traitement antiretroviral, les troubles metaboliques et l'apparition d'evenements cardiovasculaires est encore meconnue."
0376d58ed815ace99d1c07944dd32206d01d4916,
39d42f26b1f120e11aea537e365eed847b6f64b0,"We report the first case of myopericarditis after triple vaccination against diphtheria, tetanus, and poliovirus in a young adult. He presented with fever, acute chest pain, and diffuse ST-segment elevation 2 days after vaccination. Two-dimensional echocardiography findings were normal. Endomyocardial biopsy showed interstitial edema with diapedesis of erythrocytes. Laboratory findings showed inflammatory syndrome and elevated circulating immune complexes. He recovered within a few days with high-dose aspirin treatment and was without complications at 3-month follow-up. We discuss the different hypotheses for infective or hypersensitivity myocarditis."
b2520033bff2693f3e066e60a9ad29283bcfa91a,"Although it has been demonstrated recently that in patients with atrial fibrillation, protrusive atheromatous plaques of the thoracic aorta (thickness 4 mm) and left atrial abnormalities such as thrombosis, spontaneous contrast and low atrial blood flow velocities carry an additional embolic risk, this has not yet been studied in atrial flutter. Out of 2493 patients undergoing transoesophageal echocardiography between September 1993 and December 1997, 271 consecutive patients in atrial flutter (N = 41) or fibrillation (N = 230) for over 48 hours, underwent transoesophageal echocardiography before cardioversion. Patients with atrial flutter were compared with those with atrial fibrillation. Their characteristics were comparable with respect to age (68 +/- 13 and 67 +/- 12 years respectively, p = 0.628), sex ratio (men 66 and 54% respectively, p = 0.212), previous thromboembolic disease (5 and 15% respectively, p = 0.126). The incidence of protrusive aortic atheroma (12 and 11% respectively, p = 0.919), of spontaneous contrast in the thoracic aorta (15 and 14% respectively, p = 0.847) were identical in both groups. The left atrium was significantly smaller (3.1 +/- 0.7 and 6 +/- 3 cm2 respectively, p = 0.001), spontaneous atrial contrast less frequent (17 and 37% respectively, p = 0.024) and the velocities of atrial emptying higher (47 +/- 10 and 30 +/- 10 cm/s respectively, p = 0.030) in patients with flutter compared with atrial fibrillation. There was no difference in left ventricular fractional shortening (30 +/- 10 and 33 +/- 13% respectively, p = 0.630), the presence of rheumatic valvular disease (5 and 12%, p = 0.301), left atrial diameter (43 +/- 7 and 45 +/- 8, p = 0.134), right atrial surface area (16 +/- 4 and 17 +/- 6 cm2, p = 0.384) or in intraatrial thrombosis (2 and 3%, p = 0.888) respectively. These results show a high prevalence of protrusive atheroma of the thoracic aorta both in atrial flutter and in atrial fibrillation, and fewer left atrial abnormalities in patients with flutter."
bea0e7b8403a13a0acc236dbf4a19bf1ddce31fa,"The aim of this study was to test the hypothesis that Doppler study of hepatic venous flow, reflecting right atrial pressures and right ventricular dysfunction, allows prediction of increased right atrial pressure and right ventricular dysfunction in patients with right ventricular infarction. The authors studied 30 patients (27 men, mean age 54 +/- 12 years) in sinus rhythm with acute inferior myocardial infarction who underwent right heart catheterisation and Doppler echocardiography including recording of regurgitant and hepatic vein flow within 48 hours of hospital admission. Hepatic venous flow was used to measure peak velocity and velocity time integrals (VTI) of the systolic (S), diastolic (D) and atrial (a) contraction waves. The fraction of systolic filling was calculated: VTI S/VTI S + VTI D. The pressure half-time of pulmonary regurgitant flow (PHT IP) was also measured. Using haemodynamic criteria (non-compliant right atrial pressure wave form or right ventricular end diastolic pressure/pulmonary capillary pressure > or = 0.8), patients were divided into two groups: Group 1: right ventricular infarction (VD+, N = 22). Group 2: no right ventricular infarction (VD-, N = 8). No correlation was observed between Doppler parameters of hepatic venous flow and haemodynamic data, in particular right atrial pressure and pressure wave form. Moreover, no statistically significant difference was observed between the two groups with respect to the Doppler parameters derived from hepatic venous flow. On the other hand, the results confirmed good diagnostic performance of Doppler analysis of pulmonary regurgitant flow: sensitivity 80%, specificity 83%, positive predictive value 94%, negative predictive value 55%. The authors conclude that, in patients with acute inferior wall infarction, Doppler analysis of hepatic venous flow does not allow assessment of right atrial pressure or of ischaemic right ventricular dysfunction."
b2c5ceef57b5bff115869c47a4a4eeb40c6c0203,"The potential additional embolic risk of protruding aortic plaques ≥ 4 mm and left atrial abnormalities such as thrombus, spontaneous echocardiographic contrast (SEC), low left atrial appendage velocity, recently has been shown in patients with atrial fibrillation (AF). However, the presence and potential role of transesophageal echocardiographic (TEE)‐detected protruding aortic plaques ≥ 4 mm have not been systematically evaluated in patients with atrial flutter. Among 2493 patients evaluated by TEE, 271 consecutive patients with atrial flutter (n = 41) and AF (n = 230) ≥ 2 days duration were included in the study. Clinical and echocardiographic characteristics in consecutive patients with atrial flutter were compared to those in patients with AF, especially atrial morphology and function and atherosclerotic disease of the thoracic aorta. Clinical characteristics of patients with atrial flutter and AF were similar with regard to age (68 ± 13 and 67 ± 12, P = 0.628), sex ratio (men, 66% and 54%, P = 0.212), and previous embolic events (5% and 15%, P = 0.126), respectively. The frequency of protruding atherosclerotic plaques ≥ 4 mm (12% and 11%, P = 0.919) and SEC (15% and 14%, P = 0.847) in the thoracic aorta was similar in patients with atrial flutter and AF. Left atrial appendage area was smaller (3.1 ± 0.7 and 6.0 ± 3.0 cm2, P = 0.001), left atrial appendage SEC was less frequent (17% and 37%, P = 0.024), and left atrial appendage emptying velocity was higher (47 ± 10 and 30 ± 10 cm/s, P = 0.030) in patients with atrial flutter as compared to those with AF. There was no difference between the two groups regarding left ventricular fractional shortening (30 ± 10% and 33 ± 13%, P = 0.630), rheumatic valvular disease (5% and 12%, P = 0.301), left atrial diameter (43 ± 7 and 45 ± 8 mm, P = 0.134), right atrial area (16 ± 4 and 17 ± 6 cm2, P = 0.384), left atrial SEC (39% and 53%, P = 0.124), or atrial thrombus (2% and 3%, P = 0.888) respectively. Our results point to the high prevalence of protruding atherosclerotic plaques in the thoracic aorta in patients with atrial flutter."
838808f19aaf3f4a82e896e0c0030b42712781ef,"La cardiologie interventionnelle a pris ces dernieres annees une place importante dans le traitement de l’infarctus du myocarde a la phase aigue. Cependant, les resultats satisfaisants obtenus initialement doivent etre temperes par l’existence d’un taux de restenose eleve. La mise en place d’une endoprothese coronaire dans ces circonstances hautement thrombogenes a ete initialement consideree comme etant contre-indiquee. Plusieurs series cliniques ont neanmoins permis d’etablir son innocuite. Trois etudes randomisees, comparant l’angioplastie a la phase aigue de l’infarctus du myocarde, avec et sans endoprotheses, ont ete recemment publiees (PAMI stent, FRESCO et GRAMI). Elles suggerent que celles-ci pourraient ameliorer le pronostic de ces patients, en particulier en reduisant la frequence des recidives ischemiques durant les premieres semaines suivant l’infarctus."
8542c95c6dd13db3a320cb2ba4b7df2a5cd70c03,"L'echographie, un principe simple base sur le couple emission-reception des ultrasons (la retrodiffusion acoustique), a donne acces de facon non invasive a l'etude quasi complete de l'anatomie et de la fonction cardiaque. Apres la naissance du mode M dans les annees 60, l'echocardiographie bidimensionnelle permet des les [...]"
a195037cea931c670ad12ef5ce8d9a17cf80ed31,
ceb297f6f507252f777b547a2ecb7e6d4162179c,"Myocarditis is a focalised or diffuse disease of the myocardium. The principal causal agents are viruses in Europe and North America and a parasite in South America (Chagas' disease). The prevalence of acute myocarditis is variable, related to the periodic cycle of viral epidemics. The diagnosis is difficult to establish because the clinical presentation is variable, ranging from asymptomatic forms to rapidly fatal acute congestive heart failure. The diagnostic tools suffer from lack of sensitivity or specificity. Endomyocardial biopsy, despite its low sensitivity, remains the reference investigation as it provides histological proof of the myocarditis. Myocardial scintigraphy with antimyosin antibodies has the advantage of very good sensitivity but with less specificity. The authors discuss the critical indications and limitations of each investigation."
d6fef6c2f9cf9f48a00b41f8dedf038d1c723598,
67e2360c37cffcf9673c2b748f9dd1aff4ef6bcb,
d49fa4c9f32a7f2bb50566ac1976493b9dc0a5f3,"Thrombocytopaenia is rare after cardiac surgery but carries a very high risk of bleeding and thrombotic complications. It is generally due to heparinisation but may be secondary to the surgical procedure itself (cardiopulmonary bypass, sepsis, platelet consumption by the prosthesis), or associated factors (massive blood transfusion, drug reaction, rare antiplatelet allo-immunisation). One case of post-transfusion thrombocytopenia with antiplatelet anti-HPA-la allo-antibodies with a favourable outcome with high dose polyvalent gammaglobulins is reported. The authors describe the diagnostic and therapeutic approaches to this problem."
d7cae7842f222976603f37a846c46cdc1d166778,"The increasing indications of dobutamine stress echo in the investigation of myocardial ischaemia, viability and evaluation of the prognosis of coronary artery disease has made this technique a tool of everyday clinical practice. The authors reviewed the results of 600 investigations in consecutive unselected including patients aged over 75. No significant difference was observed with respect to the causes of interruption of the test between patients aged less than 75 (521 patients) and those older than 75 (79 patients). Attaining the target theoretical maximal heart rate was the commonest reason for stopping the test (47 and 48% respectively). Ventricular arrhythmias were not more common (12 and 10% respectively). Twelve cases of ventricular tachycardia were observed, 8 of which were non-sustained; 9 led to interruption of the test. No cases of ventricular fibrillation were observed. A previous history of cardiac arrhythmias was not associated with a higher frequency of arrhythmia during the test (8% in those with a previous history, 4% in those patients without). Supraventricular arrhythmias were significantly more common in patients over 75 years of age (15 versus 8%, p = 0.046). Dobutamine stress echocardiography' is feasible in a population of unselected patients, including those over 75. Therefore, age does not represent a limitation to the extension of this investigation."
129c6637cdef5902b25b2f8fd4262253bb39d501,"A case of ""hyperalgesic pseudothrombophlebitis"" in an hemophilic child with acquired immune deficiency syndrome (AIDS) is reported. Known causes such as associated blood clotting defect, collagen vascular disease, infection and homocystinuria were ruled out. Five similar cases were reported in 1985, in homosexual American adult males with AIDS, in whom venography did not show evidence of venous occlusion. Recovery occurred spontaneously and progressively, while the patients were given an anticoagulant therapy. Despite our hemophilic patient did not receive any anticoagulant treatment, a similar favorable course was observed. This AIDS related syndrome has not been reported in children yet, and its pathogenesis remains unknown."
bac963b22f074dce7f8f12a5d295ecf73a53dcbd,A case of measles encephalitis of the delayed type in a 5 year-old girl is reported. The encephalitis occurred 6 months after a measles which supervened just after the 18th monthly reinduction treatment for acute lymphoblastic leukemia. The child died one month later. This measles inclusion body encephalitis (MIBE) was confirmed by the presence of intracellular inclusions in the brain cells visualized by electron microscopy. The evidence of viral related intracellular nucleocapsides was confirmed by in situ hybridization. These nucleocapsides were identical to those seen in subacute sclerosing panencephalitis.
ce0dd35980f90f9f44591cfdc45ab6fe1490d9e7,"Background Preterm birth is the leading cause of perinatal morbidity and mortality and is associated with adverse developmental and long-term health outcomes, including several cardiometabolic risk factors and outcomes. However, evidence about the association of preterm birth with later body size derives mainly from studies using birth weight as a proxy of prematurity rather than an actual length of gestation. We investigated the association of gestational age (GA) at birth with body size from infancy through adolescence. Methods and findings We conducted a two-stage individual participant data (IPD) meta-analysis using data from 253,810 mother–child dyads from 16 general population-based cohort studies in Europe (Denmark, Finland, France, Italy, Norway, Portugal, Spain, the Netherlands, United Kingdom), North America (Canada), and Australasia (Australia) to estimate the association of GA with body mass index (BMI) and overweight (including obesity) adjusted for the following maternal characteristics as potential confounders: education, height, prepregnancy BMI, ethnic background, parity, smoking during pregnancy, age at child’s birth, gestational diabetes and hypertension, and preeclampsia. Pregnancy and birth cohort studies from the LifeCycle and the EUCAN-Connect projects were invited and were eligible for inclusion if they had information on GA and minimum one measurement of BMI between infancy and adolescence. Using a federated analytical tool (DataSHIELD), we fitted linear and logistic regression models in each cohort separately with a complete-case approach and combined the regression estimates and standard errors through random-effects study-level meta-analysis providing an overall effect estimate at early infancy (>0.0 to 0.5 years), late infancy (>0.5 to 2.0 years), early childhood (>2.0 to 5.0 years), mid-childhood (>5.0 to 9.0 years), late childhood (>9.0 to 14.0 years), and adolescence (>14.0 to 19.0 years). GA was positively associated with BMI in the first decade of life, with the greatest increase in mean BMI z-score during early infancy (0.02, 95% confidence interval (CI): 0.00; 0.05, p < 0.05) per week of increase in GA, while in adolescence, preterm individuals reached similar levels of BMI (0.00, 95% CI: −0.01; 0.01, p 0.9) as term counterparts. The association between GA and overweight revealed a similar pattern of association with an increase in odds ratio (OR) of overweight from late infancy through mid-childhood (OR 1.01 to 1.02) per week increase in GA. By adolescence, however, GA was slightly negatively associated with the risk of overweight (OR 0.98 [95% CI: 0.97; 1.00], p 0.1) per week of increase in GA. Although based on only four cohorts (n = 32,089) that reached the age of adolescence, data suggest that individuals born very preterm may be at increased odds of overweight (OR 1.46 [95% CI: 1.03; 2.08], p < 0.05) compared with term counterparts. Findings were consistent across cohorts and sensitivity analyses despite considerable heterogeneity in cohort characteristics. However, residual confounding may be a limitation in this study, while findings may be less generalisable to settings in low- and middle-income countries. Conclusions This study based on data from infancy through adolescence from 16 cohort studies found that GA may be important for body size in infancy, but the strength of association attenuates consistently with age. By adolescence, preterm individuals have on average a similar mean BMI to peers born at term."
0d8fce3be6971e2f436a70852b412d2a6b678912,"Abstract Motivation DataSHIELD is an open-source software infrastructure enabling the analysis of data distributed across multiple databases (federated data) without leaking individuals’ information (non-disclosive). It has applications in many scientific domains, ranging from biosciences to social sciences and including high-throughput genomic studies. R is the language used to interact with (and build) DataSHIELD. This creates difficulties for researchers who do not have experience writing R code or lack the time to learn how to use the DataSHIELD functions. To help new researchers use the DataSHIELD infrastructure and to improve the user-friendliness for experienced researchers, we present ShinyDataSHIELD. Implementation ShinyDataSHIELD is a web application with an R backend that serves as a graphical user interface (GUI) to the DataSHIELD infrastructure. General features The version of the application presented here includes modules to perform: (i) exploratory analysis through descriptive summary statistics and graphical representations (scatter plots, histograms, heatmaps and boxplots); (ii) statistical modelling (generalized linear fixed and mixed-effects models, survival analysis through Cox regression); (iii) genome-wide association studies (GWAS); and (iv) omic analysis (transcriptomics, epigenomics and multi-omic integration). Availability ShinyDataSHIELD is publicly hosted online [https://datashield-demo.obiba.org/], the source code and user guide are deposited on Zenodo DOI 10.5281/zenodo.6500323, freely available to non-commercial users under ‘Commons Clause’ License Condition v1.0. Docker images are also available [https://hub.docker.com/r/brgelab/shiny-data-shield]."
2291521113b0089cc5a93ebf4c6e781748e6675d,
687107e9a83f757a9487ca2efa9164bc8e227847,
9574a306b786fcead5167d630923371a6daf13b6,"Optimizing research on the developmental origins of health and disease (DOHaD) involves implementing initiatives maximizing the use of the available cohort study data; achieving sufficient statistical power to support subgroup analysis; and using participant data presenting adequate follow-up and exposure heterogeneity. It also involves being able to undertake comparison, cross-validation, or replication across data sets. To answer these requirements, cohort study data need to be findable, accessible, interoperable, and reusable (FAIR), and more particularly, it often needs to be harmonized. Harmonization is required to achieve or improve comparability of the putatively equivalent measures collected by different studies on different individuals. Although the characteristics of the research initiatives generating and using harmonized data vary extensively, all are confronted by similar issues. Having to collate, understand, process, host, and co-analyze data from individual cohort studies is particularly challenging. The scientific success and timely management of projects can be facilitated by an ensemble of factors. The current document provides an overview of the 'life course' of research projects requiring harmonization of existing data and highlights key elements to be considered from the inception to the end of the project."
c2f7a379a01697d0f31c15b6d6bbe620062c50f7,
279f3ea571ad867cb6be0ff46faf3966932abd51,

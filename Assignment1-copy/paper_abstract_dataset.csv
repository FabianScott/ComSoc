PaperId,Abstract
4498804388944416d028e11b38f7756cf2d464b5,"SUMMARY Disregarding the widely used division of skull base into anterior and lateral, since the skull base should be conceived as a single anatomic structure, it was to our convenience to group all those approaches that run from the antero-lateral, pure lateral and postero-lateral side of the skull base as “Surgery of the lateral skull base”. “50 years of endeavour” points to the great effort which has been made over the last decades, when more and more difficult surgeries were performed by reducing morbidity. The principle of lateral skull base surgery, “remove skull base bone to approach the base itself and the adjacent sites of the endo-esocranium”, was then combined with function preservation and with tailoring surgery to the pathology. The concept that histology dictates the extent of resection, balancing the intrinsic morbidity of each approach was the object of the first section of the present report. The main surgical approaches were described in the second section and were conceived not as a step-by-step description of technique, but as the highlighthening of the surgical principles. The third section was centered on open issues related to the tumor and its treatment. The topic of vestibular schwannoma was investigated with the current debate on observation, hearing preservation surgery, hearing rehabilitation, radiotherapy and the recent efforts to detect biological markers able to predict tumor growth. Jugular foramen paragangliomas were treated in the frame of radical or partial surgery, radiotherapy, partial “tailored” surgery and observation. Surgery on meningioma was debated from the point of view of the neurosurgeon and of the otologist. Endolymphatic sac tumors and malignant tumors of the external auditory canal were also treated, as well as chordomas, chondrosarcomas and petrous bone cholesteatomas. Finally, the fourth section focused on free-choice topics which were assigned to aknowledged experts. The aim of this work was attempting to report the state of the art of the lateral skull base surgery after 50 years of hard work and, above all, to raise questions on those issues which still need an answer, as to allow progress in knowledge through sharing of various experiences. At the end of the reading, if more doubts remain rather than certainties, the aim of this work will probably be achieved."
f6bbd18210c5cde2c21867a413397305d17e4a69,
70c296f882f72490d8365afe78445594f0be28ba,"High resolution computed tomography (CT) is presently the most accurate technique to study the temporal bone. Nevertheless, there is no general agreement about its usefulness in pre-operative evaluation of chronic otitis media. Indeed, if we rule out some exceptions, CT is not fundamental for diagnosis which can often be obtained through an accurate otomicroscopy. The Otology Group in Piacenza applies the following absolute indications for pre-operative CT in chronic otitis media: 1) difficult otomicroscopy evaluation; 2) suspected petrous bone cholesteatoma; 3) dubious diagnosis; 4) suspect of malformations; 5) review of cases that had previously undergone mastoidectomy; 6) suspected intracranial complications and/or meningoencephalic herniation (in this case also a magnetic resonance imaging must be performed). With the exception of these specific conditions, pre-operative CT is useless in cases of simple chronic otitis. However, when a cholesteatoma is suspected, CT can provide the surgeon, particularly when inexperienced, useful, but not indispensable, informations. Pre-operative knowledge of these informations can allow a more accurate evaluation of the case, with a better planning of the surgical procedure, in order to ensure a more specific informed consent. Finally, the Authors point out the fact that surgeon must be able to interpret by his own the CT data to have a real advantage by this examination."
2a01a2a0cf47093b08e153cb2e763adbea8d4cb4,"The translabyrinthine approach has long been, and in some centers is still, considered inadequate for the removal of large acoustic neuromas (AN). Over the years, with experience, the original technique has been modified, extending the approach to what is now called the enlarged translabyrinthine (ET) approach. Applying these modifications, between April 1987 and February 2000, the Gruppo Otologico in Piacenza, Italy removed 132 ANs, 3 cm or larger, from the cerebello-pontine angle. These tumors accounted for 25.9% of the 510 cases of AN to undergo surgery during that period. Of the 132 cases only one patient died and the percentage of complications was very low, generally lower than analogous series published in the literature. Such complications were progressively reduced in time, leading to a significant reduction in the length of post-operative hospitalization: on the average the 8.8 days were reduced to 5.7 in the last 43 cases. Ipsilateral preoperative hearing, inevitably sacrificed using the ET approach, was already significantly compromised in more than 65% of the cases. On the basis of the present data, it can be asserted that tumor diameter does not in any way preclude the use of the ET approach in AN surgery, rather the reduced morbility and shorter post-operative hospitalization make it the approach of choice for large ANs."
a572b1aeb9c1acee86946aa14319e689c42c3d99,"Geniculate ganglion meningiomas are extremely rare lesions-only 14 cases have been reported in the literature. Two new cases of these tumors are described. On computed tomography and magnetic resonance imaging, both lesions appeared centered on the area of the geniculate ganglion, extending to the tympanic cleft and eroding the middle cranial fossa floor. The first case was treated through a middle cranial fossa approach. Because the tumor was so large in the second case, a subtotal petrosectomy was used. The authors review the literature to clarify the clinical and radiological characteristics of these tumors and their surgical treatment."
e9e4872f573a1b7b6e6a1be92b54fd885cbcea75,
79c9b6fec37bb295a4fffa9eef44097f2abc637e,"Generally the main objective of acoustic neurinoma (AN) surgery is to totally remove the tumor associated with minimum morbidity. Nevertheless, in some cases residual tumor fragments are intentionally or accidentally left in place. These residues can lead to new growth. The present study provides a retrospective analysis of 14 cases of residual AN have undergone surgery at the Otology Group in Piacenza from 1987 to 1999. All these patients had previously undergone at least one retrosigmoidal exeresis although only one had been performed at the Otology Group. All patients except 1 were affected by anacusia at the time of surgery. The list of post-operative deficits included 2 hemipareses, 3 irreversible facial paralysis with consequent corneal opacity in 2 cases, 1 dysmetria and 1 paralysis of the abducent nerve. The patients had also undergone the following additional treatments: 1 emergency revision to drain a cerebellar hematoma, 3 ventricle-peritoneal derivations, 1 double application of stereotactic radiotherapy and 2 surgical procedures for facial plasty. Ten cases underwent the revision surgery at the Otologic Group using a translabyrinthine approach and 4 using a transcochlear approach. Tumor removal was deemed complete in all cases. The sole post-operative complications were a subcutaneous hematoma at the point where abdominal fat was removed and a temporary paralysis of the abducent nerve. Post-operative hospitalization was an average of 6.9 days. Analysis of the results showed that AN must be operated at selected centers in order to reduce the post-operative neurological deficit and the percentage of residual tumor. It also indicated that the retrosigmoid approach has a higher risk of accidentally leaving tumor residues than the other approaches. Finally, in the presence of a residual AN, the translabyrinthine approach offers the greatest advantages."
9c983b903a82b6f8c08c9f36d14ff6e8f7c539e4,"A case of a Jehovah's witness affected by an intracanalicular vestibular schwannoma with an extremely fast growth rate is presented. Nine months after presentation, the tumor reached 23 mm in the cerebellopontine angle. A partial removal through a retrosigmoid approach was planned. Because of the presence of a dominant high jugular bulb masquering the internal auditory canal, the intracanalicular portion of the tumor was left in place. The residual tumor grew 12 mm in 2 months. Even after a gross total removal through a middle cranial fossa approach, the tumor recurred, reaching the size of 30 mm in 17 months. A modified transcochlear approach was then performed, and the patient was free of disease at the last radiologic follow-up, 8 months after the surgery. We illustrate our strategy in treating this aggressive benign lesion with unusual behavior."
2f072f6768c4f8d16673b08ec42de748527b256d,
4effaa563b26b98b003dd5e2af4980de4c2973f0,"Since the abrupt drop in the mortality rate as a result of the introduction of microsurgical dissection techniques in the treatment of acoustic neuromas, surgeons have concentrated their efforts on preserving hearing and facial nerve function. In the translabyrinthine approach, identification of the facial nerve at the fundus of the internal auditory canal is an important step for subsequent dissection. However, the identification techniques available to date carry with them some potential risk of facial nerve injury when performed by inexperienced surgeons. In addition, they are time-consuming procedures. The authors present an alternative method for identification of the facial nerve at the fundus of the internal auditory canal during the translabyrinthine approach. The superior ampullary nerve is interrupted at the superior cribrosa area where it is not in intimate relationship with the facial nerve. Medial reflection of the superior ampullary nerve and the superior vestibular nerve facilitates identification of the facial nerve and preparation of a vestibulo-facial dissection plane."
659296928e371bb236b7bdfb87dcb2ee022453d2,"The infra-temporal fossa approach is one of the lateral approaches to the skull base. It is indicated for the treatment of tumors such as glomus tumor, petrous apex cholesteatoma, chondroma, lower cranial nerve neuroma and nasopharyngeal cancer. In the present paper, we described the surgical anatomy of the lateral skull base and the indications for the infra-temporal fossa approach with its variants. We showed the hints and pitfalls in the procedures. Five illustrative cases are also presented."
96df05e08168421c2901d021e40fee32d3872735,"Intratemporal carotid artery aneurysms are rare lesions, with only 54 cases reported in the literature. Their most common symptoms are pulsatile tinnitus, hearing loss and signs of Eustachian tube obstruction. In case of aneurysm rupture, bleeding may be so profuse as to require emergency legation of the common carotid in the neck. Arteriography is the diagnostic gold standard for this disorder. Successful treatment usually involves selective aneurysm embolization or carotid closure with detachable balloons. The authors report a new case of intratemporal carotid artery aneurysm previously treated with selective embolization. To avoid the risk of aneurysm recanalization and/or infection through the external auditory canal, middle ear obliteration and blind-sac closure of the external canal were performed in this case."
cb4fc0576db7a857e6839b0c929771495b622e2c,"OBJECTIVE
The objective of this study was to validate measures taken to reduce the number of cerebrospinal fluid (CSF) leaks after removal of vestibular schwannomas to 0.


STUDY DESIGN
This study was a retrospective case review.


SETTING
The study was conducted at an otology/neurotology tertiary referral center (Gruppo Otologico, Piacenza, Italy).


PATIENTS
Three hundred thirty-one vestibular schwannoma patients were studied.


INTERVENTIONS
The enlarged translabyrinthine approach (TLA) was used in all cases, with a number of modifications in the last 200 patients. It was extended in 22 patients with blind sac closure of the external meatus, removal of the posterior bony canal wall, and obliteration of the Eustachian tube and middle ear.


MAIN OUTCOME MEASURES
Whether patients had a leak through the wound, the nose (rhinoliquorrhea), or the ear (otoliquorrhea) was assessed.


RESULTS
In an early group, the percentage of CSF leaks was 6.9%. On the basis of the evaluated causes, as time went by, technical modifications evolved. They consisted of 1) the total conservation of the fascioperiosteal flap, 2) obliteration of all petrosal cells possibly communicating with the middle ear, 3) removing the incus in a correct way, 4) closing the attic with periosteum, 5) obliterating the surgical cavity, leaving strips of abdominal fat with their medial ends inside the cerebellopontine angle, 6) suturing the musculo-periosteal layer in a correct way, and 7) fixing the skin flap to the underlying surface. The application of these modifications resulted in a total absence of CSF leaks in 200 consecutive patients thereafter. Also, no cases of meningitis were encountered.


CONCLUSIONS
To our knowledge, this is the first series of 200 consecutive vestibular schwannoma patients operated by means of the enlarged TLA without a single CSF leak. When the appropriate measures are taken, the number of CSF leaks after removing tumors through the enlarged TLA must and can be reduced to 0."
da87d32a6b044694b53319b5951120d036ba7b5a,
2698dd193265791de105979be03668e66deb5856,
3441ba2d18e870e41e792903992b4f7cb9baef48,"Cerebrospinal fluid (CSF) leak is one of the most dangerous complications that can arise in cases of acoustic neuroma removal. It increases the risk of meningitis, requires longer postoperative recovery and often requires revision surgery. A retrospective analysis was performed on all cases of acoustic neuromas which had undergone translabyrinthine surgery at the Gruppo Otologico, Piacenza, Italy, between April 1987 and December 1997, in the aim of finding the causes of postoperative CSF leaks. The causes found were high pneumatization of the temporal bone and improper execution of some surgical steps. The technique has presently been modified on the basis of the experience gained from cases of CSF leaks. By scrupulously applying these modifications, from July 1994 to December 1997, a total of 160 patients were consecutively treated using the translabyrinthine approach without a single case of postoperative CSF leak. Routine use of this modified technique can and must lower the percentage of CSF leaks after a translabyrinthine surgery to nearly 0%."
395e036b50e3dc952e1e488b74e7379881ccd673,"OBJECTIVE
This study aimed to update the authors' experience with the modified transcochlear approach for the management of lesions of the central skull base. The surgical technique, classification, indications, and results also are presented.


STUDY DESIGN
A retrospective review of the charts of 66 consecutive patients treated in our centers by the modified transcochlear approach was conducted.


SETTING
The study was performed in two tertiary referral centers.


PATIENTS
All patients treated by the modified transcochlear approach were included. Thirty-five patients had extradural lesions, whereas 31 lesions were intradural.


INTERVENTION
All patients were treated surgically using the modified transcochlear approach either in its basic form (type A) or with its extensions (types B, C, and D).


MAIN OUTCOME MEASURES
The outcome of surgery is evaluated with particular emphasis on the incidence of morbidity, mortality, and the degree of total tumor removal.


RESULTS
Total tumor removal was accomplished in 58 cases either in single or staged procedures. A second-stage procedure for total tumor removal is planned in five other patients. Subtotal tumor removal was performed in three patients. Mortality occurred in two cases. Ipsilateral hearing loss and immediate facial nerve palsy constituted the major drawbacks of this approach. However, 67.5% recovered to grade III facial function or better 1 year after surgery.


CONCLUSIONS
The modified transcochlear approach provides a relatively safe, wide, and versatile access to large lesions of the central skull base."
be44427d953861137a7a6b0402a486acd3605659,
2a0be502e364433fc394109cd91bd1cf9e09cfd7,"The Bondy technique is a particular radical, yet conservative, treatment where the intact tympanic-ossicular system is preserved during the operation. This technique was initially introduced by Gustave Bondy in 1910 and has been used at the authors' center since December 1983 with a few modifications. In the twelve years from December 1983 to December 1995 69 cases (66 patients) were treated with this technique: 5.6% of all chronic cholesteatomatose otitis surgically treated during that period. The following indications were applied: purely epitymapnic cholesteatoma, average transmission gap equal to or lower than 25 dB and intact bone chain. During the follow-up no signs of recurrent cholestatoma were found. The average post-operative transmission gap was greater than 25 dB in only 5 of the 54 cases with at least one year of follow-up. When used in selected cases, the modified Bondy technique can be considered an excellent option in the treatment of epitympanic cholesteatoma, making it possible to maintain the good pre-operative hearing without requiring a second operation."
442ecd3baa8212682531ef0bef551d115562f634,"Thanks to ongoing development in microsurgical techniques to treat the skull base some clivus lesions, considered inoperable until a few years ago, can now be removed with relatively low mortality and morbidity. The approaches available for the treatment of lesions in this anatomical area can be broken down into anterior and lateral. The latter offer the surgeon several important advantages such as better control over the main vascular structures and the possibility of opening the intradural space without coming into communication with the pharynx. By virtue of their extreme versatility, the lateral approaches may be used in combination in a single operation or can be performed as staged procedures. Each of the lateral routes, however, has its own advantages and drawbacks. These are presented during a brief description of each technique. The main factor in selection of approach is the anatomical limits of the approach itself. Generally speaking, the largest surgical field is offered by the most destructive approaches. Although they do cause such deficits as unilateral hearing loss, these approaches permit greatly improved control over the vital structures. Nevertheless, when such deficits exist prior to surgery, the morbidity of such approaches is negligible. The histological nature of the disease, the variable relationships with the dura, the main neurovascular structures and other factors should all be considered when deciding how to manage each case. The surgeon must, however, have the range of skills required to perform the diversity of approaches to the lateral skull base so as to provide the patient with the best possible care."
606c349038c89d7a2362d69b4b0aedcf25f24e88,"Abstract In order to study high jugular bulb management in lateral skull base surgery, an anatomical study was conducted on 30 temporal bones by examining the relationship between the internal auditory canal (IAC) and the jugular bulb. The following parameters were measured: 1) Height of the jugular bulb (H) … distance between the level of the jugular bulb dome and the line passing through the confluence of the sigmoid sinus with the jugular bulb (SS-JB), 2) Mastoid length (ML) … distance between the mastoid process and middle cranial fossa dura, 3) Distance between the most inferior part of the porus acousticus and jugular bulb dome (A), 4) Distance between the porus acousticus and SS-JB (B). The jugular bulb was defined as high when it occupied more than two thirds of (B). The incidence of a high jugular bulb was 23 per cent in this study. When the jugular bulb was high, the mean (H) and (A) were 9.4 ± 1.9 mm and 2.7 ± 0.5 mm, respectively. (H) was higher on the right side than on the left side. No statistically significant difference was found between small and large mastoids (t-test: p>0.05). It was concluded that when a high jugular bulb was encountered during lateral skull base surgery, the jugular bulb position allows a very small working area inferior to the IAC. In these cases, a 3 or 4 mm depression of the jugular bulb is necessary in order to expose the lower cranial nerves. This can be accomplished by lowering the jugular bulb with the technique already described."
7244439d212a117653d84c29309261e48d9db447,"Morphometric evaluation of the infralabyrinthine approach to the internal auditory canal (IAC) was performed using 20 fresh human temporal bones in order to assess the exposure limitations, inherent risks and technical difficulties that may arise due to common anatomic variations of this region. While performing the infralabyrinthine approach to the IAC, minor problems such as an anteriorly placed sigmoid sinus were easily managed. However, in 50% of the specimens, this approach was limited due to variations of the jugular bulb, restricting access to the IAC. Sacrificing the endolymphatic duct in these specimens did not significantly improve the surgical access to the eighth nerve. Furthermore, it was noted that this approach puts the facial nerve and cochlea under the risk of inadvertent damage during drilling. The authors conclude that vestibular nerve sectioning using the infralabyrinthine approach may be performed only in few selected cases and extreme care is needed in order not to damage the structures that limit this approach."
373a55a7297c2e20dd2b18599352c51ccb1427bc,"We report on a series of 35 Ménière's disease patients, all of whom had undergone retrolabyrinthine vestibular neurectomy between 1987-1993. The overall success rate of vertigo relief was 96.7% with no serious or permanent complications resulting from the procedure. The current literature is reviewed and our results are compared with those of previous reports. The technical elements of the operation, regarding our approach and those of the others are analyzed with special attention given to the anatomical features of the region and their influence on success or failure. We conclude that the retrolabyrinthine approach for nerve section remains a safe and highly successful technique which continues to be widely used."
52a2432afb9f228da4d4668b5cf99572d5e7c47d,"Abstract With the recent advances in the management of vestibular schwannomas, it is possible not only to save the facial nerve function but also preserve hearing in a small percentage of cases. Difficulties arise while managing patients with vestibular schwannoma in their only hearing ear. In this article we summarize our experience in managing seven of these patients. We recommend a watch and wait policy with a regular follow-up with audiometric testing and gadolinium-enhanced magnetic resonance imaging (MRI). Gamma knife radiosurgery is advised in cases with deterioration of hearing or increase in tumour size. Surgery is usually avoided unless there are brainstem compression symptoms."
0354862419b2cc080f677905e6f7c34775daaf5d,"Herniation of meningeal and/or encephalic tissue into the middle ear is a pathology which, even if rarely found by the otologist, can be life-threatening for the patient because of eventual infective intracranial complications. Four different etiological types are possible, infective, post-surgical, traumatic and spontaneous. From a pathogenic point of view, all types are characterized by a bony and dural defect localized in the tegmen through which meningeal and encephalic tissue can herniate. Symptomatology is often non-specific so that some cases are diagnosed during surgery. When there is strong suspicion of herniation neuroradiological assessment procedures must be carried out in order to make a correct pre-operative diagnosis, High Resolution Computed Tomography (HRCT) of the temporal bone in particular, can show the exact limits and location of the bone defect, while Magnetic Resonance Imaging (MRI) allows the nature of the tissue in the middle ear to be determined. Surgery is the only appropriate therapy. Different approaches have been described amongst which the transmastoid with or without temporal minicraniotomy and the middle cranial fossa (MCF) are the most frequently reported literature. From June 1982 to March 1994, 27 consecutive cases underwent surgery at the Gruppo Otologico, Piacenza. As a result of the occurrence of postoperative meningitis in one case, a new surgical technique through the MCF was standardized. The main step of this procedure consist in leaving the herniated tissue in situ so as to make a barrier between the middle ear and subdural space. The technique is indicated either in the case of large, multiple or very anteriorly located bony defects or when there is an infection in the middle ear."
200a02a990dfea32912f04b966b008a64918ad93,"Facial nerve neuromas are uncommon tumors that involve the facial nerve. There is no classic presentation of these tumors. This study presents a series of 22 patients with facial neuromas managed from 1977 to 1993. Facial nerve dysfunction was the most common complaint, present in 90.5% of cases. Hearing loss was the second most common complaint and was found in 76.2% of cases. High resolution computed tomography and magnetic resonance imaging with gadolinium proved to be the most accurate methods of preoperative assessment of these tumors and are complementary in selected cases. Different surgical approaches were performed according to tumor location and preoperative hearing level. In all cases long-term follow-up showed no tumor recurrence, and acceptable return of facial function was noted in 80% of cases."
41654b42291b8eceea26c158783eabae5c4d9ac9,"In this work we present a simple, rapid, cost-effective and time-conserving method of studying the vascular anatomy of the base of the skull. This method is based on the injection of the arteries and veins with an appropriate coloring solution that possesses the property of rapid solidification. This technique of preparation of the coloring solution and the method of injection is described in detail. The advantages and disadvantages of this technique are also discussed."
63d90fded7c71f72623ffdf2384e747e13008092,"We present 44 cases of congenital cholesteatoma of the middle ear. Twenty-one patients had a cholesteatoma located in the posterosuperior mesotympanum. This finding was in complete contrast to the commonly reported anterosuperior location, seen in only 2 cases in our study. The remaining 21 patients had a cholesteatoma involving either the entire mesotympanum and/or epitympanum. The posteriorly located congenital cholesteatoma might represent a completely different entity and originate from epithelial cell debris trapped in the posterior mesotympanum during development of the temporal bone. All but one patient were treated with a closed tympanoplasty. Eight patients underwent single stage surgery. A preplanned second stage procedure was performed in 33 patients, while 3 are presently awaiting the second stage. Residual disease was seen in 19 patients (57%) who had undergone second stage surgery. No patient has had recurrent disease this far. Thirty-eight patients (85%) had a preoperative air bone gap of 30 dB or more. Of the 33 patients evaluated for hearing results, 16 (48%) had a postoperative gap within 10 dB."
832b0174176566a53f7296512c632534d2169460,"Unilateral or asymetrical sensorineural hearing loss, tinnitus and instability classically represent the main symptoms for the suspected diagnosis of acoustic neurinoma. In literature there are very few studies which refer about acoustic neurinoma in patients with normal hearing. In this article we report the results of a retrospective analysis of 155 acoustic neurinoma cases with normal hearing managed in our center in the last 7 years. These patients are classified in two groups: 1) with normal hearing at the time of diagnosis and 2) with at least a mild sensorineural hearing loss. Of this 155 cases, 21 (13.5%) had normal pure tone audiogram. An important characteristic of these patients, in comparison with the group with normal hearing, is the younger age. Tinnitus, instability, sudden hearing loss with complete recovery and vertigo are the most common symptoms. The mean pure tone threshold is 14.7 dB, speech audiometry is positive in very few cases, while a large number of cases show false negative. ABR are positive in 90.5% of cases, and demonstrate its high sensitivity for retrococlear pathology. The mean tumor size shows significant differences between the group with normal hearing and this with hearing loss. We believe that the presence of this symptoms in a young patient should necessitate complete neurotologic examination and in the patients with normal hearing a high level of suspicion represents the first step for early diagnosis of small tumors; the next step is the through evaluation of patient with pure tone audiogram, ABR, and imaging studies, preferably MRI with gadolinium, as this permits the diagnosis of small intracanalicular tumor. Thus, in cases of small tumors with good hearing we feel that will be possible to adopt hearing conservation surgical approach like FCM and retrosigmoid approach in more cases."
8a74627abd0632593bd3fca208f30df278f9abab,"Bondy operation is a type of modified radical mastoidectomy in which the mastoid cavity is exteriorized without disturbing the intact ossicular chain and pars tensa. It is indicated in cases of epitympanic cholesteatoma with intact ossicular chain, normal pars tensa, and good hearing. The advantages of the technique are one-stage surgery with preservation of preoperative hearing levels, which is not possible with any other procedure. This article presents the indications, technique, results, and complications."
d851427c12985167cc09c17c7c1f0ca51e300114,"Intratemporal vascular tumors involving the facial nerve are rare benign lesions. Because of their variable clinical features, they are often misdiagnosed preoperatively. This study presents a series of 21 patients with such lesions managed from 1977 to 1994. Facial nerve dysfunction was the most common complaint, present in 60% of the cases, followed by hearing loss, present in 40% of cases. High-resolution computed tomography, magnetic resonance imaging with gadolinium, and a high index of clinical suspicion is required for preoperative diagnosis of these lesions. Early surgical resection of these tumors permits acceptable return of facial nerve function in many patients."
e4fad37ced5e6ee0b61d65fcea80804b9fa32f7e,"Abstract Osteoblastoma is a benign bone lesion that mainly affects the long bones and rarely the temporal bones. Very few cases have been reported in the literature. This paper reviews the literature, discusses the differential diagnosis, clinical presentation, and CT scan findings of such a condition and details our experience with a young patient who had a temporal bone (mastoid process) osteoblastoma."
f1dfbe7054a3f72f6fa97a6ef8afc2c7bafad9d2,
f8d6dd278045d80fc0c0edaf737166e4fd16930e,"This paper presents the modification made for a transcochlear approach which would permit reaching the pertoclivis area and proeponten cistern. The disadvantage, inconvenience of the approach used on 5 patient is discussed here."
f9fecc94e7437e855fd5b9817b45d657f12d8b4d,"Abstract Managing patients with failed canal wall down mastoidectomy, requires a meticulous approach to control the disease and restore hearing. The present article reviews the causes of failure of the primary procedure and pitfalls encountered in 105 patients referred to our centre for revision canal wall down mastoidectomy. At post-revision surgery there were no cases with residual or recurrent cholesteatoma. The failures in our revision procedure were due to tympanic membrane perforation which occurred in five percent (n = 4) and intermittent otorrhoea in two percent (n = 2). A dry cavity with adequate middle ear space allowed for optimum audiological function even in revision canal wall down procedures."
1b9062c352a452dde0b16419855017c31ed2bb17,"The extended middle cranial fossa approach includes removal of the petrous bone from its subtemporal surface in order to expose widely the internal auditory canal and the posterior fossa dura around its porus while preserving all the important and closely related anatomical structures. We have dissected 25 temporal bones and five fresh cadavers in order to define the limits of this approach. Measurements were obtained between the different structures to find reliable angles and distances that could guide working in this area. A new method of identification of the internal auditory canal is discussed based on the measurements taken.The results of the present work showed wide variations in the different structures. The arcuate eminence was coincident with the superior semicircular canal in only 48% of bones. Dehiscence of the geniculate ganglion and of the internal carotid artery was noted in 16% and 20% of specimens, respectively. The angles measured between the different structures showed great variations. However, the angle between the internal auditory canal and superior petrosal sinus was constant. Though the extended middle cranial fossa is a versatile approach, it affords a limited access to the cerebellopontine angle. A thorough understanding of the complex and variable anatomy of this area is necessary should this approach be utilized."
260bc46ce4f8594a1d5014077aa69075ce473bce,"The extended middle cranial fossa (EMCF) approach calls for removal of the petrous bone from its subtemporal surface in order to well expose the internal auditory canal (IAC) and the posterior fossa dura (PDF) around its meatus, safeguarding, at the same time, all the important, closely related, anatomical structures (the Gasserian ganglion (GG) and its third trigeminal division, the internal carotid artery (ICA), the cochlea, the posterior labyrinth, the superior petrosal sinus (SPS), the inferior petrosal sinus (IPS) and the jugular bulb (JB). The middle meningeal artery, on the other hand, could be divided when necessary. We dissected 25 temporal bones preserved in formaldehyde and those of five cadavers in order to define the limits of this approach. Measurements were taken so as to establish the limits of the approach as well as to determine the most appropriate angles and distances in working in this area. As previously suggested by the Senior Author (MS), we found it safer to start working medially in order to identify the IAC and then to extend the dissection laterally. The most constant angle proved to be that between the IAC and the SPS. The distances and areas found appear to be highly variable and difficult to rely upon. A thorough knowledge of this anatomy is of most importance for the surgeon who intends to use this approach."
8765e914a057d69d5a8ff71575066bdf78baa9d8,"Abstract This study was carried out to validate the enlarged translabyrinthine approach for the surgical management of large vestibular schwannomas. A retrospective review of the charts of 53 patients with large tumours removed via the enlarged translabyrinthine approach at the Gruppo Otologico, Piacenza, Italy, during the last five years was carried out. The ability to control large tumours and the achievement of total removal with low morbidity and very few complications, demonstrate that tumour size does not influence the use of the enlarged translabyrinthine approach for managing large tumours."
bf79775424c194531c6c5097352564433d403a08,"Abstract Twenty-three patients with intradural lesions of the petroclival region and prepontine cistern were managed in our centres by the modified transcochlear approach. Total tumour removal was accomplished in 17 patients, while a second stage was planned for the remaining six patients. Two cases died in the immediate postoperative period. All the remaining cases showed a good outcome and returned to work. The basic approach type A is further classified according to its anterior, superior and inferior extension into types B, C and D respectively. The surgical procedure, classification, illustrative case reports and patients' summaries are presented."
d6bedc082e3c8ddca56f01633ec32ebc1f81c3d1,"Posterior fossa meningioma is the second most common tumor in the cerebellopontine angle. It has a higher rate of postoperative morbidity and mortality compared to acoustic neuroma. Forty posterior fossa meningioma patients managed in our centers were reviewed. Thirty-nine patients were managed surgically with 42 surgical procedures. The approaches used were the translabyrinthine approach in 18 patients (43%), the modified transcochlear in 11 cases (26%), the petro-occipital transsigmoid in 5 cases (12%), the suboccipital in 4 cases (10%), the petro-occipital trassigmoid transcervical in 2 cases (5%), the petro-occipital transsigmoid transtentorial in 1 case (2%), and a subtemporal transtentorial for another case (2%). Facial nerve anatomical integrity was preserved in 87% of procedures but was interrupted in 5 cases, with 4 of the latter subsequently repaired. Total tumor removal was accomplished in 38 cases. A second-stage total tumor removal is planned for the remaining case. There was only one case of perioperative death and no cases of radiological recurrence so far."
f87afec2ebb3487c0a68bdedb42eb30d97e7d4e1,A high jugular bulb is a frequent problem in the translabyrinthine approach. This article described a safe technique for effective inferior displacement of the high bulb.
407fee7b0a90e148b98e73e7f9aa22735f6a9f89,"Unilateral progressive sensorineural hearing loss, tinnitus, and unsteadiness are the usual initial symptoms of acoustic neuroma. Of the last 100 consecutive cases of acoustic neuroma detected at our Centre, 14 had atypical symptoms, Five patients manifested sudden hearing loss; one of these had complete recovery. Three patients reported long-standing unilateral hearing loss, ranging from 10 to 20 years. Six patients had normal hearing, one of whom was diagnosed incidently when the investigations were performed for contralateral glomus tumor. A second patient, a young woman, experienced weakness of lower limbs. The remaining four patients had only subjective symptoms of hearing loss or tinnitus. Acoustic tumors could have been overlooked easily in these patients. It is important to have a high index of suspicion in all cases of sudden hearing loss, asymmetric sensorineural loss of any duration, subjective sensation of hearing loss, and tinnitus. It is mandatory to investigate these cases with auditory brainstem responses, any abnormality of which makes it necessary to perform magnetic resonance imaging with gadolinium."
474e0eabaa4a1971d608c7a0f14fe085f87c7093,"Petrous bone cholesteatoma is a rare pathologic entity and may be a difficult surgical challenge because of potential involvement of the facial nerve, carotid artery, dura mater, otic capsule, and risk of cerebrospinal fluid leak. The objective of this article is to present a personal classification of petrous bone cholesteatomas, a survey of recent surgical attitudes, and our present surgical strategy based on our experience with 54 operations between 1978 and 1990. Radical petromastoid exenteration with marsupialization and the middle cranial fossa approach were used only for small pure infra- or supralabyrinthine cholesteatomas, respectively. The enlarged transcochlear approach with closure of the external auditory canal was used for infralabyrinthine, infralabyrinthine-apical, and massive petrous bone cholesteatomas. Five cases with petrous bone cholesteatomas in different locations are described in detail to present the signs and symptoms together with the management."
b4079be623cce214fd23a7f727af6b6d65bef78a,"Abstract A report of a case with a vascular malformation of the internal auditory canal (IAC) is presented. A review of the literature of this rather rare lesion is also made in an attempt to outline its clinical features, radiological diagnosis and management. The differential diagnosis and distinction between vascular malformations and other internal auditory canal tumours are discussed."
63581d50d32ae8063919bb63c500ba109013538d,Abstract Labyrinthine destruction by direct cholesteatoma invasion has always been considered a serious threat to the inner ear function. A number of reports in the literature have cited both patients who had preservation of hearing despite widespread erosion of the labyrinth by cholesteatoma and patients who had retained auditory function despite surgical removal of the matrix from the labyrinth. In most cases the vestibular portion of the inner ear was invaded but cases of cochlear involvement have been described as well. Twelve cases with pre-operative auditory function preservation despite extensive labyrinthine destruction treated at our Institution are reported. Seven cases retained cochlear function post-operatively. Possible explanations of this occurrence and implications of related with hearing preservation in the presence of widespread inner ear destruction by chronic inflammatory disease are discussed.
bff1b4dcc8e4f2d28c8c3b1ab32fa7e306d9856e,"A labyrinthine fistula is the most common complication of cholesteatomatous chronic ear disease. Its treatment remains a controversial subject. The present paper reports our approach to the management of this complication. Operations were performed on 1,226 cases of chronic otitis media with cholesteatoma between January 1971 and December 1985. A labyrinthine fistula was detected in 158 cases. We favor intact canal wall tympanoplasty even in the presence of medium or large fistulas: in the latter case, the matrix is not removed but is trimmed to cover only the bony defect and it is left in place. Open procedures with the preservation of the matrix over the fistula are done in an only-hearing ear with fistula, in ears with a wide defect of the posterior canal wall, and in ears with multiple labyrinthine fistulas. The management of the matrix over the fistula and the anatomic and functional results following each type of procedure are presented and discussed."
df224a1711b407e2734cc51ee93f8166099c1477,
3e051c1e1459cbe5b814da2a3761d61ba9ab7914,Abstract The results of treatment of 124 cases of childhood cholesteatoma are reported in the present study and compared with an adult group of patients. Intact canal wall tympanoplasty was performed in over 90 per cent of cases in children and the procedure was staged in nearly 80 per cent of cases. The children had a 43.8 per cent incidence of residual cholesteatoma and an 8.8 per cent incidence of recurrent cholesteatoma in intact canal wall tympanoplasty cases. Intact canal wall tympanoplasty remains the technique of choice in our hands for the treatment of childhood cholesteatoma; pre-planned staging of the operation is mandatory for the detection and elimination of residual cholesteatoma which occurs more frequently in children.
ac8be7fc48f5fe998ecfd214b90dc397c3bba8bc,"Recurrent cholesteatoma in a series of 534 staged intact canal wall tympanoplasties performed over a 10-year period has been reviewed for the present study. Overall detected incidence of recurrent cholesteatoma is 5.2% (28 of 534 operated ears). A steady decrease of recurrent cholesteatoma was found, however, in the second period of our surgical experience (1978 to 1982) when prevention techniques were adopted in all operations, resulting in a 1.07% incidence (four of 373 operated ears). Our present policy for prevention of recurrent cholesteatoma in intact canal wall tympanoplasties with mastoidectomy includes 1) the use of plastic sheeting with thick Silastic, 2) the repair of bony sulcus defects with cartilage shavings, 3) staging of the operation with preplanned reexploration of the middle ear and mastoid, and 4) transtympanic ventilation tube insertion in cases of refractory tubal insufficiency."
e44212f12c044fec1353e32dc9a7137cf53142e2,
022a73c86e5ced5184641af64f8884a19b558543,"BACKGROUND
Cerebral venous sinus thrombosis due to vaccine-induced immune thrombotic thrombocytopenia (CVST-VITT) is an adverse drug reaction occurring after SARS-CoV-2 vaccination. CVST-VITT patients often present with large intracerebral hemorrhages and a high proportion undergoes decompressive surgery. We describe clinical characteristics, therapeutic management and outcomes of CVST-VITT patients who underwent decompressive surgery, and explore predictors of in-hospital mortality in these patients.


METHODS
We used data from an ongoing international registry of patients who developed CVST within 28 days of SARS-CoV-2 vaccination, reported between 29 March 2021 and 10 May 2022. We included definite, probable and possible VITT cases, as defined by Pavord et al. RESULTS: Decompressive surgery was performed in 34/128 (27%) patients with CVST-VITT. In-hospital mortality was 22/34 (65%) in the surgical and 27/94 (29%) in the non-surgical group (p<0.001). In all surgical cases, the cause of death was brain herniation. The highest mortality rates were found among patients with preoperative coma (17/18, 94% vs 4/14, 29% in the non-comatose; p<0.001), and bilaterally absent pupillary reflexes (7/7, 100%, vs 6/9, 67% with unilaterally reactive pupil, and 4/11, 36%, with bilateral reactive pupils; p=0.023). Postoperative imaging revealed worsening of index hemorrhagic lesion in 19 (70%) patients and new hemorrhagic lesions in 16 (59%) patients. At median follow-up of 6 months, 8/10 of surgical CVST-VITT who survived admission were functionally independent.


CONCLUSIONS
Almost two thirds of surgical CVST-VITT patients died during hospital admission. Preoperative coma and bilateral absence of pupillary responses were associated with higher mortality rates. Survivors often achieved functional independence."
2a6845c0215475f4d9950d85b5ed917aa8856866,"
 Introduction:
 As acute stroke therapies expand to less well-resourced regions, it is imperative to study real-world data to assess outcomes and the quality of stroke care. The present study analyzes clinical, and imaging outcomes after acute IV thrombolysis (IV-tPA) compared to mechanical thrombectomy (MT) or combined therapy in the LASE.
 
 
 Methods:
 A retrospective analysis of consecutive acute ischemic stroke cases in 17 centers from 9 Latin American (LA) countries since 2012 was performed using weighted Euclidean matching of nearest neighbors in 3-D space of baseline NIHSS, age, and glucose.
 
 
 Results:
 950 patients receiving only IV-tPA were matched to 127 treated with MT+IV-tPA. Matching resulted in 97 pairs well balanced for age (69.1 vs. 69.3), baseline NIHSS (17 vs. 17), and glucose (124.3 vs. 124.5), all p>0.2. 3-month mRS 0-1 (38.3% vs 29.8%, p=0.23) and mRS 0-2 (46.8% vs 41.5%, p=0.54) were non-significantly higher in the MT+IV-tPA group, with higher hemorrhage (26.6% vs 15.5%, p=0.05) and trends for higher death (15.1% vs 9.5%, p=0.40) and symptomatic hemorrhage (7.4% vs 6.2%, p=1.0). One hundred one patients receiving only MT were matched to 127 patients with MT+IV-tPA, resulting in 61 pairs; MT+IV-tPA showed trends for higher rates of 3-month mRS 0-1 (45.0% vs 35.6%, p=0.31), mRS 0-2 (48.3% vs 42.4%, p=0.54); with trends for lower rates of death (16.9% vs 20.0%, p=1.0) and symptomatic hemorrhage (8.3% vs 11.1%, p=0.75).
 
 
 Conclusions:
 In this real-world LA sample, trends for better functional outcomes were demonstrated with MT+IV-tPA compared to either treatment alone, as in non-LA populations. A trend for higher adverse events in the combined group requires further investigation.
"
561b99f8d0b610565566ba5f0f5ca3df9a5d5a2f,"
 Introduction:
 Except for RESILIENT, there is a paucity of randomized controlled trials (RCTs) for stroke from Latin America (LA), home to growing stroke burden. Comparison between stroke population typically suffers from baseline factor imbalances. Here, we developed outcome models from RCTs to compare Latin American Stroke Registry (LASE) with similar baselines.
 
 
 Methods:
 LASE is a registry of patients receiving tPA and thrombectomy from 17 centers in 9 countries. A systematic review identified RCTs that provided median NIHSS, mean age, percentage of patients receiving tPA, time-to-randomization, 90d mRS0-2, and mortality. Akaike Information Criterion (AIC), an information theory construct, was used to select the best model amongst 15 combinations of 4 variables. 90d outcomes of LASE and RESILIENT were compared at the baseline values against the selected model.
 
 
 Results:
 34 RCTs with ~8300 subjects were identified. Models based on NIHSS and the percentage of tPA were considered the most optimum in terms of AIC. In the 3D models (Fig1), the middle surface defines the function and the bounding surfaces the ±90% intervals. The LASE registry has 950 patients that received tPA alone, 127 that received tPA & mechanical thrombectomy (MT), and 101 that received MT alone. LASE & RESILIENT outcomes were plotted onto the models at their baseline values. LASE tPA alone group was on the middle surface for mRS 0-2 (Fig1-A), indicating that outcomes were in line with the RCT-informed model. MT alone (0% tPA) and MT + tPA (100%), and the RESILIENT MT arm (68.5% tPA) were above the +90% surface, indicating superior efficacy compared to no-MT. The RESILIENT control arm (71.8% tPA) had > expected mortality, suggesting harm, while mortality of all other arms was within the ±90% intervals (Fig1-B).
 
 
 Conclusion:
 Functional outcomes and mortality of patients from the LASE that received MT and MT+tPA compared favorably to a 90-day functional and mortality predictive model.
 
 
 
"
a3773e7969c92305a342b60e8bc6e3c3e15a5612,"
 Introduction:
 Adenovirus-based COVID-19 vaccines are extensively used in low- and middle-income countries (LMICs). In India alone, 1.67 billion ChAdOx1 nCoV-19 vaccines have been administered by August 23, 2022. Surprisingly however, there are only few reports of cerebral venous sinus thrombosis due to vaccine-induced immune thrombotic thrombocytopenia (CVST-VITT) from LMICs. We aimed to gain insight into the frequency, manifestations, treatment, and outcomes of CVST-VITT in LMICs.
 
 
 Methods:
 We report data from an international registry on CVST after COVID-19 vaccination. VITT was classified according to the Pavord criteria. We compared characteristics of CVST-VITT cases from LMICs to cases from high-income countries (HICs).
 
 
 Results:
 By August 15, 2022, 228 CVST cases after vaccination were reported, of which 63 cases from LMICs (all middle-income countries [MICs]: Brazil, China, India, Iran, Mexico, Pakistan, and Turkiye). Of these, 32/63 (51%) met the criteria for definite, probable or possible VITT. Only 5/32 (16%) CVST-VITT cases from MICs had definite VITT, mostly because anti-PF4 antibodies were not tested in 21/32 (66%) cases. Patients from MICs were diagnosed in a later time period than patients from HICs (1/32 [3%] vs 65/103 [63%] cases diagnosed before May 2021, respectively). Median age was 26 (IQR 20-37) vs 47 (IQR 32-58) years, and proportion of women was 25/32 (78%) vs 77/103 (75%) in MICs vs HICs, respectively. Clinical manifestations, such as focal neurologic deficits, coma, seizures, and intracranial hemorrhages, were similar. Concomitant venous thromboembolism was less frequent in MICs (3/31 [10%] vs 26/97 [27%]). Median platelet count nadir was higher in the MICs than the HICs group (65 x10
 9
 /L [IQR 36-115] vs 33 x10
 9
 /L [IQR 18-55],
 p
 =0.001). Intravenous immunoglobulin use was similar (19/30 [63%] vs 63/99 [64%]). In-hospital mortality was lower in the MICs than the HICs group (7/32 [22%, 95%CI 11-39] vs 44/102 [43%, 95%CI 34-53],
 p
 =0.031).
 
 
 Conclusions:
 The absolute number of CVST-VITT cases reported from LMICs was small despite the widespread use of adenoviral vaccines in these countries. Clinical manifestations and treatment of CVST-VITT cases were largely similar in MICs and HICs, while mortality was lower in patients from MICs.
"
ce410221098e3041c364fa9e9cc4d55d3e0818f1,"Cerebral venous thrombosis (CVT) is an uncommon cause of stroke in young adults. We aimed to determine the impact of age, gender and risk factors (including sex-specific) on CVT onset. We used data from the BEAST (Biorepository to Establish the Aetiology of Sinovenous Thrombosis), a multicentre multinational prospective observational study on CVT. Composite factors analysis (CFA) was performed to determine the impact on the age of CVT onset in males and females. A total of 1309 CVT patients (75.3% females) aged ⩾18 years were recruited. The overall median (IQR-interquartile range) age for males and females was 46 (35–58) years and 37 (28–47) years ( p < 0.001), respectively. However, the presence of antibiotic-requiring sepsis ( p = 0.03, 95% CI 27–47 years) among males and gender-specific risk factors like pregnancy ( p < 0.001, 95% CI 29–34 years), puerperium ( p < 0.001, 95% CI 26–34 years) and oral contraceptive use ( p < 0.001, 95% CI 33–36 years) were significantly associated with earlier onset of CVT among females. CFA demonstrated a significantly earlier onset of CVT in females, ~12 years younger, in those with multiple (⩾1) compared to ‘0’ risk factors ( p < 0.001, 95% CI 32–35 years). Women suffer CVT 9 years earlier in comparison to men. Female patients with multiple (⩾1) risk factors suffer CVT ~12 years earlier compared to those with no identifiable risk factors."
033208323eae52736ab099535fdabb3d8e393918,"Background and Objectives COVID-19–related inflammation, endothelial dysfunction, and coagulopathy may increase the bleeding risk and lower the efficacy of revascularization treatments in patients with acute ischemic stroke (AIS). We aimed to evaluate the safety and outcomes of revascularization treatments in patients with AIS and COVID-19. Methods This was a retrospective multicenter cohort study of consecutive patients with AIS receiving intravenous thrombolysis (IVT) and/or endovascular treatment (EVT) between March 2020 and June 2021 tested for severe acute respiratory syndrome coronavirus 2 infection. With a doubly robust model combining propensity score weighting and multivariate regression, we studied the association of COVID-19 with intracranial bleeding complications and clinical outcomes. Subgroup analyses were performed according to treatment groups (IVT-only and EVT). Results Of a total of 15,128 included patients from 105 centers, 853 (5.6%) were diagnosed with COVID-19; of those, 5,848 (38.7%) patients received IVT-only and 9,280 (61.3%) EVT (with or without IVT). Patients with COVID-19 had a higher rate of symptomatic intracerebral hemorrhage (SICH) (adjusted OR 1.53; 95% CI 1.16–2.01), symptomatic subarachnoid hemorrhage (SSAH) (OR 1.80; 95% CI 1.20–2.69), SICH and/or SSAH combined (OR 1.56; 95% CI 1.23–1.99), 24-hour mortality (OR 2.47; 95% CI 1.58–3.86), and 3-month mortality (OR 1.88; 95% CI 1.52–2.33). Patients with COVID-19 also had an unfavorable shift in the distribution of the modified Rankin score at 3 months (OR 1.42; 95% CI 1.26–1.60). Discussion Patients with AIS and COVID-19 showed higher rates of intracranial bleeding complications and worse clinical outcomes after revascularization treatments than contemporaneous non–COVID-19 patients receiving treatment. Current available data do not allow direct conclusions to be drawn on the effectiveness of revascularization treatments in patients with COVID-19 or to establish different treatment recommendations in this subgroup of patients with ischemic stroke. Our findings can be taken into consideration for treatment decisions, patient monitoring, and establishing prognosis. Trial Registration Information The study was registered under ClinicalTrials.gov identifier NCT04895462."
07ed454f05e4721c193c7723c262c062a65f7152,"Background: Despite the high number of vaccines administered against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) worldwide, the information on the psychological/psychiatric adverse events following immunization (AEFI) with these newly developed vaccines remains scarce. Objective: To describe the frequency of psychological/psychiatric symptoms among recipients of five different anti-SARS-CoV-2 vaccines and to explore the factors associated with their development reported in the nationwide Mexican registry of AEFI against SARS-CoV-2. Methods: Descriptive study of all the psychological/psychiatric symptoms, including anxiety, panic attacks, insomnia, and agitation reported to the Mexican Epidemiological Surveillance System from 21 December 2020 to 27 April 2021, among adult (≥18 years old) recipients of 7,812,845 doses of BNT162b2, ChAdOx1 nCov-19, rAd26-rAd5, Ad5-nCoV, or CoronaVac. The factors associated with their development are determined by multivariate regression analysis. Results: There were 19,163 AEFI reports during the study period; amongst them, 191 (1%) patients had psychological/psychiatric symptoms (median age of 41 years, interquartile range of 32–54; 149 [78%] women) for an observed incidence of 2.44 cases per 100,000 administered doses (95% confidence interval [CI] 2.12–2.82), 72.8% of psychiatric AEFIs were reported among recipients of BNT162b2. The median time from vaccination to symptom onset was 35 min (interquartile range: 10–720). Overall, the most common psychological/psychiatric symptoms were anxiety in 129 (67.5%) patients, panic attacks in 30 (15.7%), insomnia in 25 (13%), and agitation in 11 (5.7%). After adjusting for the confounding factors, the odds for developing psychological/psychiatric symptoms were higher for those concurrently reporting syncope (odds ratio [OR]: 4.73, 95% CI: 1.68–13.33); palpitations (OR: 2.47, 95% CI: 1.65–3.70), and dizziness (OR: 1.59, 95% CI: 1.10–2.28). Conclusion: In our population, psychological/psychiatric symptoms were extremely infrequent AEFIs. No severe psychiatric AEFIs were reported. Immunization stress-related responses might explain most of the detected cases."
3cbc054febb81cc900aaa5e2faeab5dce9b1cdc6,"— Popcorn Brain: Familial Multiple Cerebral Cavernomatosis Introduction. Cerebral vascular malformations are a heterogeneous group of disorders with several clinical manifestations. Cerebral Cavernous Malformations (CCM) are defined by dilated vascular spaces, with a single layer of endothelium, no mature vessel wall, and no intervening brain parenchyma. CCM may be sporadic or have an autosomal dominant hereditary pattern. Case Report. Here we present the case of a 31-year-old male with a medical history of multiple vascular malformations, difficult to treat epilepsy, and multiple hemorrhagic strokes. At initial evaluation the patient revealed mild dysarthria, left hemiparesis 4/5, hyperreflexia, left dysdiadochokinesia and dysmetria with a modified Rankin scale (mRS) score of 1. Magnetic Resonance Imaging (MRI) of the brain showed multiple supra and infratentorial parenchymal lesions with heterogeneous components on T1-weighted and T2-weighted sequences, surrounded by a rim of signal loss, demonstrating a “popcorn” appearance on Susceptibility-Weighted Angiography (SWAN) MRI. Molecular testing reported a heterozygous KRIT1 c.1099dup (p. His367Profs*3). Screening of fist degree relatives was performed, and antiepileptic treatment was adjusted. As of today, our patient has a mRS of 3, with refractary epileptic seizures. Conclusion. Symptomatic CCM must receive targeted medical treatment with a multidisciplinary approach. If a patient presents with more than one lesion, a familial case should be suspected, molecular testing and a screening head computed tomography (CT) should be done in first degree relatives. It is important to follow-up any patient with CCM with an annual MRI to monitor the size and number of lesions, symptomatic medical treatment is indicated, and surgical treatment should also be assessed. Ictus 2022;3(1):e14012203002"
5fd111e6b98ec9c57b4cbd9533ef5ab4f815ce5c,"We appreciate the comment on our article.1 Dr. Stoiloudis reports the case of an otherwise healthy young male individual who developed a wake-up acute ischemic stroke after the second dose of BNT162b2. It is of interest that their patient developed a fever in close temporal association to vaccination, which may be explained by vaccination reactogenicity. Additional research on the potential pathophysiologic pathways for immunization-related strokes is needed. It would be interesting to clarify the stroke etiologic approach in this patient because the development of an acute stroke in close temporal association to the vaccine does not exclude other common stroke causes among young adults.2"
619f5c749e52e28ad3922c24f217fc57cbfd1bac,"There are more than 1 billion people with disabilities worldwide, with 16% of children having some type of neurodevelopmental disorder (ND). In Mexico, 6% of the population presented some disability; however, there is a lack of data on ND in children under 5 years of age. Objective: The objective of this study was to determine the prevalence of neurodevelopmental alterations in children under 5 years of age in urban, suburban, and rural populations from two states of Mexico. Methods: This was an observational and cross-sectional design study. We included 501 clinically healthy children from 0 to 60 months of age, from urban, suburban, and rural populations from the state of Queretaro (201) and from the State of Mexico (300). Neurodevelopmental alterations were detected through the electronic N-PED system, exploring areas of neurological development of language, psychomotor, and sensory (auditory and visual). The positive subjects were clinically assessed to confirm the diagnosis. Results: A 14.7% prevalence of ND was found, with a significant difference between Queretaro (8.9%) and the State of Mexico (18.7%). Language alterations were significantly different for both states (4.9% and 16% for Queretaro and the State of Mexico, respectively). Conclusions: The prevalence of ND and language alterations presented significant differences between the two evaluated states. Abstract Objective: The objective of the study was to know the opinion and prevalence of use of central nervous system stimulant drugs (SDs) in healthy people in open population. Methods: An electronic survey was designed with 11 questions to know the frequency of use of SD, and also questions to explore previous knowledge, opinion about the risks associated with its use and about its regulation, etc. The survey was spread by electronic social networks to persons over 18 years old of any gender. Descriptive analysis and a Chi-square test were done to find associations between variables. Results: A total of 526 surveys were conducted, 271 male participants (51.5%) and 249 female (47.3%). The median of age was 22 years old (range 18-83 years). Median scholarship was 12 years (range 6-20 years). About 49.6% were students. About 75% had prior knowledge about stimulants, 13.6% reported prior use, 26% opined these drugs could be dangerous, and 88% opined that they should be regularized. Finally, ages between 18 and 30 years old and scholarship above high school were associated with the use of SD. Conclusion: There was a frequency of 13.6% of SD use. Most of the res-pondents are worried about its safety and are in favor of their regularization. Being young with high scholarship was associated with SD use. Abstract Neurodegenerative diseases (NDDs) are a global health problem that has been on the rise in recent years. Reviewing the definition of neurodegeneration, it can be established that NDDs are not only chronic diseases since acute events can also generate neurodegeneration. Recent research has focused on identifying key factors in the development of NDDs in order to generate new therapies at different levels of the pathophysiology of these diseases. The relationship between the gut microbiota and neuroinflammation has been subject of research in recent years, discovering new linking and triggering processes. In this text, we seek to summarize the existing findings regarding three NDDs (Alzheimer’s disease, Parkinson’s disease, and Stroke) and their relationship with the Gut Brain Axis, as well as highlight the importance of maintaining a healthy microbiota and generating therapies focused on reducing gut inflammation for the management of NDDs. Abstract There are high incidences of epilepsy and autism in the preschool and school ages, in which the synaptic plasticity and synaptogenesis are more active, a primary interruption of the synaptic function due to injury or genetic mutation may result in the appearance of both pathologies as in Lennox Gastaut S., De Aircadi S, epilepsy with continuous slow waves during sleep and Landau Kleffner S. There is evidence of abnormal brain maturation in autism spectrum disorders (ASD). Normal pruning eliminates faulty connections and optimizes neuronal functioning. In autism, this pruning would result in degrees of anatomical “over-connectivity” that increases or decreases the efficiency of communication between cortical regions. The same occurs with axonal myelination that affects integrated interregional cortical communication and synchronization. Critical and vulnerable mechanisms are appreciated during the peri-neonatal period, with subsequent stabilization of the synapses to form pre-designed neural networks through genetic mechanisms and modified by environmental factors. There are anomalies in different proteins that modulate the first phase of synaptogenesis, mutations in protocadherins, cadherins, and abnormalities in glutamatergic and GABAergic systems that affect the brain. All these aspects are critical for learning, language, and memory in both autism and epilepsy."
6353116984afb846148802af450d94ad4ab2a8e3,"Background and Objectives Information on stroke among severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) vaccines remains scarce. We report stroke incidence as an adverse event following immunization (AEFI) among recipients of 79,399,446 doses of 6 different SARS-CoV-2 vaccines (BNT162b2, ChAdOx1 nCov-19, Gam-COVID-Vac, CoronaVac, Ad5-nCoV, and Ad26.COV2-S) between December 24, 2020, and August 31, 2021, in Mexico. Methods This retrospective descriptive study analyzed stroke incidence per million doses among hospitalized adult patients (≥18 years) during an 8-month interval. According to the World Health Organization, AEFIs were defined as clinical events occurring within 30 days after immunization and categorized as either nonserious or serious, depending on severity, treatment, and hospital admission requirements. Acute ischemic stroke (AIS), intracerebral hemorrhage (ICH), subarachnoid hemorrhage (SAH), and cerebral venous thrombosis (CVT) cases were collected through a passive epidemiologic surveillance system in which local health providers report potential AEFI to the Mexican General Board of Epidemiology. Data were captured with standardized case report formats by an ad hoc committee appointed by the Mexican Ministry of Health to evaluate potential neurologic AEFI against SARS-COV-2. Results We included 56 patients (31 female patients [55.5%]) for an overall incidence of 0.71 cases per 1,000,000 administered doses (95% CI 0.54–0.92). Median age was 65 years (interquartile range [IQR] 55–76 years); median time from vaccination to stroke (of any subtype) was 2 days (IQR 1–5 days). In 27 (48.2%) patients, the event was diagnosed within the first 24 hours after immunization. The most frequent subtype was AIS in 43 patients (75%; 0.54 per 1,000,000 doses, 95% CI 0.40–0.73), followed by ICH in 9 (16.1%; 0.11 per 1,000,000 doses, 95% CI 0.06–0.22) and SAH and CVT, each with 2 cases (3.6%; 0.03 per 1,000,000 doses, 95% CI 0.01–0.09). Overall, the most common risk factors were hypertension in 33 (58.9%) patients and diabetes in 22 (39.3%). Median hospital length of stay was 6 days (IQR 4–13 days). At discharge, functional outcome was good (modified Rankin Scale score 0–2) in 41.1% of patients; in-hospital mortality rate was 21.4%. Discussion Stroke is an exceedingly rare AEFI against SARS-CoV-2. Preexisting stroke risk factors were identified in most patients. Further research is needed to evaluate causal associations between SARS-COV-2 vaccines and stroke."
7ba28b4865f86fa2d354e436075b44b0e4fdf470,"The COVID‐19 pandemic led to the development and emergency approval of an array of effective vaccines against SARS‐CoV‐2. Given the relatively small number of patients included in vaccine trials, postapproval epidemiological surveillance is crucial to detect infrequent vaccine‐related adverse events. We conducted a nationwide retrospective descriptive study evaluating the incidence of seizures among recipients of SARS‐CoV‐2 vaccines in Mexico from December 24, 2020 (date of administration of first doses nationwide) to October 29, 2021. Among 81 916 351 doses of any vaccine that were administered, we documented seizures in 53 patients, of which 31 (60%) were new onset seizures. The incidence rate of seizures per million doses was highest for mRNA‐1273 (Moderna) with 2.73 per million, followed by BNT162b2 (Pfizer‐BioNTech) with 1.02 per million, and Ad5‐nCoV (CanSino) with 1.01 per million. Thus, we found that seizures following SARS‐CoV‐2 vaccination are exceedingly rare events."
96c6a8f70d2dcfa4d67bc310806818adc0cca563,"We greatly appreciate the insightful comments by Dr. Taccetta on our article.1 We agree that passive epidemiologic surveillance is less likely to detect incidence, which is why we only reported the observed incidence and not a rate or ratio. This is a limitation we acknowledged in our article.1 In addition, concerning the 30-day timeframe from immunization to event detection in our study, we recognize that some neurologic events such as cerebral venous thrombosis (CVT) and Guillain-Barré syndrome (GBS) may occur up to 42 days after immunization. However, as stated in the Methods section, the World Health Organization's noted operational definition for surveillance of adverse events following immunization (AEFI) in our country only includes events occurring within the first 30 days after immunization.2,3 Hence, we were unable to detect or collect data on those cases occurring beyond the time frame specified by our legislature—a limitation that we also acknowledged as a potential selection bias. Recently, the increasing number of vaccine-induced thrombotic thrombocytopenia with CVT and GBS reports worldwide, especially with adenovirus-based vaccines,4,5 has compelled the expansion of our country's AEFI surveillance time frame to consider cases occurring within the first 42 days. We plan to consider this information in future reports."
a79fb3394821cb382904ef3173a15f155301e982,"Information on Guillain–Barré syndrome (GBS) as an adverse event following immunization (AEFI) against SARS‐CoV‐2 remains scarce. We aimed to report GBS incidence as an AEFI among adult (≥18 years) recipients of 81,842,426 doses of seven anti‐SARS‐CoV‐2 vaccines between December 24, 2020, and October 29, 2021, in Mexico."
aeddf57c0756d25b58ca034c8158d6534ab3b72a,"OBJECTIVE
A narrative overview of regional academic research collaborations to address the increasing burden and gaps in care for patients at risk of, and who suffer from, stroke in Latin America (LA).


MATERIALS AND METHODS
A summary of experiences and knowledge of the local situation is presented. No systematic literature review was performed.


RESULTS
The rapidly increasing burden of stroke poses immense challenges in LA, where prevention and manage-ment strategies are highly uneven and inadequate. Clinical research is increasing through various academic consortia and networks formed to overcome structural, funding and skill barriers. However, strengthening the ability to generate, analyze and interpret randomized evidence is central to further develop effective therapies and healthcare systems in LA.


CONCLUSIONS
Regional networks foster the conduct of multicenter studies -particularly randomized controlled trials-, even in resource-poor regions. They also contribute to the external validity of international studies and strengthen systems of care, clinical skills, critical thinking, and international knowledge exchange."
daaf41eb85601c74eedf74fa310dd56b1c9ce0a0,"INTRODUCTION
Cerebral venous thrombosis (CVT) associated with pregnancy and puerperium has long been recognized, with poor information in terms of functional outcomes. Our objective was to analyze risk factors, clinical, imaging, and laboratory variables to predict functional outcome and death in this population.


METHODS
CVT registries from three referral centers from Pakistan, Turkey, and Mexico, recruiting prospective cases, were combined for CVT associated with pregnancy or puerperium. Datasets and variables were standardized. Demographic characteristics, presentation, risk factors, and functional outcomes in pregnancy/puerperium-related CVT were analyzed. Binary logistic regression was used to assess predictors of outcome. The main outcome was modified Rankin score >2 at 30 days and mortality at 30 days.


RESULTS
Five hundred fifty-three cases (median age 28 years [IQR 23-34]) of CVT associated with pregnancy and puerperium were included; 439 cases (79.4%) happened in the puerperium and 20.6% during pregnancy (53.5% occurred during the first trimester). Anemia (36.7%) and dehydration (22.9%) were the commonest obstetric risk factors identified. Predictors of poor outcome (mRS >2) were encephalopathy (OR 12.8, p < 0.001), cases from Mexican origin (OR 3.1, p = 0.004), fever/puerperal infection (OR 2.7, p = 0.02), and anemia (OR 2.2, p = 0.01). Cases from Mexican origin (OR 12.0, p = 0.003) and Encephalopathy (OR 7.7, p < 0.001), presented with the highest mortality association in the final adjusted model.


DISCUSSION/CONCLUSION
In CVT associated with pregnancy and puerperium, encephalopathy, fever/puerperal infection, and anemia are associated with bad functional outcomes, meanwhile encephalopathy and cases from Mexican origin with higher mortality in the acute (30-days) of CVT onset. Anemia and infection are potential reversible predictors of poor outcome that clinicians should be aware of in order to prevent poor outcomes in these patients."
e134a81194017cf60748b55195d25b3031a02bb4,
e51df55a5d777d5e22e8172ec9a73832f8277800,"Importance
Cryptogenic strokes constitute approximately 40% of ischemic strokes in young adults, and most meet criteria for the embolic stroke of undetermined source (ESUS). Two randomized clinical trials, NAVIGATE ESUS and RESPECT ESUS, showed a high rate of stroke recurrence in older adults with ESUS but the prognosis and prognostic factors among younger individuals with ESUS is uncertain.


Objective
To determine rates of and factors associated with recurrent ischemic stroke and death and new-onset atrial fibrillation (AF) among young adults.


Design, Setting, and Participants
This multicenter longitudinal cohort study with enrollment from October 2017 to October 2019 and a mean follow-up period of 12 months ending in October 2020 included 41 stroke research centers in 13 countries. Consecutive patients 50 years and younger with a diagnosis of ESUS were included. Of 576 screened, 535 participants were enrolled after 1 withdrew consent, 41 were found to be ineligible, and 2 were excluded for other reasons. The final follow-up visit was completed by 520 patients.


Main Outcomes and Measures
Recurrent ischemic stroke and/or death, recurrent ischemic stroke, and prevalence of patent foramen ovale (PFO).


Results
The mean (SD) age of participants was 40.4 (7.3) years, and 297 (56%) participants were male. The most frequent vascular risk factors were tobacco use (240 patients [45%]), hypertension (118 patients [22%]), and dyslipidemia (109 patients [20%]). PFO was detected in 177 participants (50%) who had transthoracic echocardiograms with bubble studies. Following initial ESUS, 468 participants (88%) were receiving antiplatelet therapy, and 52 (10%) received anticoagulation. The recurrent ischemic stroke and death rate was 2.19 per 100 patient-years, and the ischemic stroke recurrence rate was 1.9 per 100 patient-years. Of the recurrent strokes, 9 (64%) were ESUS, 2 (14%) were cardioembolic, and 3 (21%) were of other determined cause. AF was detected in 15 participants (2.8%; 95% CI, 1.6-4.6). In multivariate analysis, the following were associated with recurrent ischemic stroke: history of stroke or transient ischemic attack (hazard ratio, 5.3; 95% CI, 1.8-15), presence of diabetes (hazard ratio, 4.4; 95% CI, 1.5-13), and history of coronary artery disease (hazard ratio, 10; 95% CI, 4.8-22).


Conclusions and Relevance
In this large cohort of young adult patients with ESUS, there was a relatively low rate of subsequent ischemic stroke and a low frequency of new-onset AF. Most recurrent strokes also met the criteria for ESUS, suggesting the need for future studies to improve our understanding of the underlying stroke mechanism in this population."
ea292a8828d1bd1ddfac4c6c7583f892d2ed041c,"During the first hours after stroke onset neurological deficits can be highly unstable: some patients rapidly improve, while others deteriorate. This early neurological instability has a major impact on long-term outcome. Here, we aimed to determine the genetic architecture of early neurological instability measured by the difference between NIH stroke scale (NIHSS) within six hours of stroke onset and NIHSS at 24 h (ΔNIHSS). A total of 5,876 individuals from seven countries (Spain, Finland, Poland, United States, Costa Rica, Mexico and Korea) were studied using a multi-ancestry meta-analyses. We found that 8.7% of ΔNIHSS variance was explained by common genetic variations, and also that early neurological instability has a different genetic architecture than that of stroke risk. Eight loci (1p21.1, 1q42.2, 2p25.1, 2q31.2, 2q33.3, 5q33.2, 7p21.2,and 13q31.1) were genome-wide significant and explained 1.8% of the variability suggesting that additional variants influence early change in neurological deficits. We used functional genomics and bioinformatic annotation to identify the genes driving the association from each loci. eQTL mapping and SMR indicate that ADAM23 (log Bayes Factor (LBF) = 5.41) was driving the association for 2q33.3. Gene based analyses suggested that GRIA1 (LBF = 5.19), which is predominantly expressed in brain, is the gene driving the association for the 5q33.2 locus. These analyses also nominated GNPAT (LBF = 7.64)ABCB5 (LBF = 5.97) for the 1p21.1 and 7p21.1 loci. Human brain single nuclei RNA-seq indicates that the gene expression of ADAM23 and GRIA1 is enriched in neurons. ADAM23, a pre-synaptic protein, and GRIA1, a protein subunit of the AMPA receptor, are part of a synaptic protein complex that modulates neuronal excitability. These data provides the first genetic evidence in humans that excitotoxicity may contribute to early neurological instability after acute ischemic stroke."
fb0f6327acfa5f4d9555cd0a55f1b7dc42020380,"1 The George Institute for Global Health, Faculty of Medicine, University of New South Wales (UNSW), Sydney, NSW, Australia, Department of Clinical Medicine, Faculty of Medicine, Health and Human Sciences, Macquarie University, Sydney, NSW, Australia, 3 Stroke Clinic, Manuel Velasco Suárez Instituto Nacional de Neurología y Neurocirugía, Mexico City, Mexico, Hospital Dr. Rafael A. Calderon, Neuroscience Department, University of Costa Rica, San José, Costa Rica"
06e26ff1813a0e1cbb38ddc760245cf092626576,"​Background: The development of mRNA vaccines to prevent SARS-CoV-2 has been remarkably successful, with highly effective vaccines available less than one year after confirming the first case. Due to the global burden of COVID-19 on health systems, emergency approval was attained after clinical trials with relatively small sample sizes and short follow-up periods. Limited information exists about the incidence of adverse events following immunisation (AEFI), particularly neurologic ones. Here, we describe the neurologic AEFI reported by recipients of the BNT162b2 mRNA COVID-19 vaccine.  
 
Methods: We conducted a prospective observational cohort using de-identified information from a database of all systemic and neurologic AEFI reported to the Mexican Ministry of Health throughout a passive Epidemiological Surveillance System by first-dose vaccine recipients, from December 24, 2020 to February 12, 2021. The cut-off date for this data was February 18, 2021. We performed descriptive analyses on demographics, timing from vaccination to AEFI development, event duration, and current outcome. 
 
Findings: Nationwide, 704 003 first-doses were administered; 6536 AEFI were reported. Among those, 4258 (65·1%) had at least one neurologic manifestation. Non-serious neurologic AEFI occurred in 99·6%. Headache (62·2%), transient sensory symptoms (3·5%), and weakness (1%) were the most frequent. Thirty-three serious AEFI were reported, of which 17 (2·4/100 000 doses) were neurologic, seven seizures, four functional syndromes, three Guillain-Barre syndrome (GBS) cases, and two of acute transverse myelitis. All GBS cases were related to a gastrointestinal infection before vaccination, 3/7 seizure episodes were related to poor antiepileptic drug compliance, and 2/7 to anaphylactic reactions. At the time of this report, 16/17 cases of serious neurologic AEFI had been discharged with no observed deaths. 
 
Interpretation: Our data suggest that the BNT162b2 mRNA vaccine is effective and safe. Their individual and societal benefits outweigh the low-percentage of serious neurologic and non-neurologic AEFI. 
 
Funding: Consejo Nacional de Ciencia y Tecnologia, Mexico. 
 
Declaration of Interest: None to declare 
 
Ethical Approval: The study was revised and approved by the Ethics and Research Committees of the Instituto Nacional de Ciencias Medicas y Nutricion Salvador Zubiran (Ref. NER-3667-2021) and the Mexican Ministry of Health."
0c646e19faf53eb17dfb8d057e246bef4dfe72c5,"Introduction: Twenty to 40% of Guillain Barré syndrome (GBS) patients will not be able to walk independently despite effective treatment. Older patients carry additional risks for worse outcomes. Methods: A single center, ambispective cohort study was performed. Only subjects ≥18 years with a 3-month follow-up were included. Elderly patients were considered as a whole if ≥ 60 years. Demographics, CSF and nerve conduction studies were compared. A binomial logistic regression and Kaplan-Meier analyses were carried out to estimate good prognosis (Hugues ≤2) at 3-month follow-up. Results: From 130 patients recruited, 27.6% were elderly adults. They had a more severe disease, higher mEGOS and more cranial nerve involvement. Age ≥70 years, invasive mechanical ventilation and axonal subtype, portrayed an unfavorable 3-month outcome. Further analysis demonstrated an earlier recovery in independent walk at 3 months for patients <70 years. Conclusions: Elderly patients with GBS have a more severe disease at admission and encounter worse prognosis at 3-month follow-up, especially those above 70 years."
0d84d8a8a7e1e7d42c91ce5cd1bcac2630a35613,"To explore the prevalence, risk factors, time correlation, characteristics and clinical outcome of dural arteriovenous fistulas (dAVFs) in a cerebral venous thrombosis (CVT) population."
1b03ec297444b3df8b1989b1c48f4a87c9ef0027,"Background: Stroke is a leading cause of death and disability worldwide, particularly in low- and middle-income countries. We aimed to identify the main barriers to optimal acute management of stroke in a referral center. Methods: Demographic data was collected from patients assessed with acute stroke in the emergency department of the Instituto Nacional de Neurología y Neurocirugía (INNN) from January to June 2019. Additionally, a telephone interview was conducted with patients/primary caregiver to know which they considered the main reason for the delay in arrival at INNN since the onset of stroke. Results: 116 patients were assessed [age 65 ± 15 years, 67 (57.8%) men]. Patients consulted other facilities prior to arrival at INNN in 59 (50.9%) cases (range of hospitals visited 1–4), 83 (71.6%) arrived in a private car, with prenotification in only 4 (3.4%) of the total sample. The mean onset-to-door time was 17 h (45 min−10 days). Telephone interviews were done in 61 patients/primary caregivers, stating that they consider the multiple evaluations in other facilities [n = 26/61 (42.6%)] as the main reason for delay in arrival at the ED, followed by ignorance of stroke symptoms and treatment urgency [n = 21/61 (34.4%)]. Conclusion: In this small, retrospective, single center study, the main prehospital barrier to optimal acute management of stroke in a developing country is multiple medical evaluations prior to the patient's transport to a specialized stroke hospital, who mostly arrived in a private car and without prenotification. These barriers can be overcome by strengthening public education and improving patient transfer networks and telemedicine."
25e68c471f9a630ae849769c0c43bb24dac7bfaf,"Introduction
Foix-Chavany-Marie syndrome (FCMS) is a type of pseudobulbar palsy that affects facio-pharyngo-glosso-masticatory muscles.


Materials and Methods
A 62-year-old man was admitted to the emergency department after 9 hours of acute dysarthria and dysphagia. MRI showed restricted diffusion in the right operculum on diffusion-weighted imaging (DWI). No thrombolytic therapy was given. The patient had a history of mechanical aortic valve replacement under anticoagulation with a vitamin K antagonist. Work-up demonstrated suboptimal levels of INR. Due to severe dysphagia during hospitalization, a percutaneous endoscopic gastrostomy (PEG) was performed.


Results
The patient was discharged 5 days later, with a modified Rankin scale (mRs) score of 3, and secondary stroke prevention. He had achieved an excellent functional outcome (mRs 1) at 6-month follow-up.


Conclusion
Our patient had a satisfactory recovery due to prompt diagnosis, secondary stroke prevention, and compliance with treatment.


LEARNING POINTS
In the presence of acute dysarthria and dysphagia, Foix-Chavany-Marie syndrome (FCMS) should be considered.FCMS may occur in the presence of unilateral opercular stroke.Swallowing and speech therapy play an essential role in rehabilitation after the acute setting."
29c151e9d8fe3681eac0e494903b56bdfa6c7c6d,"
 Background:
 Ischemic stroke has been reported to occur in approximately 5% of COVID-19 patients, although some reports are contradictory. Proposed mechanisms of this association are hypercoagulable state, vasculitis and cardiomyopathy, together with traditional vascular risk factors. We analyzed the frequency and clinical characteristics of COVID-19 positive stroke cases during the first months of the pandemic in Latin America.
 
 
 Methods:
 A multinational study (7 countries, 18 centers) of patients admitted during the pandemic outbreak (March - June 2020). We assessed acute stroke cases associated to COVID-19 infection. Clinical characteristics, stroke etiology and severity, acute care and functional outcomes, were compared between non-COVID-19 and COVID-19 cases.
 
 
 Results:
 There were a total of 1037 stroke cases; sixty-two of them (6.0%) were diagnosed with COVID-19 infection. This group consisted of 38 men [61.3%], with a median age of 68 years [IQR 59-79 years]. From these cases, 80.6% were ischemic stroke, 16.1% hemorrhagic stroke, and 1.6% transient ischemic attack and cerebral venous thrombosis respectively. The most common etiology reported for ischemic cases was atherosclerotic large vessel occlusion (30.6% vs. 12.7% in non-COVID cases, p<0.001), and undetermined etiology for hemorrhagic stroke (55.6%). Median NIHSS for COVID-stroke patients was higher (7 IQR 2-16 vs. 5 IQR 2-11, p=0.05). Five (8.1%) patients received acute reperfusion therapy, with no differences in door-to-CT, door-to-needle and door-to-groin times, compared to non-COVID cases. Most characteristics did not differ from those of COVID-19 negative patients. Mortality was higher in COVID-stroke cases (20.9% vs. 9.6%, p<0.001).
 
 
 Conclusions:
 COVID-19 infection frequency in stroke patients in Latin America is similar to that reported in several series worldwide, with a higher frequency of atherosclerotic ischemic strokes and mortality compared to non COVID-19 strokes
"
2dcd06d6268699f3c4efbe7275766ae23b2efe37,"Background: COVID-19 pandemic has forced important changes in health care worldwide. Stroke care networks have been affected, especially acute admissions and ancillary tests availability. We assess..."
34cce0fba6a503774800541971f5aff579353e4b,
3c4ae995d4eba174515e1072027e2307b2df711d,
3e7b40a0855c3dfa047d360c902cc18f371fc6fd,"Importance
Thrombosis with thrombocytopenia syndrome (TTS) has been reported after vaccination with the SARS-CoV-2 vaccines ChAdOx1 nCov-19 (Oxford-AstraZeneca) and Ad26.COV2.S (Janssen/Johnson & Johnson).


Objective
To describe the clinical characteristics and outcome of patients with cerebral venous sinus thrombosis (CVST) after SARS-CoV-2 vaccination with and without TTS.


Design, Setting, and Participants
This cohort study used data from an international registry of consecutive patients with CVST within 28 days of SARS-CoV-2 vaccination included between March 29 and June 18, 2021, from 81 hospitals in 19 countries. For reference, data from patients with CVST between 2015 and 2018 were derived from an existing international registry. Clinical characteristics and mortality rate were described for adults with (1) CVST in the setting of SARS-CoV-2 vaccine-induced immune thrombotic thrombocytopenia, (2) CVST after SARS-CoV-2 vaccination not fulling criteria for TTS, and (3) CVST unrelated to SARS-CoV-2 vaccination.


Exposures
Patients were classified as having TTS if they had new-onset thrombocytopenia without recent exposure to heparin, in accordance with the Brighton Collaboration interim criteria.


Main Outcomes and Measures
Clinical characteristics and mortality rate.


Results
Of 116 patients with postvaccination CVST, 78 (67.2%) had TTS, of whom 76 had been vaccinated with ChAdOx1 nCov-19; 38 (32.8%) had no indication of TTS. The control group included 207 patients with CVST before the COVID-19 pandemic. A total of 63 of 78 (81%), 30 of 38 (79%), and 145 of 207 (70.0%) patients, respectively, were female, and the mean (SD) age was 45 (14), 55 (20), and 42 (16) years, respectively. Concomitant thromboembolism occurred in 25 of 70 patients (36%) in the TTS group, 2 of 35 (6%) in the no TTS group, and 10 of 206 (4.9%) in the control group, and in-hospital mortality rates were 47% (36 of 76; 95% CI, 37-58), 5% (2 of 37; 95% CI, 1-18), and 3.9% (8 of 207; 95% CI, 2.0-7.4), respectively. The mortality rate was 61% (14 of 23) among patients in the TTS group diagnosed before the condition garnered attention in the scientific community and 42% (22 of 53) among patients diagnosed later.


Conclusions and Relevance
In this cohort study of patients with CVST, a distinct clinical profile and high mortality rate was observed in patients meeting criteria for TTS after SARS-CoV-2 vaccination."
437ea28cabf5491a3e7c9d82ae32af2503c3546c,"Cerebral venous thrombosis (CVT) is an uncommon form of stroke affecting mostly young individuals. Although genetic factors are thought to play a role in this cerebrovascular condition, its genetic etiology is not well understood."
6c297e1d56ee4c433d09a04fed7184a24b6fc94c,
6e4ba3acc9688f0f055608c99cf15b80b936cbf1,"Background and Objectives There is a worldwide increase in the incidence of stroke in young adults, with major regional and ethnic differences. Advancing knowledge of ethnic and regional variation in causes and outcomes will be beneficial in implementation of regional health care services. We studied the global distribution of risk factors, causes, and 3-month mortality of young patients with ischemic stroke, by performing a patient data meta-analysis from different cohorts worldwide. Methods We performed a pooled analysis of individual patient data from cohort studies that included consecutive patients with ischemic stroke aged 18–50 years. We studied differences in prevalence of risk factors and causes of ischemic stroke between different ethnic and racial groups, geographic regions, and countries with different income levels. We investigated differences in 3-month mortality by mixed-effects multivariable logistic regression. Results We included 17,663 patients from 32 cohorts in 29 countries. Hypertension and diabetes were most prevalent in Black (hypertension, 52.1%; diabetes, 20.7%) and Asian patients (hypertension 46.1%, diabetes, 20.9%). Large vessel atherosclerosis and small vessel disease were more often the cause of stroke in high-income countries (HICs; both p < 0.001), whereas “other determined stroke” and “undetermined stroke” were higher in low and middle-income countries (LMICs; both p < 0.001). Patients in LMICs were younger, had less vascular risk factors, and despite this, more often died within 3 months than those from HICs (odds ratio 2.49; 95% confidence interval 1.42–4.36). Discussion Ethnoracial and regional differences in risk factors and causes of stroke at young age provide an understanding of ethnic and racial and regional differences in incidence of ischemic stroke. Our results also highlight the dissimilarities in outcome after stroke in young adults that exist between LMICs and HICs, which should serve as call to action to improve health care facilities in LMICs."
7fef0278bb52f696fb251c8f471dece48e3df8d5,
9f1c85c1a364c0242bc6a189fb03948e18443a85,
a67eb78980e6d0c568782f68b63af330712f5e79,Background: Identification of predictive factors for favorable functional outcome after acute ischemic stroke is crucial. 1 Minor stroke (MS) is the most common exclusion criteria for intravenous t...
ac962ea27ecab88fe33d2653108c9e7e1721f211,
acbd3de99fe3b8d4982b3c869ad5c7a5d9b9c41f,
c7598a08bfa93fcd659c0e96c31b51e88cd51d11,"Introduction: Stroke is one of the leading causes of death in Latin America, a region with countless gaps to be addressed to decrease its burden. In 2018, at the first Latin American Stroke Ministerial Meeting, stroke physician and healthcare manager representatives from 13 countries signed the Declaration of Gramado with the priorities to improve the region, with the commitment to implement all evidence-based strategies for stroke care. The second meeting in March 2020 reviewed the achievements in 2 years and discussed new objectives. This paper will review the 2-year advances and future plans of the Latin American alliance for stroke. Method: In March 2020, a survey based on the Declaration of Gramado items was sent to the neurologists participants of the Stroke Ministerial Meetings. The results were confirmed with representatives of the Ministries of Health and leaders from the countries at the second Latin American Stroke Ministerial Meeting. Results: In 2 years, public stroke awareness initiatives increased from 25 to 75% of countries. All countries have started programs to encourage physical activity, and there has been an increase in the number of countries that implement, at least partially, strategies to identify and treat hypertension, diabetes, and lifestyle risk factors. Programs to identify and treat dyslipidemia and atrial fibrillation still remained poor. The number of stroke centers increased from 322 to 448, all of them providing intravenous thrombolysis, with an increase in countries with stroke units. All countries have mechanical thrombectomy, but mostly restricted to a few private hospitals. Pre-hospital organization remains limited. The utilization of telemedicine has increased but is restricted to a few hospitals and is not widely available throughout the country. Patients have late, if any, access to rehabilitation after hospital discharge. Conclusion: The initiative to collaborate, exchange experiences, and unite societies and governments to improve stroke care in Latin America has yielded good results. Important advances have been made in the region in terms of increasing the number of acute stroke care services, implementing reperfusion treatments and creating programs for the detection and treatment of risk factors. We hope that this approach can reduce inequalities in stroke care in Latin America and serves as a model for other under-resourced environments."
d3c9c6d4341b86e00dc12f073675c4d46392b965,"Empiric anticoagulation is not routinely indicated in patients with cryptogenic stroke without documentation of atrial fibrillation (AF). Therefore, identification of patients at increased risk of AF from this vulnerable group is vital."
d40090a8623b780c1dd71fac30be6e652c3f3616,
d556f5afc0ffbb3d080895d86436383d173d1e72,"Importance
Cases of cerebral venous sinus thrombosis in combination with thrombocytopenia have recently been reported within 4 to 28 days of vaccination with the ChAdOx1 nCov-19 (AstraZeneca/Oxford) and Ad.26.COV2.S (Janssen/Johnson & Johnson) COVID-19 vaccines. An immune-mediated response associated with platelet factor 4/heparin antibodies has been proposed as the underlying pathomechanism.


Objective
To determine the frequencies of admission thrombocytopenia, heparin-induced thrombocytopenia, and presence of platelet factor 4/heparin antibodies in patients diagnosed with cerebral venous sinus thrombosis prior to the COVID-19 pandemic.


Design, Setting, and Participants
This was a descriptive analysis of a retrospective sample of consecutive patients diagnosed with cerebral venous sinus thrombosis between January 1987 and March 2018 from 7 hospitals participating in the International Cerebral Venous Sinus Thrombosis Consortium from Finland, the Netherlands, Switzerland, Sweden, Mexico, Iran, and Costa Rica. Of 952 patients, 865 with available baseline platelet count were included. In a subset of 93 patients, frozen plasma samples collected during a previous study between September 2009 and February 2016 were analyzed for the presence of platelet factor 4/heparin antibodies.


Exposures
Diagnosis of cerebral venous sinus thrombosis.


Main Outcomes and Measures
Frequencies of admission thrombocytopenia (platelet count <150 ×103/μL), heparin-induced thrombocytopenia (as diagnosed by the treating physician), and platelet factor 4/heparin IgG antibodies (optical density >0.4, in a subset of patients with previously collected plasma samples).


Results
Of 865 patients (median age, 40 years [interquartile range, 29-53 years], 70% women), 73 (8.4%; 95% CI, 6.8%-10.5%) had thrombocytopenia, which was mild (100-149 ×103/μL) in 52 (6.0%), moderate (50-99 ×103/μL) in 17 (2.0%), and severe (<50 ×103/μL) in 4 (0.5%). Heparin-induced thrombocytopenia with platelet factor 4/heparin antibodies was diagnosed in a single patient (0.1%; 95% CI, <0.1%-0.7%). Of the convenience sample of 93 patients with cerebral venous sinus thrombosis included in the laboratory analysis, 8 (9%) had thrombocytopenia, and none (95% CI, 0%-4%) had platelet factor 4/heparin antibodies.


Conclusions and Relevance
In patients with cerebral venous sinus thrombosis prior to the COVID-19 pandemic, baseline thrombocytopenia was uncommon, and heparin-induced thrombocytopenia and platelet factor 4/heparin antibodies were rare. These findings may inform investigations of the possible association between the ChAdOx1 nCoV-19 and Ad26.COV2.S COVID-19 vaccines and cerebral venous sinus thrombosis with thrombocytopenia."
d8fab12c8a8b03db4eafad95ca359fba0f52745e,Background: Cerebral Venous Thrombosis (CVT) is an infrequent type of cerebrovascular disease with variable functional outcomes. Different studies have been carried out to identify such predicting ...
091f59bac297378e615defcdf4fac32d636c7ffc,"Coronavirus disease 2019 (COVID-19) is a global pandemic. As COVID-19 cases rise in different countries, neurologists and neurologists-in-training will be increasingly involved in the care of these patients from both a general medical and neurologic perspective. In the same way, patients with preexisting neurologic conditions are in no way exempt from acquiring this infection. On the other hand, new-onset neurologic manifestations have also been recently reported in previously healthy people infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).1 It is essential for neurologists and neurologists-in-training to be aware of the repercussions that COVID-19 may have in our daily practice."
121c3addc393d0c1d9f8c9f60bea8c7392480719,
2a1d4c9c8ee1c4e2bbd9111ae3f8426754074afb,"Importance
The concept of embolic stroke of undetermined source (ESUS) unifies a subgroup of cryptogenic strokes based on neuroimaging, a defined minimum set of diagnostic tests, and exclusion of certain causes. Despite an annual stroke recurrence rate of 5%, little is known about the etiology underlying recurrent stroke after ESUS.


Objective
To identify the stroke subtype of recurrent ischemic strokes after ESUS, to explore the interaction with treatment assignment in each category, and to examine the consistency of cerebral location of qualifying ESUS and recurrent ischemic stroke.


Design, Setting, and Participants
The NAVIGATE-ESUS trial was a randomized clinical trial conducted from December 23, 2014, to October 5, 2017. The trial compared the efficacy and safety of rivaroxaban and aspirin in patients with recent ESUS (n = 7213). Ischemic stroke was validated in 309 of the 7213 patients by adjudicators blinded to treatment assignment and classified by local investigators into the categories ESUS or non-ESUS (ie, cardioembolic, atherosclerotic, lacunar, other determined cause, or insufficient testing). Five patients with recurrent strokes that could not be defined as ischemic or hemorrhagic in absence of neuroimaging or autopsy were excluded. Data for this secondary post hoc analysis were analyzed from March to June 2019.


Interventions
Patients were randomly assigned to receive rivaroxaban, 15 mg/d, or aspirin, 100 mg/d.


Main Outcomes and Measures
Association of recurrent ESUS with stroke characteristics.


Results
A total of 309 patients (205 men [66%]; mean [SD] age, 68 [10] years) had ischemic stroke identified during the median follow-up of 11 (interquartile range [IQR], 12) months (annualized rate, 4.6%). Diagnostic testing was insufficient for etiological classification in 39 patients (13%). Of 270 classifiable ischemic strokes, 156 (58%) were ESUS and 114 (42%) were non-ESUS (37 [32%] cardioembolic, 26 [23%] atherosclerotic, 35 [31%] lacunar, and 16 [14%] other determined cause). Atrial fibrillation was found in 27 patients (9%) with recurrent ischemic stroke and was associated with higher morbidity (median change in modified Rankin scale score 2 [IQR, 3] vs 0 (IQR, 1]) and mortality (15% vs 1%) than other causes. Risk of recurrence did not differ significantly by subtype between treatment groups. For both the qualifying and recurrent strokes, location of infarct was more often in the left (46% and 54%, respectively) than right hemisphere (40% and 37%, respectively) or brainstem or cerebellum (14% and 9%, respectively).


Conclusions and Relevance
In this secondary analysis of randomized clinical trial data, most recurrent strokes after ESUS were embolic and of undetermined source. Recurrences associated with atrial fibrillation were a minority but were more often disabling and fatal. More extensive investigation to identify the embolic source is important toward an effective antithrombotic strategy.


Trial Registration
ClinicalTrials.gov Identifier: NCT02313909."
4c8d6eb802d38e474d45abcf39260675f342a9da,"Background Reports on sex differences in stroke outcome and risk factors are scarce in Latin America. Our objective was to analyze clinical and prognostic differences according to sex among participants in the LASE (Latin American Stroke Registry). Methods and Results Nineteen centers across Central and South America compiled data on demographics, vascular risk factors, clinical stroke description, ancillary tests, and functional outcomes at short‐term follow‐up of patients included from January 2012 to January 2017. For the present study, all these variables were analyzed according to sex at hospital discharge. We included 4788 patients with a median in‐hospital stay of 8 days (interquartile range, 5–8); 2677 were male (median age, 66 years) and 2111 female (median age, 60 years). Ischemic stroke occurred in 4293: 3686 as cerebral infarction (77%) and 607 as transient ischemic attack cases (12.7%); 495 patients (10.3%) corresponded to intracerebral hemorrhage. Poor functional outcome (modified Rankin scale, 3–6) was present in 1662 (34.7%) patients and 38.2% of women (P<0.001). Mortality was present in 6.8% of the registry, with 7.8% in women compared with 6.0% in men (P=0.01). Death and poor functional outcome for all‐type stroke showed a higher risk in female patients (hazard ratio, 1.3, P=0.03; and hazard ratio, 1.1, P=0.001, respectively). Conclusions A worse functional outcome and higher mortality rates occurred in women compared with men in the LASE, confirming sex differences issues at short‐term follow‐up."
4d27e22ef6ee5aaf4d6c32631c22db44372809cf,"Information about clinical outcome and reperfusion treatment in Latin-America is not widely known. Our aim was to determine this association and other clinical factors according to the initial clinical severity in functional outcome in the population from the Latin-America Stroke Registry (LASE).
 
 Methods:
 Data were collected prospectively from consecutive patients admitted from January 2012-January 2017 in 20 Latin-American centers. The initial clinical severity was stratified according to the baseline NIHSS; 0-3 minor, 4-7 mild, 8-14 moderate, and ≥15 severe. Clinical outcome was defined by mRS at 3 months, dichotomized in good (0-2) and poor (3-6) groups for univariate and logistic multivariate analysis.
 
 
 Results:
 Among 5381 patients, 823(15.3%) patients received any reperfusion treatment. Of these, 84.7% received I.V. fibrinolytic, 13.2% bridging therapy, and 0.9 % primary endovascular treatment. In overall, according to initial clinical deficit by NIHSS, reperfusion treatment was associated with good outcome in mild (82.8% Vs 66.1% p=0.005), moderate (55.9 % vs. 41.6% p=0.002) and severe deficits (29.4 Vs. 18.8% p=0.002). In patients with a baseline NIHSS ≥4, multivariate analysis showed that lower NIHSS (p<0.001 OR: 1,194 CI95%: 1,169-1,219), reperfusion treatment (p<0.001 OR: 2.132 CI95%: 1.625-2.797) and urban setting (p<0.001 OR: 2.310 CI95%: 1,702-3,134) were predictors for good outcome. In reperfusion treatment patients group, multivariate analysis showed that the presence of prior ischemic cardiopathy (p=0.009 RO: 4,304 CI95%: 1,430-12,953) and lower baseline NIHSS (p< 0.001 OR: 1,232 CI95%: 1,173- 1,293) were independent variables associated with a good outcome.
 
 
 Conclusion:
 In this study, the rate of reperfusion treatment in 20 stroke Latin-American centers is lower than that reported from other world regions. A significant association between reperfusion treatment and good clinical outcome was observed in mild up to severe baseline deficit in our study. The effect of ischemic cardiopathy in clinical outcome in reperfusion treated patients deserves further studies. Its association with recanalization, remote pre-conditioning ischemia and collateral circulation could be explored in our population.
"
5cae97b64b9bc2131abc1951b2a21ef3c5cd8981,"Objectives
Left atrial disease is an independent risk factor for ischemic stroke and can be used to predict atrial fibrillation (AF). We examine whether left atrial enlargement (LAE) could predict stroke recurrence in patients with embolic stroke of undetermined source (ESUS).


Materials and methods
Sixty-four patients with a confirmed diagnosis of ESUS were followed for a median of 22 months. Clinical data and echocardiogram findings were recorded. The echocardiogram interpretation was performed centrally and blindly. The Brown ESUS - AF score was used to categorize patients into high (human resource planning [HRP]: score > 2) and low-risk patients (non-HRP score 0-1). Stroke recurrence was the primary outcome.


Results
The median age was 62 years (range: 22-85 years); and 33 (51.6%) were men. The median initial NIHSS score was three points (range: 0-27). Twelve (18.8%) patients were categorized as HRP. We found a significant tendency toward recurrence among HRP versus non-HRP patients. Three (25%) HRP versus 2 (3.8%) non-HRP experienced recurrence (OR: 8.3 95% CI 1.2-57; p=0.042); this association was related to severe atrial dilatation (OR: 14.5 95% CI 0.78-277, p = 0.02) and age > 75 years (OR: 12.7 95% CI 1.7-92.2, p = 0.03). We found no differences in recurrence in a univariate analysis.


Conclusions
Patients with severe LAE who are 75 years old or older have a significant tendency to experience stroke recurrence."
608f3af147896efb979d917e0b12fc034d0e7a33,
66a5c05f90e32f303b9b0a29717c8e182b239b70,"Objective To examine the incidence, characteristics, treatment, and predictors of late seizures (LS) after cerebral venous thrombosis (CVT), we described these features in a registry of 1,127 patients with CVT. Methods We included consecutive adult patients from an international consortium of 12 hospital-based CVT registries. We excluded patients with a history of epilepsy or with <8 days of follow-up. We defined LS as seizures occurring >7 days after diagnosis of CVT. We used multivariable Cox regression to identify predictors of LS. Results We included 1,127 patients with CVT. During a median follow-up of 2.0 years (interquartile range [IQR] 1.0–6.3), 123 patients (11%) experienced ≥1 LS (incidence rate for first LS 30 per 1,000 person-years, 95% confidence interval [CI] 25–35). Median time to first LS was 5 months (IQR 1–16 months). Baseline predictors of LS included status epilepticus in the acute phase (hazard ratio [HR] 7.0, 95% CI 3.9–12.6), decompressive hemicraniectomy (HR 4.2, 95% CI 2.4–7.3), acute seizure(s) without status epilepticus (HR 4.1, 95% CI 2.5–6.5), subdural hematoma (HR 2.3, 95% CI 1.1–4.9), and intracerebral hemorrhage (HR 1.9, 95% CI 1.1–3.1). Eighty-five patients (70% of patients with LS) experienced a recurrent seizure during follow-up, despite the fact that 94% received antiepileptic drug treatment after the first LS. Conclusion During a median follow-up of 2 years, ≈1 in 10 patients with CVT had LS. Patients with baseline intracranial bleeding, patients with acute symptomatic seizures, and those who underwent decompressive hemicraniectomy were at increased risk of developing LS. The high recurrence risk of LS justifies epilepsy diagnosis after a first LS."
77f1754d3a4b91c51af8c7eb8a11524aa2ddbb89,
7a25937ae8ce8a66289dac034919dcb8f0a2b878,"During the first hours after stroke onset neurological deficits can be highly unstable: some patients rapidly improve, while others deteriorate. This early neurological instability has a major impact on long-term outcome. Here, we aimed to determine the genetic architecture of early neurological instability measured by the difference between NIH stroke scale (NIHSS) within six hours of stroke onset and NIHSS at 24h ({Delta}NIHSS). A total of 5,876 individuals from seven countries (Spain, Finland, Poland, United States, Costa Rica, Mexico and Korea) were studied using a multi-ancestry meta-analyses. We found that 8.7% of {Delta}NIHSS variance was explained by common genetic variations, and also that early neurological instability has a different genetic architecture than that of stroke risk. Seven loci (2p25.1, 2q31.2, 2q33.3, 4q34.3, 5q33.2, 6q26 and 7p21.1) were genome-wide significant and explained 2.1% of the variability suggesting that additional variants influence early change in neurological deficits. We used functional genomics and bioinformatic annotation to identify the genes driving the association from each loci. eQTL mapping and SMR indicate that ADAM23 (log Bayes Factor (LBF)=6.34) was driving the association for 2q33.3. Gene based analyses suggested that GRIA1 (LBF=5.26), which is predominantly expressed in brain, is the gene driving the association for the 5q33.2 locus. These analyses also nominated PARK2 (LBF=5.30) and ABCB5 (LBF=5.70) for the 6q26 and 7p21.1 loci. Human brain single nuclei RNA-seq indicates that the gene expression of ADAM23 and GRIA1 is enriched in neurons. ADAM23, a pre-synaptic protein, and GRIA1, a protein subunit of the AMPA receptor, are part of a synaptic protein complex that modulates neuronal excitability. These data provides the first evidence in humans that excitotoxicity may contribute to early neurological instability after acute ischemic stroke."
7baf08bbc368817cfb9d1470117247a1b761bcff,"Objective To identify characteristics, predictors, and outcomes of acute symptomatic seizures (ASS) in cerebral venous thrombosis (CVT), we investigated 1,281 consecutive adult patients with CVT included from 12 hospitals within the International CVT Consortium. Methods We defined ASS as any seizure between symptom onset and 7 days after diagnosis of CVT. We stratified ASS into prediagnosis and solely postdiagnosis ASS. Status epilepticus (SE) was also analyzed separately. We analyzed predictors for ASS and the association between ASS and clinical outcome (modified Rankin Scale) with multivariable logistic regression. Results Of 1,281 eligible patients, 441 (34%) had ASS. Baseline predictors for ASS were intracerebral hemorrhage (ICH; adjusted odds ratio [aOR] 4.1, 95% confidence interval [CI] 3.0–5.5), cerebral edema/infarction without ICH (aOR 2.8, 95% CI 2.0–4.0), cortical vein thrombosis (aOR 2.1, 95% CI 1.5–2.9), superior sagittal sinus thrombosis (aOR 2.0, 95% CI 1.5–2.6), focal neurologic deficit (aOR 1.9, 95% CI 1.4–2.6), sulcal subarachnoid hemorrhage (aOR 1.6, 95% CI 1.1–2.5), and female-specific risk factors (aOR 1.5, 95% CI 1.1–2.1). Ninety-three (7%) patients had solely postdiagnosis ASS, best predicted by cortical vein thrombosis (positive/negative predictive value 22%/92%). Eighty (6%) patients had SE, independently predicted by ICH, focal neurologic deficits, and cerebral edema/infarction. Neither ASS nor SE was independently associated with outcome. Conclusion ASS occurred in one-third of patients with CVT and was associated with brain parenchymal lesions and thrombosis of the superficial system. In the absence of prediagnosis ASS, no subgroup was identified with sufficient risk of postdiagnosis ASS to justify prophylactic antiepileptic drug treatment. We found no association between ASS and outcome."
808a442648e01bfc581e7089e8d582f7d2ff8d81,"Supplemental Digital Content is available in the text. Background and Purpose— The hypothesis that venous recanalization prevents progression of venous infarction is not established in patients with cerebral venous thrombosis (CVT). Evidence is also scarce on the association between residual symptoms, particularly headache, and the recanalization grade. We aimed to assess, in patients with CVT treated with standard anticoagulation, (1) the rate of early venous recanalization, (2) whether lack of early recanalization was predictor of parenchymal brain lesion progression, and (3) the prevalence and features of persistent headache according to the recanalization grade achieved. Methods— PRIORITy-CVT (Pathophysiology of Venous Infarction – Prediction of Infarction and Recanalization in CVT) was a multicenter, prospective, cohort study including patients with newly diagnosed CVT. Standardized magnetic resonance imaging was performed at inclusion (≤24 hours of therapeutic anticoagulation), days 8 and 90. Potential imaging predictors of recanalization were predefined and analyzed at each anatomical segment. Primary outcomes were rate of early recanalization and brain lesion progression at day 8. Secondary outcomes were headache (days 8 and 90) and functional outcome (modified Rankin Scale at days 8 and 90). Results— Sixty eight patients with CVT were included, of whom 30 (44%) had parenchymal lesions. At the early follow-up (n=63; 8±2 days), 68% (n=43) of patients had partial recanalization and 6% (n=4) full recanalization. Early recanalization was associated both with early regression (P=0.03) and lower risk of enlargement of nonhemorrhagic lesions (P=0.02). Lesions showing diffusion restriction (n=12) were fully reversible in 66% of cases, particularly in patients showing early venous recanalization. Evidence of new or enlarged hemorrhagic lesions, headache at days 8 and 90, and unfavorable functional outcome at days 8 and 90 were not significantly different in patients achieving recanalization. Conclusions— Venous recanalization started within the first 8 days of therapeutic anticoagulation in most patients with CVT and was associated with early regression of nonhemorrhagic lesions, including venous infarction. There was an association between persistent venous occlusion at day 8 and enlargement of nonhemorrhagic lesions."
964de0939f1943de3bedd7ad77b7cef298685adf,
9b6274d02faeb5a4ceda9b1125ce10c68662ceba,
ac8e525c7219c3e70024a14fb3e153413ca1cce2,
ad51fa27a6bb8516e38ec5f7fa7f166b7570d1a9,"The diagnosis and treatment of acute ischemic stroke (AIS) have undergone great changes in the past few decades. The keys to a correct selection of the best therapeutic modality are an early identification of patients with AIS symptoms, the correct interpretation of the different neuroimaging techniques, and, if possible, to provide a reperfusion therapy as soon as possible. This review will analyze the principles of each of the new neuroimaging techniques for AIS, how these new techniques can help in clinical practice, the different treatment options, as well as the inclusion and exclusion criteria for each of them. A systematic research was conducted on MEDLINE (PubMed), using the following Medical Subject Headings terms: (AIS) + (neuroimaging/techniques) + (thrombolytic therapy) + (mechanical thrombectomy). We selected original articles, as well as clinical trials and review articles. Each article was read to completion, to check for other useful references. We also reviewed the different therapeutic options for AIS, including endovascular approaches through a selection of the most recent neuroimaging techniques. In conclusion, intravenous thrombolysis continues to be the cornerstone for the treatment of AIS. The attending physician has a fundamental role in suspecting and confirming large vessel occlusions, and in deciding which is the most appropriate therapeutic option for each case, with important prognostic implications for the patient."
bebfddae074bea72ccbc2193f43eb0f8d6f67cc6,Supplemental Digital Content is available in the text.
d7174937e665be5ea870c1662ece2e56e22630f7,
dd2fcdf05d8f42faa6943e4e50167bd9167b8e87,"Introduction Despite the extremely favourable prognosis of patients with cerebral venous thrombosis (CVT), death occurs in 10–15% of patients. In severe cases of malignant CVT with supratentorial haemorrhagic lesions, cerebral oedema and brain herniation, decompressive surgery may be the only life-saving treatment. Patient and methods We present the case of a puerperal young woman with progressive headache, seizures and decreased alertness. Thrombosis of the entire superior sagittal sinus with bifrontal venous infarcts and midline shift was confirmed by magnetic resonance imaging with venography sequencing. Despite medical treatment with anticoagulation, progressive neurological deterioration was observed, so bilateral, frontal decompressive craniectomy was performed. Results At the 6-month follow-up, we observed partial functional recovery with a modified Rankin score of 3. Discussion Bilateral decompressive craniectomy may be a life-saving therapeutic option when medical therapy fails and there are clinical and radiological features of progression in both cerebral hemispheres. LEARNING POINTS Malignant cerebral venous thrombosis presents with diffuse haemorrhagic lesions or cerebral oedema associated with brain herniation and rapid clinical deterioration. Decompressive surgery may be the only life-saving treatment in severe cases. Early and adequate intervention based on the characteristics of the lesions allowed preservation of life and improvement in long-term functionality"
e68240ccc610d60691048cd7745b88e2a6fb2e3d,
1c091d845922b79140eb8e45336c3dce930304ba,
21b4983709cf88aedc70eaabac836758f40cafca,"Objective To assess the added diagnostic value of semiquantitative imaging markers on noncontrast CT scans in cerebral venous thrombosis (CVT). Methods In a retrospective, multicenter, blinded, case-control study of patients with recent onset (<2 weeks) CVT, 3 readers assessed (1) the accuracy of the visual impression of CVT based on a combination of direct and indirect signs, (2) the accuracy of attenuation values of the venous sinuses in Hounsfield units (with adjustment for hematocrit levels), and (3) the accuracy of attenuation ratios of affected vs unaffected sinuses in comparison with reference standard MRI or CT angiography. Controls were age-matched patients with (sub)acute neurologic presentations. Results We enrolled 285 patients with CVT and 303 controls from 10 international centers. Sensitivity of visual impression of thrombosis ranged from 41% to 73% and specificity ranged from 97% to 100%. Attenuation measurement had an area under the curve (AUC) of 0.78 (95% confidence interval [CI] 0.74–0.81). After adjustment for hematocrit, the AUC remained 0.78 (95% CI 0.74–0.81). The analysis of attenuation ratios of affected vs unaffected sinuses had AUC of 0.83 (95% CI 0.8–0.86). Adding this imaging marker significantly improved discrimination, but sensitivity when tolerating a false-positive rate of 20% was not higher than 76% (95% CI 0.70–0.81). Conclusion Semiquantitative analysis of attenuation values for diagnosis of CVT increased sensitivity but still failed to identify 1 out of 4 CVT. Classification of evidence This study provides Class II evidence that visual analysis of plain CT with or without attenuation measurements has high specificity but only moderate sensitivity for CVT."
43344c87c3733dc406629de0025e63899e9c5d4d,
6099c6fc5363e7355d66df18f088670f86835bc0,Anaemia is associated with poor clinical outcome after ischaemic and haemorrhagic stroke. The association between anaemia and outcome in patients with cerebral venous thrombosis (CVT) was examined.
65538b018029c9f3ad0cea09b30097b5c948c958,
727cd6d52b4848415a47c7740e2937746fd8cefb,"Introduction Worldwide, 2 million patients aged 18–50 years suffer a stroke each year, and this number is increasing. Knowledge about global distribution of risk factors and aetiologies, and information about prognosis and optimal secondary prevention in young stroke patients are limited. This limits evidence-based treatment and hampers the provision of appropriate information regarding the causes of stroke, risk factors and prognosis of young stroke patients. Methods and analysis The Global Outcome Assessment Life-long after stroke in young adults (GOAL) initiative aims to perform a global individual patient data meta-analysis with existing data from young stroke cohorts worldwide. All patients aged 18–50 years with ischaemic stroke or intracerebral haemorrhage will be included. Outcomes will be the distribution of stroke aetiology and (vascular) risk factors, functional outcome after stroke, risk of recurrent vascular events and death and finally the use of secondary prevention. Subgroup analyses will be made based on age, gender, aetiology, ethnicity and climate of residence. Ethics and dissemination Ethical approval for the GOAL study has already been obtained from the Medical Review Ethics Committee region Arnhem-Nijmegen. Additionally and when necessary, approval will also be obtained from national or local institutional review boards in the participating centres. When needed, a standardised data transfer agreement will be provided for participating centres. We plan dissemination of our results in peer-reviewed international scientific journals and through conference presentations. We expect that the results of this unique study will lead to better understanding of worldwide differences in risk factors, causes and outcome of young stroke patients."
752fc8bc62e1f69a7b34cbce2054013be38922dc,"Introduction: Stroke in young adults is not rare, and can have a devastating, lasting impact. Up to 20% of patients with Embolic Stroke of Undetermined Source (ESUS) are under 50 years of age; thus..."
79c16e29cb05fa048faf0f2e0fd6997d6ebaccc4,
83572ee87080328a79e98a238710fcc3d07cbe1e,
b6b60f7846e17e1ebb03dc20b1473cdf019eaa3d,"Importance
The NAVIGATE ESUS randomized clinical trial found that 15 mg of rivaroxaban per day does not reduce stroke compared with aspirin in patients with embolic stroke of undetermined source (ESUS); however, it substantially reduces stroke risk in patients with atrial fibrillation (AF).


Objective
To analyze whether rivaroxaban is associated with a reduction of recurrent stroke among patients with ESUS who have an increased risk of AF.


Design, Setting, and Participants
Participants were stratified by predictors of AF, including left atrial diameter, frequency of premature atrial contractions, and HAVOC score, a validated scheme using clinical features. Treatment interactions with these predictors were assessed. Participants were enrolled between December 2014 and September 2017, and analysis began March 2018.


Intervention
Rivaroxaban treatment vs aspirin.


Main Outcomes and Measures
Risk of ischemic stroke.


Results
Among 7112 patients with a mean (SD) age of 67 (9.8) years, the mean (SD) HAVOC score was 2.6 (1.8), the mean (SD) left atrial diameter was 3.8 (1.4) cm (n = 4022), and the median (interquartile range) daily frequency of premature atrial contractions was 48 (13-222). Detection of AF during follow-up increased for each tertile of HAVOC score: 2.3% (score, 0-2), 3.0% (score, 3), and 5.8% (score, >3); however, neither tertiles of the HAVOC score nor premature atrial contractions frequency impacted the association of rivaroxaban with recurrent ischemic stroke (P for interaction = .67 and .96, respectively). Atrial fibrillation annual incidence increased for each tertile of left atrial diameter (2.0%, 3.6%, and 5.2%) and for each tertile of premature atrial contractions frequency (1.3%, 2.9%, and 7.0%). Among the predefined subgroup of patients with a left atrial diameter of more than 4.6 cm (9% of overall population), the risk of ischemic stroke was lower among the rivaroxaban group (1.7% per year) compared with the aspirin group (6.5% per year) (hazard ratio, 0.26; 95% CI, 0.07-0.94; P for interaction = .02).


Conclusions and Relevance
The HAVOC score, left atrial diameter, and premature atrial contraction frequency predicted subsequent clinical AF. Rivaroxaban was associated with a reduced risk of recurrent stroke among patients with ESUS and moderate or severe left atrial enlargement; however, this needs to be independently confirmed before influencing clinical practice."
cc2abc27370b5aaa12276ba82cad05cc39320b8e,"Objective A tool to stratify the risk of stroke recurrence in patients with embolic stroke of undetermined source (ESUS) could be useful in research and clinical practice. We aimed to determine whether a score can be developed and externally validated for the identification of patients with ESUS at high risk for stroke recurrence. Methods We pooled the data of all consecutive patients with ESUS from 11 prospective stroke registries. We performed multivariable Cox regression analysis to identify predictors of stroke recurrence. Based on the coefficient of each covariate of the fitted multivariable model, we generated an integer-based point scoring system. We validated the score externally assessing its discrimination and calibration. Results In 3 registries (884 patients) that were used as the derivation cohort, age, leukoaraiosis, and multiterritorial infarct were identified as independent predictors of stroke recurrence and were included in the final score, which assigns 1 point per every decade after 35 years of age, 2 points for leukoaraiosis, and 3 points for multiterritorial infarcts (acute or old nonlacunar). The rate of stroke recurrence was 2.1 per 100 patient-years (95% confidence interval [CI] 1.44–3.06) in patients with a score of 0–4 (low risk), 3.74 (95% CI 2.77–5.04) in patients with a score of 5–6 (intermediate risk), and 8.23 (95% CI 5.99–11.3) in patients with a score of 7–12 (high risk). Compared to low-risk patients, the risk of stroke recurrence was significantly higher in intermediate-risk (hazard ratio [HR] 1.78, 95% CI 1.1–2.88) and high-risk patients (HR 4.67, 95% CI 2.83–7.7). The score was well-calibrated in both derivation and external validation cohorts (8 registries, 820 patients) (Hosmer-Lemeshow test χ2: 12.1 [p = 0.357] and χ2: 21.7 [p = 0.753], respectively). The area under the curve of the score was 0.63 (95% CI 0.58–0.68) and 0.60 (95% CI 0.54–0.66), respectively. Conclusions The proposed score can assist in the identification of patients with ESUS at high risk for stroke recurrence."
d8fa91f6e004967f56d299b02b4be51025715234,"Background and aim The diagnosis of embolic stroke of undetermined source (ESUS) is based on excluding other more likely stroke etiologies, and therefore diagnostic testing plays an especially crucial role. Our objective was to compare the diagnostic testing by region, sex, and age among the participants of NAVIGATE-ESUS trial. Methods Participants were grouped according to five global regions (North America, Latin America, Western Europe, Eastern Europe and East Asia), age (<60, 60–74, and >75 years), and sex. Frequencies of each diagnostic test within areas of echocardiography, cardiac rhythm monitoring, and arterial imaging were described and compared across groups. A multivariable logistic regression model for each diagnostic test was fit to assess the independent influence of each of region, age, and sex and likelihood of testing. Results We included 6985 patients in the analysis (918 from North America; 746 from Latin America; 2853 from Western Europe; 1118 from Eastern Europe; 1350 from East Asia). Average age (highest in Western Europe (69 years), lowest in Eastern Europe (65 years)), % females (highest in Latin America (44%) and lowest in East Asia (31%)), and use of each diagnostic test varied significantly across regions. Region, but not sex, was independently associated with use of each diagnostic test examined. Transesophageal echocardiography and either CT or MR angiogram were more often used in younger patients. Conclusion Diagnostic testing differed by region, and less frequently by age, but not by sex. Our findings reflect the existing variations in global practice in diagnostic testing in ESUS patients."
e0d142617a11d25d7fb61b965122b49d2f9519db,"Background and Purpose— Low ankle-brachial index (ABI) identifies a stroke subgroup with high risk of recurrent stroke, cardiovascular events, and death. However, limited data exist on the relationship between low ABI and stroke in low and middle-income countries. Therefore, we evaluated the prevalence of ABI ⩽0.90 (which is diagnostic of peripheral artery disease) in nonembolic stroke patients or transient ischemic attack and assessed the correlation of low ABI with stroke risk, factors, and recurrent vascular events and death. Methods— Patients ≥45 years with acute transient ischemic attack or minor ischemic strokes were recruited consecutively from over 17 low-income and middle-income countries (Latin America [1543 patients], Middle East [1041 patients], North Africa [834 patients], and South Africa [217 patients]). The ABI measurement was performed at a single visit. Stroke recurrence and risk of new vascular events were assessed after 24 months of follow-up. Results— Among 3487 enrolled patients, abnormal ABI (<0.9) was present in 22.3 %. Patients with an ABI of ⩽0.9 were more likely (P<0.05) to be male, older, and have a history of peripheral artery disease, hypertension, and diabetes mellitus. During 2-year follow-up, the rate of major cardiovascular event was higher in patients with ABI <0.9 than those with ABI ≥0.9 (Kaplan-Meier estimates, 22.5%; 95% CI, 19.6–25.8 versus 13.7%; 21.4–15.1; P<0.001), and when ABI was categorized into 4 groups (⩽0.6; 95% CI, 0.6–0.9; 0.9–1; 1–1.4), the rate of major cardiovascular event was higher in those with ABI ⩽0.6 than the other groups (Kaplan-Meier estimates, 32.6%; 95% CI, 21.0–48.3 for ABI⩽0.6 versus 21.7%; 95% CI, 18.8–25.0 for ABI 0.6–0.9 versus 14.3%; 95% CI, 12.4–16.6 for ABI 0.9–1 versus 13.3%; 95% CI, 11.6–15.2 for ABI 1–1.4; P<0.001). Conclusions— Among patients with nonembolic ischemic stroke or transient ischemic attack, those with low ABI had a higher rate of vascular events and death in this population. Screening for ABI in stroke patients may help identify patients at high risk of future events."
0bb6adaa57b5464946312fd53f04c0bbf602fadb,"Background Embolic strokes of undetermined source represent 20% of ischemic strokes and are associated with a high rate of recurrence. Anticoagulant treatment with rivaroxaban, an oral factor Xa inhibitor, may result in a lower risk of recurrent stroke than aspirin. Methods We compared the efficacy and safety of rivaroxaban (at a daily dose of 15 mg) with aspirin (at a daily dose of 100 mg) for the prevention of recurrent stroke in patients with recent ischemic stroke that was presumed to be from cerebral embolism but without arterial stenosis, lacune, or an identified cardioembolic source. The primary efficacy outcome was the first recurrence of ischemic or hemorrhagic stroke or systemic embolism in a time‐to‐event analysis; the primary safety outcome was the rate of major bleeding. Results A total of 7213 participants were enrolled at 459 sites; 3609 patients were randomly assigned to receive rivaroxaban and 3604 to receive aspirin. Patients had been followed for a median of 11 months when the trial was terminated early because of a lack of benefit with regard to stroke risk and because of bleeding associated with rivaroxaban. The primary efficacy outcome occurred in 172 patients in the rivaroxaban group (annualized rate, 5.1%) and in 160 in the aspirin group (annualized rate, 4.8%) (hazard ratio, 1.07; 95% confidence interval [CI], 0.87 to 1.33; P=0.52). Recurrent ischemic stroke occurred in 158 patients in the rivaroxaban group (annualized rate, 4.7%) and in 156 in the aspirin group (annualized rate, 4.7%). Major bleeding occurred in 62 patients in the rivaroxaban group (annualized rate, 1.8%) and in 23 in the aspirin group (annualized rate, 0.7%) (hazard ratio, 2.72; 95% CI, 1.68 to 4.39; P<0.001). Conclusions Rivaroxaban was not superior to aspirin with regard to the prevention of recurrent stroke after an initial embolic stroke of undetermined source and was associated with a higher risk of bleeding. (Funded by Bayer and Janssen Research and Development; NAVIGATE ESUS ClinicalTrials.gov number, NCT02313909.)"
1094077326a9e4e1ec4646534b4246acb487b7e4,"Background: Most patients with cerebral venous thrombosis (CVT) have independent survival in the short term. However, identification of high-risk individuals with an unfavorable outcome is a challenging task. We aimed to develop a CVT grading scale (CVT-GS) to aid in the short-term clinical decision-making. Methods: We included 467 consecutive patients with CVT who were hospitalized from 1981 to 2015 in two third-level referral hospitals. Factors associated with 30-day mortality were selected with bivariate analyses to integrate a Cox proportional-hazards model to determine components of the final scoring. After the scale was configured, the prognostic performance was tested for prediction of short-term death or moderately impaired to death [modified Rankin scale (mRS) > 2]. CVT-GS was categorized as mild, moderate or severe for the prediction of 30-day fatality rate and a probability of mRS > 2. Results: The 30-day case fatality rate was 9.0%. The CVT-GS (0–13 points; more points predicting poorer outcomes) was composed of parenchymal lesion size > 6 cm (3 points), bilateral Babinski signs (3 points), male sex (2 points), parenchymal hemorrhage (2 points), and level of consciousness (coma: 3 points, stupor: 2, somnolence: 1, and alert: 0). CVT was categorized as mild (0–2 points, 0.4% fatality rate), moderate (3–7 points, 9.9% fatality rate), or severe (8–13 points, 61.4% fatality rate). The CVT-GS had an accuracy of 91.6% for the prediction of 30-day mortality and 85.3% for mRS > 2. Conclusions: CVT-GS is a practical clinical tool for prediction of outcome after CVT. This score may aid in clinical decision-making and could serve to stratify patients enrolled in clinical trials."
1c28a9ba0f3997a13b111ab3b5ba087927c7ccf0,"The editor-in-chief extends her appreciation to the editorial board members and to all ad hoc reviewers whose comments and criticisms ensure the timeliness and quality of the papers published in this journal. We are especially grateful to those members of the editorial board who, after serving for many years, have retired from the board, and we look forward to the new colleagues who will join us. Finally, we thank all of you, authors and contributors who give this journal the international standing in the research into dementing disorders that it enjoys."
30d8b54bc33621b430fe450c6d11e347f6146da2,"Introduction The sources of emboli in those with embolic stroke of undetermined source may differ in old and young. We assessed the frequency, features and potential embolic sources of younger vs. older embolic stroke of undetermined source patients in the embolic stroke of undetermined source Global Registry. Patients and methods Cross-sectional study of consecutive patients over age 18 years, with recent ischaemic strokes at 19 centres conducted in 2013–2014. Characteristics of embolic stroke of undetermined source patients who aged ≤50 years were analysed and compared with embolic stroke of undetermined source patients who aged >50 years. Results Among 2144 patients with ischaemic stroke, 323 (15.1%, 95% confidence interval: 13.6–16.7%) were ≤50 years old and, 1821 >50 years. 24% (n = 78) of young vs. 15% (n = 273) of older patients met embolic stroke of undetermined source criteria. The mean age of young embolic stroke of undetermined source patients was 40 years (standard deviation +/−9), 33% were women and the most prevalent vascular risk factor was hypertension (38%). Conventional vascular risk factors were less frequent in younger embolic stroke of undetermined source patients. Fewer young embolic stroke of undetermined source patients (63%) had potential minor risk embolic sources identified vs. older embolic stroke of undetermined source patients (77%) (p = 0.02). Stroke severity on admission was similar in younger vs. older patients (National Institute of Health Stroke Scale (NIHSS) 3 vs. 4, p = 0.06). Discussion Young embolic stroke of undetermined source patients comprise an important subset of ischaemic stroke patients around the world. Severity of stroke on admission and 30-day mortality rates are similar among young and older patients. However, there are important differences between younger vs. older embolic stroke of undetermined source patients with respect to risk factors, and potential embolic sources that could affect response to anticoagulants vs. antiplatelet therapies. Conclusion This study provides a benchmark for the global frequency and characteristics of young embolic stroke of undetermined source patients and shows consistent high frequency of embolic stroke of undetermined source in young adults."
334c2e97d49adc2328f29dd9d7bc5bbfa23fe283,
60b9c754c28daad68a3595b8027b26130303e9a0,
8c161116e1e2aab5a716614c3ea5d2cd8000a9a9,"Background and purpose Stroke has been scarcely studied in Latin America (LA). The Mexican Institute of Neurology Stroke Registry was established in 1990 as a prospective computer-based database to register data obtained from patients admitted with stroke. Using this data, we attempted to define the profile of risk factors and outcomes. Methods The demographic data, stroke description, ancillary tests, vascular risk factors, and modified Rankin scale (mRs) were registered. Ischemic stroke subtyping was based on the Trial of Org 10,172 of the Acute Stroke Treatment classification. We followed-up patients using multiple overlapping methods. Primary outcomes included mRs, recurrence, and death at 30 days and at the end of follow-up. Results We included 4,481 patients with a median follow-up of 27 months, (17,281 person-years follow-up). The mean age was 52.8 ± 18 years. There were 2,229 males (50%) included in the study. CI was present in 64.9%, intracerebral hemorrhage (ICH) in 25.6%, and cerebral venous thrombosis (CVT) in 6.3%. Hypertension was the major risk factor (46.5%). The most common cause of CI was atherosclerosis (27%). ICH was mainly hypertensive (58%), and 60% of CVT were puerperal. Overall, the mortality rate was 24.5%. The recurrence rate was 16.9%. Poor outcome (mRs ≥ 3) was found in 56.2% of patients. The best outcomes were observed in CVT patients (74.5% mRs ≤ 2), whereas 72.1% ICH patients had mRs ≥3. Conclusion This is one of the largest hospital-based registries in LA and shows significant differences with other previously published registries, including a younger age, relatively less hypertension, and larger proportion of CVT. Poor functional outcome was common. This study adds to the understanding of geographic differences in stroke characteristics and outcomes."
8c494aea96cb6d30b7799e809feb1ba04a80d6dc,"Citation: Toledo A, Osorio R, Matus C, Martinez Lopez Y, Ramirez Cruz N, Sciutto E, Fragoso G, Arauz A, Carrillo-Mezo R and Fleury A (2018) Human Extraparenchymal Neurocysticercosis: The Control of Inflammation Favors the Host...but Also the Parasite. Front. Immunol. 9:2652. doi: 10.3389/fimmu.2018.02652 Human Extraparenchymal Neurocysticercosis: The Control of Inflammation Favors the Host...but Also the Parasite"
a33449cd6a10932ceb44e5885612c70bf8aecdec,"Rationale After a cerebral vein thrombosis, there is an increased risk of further venous thromboembolic events. The optimal duration of anticoagulation after cerebral vein thrombosis is unknown. Aim To compare efficacy and safety of a policy of short- (3–6 months) versus long-term (12 months) anticoagulation (any type venous thromboembolic events) after cerebral vein thrombosis for the prevention of venous thromboembolic events. Sample size estimates A sample of 1428 patients (749 per arm) allows detecting a reduction from 10 to 5% in the risk of venous thromboembolic event recurrence with 80% power at 5% significance, with 3% dropout rate. Methods and design An international multicenter, prospective cluster-randomized trial with equal allocation between both interventions (ISRCTN25644448). Each cluster is a participating center, which accepted to be randomly allocated to one of the anticoagulation policies. Eligible patients are adults with radiologically confirmed cerebral vein thrombosis within 30 days, and stable to initiate post-acute anticoagulation. Patients judged by the investigator to be an absolute indication for permanent anticoagulation are excluded. Follow-up is at 6, 12 and 24 months. Study outcomes Primary efficacy outcome is any symptomatic and confirmed fatal/nonfatal venous thromboembolic event (recurrent-cerebral vein thrombosis or non-cerebral venous thromboembolic event). Primary safety outcomes include bleeding events during treatment periods and death from any cause. Discussion This study responds to a knowledge gap in the post-acute management of cerebral vein thrombosis patients by comparing short- versus long-term anticoagulation for the prevention of venous thromboembolic event recurrence."
aaca92e617752bef9520167a9c6f068be71260cf,"Systemic lupus erythematosus (SLE) patients are susceptible to the development of posterior reversible encephalopathy syndrome (PRES). The main theory concerning the physiopathology of PRES suggests that there is brain–blood barrier damage, which is associated with endothelial dysfunction, and characterized by vasogenic oedema. However, current evidence regarding its physiopathogenic mechanisms is quite scant. The aim of this study was to analyse the expression of different serum cytokines, as well as vascular endothelial growth factor (VEGF) and soluble CD40 ligand (sCD40L), in patients with PRES/systemic lupus erythematosus (SLE) and to compare them with levels in SLE patients without PRES and in healthy controls. We performed a transversal study in a tertiary care centre in México City. We included 32 subjects (healthy controls, n = 6; remission SLE, n = 6; active SLE, n = 6 and PRES/SLE patients, n = 14). PRES was defined as reversible neurological manifestations (seizures, visual abnormalities, acute confusional state), associated with compatible changes by magnetic resonance imaging (MRI). Serum samples were obtained during the first 36 h after the PRES episode and were analysed by cytometric bead array, Luminex multiplex assay or enzyme‐linked immunosorbent assay (ELISA). Interleukin (IL)‐6 and IL‐10 levels were significantly higher in PRES/SLE patients (P = 0·013 and 0·025, respectively) when compared to the other groups. Furthermore, IL‐6 and IL‐10 levels displayed a positive correlation (r = 0·686, P = 0·007). There were no differences among groups regarding other cytokines, sCD40L or VEGF levels. A differential serum cytokine profile was found in PRES/SLE patients, with increased IL‐6 and IL‐10 levels. Our findings, which are similar to those described in other neurological manifestations of SLE, support the fact that PRES should be considered among the SLE‐associated neuropsychiatric syndromes."
bc89d90d043475276c9b048354ff195f49266c50,"Background and Purpose— We aimed to assess if renal function can aid in risk stratification for ischemic stroke or transient ischemic attack (TIA) recurrence and death in patients with embolic stroke of undetermined source (ESUS). Methods— We pooled 12 ESUS datasets from Europe and America. Renal function was evaluated using the estimated glomerular filtration rate (eGFR) and analyzed in continuous, binary, and categorical way. Cox-regression analyses assessed if renal function was independently associated with the risk for ischemic stroke/TIA recurrence and death. The Kaplan-Meier product limit method estimated the cumulative probability of ischemic stroke/TIA recurrence and death. Results— In 1530 patients with ESUS followed for 3260 patient-years, there were 237 recurrences (15.9%) and 201 deaths (13.4%), corresponding to 7.3 ischemic stroke/TIA recurrences and 5.6 deaths per 100 patient-years, respectively. Renal function was not associated with the risk for ischemic stroke/TIA recurrence when forced into the final multivariate model, regardless if it was analyzed as continuous (hazard ratio, 1.00; 95% CI, 0.99–1.00 for every 1 mL/min), binary (hazard ratio, 1.27; 95% CI, 0.87–1.73) or categorical covariate (likelihood-ratio test 2.59, P=0.63 for stroke recurrence). The probability of ischemic stroke/TIA recurrence across stages of renal function was 11.9% for eGFR ≥90, 16.6% for eGFR 60–89, 21.7% for eGFR 45–59, 19.2% for eGFR 30–44, and 24.9% for eGFR <30 (likelihood-ratio test 2.59, P=0.63). The results were similar for the outcome of death. Conclusions— The present study is the largest pooled individual patient-level ESUS dataset, and does not provide evidence that renal function can be used to stratify the risk of ischemic stroke/TIA recurrence or death in patients with ESUS."
d6f8718e010bd296772135abfbb7689ae1fe1bbc,
ddc9c31e0d3d6bfb58d1b7eb4ba62d4d650fe49a,
e082aa86a6bb71ce4b37ecb83ccccd4621f0c0f1,
e17fd74b6a0e2e301e42599a6e02ced801f2aa41,"Thrombin-activatable fibrinolysis inhibitor (TAFI) gene polymorphisms have been proposed as a predisposing factor for cerebral venous thrombosis (CVT). We analyzed the association between CVT and TAFI single-nucleotide polymorphisms (rs3742264, rs2146881, and rs1926447) compared to healthy controls. Mexico Mestizo confirmed cases with CVT and age- and sex-matched controls with no history of venous thrombotic events were recruited from July 2006 to July 2015. Demographic, clinical, and imaging information was included in the analysis. Genotyping single-nucleotide polymorphisms were performed by allele-specific polymerase chain reaction. Allelic univariate analysis, haplotype association, and Hardy-Weinberg equilibrium were assessed. A total of 113 CVT cases (94 females [83.2%]; median age 35 years [interquartile range 27-43 years]) and 134 age- and sex-matched controls were included. The main risk factors for CVT were pregnancy/puerperium (30.9%), oral contraceptive use (19.5%), and hereditary thrombophilia (7.1%). We found no significant association for heterozygous and homozygous models for rs3742264 (P = .30 and P = .69, respectively), rs2146881 (P = .90 and P = .17, respectively), or rs1926447 (P = .40 and P = .52, respectively) compared to controls; these findings were consistent in subgroup and haplotype analyses. In conclusion, TAFI rs3742264, rs2146881, and rs1926447 polymorphisms do not increase the risk of CVT in comparison to healthy controls."
e276190ca0c1b22bec24d251b8a41a37dc690a13,
fac26e93feda0254b2de2b1254b9798ff624d402,
05ae4e8663a3fcef2f844e8ec7bc6c1b35ca3729,"s 26th European Stroke Conference Berlin, Germany, May 24–26, 2017 (available online as E-Book)"
084d99a85636bdbafd4540d7b8c1789215d475dd,"Background: Stroke is largely preventable, and therefore, a better understanding of risk factors is an essential step in reducing the population stroke rate and resulting disease burden in Arab countries. Summary: We performed 2 separate analyses in 2 similar populations of patients with noncardioembolic ischemic stroke. This first involved 3,635 patients in the Outcomes in Patients with TIA and Cerebrovascular disease (OPTIC) registry (followed for 2 years), with baseline collection of the usual risk factors and 5 socioeconomic variables (unemployment status, residence in rural area, living in fully serviced accommodation, no health-insurance coverage, and low educational level). The second involved patients in the PERFORM trial (n = 19,100 followed up for 2 years), with baseline collection of the usual risk factors and 1 socioeconomic variable (low educational level). The primary outcome was a composite of nonfatal stroke, nonfatal myocardial infarction, or cardiovascular death. Stroke risk factors were more prevalent in patients in Arab countries. The incidence of major cardiovascular events (MACE; age- and gender-adjusted) was higher in Arab countries (OPTIC, 18.5 vs. 13.3%; PERFORM, 18.4 vs. 9.7%; both p ≤ 0.0001). These results remained significant after adjustment on risk factors and were attenuated in OPTIC after further adjustment on socioeconomic variables (hazard ratio 1.24; 95% CI 0.98-1.55; p = 0.07). Key Messages: Patients with ischemic stroke living in Arab countries had a lower mean socioeconomic status, a much higher prevalence of diabetes mellitus, and a higher rate of MACE compared with patients from non-Arab countries. This finding is partly explained by a higher prevalence of risk factors and also by a high prevalence of poverty and low educational level."
1f41c11e0ea1b118a4c77c697a18f803a3b68d91,
3884eb846b1cb35c74f0822b64f0ed1aaa1992f0,"Objective: To investigate whether the correlation of age and sex with the risk of recurrence and death seen in patients with previous ischemic stroke is also evident in patients with embolic stroke of undetermined source (ESUS). Methods: We pooled datasets of 11 stroke registries from Europe and America. ESUS was defined according to the Cryptogenic Stroke/ESUS International Working Group. We performed Cox regression and Kaplan-Meier product limit analyses to investigate whether age (<60, 60–80, >80 years) and sex were independently associated with the risk for ischemic stroke/TIA recurrence or death. Results: Ischemic stroke/TIA recurrences and deaths per 100 patient-years were 2.46 and 1.01 in patients <60 years old, 5.76 and 5.23 in patients 60 to 80 years old, 7.88 and 11.58 in those >80 years old, 3.53 and 3.48 in women, and 4.49 and 3.98 in men, respectively. Female sex was not associated with increased risk for recurrent ischemic stroke/TIA (hazard ratio [HR] 1.15, 95% confidence interval [CI] 0.84–1.58) or death (HR 1.35, 95% CI 0.97–1.86). Compared with the group <60 years old, the 60- to 80- and >80-year groups had higher 10-year cumulative probability of recurrent ischemic stroke/TIA (14.0%, 47.9%, and 37.0%, respectively, p < 0.001) and death (6.4%, 40.6%, and 100%, respectively, p < 0.001) and higher risk for recurrent ischemic stroke/TIA (HR 1.90, 95% CI 1.21–2.98 and HR 2.71, 95% CI 1.57–4.70, respectively) and death (HR 4.43, 95% CI 2.32–8.44 and HR 8.01, 95% CI 3.98–16.10, respectively). Conclusions: Age, but not sex, is a strong predictor of stroke recurrence and death in ESUS. The risk is ≈3- and 8-fold higher in patients >80 years compared with those <60 years of age, respectively. The age distribution in the ongoing ESUS trials may potentially influence their power to detect a significant treatment association."
76f5b46f30aae5f96b6b379aefbb61c08bf7ce5a,
920b4615286b9359ceab02560072ac1a027058ce,"Introduction: return to work is a challenging task among work-active stroke survivors, which has been widely analyzed, but few data has been collected regarding the performance quality compared to ..."
c9730bb7c8b11677fda0ac7c32d250fc27c1248a,"Background and Purpose— Pregnancy is associated with increased risk of venous thrombotic events, including cerebral venous thrombosis. We aimed to study the complications and outcome of subsequent pregnancies in women with previous cerebral venous thrombosis. Methods— Follow-up study of women with acute cerebral venous thrombosis at childbearing age included in a previously described cohort (International Study of Cerebral Vein and Dural Sinus Thrombosis). Patients were interviewed by local neurologists to assess rate of venous thrombotic events, pregnancy outcomes, and antithrombotic prophylaxis during subsequent pregnancies. Results— A total of 119 women were included, with a median follow-up of 14 years. Eighty-two new pregnancies occurred in 47 women. In 83% (68 of 82), some form of antithrombotic prophylaxis was given during at least 1 trimester of pregnancy or puerperium. Venous thrombotic events occurred in 3 pregnancies, including 1 recurrent cerebral venous thrombosis. Two of the 3 women were on prophylactic low-molecular-weight heparin at the time of the event. Outcomes of pregnancies were 51 full-term newborns, 9 preterm births, 2 stillbirths, and 20 abortions (14 spontaneous). Conclusions— In women with prior cerebral venous thrombosis, recurrent venous thrombotic events during subsequent pregnancies are infrequent."
cbd82dcde7dfb55b02c3dd2a2c6b521d9df53134,
e478c549108b1160054619cf97bd09d7aad4b17a,"Background and purpose: Atherosclerotic ischemic stroke is the second most frequent etiology of stroke in the adult population. Functional outcome, mortality and recurrence of stroke rates on the long-term follow-up are poorly studied. This study investigates long-term outcome among patients with ischemic stroke secondary to atherosclerotic causality, and identifies the main factors associated with poor outcome, recurrence, and death. Methods: We analyzed data from our consecutive acute ischemic stroke database, over a period of 25 years (1990-2015). The endpoints were: bad outcome (Modified Rankin Score ≥3), recurrence and mortality at discharge, and final follow-up. Multivariate Cox and Kaplan-Meier analysis were used to estimate the probability of death and recurrence. Results: A total of 946 consecutive atherosclerotic stroke patients were included (571 [60.4%] males, median age 65 years [interquartile range 57-73 years] for the entire population); dyslipidemia (64.2%), hypertension (63.3%), diabetes (35.0%), and active smoking history (31.8%) were the most prevalent risk factors.After a median follow-up of 38 months (IQR 12-75 months), 59.3% patients had a bad outcome at discharge. A result of 26.1% had stroke recurrence (median time until recurrence: 9 months [IQR 12-84 months], with 12.9% cases presenting ≥2 recurrences), and 24.1% were dead (median time to death: 18.5 months [IQR 11-74 months]) at the final follow-up period. After multivariate adjustment, hypertension (HR 4.2, CI 95% 2.8-6.1; p Conclusions: Atherosclerotic ischemic stroke has a high rate of recurrence, associated mainly with hypertension. Mortality is predicted by diabetes, bad functional outcome at recurrence, and older age."
fb9cb92e240a7684b18236626f908dbc168fca59,"Purpose: We sought to evaluate the long-term functional outcomes and identify the potential risk factors for rebleeding in patients with brain stem cavernous malformations (BCMs) who presented with hemorrhages and were surgically or conservatively treated and prospectively monitored. Methods: From January 1990 to July 2015, we included patients with first hemorrhagic episodes secondary to single BCMs. Modified Rankin score (mRS) was used for neurological status assessment. Univariate and multivariate regression statistics were used to identify the risk factors for rebleeding. Results: A total of 99 patients with BCMs hemorrhages were included (59 [59.6%] women, mean age 37± 13 years). As initial treatments, 37 patients (37.4%) underwent surgery and 62 (62.6%) received conservative treatment. The median follow-up was 3.33 years (interquartile range 1.16-7 years; 408.3 patient/years). The rebleeding rate by patient/year was 10% in conservatively treated patients. Deterioration was significantly more frequent in patients with rebleeding (p = 0.0001). At the end of the follow-up, the mRS were favorable in 49 patients (65.3%) without rebleeding, whereas only 8 (33.3%) with rebleeding evolved to favorable outcomes (p = 0.006). Lesion size >18 mm (hazards ratio, HR 3.34, 95% CI 1.54-7.26; p = 0.0001) and ventral location or crossing the brain stem's midpoint (HR 2.5, 95% CI 1.14-5.46; p = 0.022) were associated with a major risk of rebleeding in the univariate analysis, but only a lesion >18 mm remained statistically significant (HR 2.7, 95% CI 1.2-6.21; p = 0.016) in the multivariate analysis. Conclusion: A lesion size >18 mm was the principal factor associated with hemorrhage recurrence. The overall functional outcome was good. However, significant morbidity was attributable to rebleeding."
0fd7d38801797b48a878b174101dade2536a9de9,"Background: Transverse sinuses (TS) are frequently asymmetric. Hypoplasia or aplasia of TS is a common anatomical variation, right TS is dominant in 61% of cases. The relationship between hypoplastic TS and cerebral venous thrombosis is not well established. Hypothesis: Transverse sinus hypoplasia is a predisposing factor for ipsilateral transverse sinus thrombosis Methods: We retrospectively evaluated 27 confirmed cases with isolated transverse sinus thrombosis and 54 age-and-sex matched controls, treated in a Neurological tertiary center from 2010 to 2015. A stroke neurologist and a neuroradiologist measured TS using an MRI sequence (Inhance 3D Inflow IR); interrater reliability was calculated using Bland-Altman plots. Hypoplasia was defined as a transverse sinus diameter less than 50% of the cross-sectional diameter of the lumen of the distal superior sagittal sinus. Univariate analysis was performed to evaluate the association between transverse sinus hypoplasia (TSh) and thrombosis. Results: There was a good inter-rater reliability (p=0.55 on the Bland-Altman plot by ANOVA test). There were a total of 45 left hypoplastic transverse sinuses (TS) (19 [70.4%] cases vs. 26 [48.1%] controls), and 16 right hypoplastic TS (11 [40.7%] cases vs. 5 [9.3%] controls). Ipsilateral thrombosis was present in 9 (33.3%) right and 15 (55.5%) left hypoplastic transverse sinuses. Transverse sinus thrombosis was more likely to be present when associated with left TSh (RR 2.57, 95% CI 1.17-5.69; p=0.001), than right TSh and ipsilateral thrombosis (RR 0.15, 95% CI 0.04-0.57; p Conclusion: Isolated transverse sinus hypoplasia might be a predisposing factor for ipsilateral transverse sinus thrombosis."
104be3d04fc0cb3fc96ec563625f60995cc50019,"Background Recent evidence supports that most non-lacunar cryptogenic strokes are embolic. Accordingly, these strokes have been designated as embolic strokes of undetermined source (ESUS). Aims We undertook an international survey to characterize the frequency and clinical features of ESUS patients across global regions. Methods Consecutive patients hospitalized for ischemic stroke were retrospectively surveyed from 19 stroke research centers in 19 different countries to collect patients meeting criteria for ESUS. Results Of 2144 patients with recent ischemic stroke, 351 (16%, 95% CI 15% to 18%) met ESUS criteria, similar across global regions (range 16% to 21%), and an additional 308 (14%) patients had incomplete evaluation required for ESUS diagnosis. The mean age of ESUS patients (62 years; SD = 15) was significantly lower than the 1793 non-ESUS ischemic stroke patients (68 years, p ≤ 0.001). Excluding patients with atrial fibrillation (n = 590, mean age = 75 years), the mean age of the remaining 1203 non-ESUS ischemic stroke patients was 64 years (p = 0.02 vs. ESUS patients). Among ESUS patients, hypertension, diabetes, and prior stroke were present in 64%, 25%, and 17%, respectively. Median NIHSS score was 4 (interquartile range 2–8). At discharge, 90% of ESUS patients received antiplatelet therapy and 7% received anticoagulation. Conclusions This cross-sectional global sample of patients with recent ischemic stroke shows that one-sixth met criteria for ESUS, with additional ESUS patients likely among those with incomplete diagnostic investigation. ESUS patients were relatively young with mild strokes. Antiplatelet therapy was the standard antithrombotic therapy for secondary stroke prevention in all global regions."
36ec3ada6d80b66e195863f7a31c504263bab9d8,"Background and Purpose— The risk of stroke recurrence in patients with Embolic Stroke of Undetermined Source (ESUS) is high, and the optimal antithrombotic strategy for secondary prevention is unclear. We investigated whether congestive heart failure, hypertension, age ≥75 years, diabetes mellitus, and stroke or transient ischemic attack (TIA; CHADS2) and CHA2DS2-VASc scores can stratify the long-term risk of ischemic stroke/TIA recurrence and death in ESUS. Methods— We pooled data sets of 11 stroke registries from Europe and America. ESUS was defined according to the Cryptogenic Stroke/ESUS International Working Group. Cox regression analyses were performed to investigate if prestroke CHADS2 and congestive heart failure, hypertension, age ≥75 years, diabetes mellitus, stroke or TIA, vascular disease, age 65–74 years, sex category (CHA2DS2-VASc) scores were independently associated with the risk of ischemic stroke/TIA recurrence or death. The Kaplan–Meier product limit method was used to estimate the cumulative probability of ischemic stroke/TIA recurrence and death in different strata of the CHADS2 and CHA2DS2-VASc scores. Results— One hundred fifty-nine (5.6% per year) ischemic stroke/TIA recurrences and 148 (5.2% per year) deaths occurred in 1095 patients (median age, 68 years) followed-up for a median of 31 months. Compared with CHADS2 score 0, patients with CHADS2 score 1 and CHADS2 score >1 had higher risk of ischemic stroke/TIA recurrence (hazard ratio [HR], 2.38; 95% confidence interval [CI], 1.41–4.00 and HR, 2.72; 95% CI, 1.68–4.40, respectively) and death (HR, 3.58; 95% CI, 1.80–7.12, and HR, 5.45; 95% CI, 2.86–10.40, respectively). Compared with low-risk CHA2DS2-VASc score, patients with high-risk CHA2DS2-VASc score had higher risk of ischemic stroke/TIA recurrence (HR, 3.35; 95% CI, 1.94–5.80) and death (HR, 13.0; 95% CI, 4.7–35.4). Conclusions— The risk of recurrent ischemic stroke/TIA and death in ESUS is reliably stratified by CHADS2 and CHA2DS2-VASc scores. Compared with the low-risk group, patients in the high-risk CHA2DS2-VASc group have much higher risk of ischemic stroke recurrence/TIA and death, approximately 3-fold and 13-fold, respectively."
5f622cce5807f5a01f069af6480c58e109ab9229,"Background Embolic strokes of undetermined source comprise up to 20% of ischemic strokes. The stroke recurrence rate is substantial with aspirin, widely used for secondary prevention. The New Approach riVaroxaban Inhibition of Factor Xa in a Global trial versus ASA to prevenT Embolism in Embolic Stroke of Undetermined Source international trial will compare the efficacy and safety of rivaroxaban, an oral factor Xa inhibitor, versus aspirin for secondary prevention in patients with recent embolic strokes of undetermined source. Main hypothesis In patients with recent embolic strokes of undetermined source, rivaroxaban 15 mg once daily will reduce the risk of recurrent stroke (both ischemic and hemorrhagic) and systemic embolism (primary efficacy outcome) compared with aspirin 100 mg once daily. Design Double-blind, randomized trial in patients with embolic strokes of undetermined source, defined as nonlacunar cryptogenic ischemic stroke, enrolled between seven days and six months from the qualifying stroke. The planned sample size of 7000 participants will be recruited from approximately 480 sites in 31 countries between 2014 and 2017 and followed for a mean of about two years until at least 450 primary efficacy outcome events have occurred. The primary safety outcome is major bleeding. Two substudies assess (1) the relative effect of treatments on MRI-determined covert brain infarcts and (2) the biological underpinnings of embolic strokes of undetermined source using genomic and biomarker approaches. Summary The New Approach riVaroxaban Inhibition of Factor Xa in a Global trial versus ASA to prevenT Embolism in Embolic Stroke of Undetermined Source trial is evaluating the benefits and risks of rivaroxaban for secondary stroke prevention in embolic strokes of undetermined source patients. Main results are anticipated in 2018."
6e0110977f4fbe27636c7235a75b50ab5d61b6ca,"Background Embolic stroke of undetermined source (ESUS) recurrence and functional outcome from long-term follow-up is not well delineated. The purpose of this study is to compare these functional variables between ESUS vs. cardioembolic stroke (CS) patients. Methods We analyzed data of consecutive ESUS and CS patients from our institutional database, from January 2003 until April 2015. The endpoints were stroke recurrence, mortality and poor clinical outcome (Modified Rankin Score 3–6), at discharge, 6 months and final follow-up. Adjusted multivariate Cox analysis and Kaplan-Meier curves were used to estimate the probability of recurrence and death. Results 149 ESUS (median age 44 years) and 235 CS (median age 66 years) consecutive patients were included in the study. Median follow-up period for the entire sample was 19 months (interquartile range 6.0–45.0 months). Stroke recurrence was similar between ESUS and CS patients (5.4% vs. 9.8% respectively, p = 0.12). Death occurred in 30 CS cases (12.8%), with a cumulative probability of survival of 77%. Poor functional outcome was present in 58.3%, 54.0% and 54.9% at discharge, 6 months and final follow-up respectively in CS patients, significantly worst compared to ESUS cases (HR 3.1; CI 95% 1.96–4.68). Oral anticoagulation presents with a HR 8.01 for recurrence, and antiplatelet therapy had the highest risk for recurrence for both groups (HR 24.3). Conclusion ESUS patients are substantially younger than CS patients but have a stroke recurrence rate similar to CS patients, with a lower mortality rate, and better functional outcome on long-term follow-up."
70c5c66d6f5c008ca429c7fc893665c950f3fcd4,"Introduction Cerebral venous thrombosis (CVT) is a rare cerebrovascular condition accounting for <1% of all stroke cases and mainly affects young adults. Its genetic aetiology is not clearly elucidated. Methods and analysis To better understand the genetic basis of CVT, we have established an international biobank of CVT cases, Biorepository to Establish the Aetiology of Sinovenous Thrombosis (BEAST) which aims to recruit highly phenotyped cases initially of European descent and later from other populations. To date we have recruited 745 CVT cases from 12 research centres. As an initial step, the consortium plans to undertake a genome-wide association analysis of CVT using the Illumina Infinium HumanCoreExome BeadChip to assess the association and impact of common and low-frequency genetic variants on CVT risk by using a case–control study design. Replication will be performed to confirm putative findings. Furthermore, we aim to identify interactions of genetic variants with several environmental and comorbidity factors which will likely contribute to improve the understanding of the biological mechanisms underlying this complex disease. Ethics and dissemination BEAST meets all ethical standards set by local institutional review boards for each of the participating sites. The research outcomes will be published in international peer-reviewed open-access journals with high impact and visibility. The results will be presented at national and international meetings to highlight the contributions into improving the understanding of the mechanisms underlying this uncommon but important disease. This international DNA repository will become an important resource for investigators in the field of haematological and vascular disorders."
7bef1709273aea21e15a8d8e16b89230e939622c,"Intracranial aneurysms uncommonly present with ischemic stroke. Parent artery occlusion due to local extension of the luminal thrombus, aneurysms ejecting emboli to distal arteries, or increased mass effect have been described as possible pathogenic mechanisms. Guidelines for the management of these patients are absent. We present the clinical outcome and radiological characteristics of three patients with spontaneous thrombosis of intracranial aneurysms as a cause of ischemic stroke. This information is relevant given the possible benign history in terms of stroke recurrence and risk of bleeding."
82357378ed075753c24666c7fe51dbcb99402632,
8584dd9f133b6be2e0d65b5e24994d3a10498785,"Objective: 
To examine the association between hypoplasia of the transverse sinus and ipsilateral transverse sinus (TS) thrombosis
 Background: 
Transverse sinuses (TS) are frequently asymmetric. Hypoplasia or aplasia of TS is a common anatomical variation, right TS is dominant in 61[percnt] of cases. The relationship between hypoplastic TS and cerebral venous thrombosis is not well established.
 Methods: 
We retrospectively evaluated 27 confirmed cases with isolated transverse sinus thrombosis and 54 age-and-sex matched controls, treated in a Neurological tertiary center from 2010 to 2015. A stroke neurologist and a neuroradiologist measured TS using an MRI sequence (Inhance 3D Inflow IR); interrater reliability was calculated using Bland-Altman plots. Hypoplasia was defined as a transverse sinus diameter less than 50[percnt] of the cross-sectional diameter of the lumen of the distal superior sagittal sinus. Univariate analysis was performed to evaluate the association between transverse sinus hypoplasia (TSh) and thrombosis
 Results: 
There was a good inter-rater reliability (p=0.55 on the Bland-Altman plot by ANOVA test). There were a total of 45 left hypoplastic transverse sinuses (TS) (19 [70.4[percnt]] cases vs. 26 [48.1[percnt]] controls), and 16 right hypoplastic TS (11 [40.7[percnt]] cases vs. 5 [9.3[percnt]] controls). Ipsilateral thrombosis was present in 9 (33.3[percnt]) right and 15 (55.5[percnt]) left hypoplastic transverse sinuses. Transverse sinus thrombosis was more likely to be present when associated with left TSh (RR 2.57, 95[percnt] CI 1.17-5.69; p=0.001), than right TSh and ipsilateral thrombosis (RR 0.15, 95[percnt] CI 0.04-0.57; p<0.001).
 Conclusions: 
Isolated transverse sinus hypoplasia might be a predisposing factor for ipsilateral transverse sinus thrombosis. Disclosure: Dr. Chavarria has nothing to disclose. Dr. Barboza has nothing to disclose. Dr. Varela has nothing to disclose. Dr. Patino-Hernandez has nothing to disclose. Dr. Becerril has nothing to disclose. Dr. Arauz has nothing to disclose."
9bcbbb039a05b463921619b0749b48029523b08d,"Introduction: Cerebral venous thrombosis (CVT) is an uncommon cerebrovascular condition, which presents with a wide spectrum of symptoms9 onset and clinical syndromes. Hypothesis: We assessed the hypothesis that there is a correlation among the main clinical syndromes in CVT and the sites of venous occlusion; also we analyzed functional outcome on each clinical syndrome in the acute setting (30-days). Methods: This is a retrospective analysis from a systematic database of hospitalized patients from January 1979 to December 2014. Univariate and adjusted multivariate models were used to evaluate in a first step, association between clinical syndromes and affected vessels, and in a second step functional outcome in the acute setting (30-day follow-up). Clinical syndromes were classified as: focal syndrome, encephalopathy, isolated intracranial hypertension, meningeal syndrome. Affected vessels were classified as isolated thrombosis or vessels combinations. Functional outcome was based on modified Rankin score (mRs) at 30- and 90-day (good functional outcome, mRs = 0-2). Results: 467 confirmed CVT patients (81.6% women, median age: 29 years, IQR: 22-38 years). Isolated superior sagittal sinus (82.0%), lateral sinus (50.1%), and the combination of them (22.1%) were the most prevalent affected vessels. Good functional outcome was present in 359 (76.9%) and 394 (84.4%) of all patients, at 30- and 90-day respectively. Focal syndrome was associated with hemorrhagic (OR 11.8, 95% CI 5.59-25.0); encephalopathy with the combination of Vein of Galen + Straight sinus (OR 6.52, 95% CI 2.13-19.9); isolated intracranial hypertension was associated with the absence of parenchymal lesion (OR 71.8, 95% CI 25.1-205); meningeal syndrome was associated with the combination of deep and superficial venous thrombosis (OR 3.22, 95% CI 1.61-6.43). Good functional outcome at 30- and 90-day was mainly associated to absence of encephalopathy (HR 0.74) and absence of meningeal syndrome (HR 0.85). Conclusions: Focal syndrome depends on the type of parenchymal lesion; encephalopathy depends on the compromise of deep venous system; the strongest associations for 30-day mortality were found on the presence of meningeal and focal syndromes."
b7f3024cbf429ced7b87dda6ae41fa25ad9dcee0,"Background Pontine infarcts are common and often attributed to small vessel disease (“small deep infarcts”) or basilar branch atherosclerosis (“wedge shaped”). A well-described morphological differentiation using magnetic resonance images has not been reported. Furthermore, whether risk factors and outcomes differ by morphology, or whether infarct morphology should guide secondary prevention strategy, is not well characterized. Methods All participants in the Secondary Prevention of Small Subcortical Strokes Study with magnetic resonance imaging -proven pontine infarcts were included. Infarcts were classified as well-circumscribed small deep (small deep infarct, i.e. lacunar), paramedian, atypical paramedian, or other based on diffusion-weighted imaging, T2/fluid-attenuated inversion recovery, and T1-magnetic resonance images. Inter-rater reliability was high (90% agreement, Cohen’s kappa = 0.84). Clinical and radiologic features independently associated with small deep infarct versus paramedian infarcts were identified (multivariable logistic regression). Differences in stroke risk and death were assessed using Cox proportional hazards. Results Of the 3020 patients enrolled, 644 had pontine infarcts; 619 images were available: 302(49%) small deep infarct, 245 (40%) paramedian wedge, 35 (6%) atypical paramedian, and 37 (6%) other. Among vascular risk factors, only smoking (OR 2.1, 95% CI 1.3–3.3) was independently associated with small deep infarct versus paramedian infarcts; on neuroimaging, old lacunes on T1/fluid-attenuated inversion recovery (OR 1.8, 1.3–2.6) and intracranial stenosis (any location) ≥50% (OR 0.62, 0.41–0.96). Small deep infarct versus paramedian was not predictive of either recurrent stroke or death, and there was no interaction with assigned treatment. Conclusions Pontine infarcts can be reliably classified based on morphology using clinical magnetic resonance images. Few risk factors differed between small deep infarct and paramedian infarcts with no differences in recurrent stroke or mortality. There was no difference in response to different antiplatelet or blood pressure treatment strategies between these two groups. Registration http://www.clinicaltrials.gov/NCT00059306."
ded0b085c24a9a6ef8b053e1774d768c584b12f1,"Brainstem cavernous malformation (BCM) account for 8-22% of all intracranial cavernomas. Currently, they can be treated microsurgically or conservative but it is still difficult to choose the best treatment for each patient. The main objective of our series was to evaluate the long-term functional outcome and recurrence in patients with BCM treated with conservative or surgical treatments. Hypothesis: We assessed the hypothesis that surgical and conservative treatments are associated with different functional outcome and re-hemorrhage rate in long-term follow-up. Methods: In this non-randomized, clinical series, we compared the clinical and radiological findings of patients with their first hemorrhage secondary to confirmed BCM, treated in a tertiary neurological center, during a twenty five- year period. Treatment of each patient was selected by the attending physician and consisted of either conservative or surgical evacuation of BCM. The primary end-points were recurrent hemorrhage and functional outcome. Favorable prognosis was defined as modified Rankin scale (mRs) of 0 to 2. Results: From January of 1990 to July of 2015; 99 patients with BCM hemorrhage were treated (59 [59,6%] female; mean age 37± 13 years). 37 patients (37,4%) were surgically treated and 62 (62,6%) received conservative treatment. During the follow-up; 20 patients in the medical group (median time of recurrence: 34,5 months; IQR: 13,75-93) and 4 patients in the surgical group (median time of recurrence: 22 months; IQR: 9-46,5) had a recurrence (OR: 0,255; 95% IC: 0,079-0,817), with a cumulative incidence of 5,1 per 100 years-person and 3,96 per 100 years-person respectively. Because of rebleeding, 11 patients of the conservative group were taken to surgery and 3 of the surgical group were to required re-intervention. At the end of follow-up (median: 51 months; IQR: 19-104) 51 patients remained in the conservative group and 28 (54,9%) had a favorable mRs. 48 patients remained in the surgical group and 27 (56,2%) had a favorable mRs (OR:0,94 95% IC: 0,42-2,09). Conclusion: Despite a significant high recurrent hemorrhage rate was observed in conservative treated patients, we did not found difference in clinical outcome between both groups of patients with BCM."
09accd830a28bb6f959f48640b9970ea46f5f502,"Background Cognitive impairment is frequent in lacunar stroke patients. The prevalence and pattern among Spanish-speaking patients are unknown and have not been compared across regions or with English-speaking patients. Aims The aim of this study was to characterize cognitive impairment in Spanish-speaking patients and compare it with English-speaking patients. Methods The baseline neuropsychological test performance and the prevalence of mild cognitive impairment, defined as a z-score ≤ -1·5 on memory and/or non-memory tests, were evaluated in Spanish-speaking patients in the Secondary Prevention of Small Subcortical Strokes trial. Results Out of 3020 participants, 1177 were Spanish-speaking patients residing in Latin America (n = 693), the United States (n = 121), and Spain (n = 363). Low education (zero- to eight-years) was frequent in Spanish-speaking patients (49–57%). Latin American Spanish-speaking patients had frequent post-stroke upper extremity motor impairment (83%). Compared with English-speaking patients, all Spanish-speaking patient groups had smaller memory deficits and larger non-memory/motor deficits, with Latin American Spanish-speaking patients showing the largest deficits median z-score -1·3 to −0·6 non-memory tests; ≤5·0 for Grooved Pegboard; −0·7 to −0·3 for memory tests). The prevalence of mild cognitive impairment was high and comparable with English-speaking patients in the United States and Latin American Spanish-speaking patients but not the Spanish group: English-speaking patients = 47%, Latin American Spanish-speaking patients = 51%, US Spanish-speaking patients = 40%, Spanish Spanish-speaking patients = 29%, with >50% characterized as non-amnestic in Spanish-speaking patient groups. Older age [odds ratio per 10 years =1·52, confidence interval = 1·35–1·71), lower education (odds ratio 0–4 years = 1·23, confidence interval = 0·90–1·67), being a Latin American resident (odds ratio = 1·31, confidence interval = 0·87–1·98), and post-stroke disability (odds ratio Barthel Index <95=1·89, confidence interval = 1·43–2·50) were independently associated with mild cognitive impairment. Conclusions Mild cognitive impairment in Secondary Prevention of Small Subcortical Strokes Spanish-speaking patients with recent lacunar stroke is highly prevalent but has a different pattern to that observed in English-speaking patients. A combination of socio-demographics, stroke biology, and stroke care may account for these differences."
14fa0287a29b6688b98cff6bb5bb6822179afe44,"Regrettably, stroke has been scarcely studied in México, and the available data suffer from great variability in diagnostic testing, risk factor definitions, and poor generalizability. The current cumulative incidence of stroke in Mexico is 232·2 per 100 000, whereas prevalence among people aged 60 years or older is 18·2 per 1000. Hypertension and diabetes are the main risk factors. Ischemic stroke is the most frequent sub-type. Stroke mortality has been increasing during last years, and 30-day case fatality rate doubles at one-year follow-up. A remarkable finding of a hospital-based registry was that most of ischemic stroke cases are of undetermined etiology and even when a quarter of patients arrive on time for thrombolysis, less than 1% received this management."
1904e4a387c4319405756f3f729d3850415a2055,"Background and purpose Few studies have investigated the rates of recanalisation after cerebral venous thrombosis (CVT). Our objective was to investigate the recanalisation rate and to identify predictors of recanalisation in patients with CVT. Methods We included 102 patients with confirmed first-ever, non-septic CVT. All patients received anticoagulation for 12 months or until complete recanalisation. To assess recanalisation, patients underwent MR venography every 3 months until partial or complete recanalisation or for 12 months after diagnosis. We conducted two parallel analyses of complete recanalisation versus partial and no recanalisation versus any recanalisation. As a secondary objective we explored the influence of recanalisation on outcome and recurrent events. We calculated the probability of recanalisation using Kaplan-Meier analysis and conducted multivariate analysis using a Cox model. Results The mean age of patients was 33.5±11 years (80 (78.4%) women). Survival analysis indicated that 50% of the patients had any recanalisation (grades I, II and III) by 64 days and complete recanalisation (grade III) by 169 days. Adjusted Cox proportional model revealed that age <50 years (HR=11.5 95% CI=1.58 to 84.46, p=0.01) and isolated superior sagittal sinus thrombosis (HR=0.39, 95% CI=0.14 to 1.04, p=0.05) predict complete recanalisation, while age <50 years (HR=4.79; 95% CI=1.69 to 13.5, p=0.003) predicts any recanalisation. Patients with complete recanalisation had a greater chance of good functional outcome (HR=5.17; 95% CI=2.8 to 9.53, p<0.001). Conclusions We found that recanalisation occurs over time, until month 11. Complete recanalisation may influence functional outcome."
3543bdf2794f563e7118fd2b45b837a7fc94e660,"Background and Purpose— Our aim was to identify whether particular subgroups of patients had an unacceptably high risk of symptomatic intracranial hemorrhage or low chance of benefit when treated with alteplase (recombinant tissue-type plasminogen activator). Methods— Third International Stroke Trial was an international randomized trial of the intravenous (IV) recombinant plasminogen activator alteplase (0.9 mg/kg) versus control in 3035 (1515 versus 1520) patients. We analyzed the effect of recombinant tissue-type plasminogen activator on 6-month functional outcome, early death, and symptomatic intracranial hemorrhage (both ⩽7 days). We tested for any differences in treatment effect between subgroups by a test of interaction. Our 13 protocol prespecified subgroups were time to randomization, age, sex, stroke subtype, atrial fibrillation, early ischemic change (clinician and expert panel), prior antiplatelet use, stroke severity, diastolic and systolic blood pressure at randomization, center’s thrombolysis experience, and trial phase. Analyses were adjusted for key baseline prognostic factors. Results— There were no significant interactions in the subgroups analyzed that were consistent across all 3 outcomes. Treatment with recombinant tissue-type plasminogen activator increased the odds of symptomatic intracranial hemorrhage by a greater amount in patients taking prior antiplatelets than those who were not (P=0.019 for test of interaction), but had no clear detrimental effect on functional outcome at 6 months in this group (P=0.781 for test of interaction). Conclusions— Among the types of patient in the Third International Stroke Trial, this secondary analysis did not identify any subgroups for whom treatment should be avoided. Given the limitations of the analysis, we found no clear evidence to avoid treatment in patients with prior ischemic stroke, diabetes mellitus, or hypertension. Clinical Trial Registration— URL: http://www.controlled-trials.com. Unique identifier: ISRCTN25765518. http://www.controlled-trials.com/ISRCTN25765518."
488ab05bc48bbeea703eda522610a34e9f559d61,"Background and purpose: Cerebral venous thrombosis (CVT) not always implies a good prognosis. There is a need for robust and simple classification systems of severity after CVT that help in clinical decision-making.

Methods: We studied 467 patients (81.6% women, median age: 29 years, interquartile range: 22-38 years) with CVT who were hospitalized from 1980 to 2014 in two third-level referral hospitals. Bivariate analyses were performed to select variables associated with 30-day mortality to integrate a further multivariate analysis. The resultant model was evaluated with the Hosmer-Lemeshow test for goodness of fit, and on Cox proportional hazards model for reliability of the effect size. After the scale was configured, security and validity were tested for 30-day mortality and modified Rankin scale (mRS) >2. The prognostic performance was compared with that of the CVT risk score (CVT-RS, 0-6 points) as the reference system.

Results: The 30-day case fatality rate was 8.7%. The CVT grading scale (CVT-GS, 0-9 points) was integrated by stupor/coma (4 points), parenchymal lesion >6 cm (2 points), mixed (superficial and deep systems) CVT (1 point), meningeal syndrome (1 point) and seizures (1 point). CVT-GS was categorized into mild (0-3 points, 1.1% mortality), moderate (4-6 points, 19.6% mortality) and severe (7-9 points, 61.4% mortality). For 30-day mortality prediction, as compared with CVT-RS (cut-off 4 points), CVT-GS (cut-off 5 points) was globally better in sensitivity (85% vs 37%), specificity (90% vs 95%), positive predictive value (44% vs 40%), negative predictive value (98% vs 94%), and accuracy (94% vs 80%). For 30-day mRS >2 the performance of CVT-GS over CVT-RS was comparably improved.

Conclusion: The CVT-GS is a simple and reliable score for predicting outcome that may help in clinical decision-making and that could be used to stratify patients recruited into clinical trials."
5f6e37c0796b55559dd593b7887223be1dbc1b96,"Background and purpose Few studies have examined collateral formation in patients with cerebral venous thrombosis (CVT). The aim of this study was to analyse the impact of baseline intracranial venous collaterals on the clinical outcome and imaging features of patients with acute CVT. Material and methods MRIs from consecutive patients with acute CVT were retrospectively analysed. The category system described by Qureshi was used to assess the pattern of venous collaterals. Clinical and imaging features and outcomes were analysed using bivariate and multivariate models to assess the association of collateral patterns with the type of parenchymal lesions and clinical outcome (modified Rankin Scale) at 30 and 90 days. Results One hundred patients were included (77 women; median age 32 years; and median of 18 months of follow-up). Venous collaterals were present in 88% of the patients; type I collaterals in 3 patients, type II collaterals in 27 patients, and type III collaterals in 58 patients. Twelve patients did not exhibit any collaterals. Cohen's κ coefficient between evaluators was 0.86. In the bivariate analysis, type III collaterals were associated with isolated intracranial hypertension and complete recovery, whereas type I collaterals were associated with encephalopathy. However, in the multivariate regression analysis, the collateral pattern was not associated with clinical presentation, type of brain lesion or outcome. Conclusions Intracranial venous collaterals are frequently found in patients with CVT during the acute phase. However, they do not have an independent effect on the type of brain damage, clinical manifestations or prognosis."
7808f0403682a902655fc84c1c9d038c012b7e6a,"UNLABELLED
Vascular complications related to multiple hereditary exostoses are uncommon. We present a 39-year-old male patient with multiple exostoses in the upper and lower limbs with an associated positive familial history of such lesions. He experienced a sudden onset of left-side ataxia and hypoesthesia secondary to a left lateral medullary infarction, which was due to a stenotic-pattern vertebral artery dissection (V1-V4). This complication is very rare as a differential diagnosis in the vertebro-basilar dissection spectrum, and a nonspecific relation has been found.


ABBREVIATIONS
MHEMultiple hereditary exostosesATangiotomographyVADvertebral artery dissectionCADcervical artery dissectionOIosteogenesis imperfecta."
9f47013ed7376d2da354b0ca9968fcac5d4b2b38,"
 Background and purpose:
 A significant proportion of patients with cerebral venous thrombosis (CVT) present neurological deterioration within the following days after arriving to hospital without significant altered consciousness. This has been a poorly studied issue. We aimed to determine factors associated with neurological worsening in hospitalized patients with CVT.
 
 
 Methods:
 We analyzed a collaborative database on patients with autopsy or neuroimaging confirmed CVT who were hospitalized from 1980 to 2014 in two third-level referral hospitals. For the present analysis we excluded cases arriving to hospital with profound altered consciousness. Neurological worsening was defined as the progression to stupor or coma in patients initially presenting alert or with somnolence. Bivariate analyses were performed to identify variables associated with neurological worsening. A multivariate analysis by logistic regression was performed to determine independent variables associated with this study outcome.
 
 
 Results:
 A total of 426 patients were here analyzed (94% aseptic CVT, 81.2% women, median age: 29 years, interquartile range: 22-38 years), of whom 51 (12%) presented neurological worsening at least 24 h after hospital arrival (median time to deterioration: 8 days, interquartile range: 5-13 days). Among patients with neurological worsening, 28 (54.9%) deteriorated after the first week upon arrival, 21 (41.2%) died during their hospital stay, and 36 (70.6%) attained a modified Rankin scale >3 at hospital discharge. Independent predictors of neurological worsening were age >40 years (OR: 5.79, 95% CI: 2.53-13.24), parenchymal lesion >6 cm in its greater diameter (OR: 11.27, 95% CI: 5.00-25.41) and bilateral pyramidal syndrome at arrival (OR: 15.40, 95% CI: 6.90-34.40). Etiological factors, anticoagulant vs antiplatelet therapy, seizures, presence of extracranial venous thrombosis or CVT topography were not independent predictors for neurological worsening after adjustment.
 
 
 Conclusion:
 Worsening in CVT patients arriving stable is not uncommon. More than half of cases of neurological deterioration occur after the first week of hospitalization, nonetheless, its determinants can be easily anticipated upon hospital arrival.
"
9fc3acef566f2eadd672b9b84f0d18f86c22ca00,"T2101 Safety of pregnancy following cerebral venous thrombosis (ISCVT2 PREGNANCY) D. Aguiar de Sousa1, P. Canhao1, I. Crassard2, J. Coutinho3, A. Arauz4, A. Conforto5, M. Giroud6, J.M.M.C. Ferro1 1Hospital de Santa Maria, University of Lisbon, Department of Neurosciences (Neurology), Lisbon, Portugal, 2Hôpital Lariboisière, Department of Neurology, Paris, France, 3Academic Medical Centre, Department of Neurology, Amsterdam, Netherlands, 4Instituto Nacional de Neurologia y Neurocirurgia, Department of Neurology, Mexico City, Mexico, 5Hospital das Clínicas, São Paulo University, Stroke Unit, São Paulo, Brazil, 6University Hospital and Medical School of Dijon, University of Burgundy, Stroke Unit, Dijon, France Background and aims: Pregnancy is associated with an increased risk of venous thrombotic events (VTE), including cerebral venous thrombosis (CVT). Data regarding the prognosis of pregnancy following CVT and the most appropriate preventive strategy are scarce. We aimed to study the course of subsequent pregnancies in women with previous CVT. Methods: Women of childbearing age (<45 years) included in the International Study of Cerebral Vein and Dural Sinus Thrombosis (ISCVT, 1998 to 2001) were eligible. Patients were interviewed (consultation or phone contact) to assess rate of VTE recurrence, outcomes of subsequent pregnancies and antithrombotic prophylaxis regimen. Results: 30/75 eligible centers agreed to participate. Follow-up was obtained in 108/185 patients (median follow-up 169±10 months). There were 76 new pregnancies in 44 women. In the 3rd trimester, 70% of women used low molecular weight heparin (LMWH) (52% in prophylactic dosage and 18% in therapeutic dosage). In 3/76 pregnancies, 4 VTE recurrences during pregnancy or puerperium were reported, including one CVT. 2/3 women were receiving prophylactic LMWH at the time of the event. Outcome of pregnancies was as follows: 47 full-term newborns, 8 preterm births, 2 stillbirths, 13 spontaneous and 6 induced abortions. Women with index CVT associated only to transient risk factors had a lower abortion rate (11%) than women in whom a predisposing condition for CVT was identified (24%). Conclusion: The absolute risk of pregnancy related VTE in women with previous CVT was low. Miscarriage rate in women with previous CVT associated to transient risk factors was similar to that estimated for the general population. Disclosure: Nothing to disclose T2102 Connectivity-based parcellation of the thalamus in multiple sclerosis and its implications for cognitive impairment: a multicenter study A. Bisecco1, M.A. Rocca1, E. Pagani1, L. Mancini2, C. Enzinger3, A. Gallo4, H. Vrenken5, M.L. Stromillo6, M. Copetti7, D. Thomas2, F. Fazekas3, G. Tedeschi4, F. Barkhof5, N. De Stefano6, M. Filippi1 1San Raffaele Scientific Institute, Vita-Salute San Raffaele University, Neuroimaging Research Unit, Milan, Italy, 2National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, London, United Kingdom, 3Medical University of Graz, Department of Neurology, Graz, Austria, 4Second University of Naples, MRI Center “SUN-FISM” , Naples, Italy, 5Free University Medical Center, Department of Radiology & Nuclear Medicine, Amsterdam, Netherlands, 6University of Siena, Department of Neurological and Behavioral Sciences, Siena, Italy, 7IRCCS-Ospedale Casa Sollievo della Sofferenza, Biostatistics Unit, San Giovanni Rotondo, Italy Background and aims: In this multicenter study, we performed a tractography-based parcellation of the thalamus and its white matter connections to investigate the relationship between thalamic connectivity abnormalities and cognitive impairment in multiple sclerosis (MS). Methods: Dual-echo, morphological and diffusion tensor (DT) MRI scans were collected from 52 relapsing-remitting MS patients and 57 healthy controls from six European centers. Patients underwent an extensive neuropsychological assessment. Thalamic connectivity defined regions (CDRs) were segmented based on their cortical connectivity using Diffusion Tractography-Based Parcellation. Between-group differences of CDRs and cortico-thalamic tracts DT MRI indices were assessed. A vertex analysis of thalamic shape was also performed. A random forest analysis was run to identify the best imaging predictor of global cognitive impairment and deficits of specific cognitive domains. Results: Twenty-two (43%) MS patients were cognitively impaired (CI). Compared to cognitively preserved (CP), CI MS patients had: 1) increased fractional anisotropy (FA) of motor, post-central and occipital connected CDRs (0.003<p<0.05) and 2) decreased FA of temporal connected CDRs (0.01<p<0.02). They also experienced more pronounced atrophy in anterior thalamic regions and abnormal DT MRI indices of all cortico-thalamic tracts. Damage of specific cortico-thalamic tracts explained global cognitive dysfunction and impairment of selected cognitive domains better than all other MRI variables. Thalamic CDR DT MRI abnormalities were correlated with abnormalities of the corresponding cortico-thalamic tracts. Tournament"
c260874ef2cbb939ca670d12cbf047c1ac6cb734,"Background Posterior reversible encephalopathy syndrome (PRES) is a well-known but rare complication in patients (<1%) with systemic lupus erythematosus (SLE). However, current epidemiological data are quite scant. The aim of the present study was to describe potentially unrecognised risk factors. Patients and methods We performed a multicentre, retrospective case–control study in Mexico between 1999 and 2014. We included a total of 168 patients who accounted for 77 episodes of PRES, as follows: SLE/PRES, 43 patients with 48 episodes; SLE without PRES, 96 patients; and PRES without SLE, 29 patients. SLE diagnosis was considered when patients fulfilled ≥4 American College of Rheumatology criteria. PRES was defined by reversible neurological manifestations and MRI changes. Results Patients with SLE/PRES were younger, presented with seizures as the most common manifestation (81%) and 18% had the typical occipital MRI finding. Hypertension (OR=16.3, 95% CI 4.03 to 65.8), renal dysfunction (OR=6.65, 95% CI 1.24 to 35.6), lymphopenia (OR=5.76, 95% CI 1.36 to 24.4), Systemic Lupus Erythematosus Activity Index ≥ 6 points (OR=1.11, 95% CI 1.01 to 1.22) and younger age (OR=0.86, 95% CI 0.81 to 0.91, p<0.001) were independent risk factors for development of PRES in SLE. Furthermore, dyslipidemia also characterised the association between PRES and SLE (OR=10.6, 95% CI 1.17 to 96.4). Conclusions This is the largest reported series of patients with SLE and PRES. We were able to corroborate the known risk factors for of PRES, and found two previously undescribed factors (lymphopenia and dyslipidemia), which suggests that endothelial dysfunction is a key element in PRES pathogenesis in lupus patients."
e65a98a9568ecd60a2e55ec5632fa07e7ad20585,"
 Background:
 Embolic stroke of undetermined source (ESUS) is a recently introduced clinical construct to define non-lacunar cryptogenic ischemic stroke and is the basis for two large international randomized trials recently underway. Diagnostic criteria are a visualized non-lacunar stroke on CT or MRI, absence of atrial fibrillation by history, ECG and at least 24 hours of rhythm monitoring, absence of intracardiac thrombus by echocardiography, no occlusive atherosclerosis based on imaging of the cervical and intracranial arteries, and no specific cause of stroke identified. The frequency and features of ESUS patients have not been characterized previously.
 
 
 Methods:
 Consecutive patients with recent ischemic stroke were retrospectively reviewed at 20 stroke research centers in 20 different countries to identify those meeting criteria for ESUS, as well as reasons not meeting criteria for the remaining patients.
 
 
 Results:
 Of 2.090 patients with recent ischemic stroke, 18% met ESUS criteria (range 2% to 36%). Of those not meeting ESUS criteria, major reasons were atrial fibrillation (26%), lacunar stroke (19%), incomplete diagnostic evaluation (18%), carotid artery stenosis >50% (17%), and intracranial stenosis (13%). Excluding those without the required diagnostic evaluation, 22% met criteria for ESUS. Of the 378 ESUS patients, the mean age was 64 years (vs. 68 years for non-ESUS), 56% were men (54% non-ESUS), 16% received tPA, and 33% were taking antiplatelet therapy at stroke onset. Hypertension, diabetes, and coronary artery disease were present in 66%, 26%, and 11%, respectively. At hospital discharge, 93% received antiplatelet therapy, 6% anticoagulant therapy, and 1% no antithrombotic therapy.
 
 
 Conclusions:
 Ischemic stroke patients meeting criteria for ESUS are not uncommon (22% of ischemic strokes) in stroke centers performing the required diagnostic evaluation. Antiplatelet therapy is overwhelmingly used for secondary prevention in these patients with non-lacunar cryptogenic stroke.
"
ef4744ea09df670f2260c009928cf28de4e5ff62,
1dda7f12c435f40c95c1c8c63ba538fcc48cf246,"Objective We aimed to stratify the risk of vascular event recurrence in patients with cerebral infarction according to living and socioeconomic characteristics and geographic region. Method The Outcomes in Patients with TIA and Cerebrovascular Disease (OPTIC) study is an international prospective study of patients aged 45 years or older who required secondary prevention of stroke [following either an acute transient ischemic attack, minor ischemic strokes, or recent (less than six-months previous), stable, first-ever, nondisabling ischemic stroke]. A total 3635 patients from 245 centers in 17 countries in four regions (Latin America, Middle East, North Africa, South Africa) were enrolled between 2007 and 2008. The outcome measure was the two-year rate of a composite of major vascular events (vascular death, myocardial infarction and stroke). Results During the two-year follow-up period, 516 patients experienced at least one major cardiovascular event, resulting in an event rate of 15·6% (95% confidence interval 14·4–16·9%). Event rates varied across geographical region (P < 0·001), ranging from 13·0% in Latin America to 20·7% in North Africa. Unemployment status, living in a rural area, not living in fully serviced accommodation (i.e., house or apartment with its own electricity, toilet and water supply), no health insurance coverage, and low educational level (less than two-years of schooling) were predictors of major vascular events. Major vascular event rates steeply increased with the number of low-quality living/socioeconomic conditions (from 13·4% to 47·9%, adjusted P value for trend <0·001). Conclusion Vascular risk in stroke patients in low- and middle-income countries varies not only with the number of arterial beds involved but also with socioeconomic variables."
1fcfebc32fce20ba88cd56216700dd66730b5a07,"Stroke is not only a leading cause of death worldwide but also a main cause of disability. In developing countries, its burden is increasing as a consequence of a higher life expectancy. Whereas stroke mortality has decreased in developed countries, in Latin America, stroke mortality rates continue to rise as well as its socioeconomic dramatic consequences. Therefore, it is necessary to implement stroke care and surveillance programs to better describe the epidemiology of stroke in these countries in order to improve therapeutic strategies. Advances in the understanding of the pathogenic processes of brain ischemia have resulted in development of effective therapies during the acute phase. These include reperfusion therapies (both intravenous thrombolysis and interventional endovascular approaches) and treatment in stroke units that, through application of management protocols directed to maintain homeostasis and avoid complications, helps to exert effective brain protection that decreases further cerebral damage. Some drugs may enhance protection, and besides, there is increasing knowledge about brain plasticity and repair mechanisms that take place for longer periods beyond the acute phase. These mechanisms are responsible for recovery in certain patients and are the focus of basic and clinical research at present. This paper discusses recovery strategies that have demonstrated clinical effect, or that are promising and need further study. This rapidly evolving field needs to be carefully and critically evaluated so that investment in patient care is grounded on well-proven strategies."
20dfc953219ae9f955194f3af8e58a28dec62a0d,"Background and Purpose— Inflammatory biomarkers predict incident and recurrent cardiac events, but their relationship to stroke prognosis is uncertain. We hypothesized that high-sensitivity C-reactive protein (hsCRP) predicts recurrent ischemic stroke after recent lacunar stroke. Methods— Levels of Inflammatory Markers in the Treatment of Stroke (LIMITS) was an international, multicenter, prospective ancillary biomarker study nested within Secondary Prevention of Small Subcortical Strokes (SPS3), a phase III trial in patients with recent lacunar stroke. Patients were assigned in factorial design to aspirin versus aspirin plus clopidogrel, and higher versus lower blood pressure targets. Patients had blood samples collected at enrollment and hsCRP measured using nephelometry at a central laboratory. Cox proportional hazard models were used to calculate hazard ratios (HRs) and 95% confidence intervals (95% CIs) for recurrence risks before and after adjusting for demographics, comorbidities, and statin use. Results— Among 1244 patients with lacunar stroke (mean age, 63.3±10.8 years), median hsCRP was 2.16 mg/L. There were 83 recurrent ischemic strokes (including 45 lacunes) and 115 major vascular events (stroke, myocardial infarction, and vascular death). Compared with the bottom quartile, those in the top quartile (hsCRP >4.86 mg/L) were at increased risk of recurrent ischemic stroke (unadjusted HR, 2.54; 95% CI, 1.30–4.96), even after adjusting for demographics and risk factors (adjusted HR, 2.32; 95% CI, 1.15–4.68). hsCRP predicted increased risk of major vascular events (top quartile adjusted HR, 2.04; 95% CI, 1.14–3.67). There was no interaction with randomized antiplatelet treatment. Conclusions— Among recent lacunar stroke patients, hsCRP levels predict the risk of recurrent strokes and other vascular events. hsCRP did not predict the response to dual antiplatelets. Clinical Trial Registration— URL: http://www.clinicaltrials.gov. Unique identifier: NCT00059306."
2a67eb8239c27d2ad6f6771d3df1d09020fd23a3,"
 Background and purpose:
 Headache is the most frequent presenting symptom of cerebral venous thrombosis (CVT), but many aspects of its profile are unknown because large scale studies are scarce. We aimed to analyze the headache characteristics in the largest cohort of Hispanic Mestizo patients with CVT.
 
 
 Methods:
 Since 1986 a systematic CVT registry has been carried out in 2 tertiary referral Neurological centers in Mexico City. Herein we examine the headache profile and the association of demographic, clinical and neuroimaging factors with the occurrence of headache in 408 consecutive patients.
 
 
 Results:
 Among 408 patients (82% women) included in this registry, headache occurred in 346 (85%; 86% in women, 79% in men, p NS), being the first neurological manifestation in 309 (76%). Headache onset was acute in 46%, of gradual onset in 51%, and a sudden explosive onset like thunderclap occurred in 3%. A diffuse headache pattern occurred in 56%, bifrontal in 22%, unilateral in 15%, and suboccipital in 8%. It was of pulsatile quality in 73%, and oppressive in 24.3%, and it was mostly deemed as severe (49%) or moderate (47%) in intensity, described as progressive in 19%, and with nausea or vomiting in 52% patients. Headache was the only neurological symptom in 58 patients (14%): as part of an isolated intracranial syndrome in 52 (13%) and as the sole manifestation of CVT in 6 (1.5%). Headache was followed by other neurological symptoms in 70%, appearing within 48 hours of headache onset in 33%, within 3-6 days in 34%, within 7-10 days in 16%, and after 10 days in 18%. CVT diagnosis was significantly delayed in patients with headache, as compared with other neurological presentations (delay >15 days: 30% vs. 8%, respectively; p<0.05). There were not differences in anatomical topography of CVT in patients presenting with headache, as compared with other neurological features.
 
 
 Conclusions:
 There is no identifiable, uniform, recognizable pattern of headache in CVT and often precedes the development of other neurological deficits for days or even weeks delaying the diagnosis of CVT. Interestingly, neither the presence nor the quality of headache is associated with the topography or extension of CTV.
"
429807a26322910cba71a353735d1695e229b650,
70d8c819539d699484616f783a0e49055bfef438,
8e459caa8303138464632bc066090ce24e28bfbf,
971e15203b4bee9d07490fea11ca198bb87c71ff,
aaf5884ca3870b8a369ad24eff5926aacbea003b,
ad32916c9ed3c2c108e921ee14ab1d779eacb196,
af8e514f18104ecddce4612d494e5fab63169a68,"Cerebral venous thrombosis (CVT) is a relatively rare form of stroke usually affecting young individuals. CVT is characterized by the diversity of its neurologic manifestations, which require a high level of clinical suspicion for diagnosis and prompt, appropriate treatment. Multiple circumstances have been associated with CVT, such as prior medical conditions, transient situations, certain medications, and some predisposing conditions. Headache, focal neurologic deficit, and seizures are the most frequent clinical manifestations at onset. MRI in combination with venography has become the imaging modality of choice, as this technique has a high sensitivity and specificity for establishing a diagnosis. CT venography is an alternative to MRI because this technique produces similar diagnostic results. Pharmacologic treatment of CVT with anticoagulants is widely accepted. Per 100 cases, the recurrence of CVT is 2.8% and the mortality of CVT is 10%."
c72cef6b190c6fb04110e386258bca2514286667,"PURPOSE OF REVIEW
The third International Stroke Trial (IST-3) was a randomized controlled trial of thrombolysis with intravenous recombinant tissue plasminogen activator in patients with acute ischemic stroke within 6 h of onset. It sought to determine whether a wider variety of patients might benefit from treatment than were eligible under the prevailing European Union approval for the drug, especially among those aged over 80 years.


RECENT FINDINGS
The entry criteria were broad, and there was no upper age limit for inclusion; over half the 3035 patients were aged over 80 years. For the types of patient recruited in IST-3, despite the early hazards (chiefly of fatal intracerebral hemorrhage), thrombolysis within 6 h did not affect longer-term survival and improved functional outcome. Benefit was greatest among patients treated within 3 h, and benefit did not appear to be diminished among elderly patients or those with severe stroke.


SUMMARY
These results should, therefore, encourage clinicians to: consider thrombolytic treatment for a wider variety of patients (particularly those aged over 80 years); treat those with more severe strokes; reinforce their efforts to increase the proportion of ischemic strokes treated within 3 h; and, have greater confidence that mortality is not increased by treatment.


VIDEO ABSTRACT
http://links.lww.com/CONR/A23."
cd91ea0d46fa75bfba79ea4b51fb7d23621ae484,"Background and Purpose The study aims to compare lipid profiles among ischemic stroke patients in a predominantly Caribbean-Hispanic population in Miami and a Mestizo Hispanic population in Mexico City. Methods We analyzed ischemic stroke Hispanic patients with complete baseline fasting lipid profile enrolled contemporaneously in the prospective registries of two tertiary care teaching hospitals in Mexico City and Miami. Demographic characteristics, risk factors, medications, ischemic stroke subtype, and first fasting lipid profile were compared. Vascular risk factor definitions were standardized. Multiple linear regression analysis was performed to compare lipid fractions. Results A total of 324 patients from Mexico and 236 from Miami were analyzed. Mexicans were significantly younger (58·1 vs. 67·4 years), had a lower frequency of hypertension (53·4% vs. 79·7%), and lower body mass index (27 vs. 28·5). There was a trend toward greater prevalence of diabetes in Mexicans (31·5 vs. 24·6%, P = 0·07). Statin use at the time of ischemic stroke was more common in Miami Hispanics (18·6 vs. 9·4%). Mexicans had lower total cholesterol levels (169·9 ± 46·1 vs. 179·9 ± 48·4 mg/dl), lower low-density lipoprotein (92·3 ± 37·1 vs. 108·2 ± 40·8 mg/dl), and higher triglyceride levels (166·9 ± 123·9 vs. 149·2 ± 115·2 mg/dl). These differences remained significant after adjusting for age, gender, hypertension, diabetes, body mass index, smoking, ischemic stroke subtype, and statin use. Conclusion We found significant differences in lipid fractions in Hispanic ischemic stroke patients, with lower total cholesterol and low-density lipoprotein, and higher triglyceride levels in Mexicans. These findings highlight the heterogeneity of dyslipidemia among the Hispanic race-ethnic group and may lead to different secondary prevention strategies."
e0e91d1cecab61bd840c44509a8c4d3d88124bd4,
fa53bcd872c522be74d7900ad5467bf10880fe32,"Background and Purpose: Stroke is the major cause of vascular behavior and cognitive disorders worldwide. In developing countries, there is a dearth of information regarding the public health magnitude of stroke. The aim of the Fogarty-Mexico cohort was to assess the prevalence of vascular behavioral and cognitive disorders, ranging from mild vascular cognitive impairment (VCI) to vascular dementia (VaD), in a cohort of acute first-ever symptomatic stroke patients in Mexico. Methods: A total of 165 consecutive, first-ever stroke patients admitted to the National Institute of Neurology and Neurosurgery in Mexico City, were included in the cohort. Patients were eligible if they had an ischemic stroke, primary intracerebral hemorrhage, or cerebral venous thrombosis (CVT). Stroke diagnosis required the presence of an acute focal deficit lasting more than 24 h, confirmed by a corresponding lesion on CT/MRI. Stroke severity was established with the NIH Stroke Scale. The pre-stroke functional status was determined by the IQCODE. Three months after the occurrence of stroke, 110 survivor patients returned for follow-up and were able to undergo functional outcome (modified Rankin scale, Barthel index), along with neurological, psychiatric, neuropsychological, laboratory, and imaging assessments. We compared depression, demographic, and clinical and imaging features between patients with and without dementia, and between patients with VCI and those with intact cognition. Results: Of the 110 patients (62% men, mean age 56 ± 17.8, education 7.7 ± 5.2 years) 93 (84%) had ischemic strokes, 14 (13%) intracerebral hemorrhage, and 3 (3%) CVT. The main risk factors were hypertension (50%), smoking (40%), hypercholesterolemia (29%), hyperhomocysteinemia (24%), and diabetes (22%). Clinical and neuropsychological evaluations demonstrated post-stroke depression in 56%, VCI in 41%, and VaD in 12%; 17% of the latter had pre-stroke functional impairment (IQCODE >3.5). Cognitive deficits included executive function in 69%, verbal memory in 49%, language in 38%, perception in 36%, and attention in 38%. Executive dysfunction occurred in 36% of non-demented subjects, 65% of them with mild-moderate deficits in daily living activities. Female gender (p ≤ 0.054), older age (mean age 65.6 years vs. 49.3, p < 0.001), diabetes (p ≤ 0.004), illiteracy and lower education (p ≤ 0.001), and PSD (p = 0.03) were significantly higher in VCI-VaD compared with cognitively intact post-stroke subjects. We could not demonstrate an association with lesion site and distribution of the cognitive deficits. Conclusions: The Fogarty-Mexico cohort recruited relatively young acute stroke patients, compared with other Mexican stroke cohorts. PSD and VCI occurred frequently but prevalence of VaD (12%) was lower than expected. A high prevalence of treatable stroke risk factors suggests that preventive interventions are advisable."
1d3d118465d827916719e9b0a6abf9927cefc3fe,"Purpose of review The third International Stroke Trial (IST-3) was a randomized controlled trial of thrombolysis with intravenous recombinant tissue plasminogen activator in patients with acute ischemic stroke within 6 h of onset. It sought to determine whether a wider variety of patients might benefit from treatment than were eligible under the prevailing European Union approval for the drug, especially among those aged over 80 years."
3cc82218d8f80c3e4f0084a548e3563c08065a0f,"Objective To compare the effects of antiplatelets and anticoagulants on stroke and death in patients with acute cervical artery dissection. Design Systematic review with Bayesian meta-analysis. Data Sources The reviewers searched MEDLINE and EMBASE from inception to November 2012, checked reference lists, and contacted authors. Study Selection Studies were eligible if they were randomised, quasi-randomised or observational comparisons of antiplatelets and anticoagulants in patients with cervical artery dissection. Data Extraction Data were extracted by one reviewer and checked by another. Bayesian techniques were used to appropriately account for studies with scarce event data and imbalances in the size of comparison groups. Data Synthesis Thirty-seven studies (1991 patients) were included. We found no randomised trial. The primary analysis revealed a large treatment effect in favour of antiplatelets for preventing the primary composite outcome of ischaemic stroke, intracranial haemorrhage or death within the first 3 months after treatment initiation (relative risk 0.32, 95% credibility interval 0.12 to 0.63), while the degree of between-study heterogeneity was moderate (τ2 = 0.18). In an analysis restricted to studies of higher methodological quality, the possible advantage of antiplatelets over anticoagulants was less obvious than in the main analysis (relative risk 0.73, 95% credibility interval 0.17 to 2.30). Conclusion In view of these results and the safety advantages, easier usage and lower cost of antiplatelets, we conclude that antiplatelets should be given precedence over anticoagulants as a first line treatment in patients with cervical artery dissection unless results of an adequately powered randomised trial suggest the opposite."
459746ab8844fa355567de3a257e3b24ec19d14c,"Although tobacco smoking is an independent risk factor for cerebrovascular disease (CVD) (1,2), CVD health-care costs generated by smoking remain unknown in many countries (3,4), including Mexico (5). To estimate the direct health-care cost of CVD from a provider’s perspective, we conducted a cost of illness (COI) analysis using data from the medical records of patients who visited the Mexican National Institute of Neurology and Neurosurgery in 2010. Cost estimates in Mexican pesos were converted into US dollars using the 2010 FIX exchange rate for foreign transactions. The fraction of CVD costs attributable to smoking was used to estimate the costs of smoking. The total cost of smoking-attributable CVD at our center in 2010 was $1 179 607. The average annual cost per patient by CVD type was $3904 for ischemic stroke, $6098 for hemorrhagic stroke, $2416 for cerebral venous thrombosis, and $11 477 for subarachnoid hemorrhage. The medical care costs of newly diagnosed with CVD patients are mainly attributable to immediate care in the emergency room and initial hospitalization, when the patient is stabilized and diagnostic tests are conducted (Fig. 1). This study in a developing country confirmed the high direct medical care costs of patients with CVD attributable to smoking. Implementing the WHO Framework Convention of Tobacco Control should work to reduce these costs."
4a51b661b4969aa9b6f9e4f05adf873c8c4aa825,
644c05089a513318a65bee78f3c791428d70958c,To evaluate the incidence and predictors of ischaemic recurrent stroke and the adverse events of antithrombotic therapy in patients with first intra‐ or extracranial vertebral artery dissection (VAD) who were treated with aspirin or oral anticoagulation (OA).
673641c8e08e63eded3e5ae3719780f23c7fd4b1,
9dd90ab4419acad828649e32c26ffc72be1d38c2,
aed47450d839dc23d87c30d8e5f78bf65371d3f5,"OBJECTIVE: To compare lipid profiles among ischemic stroke (IS) patients in a predominantly Caribbean-Hispanic population in Miami and a Mestizo Hispanic population in Mexico City. BACKGROUND: Significant differences in stroke mortality, risk factors and IC subtypes have been described between Hispanics and other subgroups. No study has systematically examined differences in lipid profiles across different Hispanic groups. DESIGN/METHODS: We analyzed IS Hispanic patients with complete baseline fasting lipid profile enrolled contemporaneously in the prospective registries of 2 tertiary care teaching hospitals in Mexico City and Miami. Demographic, risk factors, medications, TOAST subtype, and first fasting lipid profile were compared. Vascular risk factor definitions were standardized. Multiple lineal logistic regression analysis was performed to compare lipid fractions. RESULTS: A total of 560 patients (324 from Mexico and 236 from Miami) were analyzed. Mexicans, were significantly younger (58.1 vs. 67.4 years), had a lower frequency of hypertension (53.4% vs. 79.7%), and lower body mass index (27 vs. 28.5). There was a trend towards greater prevalence of diabetes in Mexicans (31.5 vs. 24.6%, p=.07). Antilipidemics was more common in Miami Hispanics (18.6 vs. 9.4%). Mexicans had lower total cholesterol (TC) levels (169.9 ± 46.1 vs. 179.9 ± 48.8 mg/dl), low density lipoprotein (LDL) (92.3 ±37.1 vs. 108.2±40.8 mg/dl), non-high-density lipoprotein cholesterol (124.8±46.2 vs. 137.8±47.6 mg/dl) and lower lipid ratios (TC/HDL 4.2±1.6 vs. 4.6±1.8 and LDL/HDL 2.3±1.2 vs. 2.8±1.3). Mexicans had higher triglyceride levels (166.9±123.9 vs. 149.2±115.2 mg/dl). High-density lipoprotein (HDL) was similar. These differences remained significant after adjusting for age, sex, hypertension, diabetes, body mass index, smoking, stroke subtype and statin use. CONCLUSIONS: We found significant differences in lipid fractions, with lower LDL levels and higher TG levels in Mexicans. These findings highlight the heterogeneity of dyslipidemia among the Hispanic race-ethnic group. The underlying genetic and environmental contributions to these differences need to be further explored. Disclosure: Dr. Arauz has nothing to disclose. Dr. Romano has received personal compensation for activities with NovaVision. Dr. Romano holds stock and/or stock options in NovaVision. Dr. Romano has received research support from NovaVision. Dr. Ruiz-Navarro has nothing to disclose. Dr. Rundek has nothing to disclose. Dr. Shang has nothing to disclose. Dr. Ruiz-Navarro has nothing to disclose. Dr. Dong has nothing to disclose. Dr. Koch has nothing to disclose. Dr. Rojas has nothing to disclose. Dr. Katsnelson has nothing to disclose. Dr. Hernandez has nothing to disclose. Dr. Sacco has nothing to disclose."
b8bc2476dfa589e9e46151739441ff36bea0f8dd,"Objective
Non-valvular atrial fibrillation (NVAF) is a major risk factor for ischemic stroke (IS) and a powerful predictor of mortality. This study investigates early and long-term outcome among patients with IS secondary to NVAF and identify the main factors associated with poor outcome, recurrence, and death.


Methods
We analyzed the data from our consecutive NVAF acute IS database, over a period of 23 years. The endpoints were bad outcome (Modified Rankin Score ≥3), recurrence, and mortality at discharge, after 6 months, 12 months, and final follow-up. Multivariate Cox and Kaplan-Meier analysis were used to estimate the probability of death.


Results
129 consecutive acute IS patients were included (77 [59.7%] females, mean age 70.2 ± 10.1 years). Discharge, 6 and 12 months bad outcome was 62%, 63%, and 61%, respectively. After a median follow-up of 17 months (IQR 6-54.5), 35.6% patients had bad outcome, 21.7% had recurrence and 36.4% died. The recurrence and death annual rates were 19.1% and 6.32%. The absence of oral anticoagulation (OAC) and NIHSS score > 12 were the strongest predictors of mortality.


Conclusions
IS secondary to NVAF has a high rate of stroke recurrence and mortality in our population, with the absence of OAC and major stroke as the main risk factors."
c2ba263afa35a47837c947ebbbba38c7ce0d576e,"Background: Current evidence shows that uric acid is a potent antioxidant whose serum concentration increases rapidly after acute ischemic stroke (AIS). Nevertheless, the re-lationship between serum uric acid (SUA) levels and AIS outcome remains debatable. We aimed to describe the prognostic significance of SUA in AIS. Methods: We studied 463 patients (52% men, mean age 68 years, 13% with glomerular filtration rate <60 ml/min at hospital arrival) with AIS pertaining to the multicenter registry PREMIER, who had SUA measurements at hospital presentation. Multivariate models were constructed to analyze the association of SUA with functional outcome as assessed by the modified Rankin scale (mRS) at 30-day, 3-, 6- and 12-month follow-up. A mRS 0-1 was regarded as a very good outcome. Results: Mean SUA concentration at hospital arrival was 6.1 ± 3.7 mg/dl (362.8 ± 220.0 μmol/l). Compared with cases with higher SUA levels at hospital admission, patients with ≤4.5 mg/dl (≤267.7 μmol/l; the lowest tertile of the sample) had more cases of a very good 30-day outcome (30.5 vs. 18.9%, respectively; p = 0.004). SUA was not associated with mortality or functional dependence (mRS >2) at 30 days, or with any outcome measure at 3, 6 or 12 months poststroke. After adjustment for age, gender, stroke type and severity (NIHSS <9), time since event onset, serum creatinine, hypertension, diabetes and smoking, a SUA ≤4.5 mg/dl (≤267.7 μmol/l) was positively associated with a very good short-term outcome (odds ratio: 1.76, 95% confidence interval: 1.05-2.95; negative predictive value: 81.1%), but not at 3, 6 or 12 months of follow-up. When NIHSS was entered in the multivariate model as a continuous variable, the independent association of SUA with outcome was lost. Compared with cases with higher levels, patients with SUA ≤4.5 mg/dl (≤267.7 μmol/l) were more frequently younger than 55 years, women, with mild strokes, with normal serum creatinine and fewer had hypertension. The time since event onset to hospital arrival was not significantly associated with AIS severity or SUA levels; nevertheless, a nonsignificant tendency was observed for patients with severe strokes and high SUA levels arriving in <24 h. Conclusions: A low SUA concentration is modestly associated with a very good short-term outcome. Our findings support the hypothesis that SUA is more a marker of the magnitude of the cerebral infarction than an independent predictor of stroke outcome."
d6e02120870ea4aaef265d3307632fcb26f94936,"OBJECTIVE: Describe the clinical, imaging and genetic features of the first Mexican family with CADASIL. BACKGROUND: Mutations in the NOTCH3 gene are responsable of (CADASIL, an adult onset hereditary angiopathy leading to ischemic stroke, vascular dementia and psychiatric disorders. All mutation of NOTCH3 described so far are striking stereotyped leading to the gain or loss of cystiene residue in a given epidermal growth factor (EGF). DESIGN/METHODS: Three members of a single Mexican family from the state of Sinaloa (North-East region of Mexico) were genetically studied in search of CADASIL confirmation. The patients come from four generations family in which there were at least 18 members with clinical data suggestive of of CADASIL (Migraine, Depression; Strokes, Dementia). DNA was extracted from whole blood using standard procedures. RESULTS: The family is a native Mexican family settled in Sinaloa, Northwestern part of the country. The proband is a female seen at age 48 years with history of migraine. An brain MRI disclosed whitte matter and periventricular abnormalities as well FLAIR and T2 hyperintensities in temporal anterior poles and basal ganglia. Her father died at age 63 years after 6 years of history with repeated strokes and dementia. Two patternal uncles with history of multiple strokes and dementia. She has fourteen siblings, six of them with history of depression, multiple stroke and dementia. Five of them are alive and with abnormalities in brain MRI.One nephew with migraine and a brother with clinical spectrum of CADASIL were eavluated in search of notch3 mutation. We identified a heterozygous point mutation in exon 3 of NOTCH 3 (c.304T>A). This mutation was detected in all 3 patients. This is a mutation typical of CADASIL. CONCLUSIONS: Clinical, imaging and genetic characterisitics of the first mexican family with CADASIL are presented. The clinical findings are similar to those presented in other races with CADASIL. Disclosure: Dr. Barinagarrementeria has nothing to disclose. Dr. Arauz has nothing to disclose. Dr. Mine has nothing to disclose."
f508949fe04a80f00a99b62013df6df55adbdb10,"Background There is a paucity of data on patients with stroke/transient ischaemic attack in low- and middle-income countries. We sought to describe the characteristics and management of patients with an ischaemic stroke and recent transient ischaemic attack or minor ischaemic strokes in low- or middle-income countries. Methods The Outcomes in Patients with TIA and Cerebrovascular disease registry is an international, prospective study. Patients ≥45 years who required secondary prevention of stroke (either following an acute transient ischaemic attack or minor ischaemic strokes (National Institutes of Health Stroke Scale <4) of <24 h duration, or recent (<6 months), stable, first-ever, non-disabling ischaemic stroke) were enrolled in 17 countries in Latin America, the Middle East, and Africa. The main measures of interest were risk factors, comorbidities, and socio-economic variables. Results Between January 2007 and December 2008, 3635 patients were enrolled in Latin America (n =1543), the Middle East (n =1041), North Africa (n = 834), and South Africa (n = 217). Of these, 63% had a stable, first-ever ischaemic stroke (median delay from symptom onset to inclusion, 25 days interquartile range, 7–77); 37% had an acute transient ischaemic attack or minor ischaemic stroke (median delay, two-days; interquartile range, 0–6). Prevalence of diabetes was 46% in the Middle East, 29% in Latin America, 35% in South Africa, and 38% in North Africa; 72% had abdominal obesity (range, 65–78%; adjusted P < 0·001); prevalence of metabolic syndrome was 78% (range, 72–84%, P < 0·001). Abnormal ankle brachial index (<0·9) was present in 22%, peripheral artery disease in 7·6%, and coronary artery disease in 13%. Overall, 24% of patients had no health insurance and 27% had a low educational level. Interpretation In this study, patients in low- and middle-income countries had a high burden of modifiable risk factors. High rates of low educational level and lack of health insurance in certain regions are potential obstacles to risk factor control. Funding The Outcomes in Patients with TIA and Cerebrovascular disease registry is supported by Sanofi-Aventis, Paris, France."
fe0f146ff9a1ff1aea9d2e391d3b31f9801f1302,
1f94a2f05ee4d41fb0dd924a851d0af24191f15e,"Stroke is recognised as a leading cause of the global disease burden. In high-income countries, 20% of strokes occur in people of working age, many of whom are in paid employment.1 This figure is probably much higher in low-to middle-income countries where the average age of onset of stroke is lower. Employment is one of the most important social roles that a person fulfills and not working has negative impacts on one's overall quality of life, health, finances, social isolation and self-efficacy. Because younger adults are responsible for generating income and supporting family members, returning to work is a key goal in the recovery from disabling illnesses such as stroke.

Given improved survival after stroke and an ageing population and workforce, the cost of stroke are expected to further increase, with lost productivity projected to be …"
39fddf33eb2a714e139ea4a7f80ab40b8c2a0623,
690ad550e09b273d6f2726777c846294971aba40,
7a19bc9c4fee2da31109fe6936b06919ee0184c8,"Background Among patients with a patent foramen ovale and cryptogenic ischemic stroke, the long-term prognosis is unclear. Aims This study aims to estimate the recurrence rate in young cryptogenic stroke patients with and without patent foramen ovale. Patients and methods One hundred eighty-six cryptogenic stroke patients (aged 18–45 years) were prospectively followed for up to five-years. They were divided into two groups according to the echocardiographic presence of patent foramen ovale. All patients received aspirin (100mg/day) for secondary prevention. Results Mean age was 32·3 (standard deviation 7·9) years. During the mean follow-up of 66 months five patients with patent foramen ovale had recurrent strokes compared with 11 patients without patent foramen ovale. The average annual rate of recurrent cerebral ischemia was 1·1% and 1·6% for patients with and without patent foramen ovale, respectively. The recurrence rate did not increase with the presence of patent foramen ovale, atrial septal aneurysm or other variables. More than 60% of the reported cases achieved a good functional outcome. Conclusions Young patients with cryptogenic ischemic stroke with and without patent foramen ovale have a low recurrence rate in a long-term follow-up and most present a favorable outcome. Patent foramen ovale with or without atrial septal aneurysm did not increase the risk of recurrence."
9a5e4a698b84d774162ce2d2f8262cd041b45fe2,"Background:It is well established that several infectious diseases can directly lead to ischemic or hemorrhagic stroke. Neurocysticercosis (NCC), caused by infection of the human central nervous system with the parasite Taenia solium, is recognized as an important public health problem in developing countries. The clinical manifestations of NCC are nonspecific and varied depending on the number and topography of lesions. Cerebrovascular disease is a relatively common but underrecognized complication of NCC; published data indicate that the incidence of stroke is between 4% and 12% in patients with NCC, depicting a clear relationship among these 2 pathologies. Review Summary:We review the cerebrovascular complications of NCC including the possible role of NCC as a cerebrovascular risk factor, including epidemiology, pathogenesis, diagnosis, and management of the cerebrovascular complications derived from cysticercal infarction and those associated with the use of anticysticercal drugs. Common and uncommon clinical manifestations, localization of stroke, and associated syndromes are discussed along with their prognostic significance. Conclusions:Although an underrecognized cause of stroke, present preponderantly in undeveloped countries, NCC still causes significant incapacity and even death in young patients suffering from stroke in the absence of cerebrovascular risk factors; hence, neurologists should become familiar with this potential complication."
a20191eb44071aea26ed04b6dea4accd1dad660f,"
 Background.
 It is assumed that cerebral venous thrombosis (CVT) implies a lower risk of death and recurrence at short- and mid-term follow-up; however, few information exist on risk factors and outcome of thrombotic venous recurrence after CVT in the long run. We aimed to describe the 20-year recurrence of intra and extracranial venous thrombotic events after an index CVT in a large cohort.
 
 
 Methods.
 Among 412 consecutive patients, we analyzed 336 (81.6%) cases (82,7% women, mean age: 30.9 years, range: 14 to 83 years) registered from 1986 to 2010 in a third-level referral center of Mexico City, for whom complete information of recurrent events was available and who survived the acute phase of CVT.
 
 
 Results.
 In a median follow-up of 28 months (range 2 to 288 months), the recurrence rate of either systemic or intracranial venous thrombotic events was 6.8% (n=23): 2.7% of recurrent CVT, 3% lower-limb thrombosis and 1.1% pulmonary embolism). Recurrent venous events occurred by a median of 5 months (range: 1 to 113 months), with 78% cases recurring in the following year after index CVT. Nonetheless, recurrent CVT occurred within 5 months in 78% cases. None of the recurrent CVT events caused death, but 50% of pulmonary embolisms were mortal. In a Cox proportional hazards model adjusted for multiple confounders, baseline factors associated with an increased risk of recurrent intra or extracranial venous thrombosis were thrombosis of jugular veins [hazard ratio (HR): 5.13, 95% confidence interval (CI): 1.39-18.9], diagnosis of anti-phospholipid syndrome (HR: 2.67, 95% CI: 1.01-7.07), mechanical ventilation (HR: 2.07, 95% CI: 1.08-3.93), and alcoholism (HR: 3.44, 95% CI: 1.12-10.58).
 
 
 Conclusion.
 Most venous recurrent events occur in the following year after index CVT. Recurrent CVT is not associated with a high mortality, but extracranial venous thromboembolisms can be fatal. This study is the largest to date on this topic.
"
b15be818fa50b73eebf5543e847fe4efe1789e6c,"Background : The impact of socioeconomic factors (SEF) on the risk of future vascular events in stroke patients has been understudied. The Outcomes in Patients with TIA and Cerebrovascular disease (OPTIC) registry included patients in secondary prevention of stroke. Objective : to stratify the risk of vascular event recurrence in patients with cerebral infarction according to presence of PAD, ankle-brachial index (ABI), known coronary artery disease (CAD), involvement of several arterial beds, geographic variations and SEF. Method : Between January 2007 and December 2008, 3635 patients aged 45 years or older were enrolled in the OPTIC registry from 245 sites in 17 countries in the following regions: Latin America (1543 patients), Middle East (1041 patients), North Africa (834 patients), and South Africa (217 patients). PAD was present in 7.8%, ABI in 22%, CAD in 12.8%, and 31.1% were unemployed, 26.2% had less than 2 school years, 23% of patients had no health insurance, 12.8% lived in rural area, 8.4% lived alone, 7.5% did not live in a house/flat. Primary endpoint included vascular death (VD), myocardial infarction (MI) and stroke. Results : During median follow-up of 731 days, 524 patients had at least 1 primary event; 190 patients had VD, 88 nonfatal MI, and 296 nonfatal stroke. The estimated risk of primary endpoint was 15.6% (95%CI, 14.4-17.0%) at 2-year. The risk increased with the number of vascular beds involved from 13.1% to 30.7% (p for trend Conclusions : vascular risk in stroke patients in North and South Africa, Middle East and Latin America varies not only with the number of arterial beds involved but also with socio-economic variables, particularly poor health insurance cover, not living in a house/flat and low education level"
b4e8d0adff70f7b87b0ca3673f252b4e5836fd7d,"Cerebral microbleeds (CMB) are focal haemosiderin deposits that result from minimal blood leakage from damaged small vessels, and they can be regarded as markers of pathological vascular changes. In addition to their association with small vessel disease, CMB have also been pathologically linked to cerebral amyloid angiopathy.1 The prevalence of CMB in the general population increases from 20% in subjects aged 60–69 years to 40% in subjects aged 80 years and older.2 Clinically, CMB have been associated with cerebrovascular disease and some of its risk factors, including lacunar infarcts, intracerebral haemorrhage, white matter changes and hypertension. CMB have also been reported to be increased in Binswanger disease, mild cognitive impairment and Alzheimer's disease (AD).1 ,2 The location and distribution of CMB …"
bd1e49e92eaac9a79c5f3f614c538f6e151b70d5,"Background. Seizures is a very common clinical presentation of cerebral venous thrombosis (CVT); however, little is known about the future risk of epilepsy in patients suffering CVT. Our objective was to analyze risk factors for epilepsy in a long-term follow-up after CVT. Methods. This is a cohort descriptive study of consecutive non-selected patients with acute cerebrovascular disease, systematically registered from 1986 to 2010 in a third-level referral center of Mexico City. Here we analyzed 340 patients who survived the first 6 months after CVT, who were not epileptic at baseline and for whom complete long-term information on neurological outcome was available. Results. Seizures occurred in 183 (54%) patients, in 26% of them as a clinical presentation and 74% at some point during follow-up. Focal motor seizures occurred in 6.5%, secondary generalized focal seizures in 13.8% and generalized tonic-clonic seizures in 22.4%. Status epilepticus occurred in 13 (7%) cases. In all, during a median follow-up of 28 months (range 2 to 288 months), epilepsy was present in 14.7% (27.3% of those who presented seizures). In a multivariate analysis adjusted for multiple confounders, risk factors associated with an increased risk of epilepsy during follow-up were presenting seizures as a clinical presentation [odds ratio (OR): 4.32, 95% confidence interval (CI): 2.20-8.48], pregnancy and puerperium (OR: 2.03, 95% CI: 1.11-3.71) and thrombosis of the longitudinal sinus (OR: 1.86, 95% CI: 1.01-3.41). Conclusion. Seizures are common at CVT presentation, but risk increases during the acute phase after thrombotic event. Most seizures resolve during the first month, but epilepsy occurred in 15% of patients with CVT in the long run."
cb9f320b9240739c375b653d0b36c4084483f2cc,"
 Objective:
 Conventional treatment of cerebral venous sinus thrombosis (CVST) has been systemic heparinization. A small percentage of CVST present with subarachnoid hemorrhage (SAH). We retrospectively evaluated the efficacy and safety of anticoagulation in consecutive patients with SAH due to CSVT.
 
 
 Materials and Methods:
 A retrospective review of our stroke database from November, 1994 to August, 2010 identified 21 consecutive patients who had presented with SAH secondary to CVST. CVST was documented through angiography, venous angioTC or venous phase angioresonance. CVST was documented through CT scan or lumbar puncture when image was equivocal. Diagnosis was performed by a neurologist and confirmed by neuorradiologists. Patient histories were reviewed to collect data on presentation, presence of venous infarction or hemorrhagic transformation, affected sinus, treatment with anticoagulation, follow up, and functional outcome.
 
 
 Results:
 21 patients were included for analysis. The mean patient age was 39years (20-83). Sixty-seven percent of patients were female. The initial symptom was thunderclap headache in 11 patients (52%), acute progressive headache in 9 patients (43%) and seizures in 1 patient (5%). 15 patients (71.4%) were treated with anticoagulants. 10 patients (48%) were treated with low molecular weight heparin, 5 patients (24%) with non fractioned heparin. All these patients continued treatment with oral anticoagulation. 3 patients (14.3% received treatment with antiplatelet agents. 3 patients (14%) received no treatment. No patients developed intracaranial or extracranial hemorrhagic complications after initial treatment. One patient died due to cerebral edema. All other patients had good functional outcome (modified Rankin Scale<2) regardless of treatment.
 
 
 Conclusion:
 Treatment with anticoagulant drug appear to be safe in the setting of CVST related HSA. Outcome appears to be good in most patients. Our study is not powered to detect the efficacy of treatment in these patients.
"
1d001a47918af82ff0237911950238aa98c95686,
364f3c25ab32a7626529ff032fb2782797885196,
5ec086dac9115e707324cfb3b18f0ccce5458417,"INTRODUCTION
The primary antiphospholipid syndrome (PAS) is an independent risk factor for cerebral infarction. AIM. To evaluate the risk of recurrence, to compare different treatments and determine the risk factors associated with recurrence and hemorrhagic complications in patients with cerebral infarction and PAS.


PATIENTS AND METHODS
Prospectively collected data from 92 patients under 45 years (71% female, mean age 33.8 ± 8.9 years) with confirmed diagnoses of cerebral infarction and PAS, treated with anticoagulants (n = 54) or aspirin (n = 38) were retrospectively analyzed. Clinical follow-up was obtained by neurological examination every 6 to 12 months. Outcome measures were: recurrence of CI, symptomatic intracerebral hemorrhage, and minor bleeding.


RESULTS
During a median follow-up of 54 months (range: 12-240 months), there were 8 (9%) recurrent cerebral infarctions, with no difference between treatment with aspirin (n = 0) or anticoagulants (n = 8). The annual rate of recurrence was 0,014 person-years of follow-up. The history of previous thrombosis and spontaneous abortions were more frequent in patients with recurrence. Aspirin-treated patients more frequently came from rural areas. Four anticoagulated patients developed bleeding complications, two minor bleeding and two subdural hematomas. 76% of the cases evolved with good outcome (modified Rankin scale: 0-2).


CONCLUSION
With the limitations of a nonrandomized study, our data suggest that the risk of recurrent arterial cerebral infarction in young patients with cerebral infarction secondary to PAS is low, probably non-uniform and independent of the type of antithrombotic."
6d4936a04f7760911b7d1686e5811a376c1da3c3,
6dcc1f484623d5063f93efed8b44601705b77437,"OBJECTIVE
To analyze the association between the admission systolic blood pressure (SBP) and 30-day outcome in patients with acute cerebrovascular disease.


METHODS
The REgistro NAcional Mexicano de Enfermedad VAScular Cerebral (RENAMEVASC) is a hospital-based multicenter registry performed between November 2002 and October 2004. A total of 2000 patients with clinical syndromes of acute cerebrovascular disease confirmed by neuroimaging were registered. The modified Rankin scale was used for outcome stratification.


RESULTS
We analyzed 1721 patients who had registered their SBP: 78 (4.5%) had transient ischemic attack, 894 (51.9%) brain infarction, 534 (30.9%) intracerebral hemorrhage, 165 (9.6%) subarachnoid hemorrhage and 50 (2.9%) cerebral venous thrombosis. Among 1036 (60.2%) patients with the antecedent of hypertension, only 32.4% had regular treatment. The 30-day case fatality rate presented a J pattern with respect to SBP, so that the risk of death was highest in <100 mmHg (37.5%), decreased between 100 and 139, and reached gradually a new zenith in ?220 mmHg (35.3%). The best functional outcome corresponded to patients who had SBP between 100 mmHg and 159 mmHg. In a Cox proportional hazards model, SBP <100 mmHg or ?220 mmHg was an independent risk factor for 30-day mortality (RR: 1.52, IC 95%: 1.07 - 2.15), as well as the antecedent of hypertension (RR: 1.33, IC 95%: 1.06 - 1.65) and age >65 years (RR: 2.16, IC 95%: 1.74 - 2.67).


CONCLUSION
Both hypotension and significant arterial hypertension at hospital admission are associated with an adverse outcome after acute cerebrovascular disease. Nevertheless, a good functional outcome can be attained in a wide range of SBP."
70b30480d40b1195639afc6c025fad4f3149162f,"American College of Cardiology/American Heart Association Task Force on practice guidelines and the European Society of Cardiology Committee for Practice Guidelines. Europace 2006; 8:651–745. 2 Page RL, Wilkinson WE, Clair WK, McCarthy EA, Pritchett EL. Asymptomatic arrhythmias in patients with symptomatic paroxysmal atrial fibrillation and paroxysmal supraventricular tachycardia. Circulation 1994; 89:224–7. 3 Hindricks G, Pokushalov E, Urban L et al. Performance of a new leadless implantable cardiac monitor in detecting and quantifying atrial fibrillation. Results of the XPECT trial. Circ Arrhythm Electrophysiol 2010; 3:141–7."
7b1360dd9d4205d18399e6174fed4769e9ae724f,
c7b39ec5c2c4fc8fa2a3cb041bdf1dacd57b1cc2,"INTRODUCTION
Scarce information exists on intracerebral hemorrhage (ICH) in Latin America, and the existent is derived from single-center registries with non-generalizable conclusions. The aim of this study is to describe the frequency, etiology, management and outcome of ICH in Mexico.


PATIENTS AND METHODS
We studied consecutive patients with ICH pertaining to the National Multicenter Registry on Cerebro-vascular Disease (RENAMEVASC), conducted in 25 centers from 14 states of Mexico. The Intracerebral Hemorrhage Grading Scale (ICH-GS) at admission was used to assess prognosis at 30 days follow-up.


RESULTS
Of 2,000 patients with acute cerebrovascular disease registered in RENAMEVASC, 564 (28%) had primary ICH (53% women; median age: 63 years; interquartile range: 50-75 years). Hypertension (70%), vascular malformations (7%) and amyloid angiopathy (4%) were the main etiologies. In 10% of cases etiology could not be determined. Main ICH locations were basal ganglia (50%), lobar (35%) and cerebellum (5%). Irruption into the ventricular system occurred in 43%. Median score of ICH-GS was 8 points: 49% had 5-7 points, 37% had 8-10 points and 15% had 11-13 points. The 30-day case fatality rate was 30%, and 31% presented severe disability. The 30-day survival was 92% for patients with ICH-GS 5-7 points, whereas it decreased to 27% in patients with ICH-GS 11-13 points.


CONCLUSIONS
In Mexico, ICH represents about a third of the forms of acute cerebrovascular disease, and the majority of patients present severe disability or death at 30 days of follow-up. Hypertension is the main cause; hence, control of this important cardiovascular risk factor should reduce the health burden of ICH."
e86d7ae2ac2fef1b0a25b03e7346798713f8b67b,
0e69e41217b2a870704943c1e7906751d531e5fe,
1b00c61721b773056acd4767b5817497645cdf2a,"INTRODUCTION
The 'obesity paradox' is the decreasing risk of death after cardiovascular disease, with a high body mass index (BMI), even when BMI is a risk factor for vasculopathy, in the first place. Our aim was to analyze the influence of obesity on the functional recovery after ischemic stroke.


PATIENTS AND METHODS
We studied 510 patients who survived a first-ever acute ischemic stroke, without cerebrovascular disease history, and without recurrence or death after 12 months of follow-up. We also studied 501 healthy subjects who received tetrapolar bioimpedance analysis to compare the waist-to-height ratio (WHtR), abdominal circumference and BMI, as adiposity indices, in order to apply them in stroke patients.


RESULTS
In healthy individuals, WHtR performed better than BMI or abdominal circumference in predicting body fat. In a Cox proportional hazards model adjusted for multiple covariables, age (hazard ratio, HR = 1.11; 95% confidence interval, 95% CI = 1.08-1.14), NIHSS score (HR = 1.03; 95% CI = 1.01-1.05) and WHtR > 70 (HR = 2.44; 95% CI = 1.33-4.48) were associated with a high risk of attaining a modified Rankin scale more or equal than 3 at 12 months after stroke; whereas BMI > 35 (HR = 0.33; 95% CI = 0.11-0.98) was protector.


CONCLUSION
As reflected by WHtR, the excess of adiposity increases the chance of severe disability after ischemic stroke. Since BMI reflects also total lean mass, it is risky to conclude that there is a protective effect of obesity alone in the functional recovery after stroke; nevertheless, it is possible that a certain magnitude of body mass is necessary to prevent severe disability in stroke survivors."
33adb4f851df64f385ad9d5299b61be0fc3faac1,
78e6167ffe6809c2595280c91fd8f38713c636b7,
8c8f843641e648ca71ff877123ed74628047a2c6,"Importancia de la enfermedad ateroesclerosa carotidea y generalidades sobre las estrategias de intervencion en la arteria carotida con fines de prevencion A nivel mundial, la tercera parte de los infartos cerebrales (IC) son atribuidos a embolismo arteria-arteria, lo cual se encuentra directamente relacionado a enfermedad aterosclerosa carotidea. En Mexico, los resultados de un estudio hospitalario prospectivo y multicentrico, mostraron que la frecuencia de enfermedad de grandes arterias representa 8.4% de la enfermedad cerebrovascular isquemica. Es probable, sin embargo, que esta cifra subestime la frecuencia real, toda vez que en dicho registro la frecuencia de eventos isquemicos de origen indeterminado fue elevado (41%). Esto parece deberse a un bajo escrutinio en busca de enfermedad aterosclerosa de grandes vasos, en particular de las arterias carotidas, lo cual limita la realizacion de procedimientos invasivos como angioplastia con colocacion de stent y endarterectomia carotidea, esta ultima la estrategia de mayor impacto en la prevencion secundaria del IC."
97d8a787346aa0fe0b66af2dcca930c5e79f2b5f,
9d0e617f400719da509fa88616c3d4a666cc6a0c,
a9b9c1540053b1279bf7c1da49b3e3a168b0ca20,"Aims The pathogenesis of spontaneous cervical artery dissection remains unknown. We examined the association between different polymorphisms frequently found in young patients with cryptogenic stroke [methylenetetrahydrofolate reductase (MTHFR) C677T, factor II (prothrombin) G20210A, factor V G1691A (Leiden), nitric oxide synthase 3 (NOS3) intron 4 VNTR, and apolipoprotein E (APOE) ε4 gene] in patients with a cerebral infarct caused by spontaneous cervical artery dissection. Methods Forty-eight patients (27 males) and 96 matching control subjects were recruited. Clinical history, including cardiovascular risk factors, was assessed in all subjects. Genotypes were determined by a polymerase chain reaction with and without a restriction fragment length polymorphism. The genotypes and allele frequencies of the five genetic variants studied were compared between spontaneous cervical artery dissection cases and controls. We also incorporated our data into a meta-analysis of the MTHFR/C677T variant. Results Of 48 patients with spontaneous cervical artery dissection (28 vertebral and 20 carotid), the mean age of the patients was 36·6 ± SD 9·9 years. There were no significant associations between the alleles of the five genetic polymorphisms studied and spontaneous cervical artery dissection. In the meta-analysis of the MTHFR/C677T variant, a total of 564 individuals (231 cases and 333 controls) were analysed; no significant association was observed. Conclusions The results from this exploratory case-control study show the lack of an association between MTHFR, factor II G20210A, factor V G1691A, NOS3, intron 4 VNTR, and APOE ε4 gene polymorphisms and the development of spontaneous cervical artery dissection. Our findings contribute towards a better understanding of the genetic risk factors associated with spontaneous cervical artery dissection."
d470c978471374f24cd4fe51aee25d34f1f51ee5,"Background and Purpose— We investigated the predictors and time course for recanalization after vertebral artery dissection. Methods— We prospectively studied 61 consecutive patients with confirmed diagnoses of vertebral artery dissection without intracerebral hemorrhage. Neuroimaging and clinical follow-up were performed at presentation and at 3, 6, and 12 months. Results— We included 61 patients with confirmed vertebral artery dissection; 19 were evaluated and followed up with conventional angiography, 24 with MR angiography, and 18 with CT angiography. Fifty-one patients had a stenotic dissection, 7 had an occlusive dissection, one had a double-lumen image, and 2 had a pseudoaneurysm. The estimated rate of complete recanalization after vertebral artery dissection was 45.9% at 3 months, 62.3% at 6 months, and 63.9% at 12 months. We found no association between outcome and complete or partial recanalization nor did we find any factors associated with recanalization. Conclusions— These results suggest that recanalization of vertebral artery dissection occurs mainly within the first 6 months after the onset of symptoms regardless of the location or pattern of the dissection."
dd5e706077d9ad9b167e0906560e4d229e8b8927,
ebca9db514505d727d021108b0e1b88e2f892bfc,
fca2f36669f33711dfc02832af9c578e6f3b04e9,"INTRODUCTION
Information on acute care and outcome of Mexican patients with ischaemic stroke is lacking. The aim of this report is to provide results of a first step stroke surveillance system and outcome at one year of follow-up.


PATIENTS AND METHODS
In the PREMIER study 1,376 patients from 59 Mexican hospitals were included from January 2005 to June 2006. Of these, 1,040 (52% women, mean age 67.5 years) with first-ever cerebral infarction are here analyzed. Five visits were completed during the one year follow-up.


RESULTS
Main risk factors were hypertension (64%), obesity (51%) and diabetes (35%). Total anterior circulation stroke syndrome occurred in 19% of patients, partial anterior in 38%, lacunar in 26% and posterior stroke syndrome in 17% cases. In 8% the stroke mechanism was large-artery atherosclerosis, in 18% cardioembolism, in 20% lacunar, in 6% miscellaneous mechanisms and in 42% the mechanism was undetermined, mainly due to a low use of diagnostic resources. Although 17% of patients arrived in < 3 h from stroke onset, only 0.5% had IV thrombolysis. Only 1% received endarterectomy or stenting. The 30-day case fatality rate was 15%. At one-year of follow-up, 47% had a modified Rankin score 0-2 (independent), 23% had 2-5 (dependent) and 29% died. One-year acute ischaemic stroke recurrence rate was 8%.


CONCLUSION
In Mexico a significant proportion of patients arrive on time for thrombolysis, but very few receive this therapy. There is a low use of diagnostic resources to assign aetiology. Thirty-day case fatality rate doubles at 1-year after acute ischaemic stroke."
ffbab3920cff245ecfcb8734ce33488e5e46f536,
6a3d171ae7789895281a79cbb7e76668b39dc222,
050fa874add69f1e5bd978ae66e6279038874e8c,
7de8a84a80aa37d6ee35597328b293d8a216d83b,
2e9682540344f196142a5032e7f3ef26349c8a00,"INTRODUCTION
Nonaneurysmal subarachnoid hemorrhage (SAH) accounts for 15% to 20% of all the cases of SAH. Its prognosis may vary from complete recovery to different and serious complications. We describe a series of cases with nonaneurysmal SAHs, their clinical and tomographic characteristics and causes as well as long term prognosis.


PATIENTS AND METHODS
50 patients diagnosed of SAH and two negative brain angiographies for aneurysm were followed-up for an average period of 62 months. The demographic data of importance, vascular risk factors, were recorded. They were evaluated during the acute phase with the Hunt and Hess clinical scale and Fisher topographic scale. The distribution of the hemorrhage was listed as absent, perimesencephalic, focal, ventricular or diffuse. Presence of rebleeding, death and the functional course, measured by the Rankin modified scale, were recorded during the follow-up. According to this scale, Rankin of 0 to 2 was considered as a favorable prognosis.


RESULTS
This series represents 8.6 of all the SAH cases in our hospital. In 6 cases (12%), there was a causal relationship between the use of sympathicomimetic drugs and the development of SAH. In 80% of them, it was not possible to document the cause of the hemorrhage, while difference causes )cerebral venous thrombosis in 4 [8%], spontaneous dissection of the vertebral artery in 2 [4%], vasculitis secondary to neurocystecerosis in 2 [4%], cavernous angioma in 1 [2%] and spinal arteriovenous malformation in 1) were found. Rebleeding did not occur in any of the cases and only one patient died. In 45 patients (90%), the final functional prognosis was good (Rankin 0-2). We found no significant differences between the tomographic pattern of the hemorrhage, initial clinical condition and long term prognosis.


CONCLUSIONS
Our findings show a low frequency of nonaneurysmal SAH in our population and a diversity of causes greater than those reported by other series. The good functional prognosis in these cases was confirmed."
475417590b54d05cf19ffd754652c6561ddda5ae,
4c65d927602df24e2dfb33a1cce15d77a29279a8,"OBJECTIVES
To demonstrate that inflammatory atheromatose carotid plaques can be visualized with positron emission tomography with 18F-fluorodeoxyglucose (18FDG PET) in symptomatic patients, in order to correlate them with systemic inflammatory markers, such as CRP.


METHOD
Fifteen patients with cerebral ischemia due to atherosclerotic carotid disease were studied. 18FDG uptake with PET was considered and blood samples were taken for determining high sensibility C reactive protein (HsCRP).


RESULTS
The mean age of the patients was 66 years; 11 of them were males (73%) and 4 were females (27%). 18FDG PET was positive in 12 patients (80%), while 100% of the studied population had low risk HsCRP with normal white cell count.


CONCLUSIONS
18FDG PET proves active inflammation in carotid atheromatose plaques. There was no significant correlation between the presence of ahteromatose carotid plaques, HsCRP serum levels, and 18FDG PET study."
c7bc37a315dabc85e3356b6e04c549a56b148f62,"Background and Purpose: Elevated homocysteine (Hcy) plasma levels are associated with an increased risk of spontaneous cervical artery dissection (sCAD). We examined the potential association between Hcy, folate, vitamin B12 levels and 5,10-methylenetetrahydrofolate reductase (MTHFR) polymorphisms in patients with cerebral infarct caused by sCAD. Patients and Methods: 39 patients who survived a cerebral infarct caused by sCAD [20 (51%) women; 24 (61.5%) vertebral and 15 (38.5%) internal carotid arteries], and 76 healthy control subjects were included. Hcy plasma levels (fasting and after methionine load), folate and vitamin B12 levels were measured. We also performed polymorphisms of MTHFR. Hcy, vitamin B12, folates and polymorphisms of MTHFR were assessed and any associations were analyzed using multivariate statistics. Results: Mean plasma fasting Hcy level was 9.81 µmol/l for cases and 6.38 for controls (p = 0.001). The occurrence of sCAD was associated with elevated fasting Hcy levels (>95th percentile over the control group) with an adjusted odds ratio of 7.9 (95% CI 1.66–35). The association between low plasma folate values (<5th percentile) and the presence of CAD was 7.9 (95% CI 1.6–31) after adjusting for confounding variables. The distribution of the MTHFR genotype showed a higher TT mutant frequency among CAD patients (p = 0.034). Conclusions: High plasma concentrations of Hcy and low plasma levels of folate were associated with an increased risk of sCAD in the sample studied. We conclude that deficiencies in nutritional status may contribute to the relatively high incidence of CAD in Mexico."
0009f89715b482c564ff8388e2d3ffb407531504,"BACKGROUND
There are no data on Mexican population referring to frequency and prognosis of transient ischemic attacks (TIA). The purpose of the present study was to: (1) estimate the prevalence, vascular risk factors and short-term outcome in patients with TIA included in the first Mexican registry of cerebrovascular disease, and (2) analyze the acute care provided in these patients.


PATIENTS AND METHODS
This national registry of cerebrovascular diseases is a multicenter, observational, and hospital-based registry that was conducted from November 2002 to October 2004. The registry was developed to improve our knowledge in Mexico regarding risk factors profile, outcome, current diagnostic and treatment strategies, and short-term follow-up in patients with acute cerebral ischemia. Standardized data assessment was used by all centers which included information on demographics, pre-hospital events (including stroke onset and arrival to hospital), emergency department triage and workup. Short-term outcome was evaluated at day 30. Of this registry, TIA cases were selected and associated risk factors, clinical characteristics, diagnosis and treatment were analyzed.


RESULTS
During the study time period, 2,000 patients were enrolled; 97 (5%) with diagnosis of TIA; 51 women and 46 men, mean age 69.3 +/- 11.4 years. Among these 97 patients; 51 (52.6%) were admitted to the hospital for evaluation. The main risk factors were; age > or = 65 years in 74%, hypertension in 64%, diabetes in 45%, and dislipidemia in 36% and obesity in 31%. The affected arterial territory was carotid TIA in 74% and vertebrobasilar in 26%. TIA was attributed to atherosclerosis in 63% of the patients, cardioembolism in 17%, and small vessels disease in 5%. At 30 days follow-up; three patients died during the initial evaluation (two secondary to cardiac arrhythmia, and one secondary to pneumonia). Among 14 of the 94 survivors (14.9%) we documented an early stroke recurrence, including cerebral infarction in nine patients (9.6%) and new TIA in five cases (5.3%). Considering death and cerebral infarction, the frequency of unfavorable major events was 12.4%. There were only three cases treated with carotid endarterectomy.


CONCLUSIONS
The short-term risk of ischemic stroke, death or recurrent in TIA patients is high. These findings emphasize that all patients with TIA should undergo rapid investigation and management to prevent a major stroke and other vascular events."
1f7ec072bb297ffe1b1cb513eaee73b8bb39bf83,"Background and Purpose— Hypertensive intracerebral hemorrhage (ICH) in young people has been the object of only succinct analyses. Therefore, it is unclear whether extrapolation of the information obtained from older patients is also valid for the young. Here we describe young persons with hypertensive ICH and compare them with their older counterparts to determine whether age-related clinical differences exist. Methods— From 1988 to 2004, we studied 35 consecutive young patients with ICH (60% men; mean age, 33 years; range, 15 to 40 years) for whom the etiology of the brain hemorrhage was hypertension. For clinical comparisons, sex-matched persons with hypertensive ICH, aged >40 years, were randomly selected by a factor of 3:1 (n=105). Results— Essential hypertension was present in 26 (74%) young patients and secondary hypertension in 9 (26%), with renovascular hypertension being the most common cause (n=5, 55%). Compared with older patients, the young had higher blood pressures, smaller hemorrhage volumes, lower rates of ventricular extensions (for all, P<0.05), and different distribution pattern of ICHs (P=0.05), without cerebellar and lobar locations. Thirty-day mortality was markedly lower in the young than in older persons (P=0.001), nevertheless at the expense of more incapacitating disabilities. Conclusions— Young people presenting with hypertensive ICH differ in clinical characteristics and have a different prognosis when compared with their older counterparts. These findings suggest underlying age-related differences in disease pathogenesis."
7e7f43bb2ed66f3e41bd0f94d8e82caea9349e21,
dffe008970450a9b067187aafcd0103f07295d00,"Background and Purpose: We describe the natural history, functional prognosis and long-term recurrences of patients with dissection of cervical arteries (DCA) in a sequential observational study. Methods: We describe 130 patients with angiographically-proven DCA admitted to the Neurology Institute in Mexico City (Mexico), and analyzed clinical and neuroimaging data, treatment and outcome. Treatment with either anticoagulation or aspirin was decided by the primary physician. Primary outcome measures were recurrence (stroke and death) and clinical outcome at 6 months. Follow-up studies were performed to determine recanalization. Results: Mean age was 35.4 years; 4 patients died (3%) and 126 were followed for 3,906 person/years; 17 patients (13%) had a heralding ischemic cerebral event (6 strokes, 11 TIAS) about 8 days before the diagnosis of DCA. After diagnosis, recurrent ischemic stroke occurred in 6 patients (4.8%) within the 2 first weeks (1.5 persons/1,000 follow-up years). No significant differences were found between aspirin and anticoagulation. Recanalization was more frequent in vertebral dissections. Complete recanalization of vertebral dissections was associated with a favorable prognosis [OR 3.2 (95% CI 1.1–8.8; p = 0.02)]. Conclusions: In Mexico, DCA affects young adults and may present with a heralding stroke or TIA. We found rare, early ischemic recurrences. Vertebral territory dissections had better prognosis than carotid ones, particularly in patients with demonstrated complete recanalization."
02d004e8e880db40d0933fab7f599da9ea543a11,"Abstract Objectives: Prediction of intracerebral hemorrhage (ICH) in patients with cavernous angiomas is not totally elucidated. The aims of our study were to determine the rate of cerebral hemorrhage, its associated factors, and the clinical outcome in patients with cavernous angiomas in a Hispanic population. Methods: We studied 133 patients with cavernous angiomas. The patients were classified into two groups depending on whether they presented an ICH. A comparative analysis of demographics and clinical data, neuroimaging characteristics, and prognosis was carried out in patients with and without hemorrhage. The hemorrhage rate (expressed as the percentage per patient per year) was also estimated. Results: Seventy–eight patients (59%) had hemorrhage. Non–lobar location of angiomas was associated with hemorrhage [OR 4.82 (CI 95% 2.17–10.73; p=<0.001)]. In contrast, factors associated with a decreased risk of hemorrhage were a family history of epilepsy [OR 0.30 (CI 95% 0.10–0.79; p=0.016)] and lobar location of the angiomas [OR 0.21 (CI 95% 0.09–0.46; p=<0.001)]. The hemorrhagic rate of 1.71% per patient per year was influenced by the location. It was only 1.22% per patient per year in lobar angiomas and 2.33, 2.39, and 2.82% per patient per year for brainstem, cerebellum, and deep hemispheric angiomas, respectively. Conclusions: The non–lobar location of cavernous angiomas gives a higher risk of hemorrhage in our Mexican mestizo population, without the hemorrhage being related to either age or sex."
6c7d6c09dba972569b3291a03e4e1fbdb6f6d051,"Abstract Objectives: To estimate the incidence of cognitive impairment (CI) among cognitively healthy, Mexican subjects, and to evaluate the impact of demographic and vascular factors on the conversion to CI and mortality. Methods: 734 eligible subjects (aged 55 to >90 years) from a population-based sample were examined. The cognitive function of participants was assessed using the Mini-Mental State Examination (MMSE) every 2 years. The subjects were followed for an average of 3.2 years. The CI was defined using two sets of criteria: (i) moderate CI, as a drop to 25–21 on the MMSE at 2-year follow-up or a decrease of at least four points and (ii) severe CI, defined as a drop of 21 or less in MMES at follow-up. The incidence density and period prevalence were determined as epidemiological measures as well as the cumulative incidence as a risk measure. Kaplan—Meier survival curves were used to analyse the main points of interest: CI, dementia and mortality. Results: The period prevalence of moderate CI was 20%, and 10% for severe CI. During 1959 person-years of follow-up, severe CI developed in 33 of the 361 participants. While during 2096 person-years of follow-up; 80 of 361 participants developed moderate CI. The rate of progression to severe CI in moderate CI subjects gradually increases with follow-up. Both, moderate and severe CI were associated with low educational level, higher age and higher mortality. Conclusions: Elderly people with moderate CI have an increased risk of severe CI. Moderate and severe CI are both predictive of higher mortality in Mexican subjects."
ebc3b0a2a783bafe61e2b7ba0c96c3c932536e5f,"The authors report on the case of a 38-year-old woman who had experienced incapacitating pulsatile tinnitus in the left ear for 6 months. Angiographic studies revealed a wide-necked venous aneurysm of the left transverse-sigmoid sinus. Solitary stent placement across the aneurysm neck resulted in a slight modification in the lesion's characteristics. A second session, in which embolization with Guglielmi Detachable Coils was performed, resulted in a 100% occlusion of the aneurysm, with patency of the parent vessel and resolution of the tinnitus."
3ba674cf360334ac7b4bf5b1d236e64317583906,"Background and Purpose: Phenylpropanolamine (PPA) has been associated with an increased risk of intracranial hemorrhage (ICH). The aim of this study was to assess the association between PPA intake and ICH in a Mexican population. Methods: We included all patients with ICH aged 18 to 51 years, with no known structural etiology, diagnosed from January 1991 to December 2000. Three to 4 controls per patient matched by sex, age (within 5 years) and place of residence were included. Patients and controls were asked about use of cold medication or appetite suppressant medications within the previous year before the interview. We considered a PPA related hemorrhage when there was a temporal relationship between the use of medication and the development of the hemorrhage, and when other causes could be ruled out. Associated risks for PPA use and other possibly related variables were estimated. Results: 177 patients (mean age 39 ± 12 years) were included; 58 (33%) were diagnosed with subarachnoid hemorrhage (SAH) and 119 (67%) with ICH. 41.2% (73 of 177) of patients had documented use of PPA within the past year and 10 (5.7%) of them had a temporal relationship between ingestion of PPA and ICH. In control subjects 42.4% (422 of 996) had been exposed to PPA and none of them developed hemorrhage. The time from PPA exposure to the onset of ICH varied from 30 minutes to 24 hours. The risk of PPA exposure for hemorrhage was not significant in cases or controls, OR 0.95 (95% CI, 0.68 to 1.34; p = 0.77). No subjects (cases or controls) reported use of PPA as an appetite suppressant. Conclusions: We found no association between ingestion of PPA and cerebral hemorrhage with respect to ingestion of PPA in the previous year. When recent use was looked at an apparent risk was evident."
7f44796ccb585a64b6dbdb5e4164e7217f604b1e,"Background and Purpose— We investigated whether lacunar infarct (LI) patients with >1 lacune have different vascular risk factors, a different prognosis, and poorer functional outcome than those with a single lacune. Methods— The study included 175 first-ever LI patients. The group was divided according to the presence of multiple (n=136) or single (n=39) LI. The association of single or multiple LI with the principal vascular risk factors, leukoaraiosis, outcome, and recurrence was investigated with logistic regression models that included age, sex, and cardiac disease. Results— No significant differences were found between single and multiple LI with respect to age, hypertension, hyperlipidemia, smoking, and heavy alcohol drinking. Diabetes mellitus (odds ratio [OR], 2.43; 95% CI, 1.09 to 5.4), high levels of hematocrit (>0.47) (OR, 1.09; 95% CI, 1.04 to 1.21), and leukoaraiosis (OR, 3.58; 95% CI, 1.77 to 7.51) were significantly related to multiple but not to single LI. Stroke recurrence rate was 7.7% in patients with single LI and 24.3% in the multiple LI group (OR, 3.84; 95% CI, 1.1 to 13.3). During a median follow-up of 12 months (range, 6 to 156 months), 94% of the single LI patients and 77.2% of the multiple LI patients had favorable outcomes (Rankin Scale score 0 to 2) (OR, 5.4; 95% CI, 1.25 to 23.9). Conclusions— Diabetes mellitus, leukoaraiosis, and high levels of hematocrit are important risk factors in patients with >1 LI. The presence of multiple LI may be an important prognostic indicator not only for functional recovery but also for a higher rate of recurrence."
a4b266f23152bb68ff9ae60e459b82b27e6d4d9a,"Edaravone, a novel free radical scavenger, demonstrates neuroprotective effects by inhibiting vascular endothelial cell injury and ameliorating neuronal damage in ischemic brain models. The present study was undertaken to verify its therapeutic efficacy following acute ischemic stroke. We performed a multicenter, randomized, placebo-controlled, double-blind study on acute ischemic stroke patients commencing within 72 h of onset. Edaravone was infused at a dose of 30 mg, twice a day, for 14 days. At discharge within 3 months or at 3 months after onset, the functional outcome was evaluated using the modified Rankin Scale. Two hundred and fifty-two patients were initially enrolled. Of these, 125 were allocated to the edaravone group and 125 to the placebo group for analysis. Two patients were excluded because of subarachnoid hemorrhage and disseminated intravascular coagulation. A significant improvement in functional outcome was observed in the edaravone group as evaluated by the modified Rankin Scale (p = 0.0382). Edaravone represents a neuroprotective agent which is potentially useful for treating acute ischemic stroke, since it can exert significant effects on functional outcome as compared with placebo."
bea152b094291cb963c28431eb8ed4c440f1513f,"Background and Purpose— Phenylpropanolamine (PPA) and pseudoephedrine are sympathomimetics contained in over-the-counter cold preparations. A case-control study linked PPA use with hemorrhagic stroke in women. Twenty-two patients with stroke associated with use of these drugs are described. Methods— In a consecutive stroke registry since 1988, 22 patients had stroke associated with over-the-counter sympathomimetics. Sympathomimetic dosage and type, time interval until stroke onset, and neuroimaging findings are described. Results— Ten male and 12 female patients were included. Intracerebral hemorrhage occurred in 17 patients, subarachnoid hemorrhage in 4, and ischemic stroke in 1. Stroke was associated with PPA use in 16 patients (dose 75 to 675 mg), with pseudoephedrine in 4 (dose 60 to 300 mg), and with others administered by the nasal route in 2 (oxymetazoline and phenylephrine). Stroke occurred after a single dose in 17 patients and after daily use during several days in 5. The interval between drug exposure and clinical onset varied from 30 minutes to 24 hours. Stroke occurred after recommended doses of PPA (50 to 75 mg) in 32% and pseudoephedrine (60 mg) in 50% of patients. Eight patients had acute hypertension at stroke onset. Cerebral angiography was normal in 8 cases and showed diffuse vasospasm or beading in 10 patients. Conclusions— Stroke related to over-the-counter sympathomimetics was associated with acute hypertension and/or vasospasm or angiitis mechanisms, most related to the use of PPA; however, stroke also occurred with the use of other sympathomimetics, particularly pseudoephedrine. Although stroke complications occurred when doses were used that were higher than recommended doses, apparently there is also a stroke risk when these agents are taken properly."
1b6e54e86d3015a6c312991c0de3fd543dcca2bc,"La prevencion secundaria implica el evitar la recurrencia de episodios de isquemia cerebral. Para elegir la estrategia ideal de prevencion debe conocerse en detalle los mecanismos fisiopatologicos productores del evento isquemico: enfermedad de grandes vasos, enfermedad de pequenos vasos y cardioembolismo. Antiagregantes plaquetarios Los antiagregantes plaquetarios deben administrarse en todos los pacientes con EVC isquemico excepto cuando hay evidencia de una fuente cardioembolica. Acido acetil-salicilico Es el antiagregante mas utilizado. Tiene accion a nivel plaquetario y endotelial. En las plaquetas previene la formacion de tromboxano A2 y en el endotelio previene la sintesis de prostaciclina"
60e441ff585b99c3abede5c4f840ff4d5cd39b6d,"Se estima que aproximadamente la tercera parte de los infartos cerebrales estan relacionados con embolismo de arteria a arteria, a su vez en relacion directa con enfermedad aterosclerosa carotidea. La racionalidad de la endarterectomia carotidea se basa en la supuesta reduccion del riesgo de EVC isquemico en pacientes con lesiones ateromatosas de la bifurcacion carotidea (extracraneal), al realizar la reconstruccion de la arteria carotida interna y disecar por completo el ateroma, dejando la luz del vaso libre de restos del mismo. Como toda opcion de tratamiento, el riesgo y el beneficio deben ser evaluados para conocer la bondad del recurso terapeutico. Resulta fundamental, por tanto, conocer el riesgo de EVC isquemico, que corre el paciente con lesion ateromatosa de la carotida cuando se le practica una endarterectomia, contrastandolo con el propio de seguir solo un tratamiento medico (i. E, control de factores de riesgo y antiplaquetarios). Los estudios epidemiologicos primero, y los ensayos terapeuticos controlados y aleatorios despues, han dejado en claro que los riesgos no son iguales para el paciente que nunca ha tenido un EVC en relacion con la lesion carotidea aterosclerosa (sujetos asintomaticos), que para el sintomatico. Por tanto, los beneficios y riesgos de la endarterectomia deben evaluarse de manera separada."
705c95527c173903e9de6802b40f0e477a4a5195,"A nivel mundial, la enfermedad vascular cerebral (EVC) es la segunda causa de muerte y la principal de invalidez. La gravedad del problema de salud que representa la EVC queda de manifiesto al revisar la evolucion natural de los eventos cerebrovasculares: la recurrencia de EVC es de 5 a 15% durante el primer ano y hasta de 40% a los cinco anos; la mortalidad durante el evento agudo es de 25 a 30%, durante el primer ano de 15 a 25% y hasta de 60% a los cinco anos, disminuyendo en forma notable la esperanza de vida; entre 25 y 40% de los supervivientes permanece con secuelas que llevan a la dependencia parcial o total; y se estima que hasta 30% desarrollan demencia en los meses siguientes. Con base en la informacion previa, es evidente que el abordaje mas efectivo para disminuir la repercusion de la EVC es la prevencion. La prevencion primaria es el conjunto de medidas tendientes a disminuir la probabilidad de desarrollar un primer evento vascular cerebral identificando a individuos de alto riesgo para EVC en quienes deben aplicarse diferentes intervenciones necesarias para modificar su perfil de riesgo."
7d1313907b738990c2cfc05d2960522abbaf1fc3,"El paciente con EVC puede requerir de cuidados intensivos en algun momento de su evolucion. Desdichadamente, los mejores niveles de evidencia y recomendacion no estan aun disponibles para el manejo de todos los problemas de los pacientes en estado critico que cursan con un evento vascular cerebral agudo. A continuacion se exponen algunos de los problemas mas comunes en el manejo de un paciente con EVC agudo y que se encuentra en estado critico. Medidas generales El EVC agudo es una urgencia. El tejido cerebral debe preservarse en la mayor magnitud posible de acuerdo con los siguientes lineamientos: 1. Garantizar la presion de perfusion cerebral (PPC), manteniendo una adecuada presion arterial media (PAM), entre 90 y 100 torr (nivel de evidencia III). Mantener la presion intracraneana (PIC), lo mas cercana a los valores normales, idealmente por debajo de 20 torr, si es que esta se esta midiendo en forma directa. Si no se cuenta con la medicion directa de PIC, se debe mantener la PPC mediante la optimizacion de la PAM, y evitar los factores que pueden elevar la PIC. Las medidas para evitar elevacion de la PIC son: elevacion de la cabecera del enfermo a 30-45 grados, evitar maniobras de Valsalva, tos y pujidos; evitar dolor, fiebre y vomitos."
93b8892fe92c723890a48cd95af5a9f02af745e4,
9c3b48b4a7ada53510f2371f2c68a0d1ab9ba23e,
a1ac592a888c539a207471c5a82fbacabd6959a9,"Las estrategias terapeuticas contra el infarto cerebral agudo estan encaminadas, por una parte, a restaurar el flujo sanguineo cerebral (FSC) y, por otra, a modificar las alteraciones bioquimicas que ocurren en la cascada isquemica (neuroproteccion) con el fin de reducir el volumen del tejido cerebral danado. Trombolisis intravenosa La trombolisis se basa en el reconocimiento de que la mayoria de los eventos isquemicos cerebrales se deben a la oclusion arterial cerebral. Administrada por via intravenosa es posible reestablecer el FSC, aunque su uso se asocia con un elevado riesgo de hemorragia cerebral. En los ultimos cinco anos se reportaron cinco ensayos clinicos controlados en los que se utilizaron diferentes tromboliticos. Los estudios realizados con estreptokinasa fueron suspendidos prematuramente por la elevada frecuencia de complicaciones hemorragicas, por lo que su uso esta proscrito en EVC. Por otra parte, en 1996 la Food and Drug Administration, con base en un estudio realizado en los Institutos Nacionales de Salud de los Estados Unidos, aprobo el uso del activador tisular del plasminogeno humano (rtPA) para su uso en las primeras horas de un infarto cerebral. A continuacion se resumen los aspectos mas importantes de este y el resto de los estudios relacionados con el tema."
9bb10e727273ca63a9f3f74bc591fe94ad945106,
adf229d57440755b230ed49cd1f3e692c28f000f,"Erik Lindgren1,2 | Alexandros Rentzos3,4 | Sini Hiltunen5 | Fabiola Serrano6 | Mirjam R. Heldner7 | Susanna M. Zuurbier8 | Suzanne M. Silvis8 | Maryam Mansour9 | William Allingham10 | Martin N. M. Punter11,12,13 | Blake F. Giarola14 | Jeremy Wells15 | Mayte Sánchez van Kammen8 | Eike I. Piechowiak16 | Nicole ChiotaMcCollum15 | Carlos GarciaEsperon17 | Christophe Cognard18 | Timothy Kleinig19 | Masoud Ghiasian9 | Jonathan M. Coutinho8 | Marcel Arnold7 | Antonio Arauz6 | Jukka Putaala5 | Katarina Jood1,2 | Turgut Tatlisumak1,2,5 | the International CVT Consortium"
2aa075183faf6a62d67803d54ace2c88a4c97b84,
2b7dbe4e266158d2a0b0f0dfc7582cbea4652989,
0cc7f3ab5e4c94117734f332e2c39b98b4a6aba8,
0fbd22d058f326c9cf4b054b289ee91edfd241fa,
1abba028de4eefb180ea695e734187f5228c4085,"Objective: As a widespread toxic metal, arsenic had potential effect for hypertensive. We evaluated the association between urinary arsenic and the incidence of hypertension in adult residents along the Yangtze River of China. Methods: We conducted the study of 1358 adults 18 to 74 years of age from Chizhou, Maanshan, and Tongling of Anhui province, who participated in the baseline study in 2014 to 2015. Inductively coupled plasma optical emission spectrometry was used to measure urinary as of residents, and follow-up extended through 2016 to 2017. Results: We identified 275 hypertension events. The hazard ratios (HRs) of highest quartile arsenic compared with lowest quartile was 1.49 for hypertension events (95% confidence interval [CI] = 1.05 to 2.12), and HRs (≥P20 vs <P20) was 1.37 (95% CI = 1.02 to 1.84). Conclusions: Higher level of arsenic exposure might play a role in increasing the incidence of hypertension."
3b463c041283e710c4b4680ddd5a1fe093f84ac8,
50e1953c047a2fc461e9fd468eb669985a74e237,"This study was to investigate the association of fasting serum glucose (FSG), thyroid‐stimulating hormone (TSH), and thyroid hormones with papillary thyroid cancer (PTC)."
606b736b8eb412674090488d0258a19eda304d9e,
94e03f4aa9f002749b11aec421822ce28a39d167,
c530849524dc3a059e7c91e46f387b063e2c14d0,
cecbfcf6c5f3b946e7ceae5729c37dc5aab5e7cf,"The goal of the present study was to determine the disparities in prevalence and risk factors of loneliness between rural empty nest and non‐empty nest older adults in Chizhou, China."
7d6753b5a4e587a732c77284a3143976212375e1,"The association between glutathione-S-transferase polymorphisms (GSTM1, GSTT1 and GSTP1) and risk of acute leukemia in Asians remains controversial. This study was therefore designed to evaluate the precise association in 23 studies identified by a search of PubMed and several other databases, up to December 2013. Using random or fixed effects models odds ratios (ORs) with corresponding 95% confidence intervals (CIs) were calculated. Heterogeneity across studies was assessed, and funnel plots were constructed to test for publication bias. The meta-analysis showed positive associations between GST polymorphisms (GSTM1 and GSTT1 but not GSTP1) and acute leukemia risk [(OR=1.47, 95% CI 1.18-1.83); (OR=1.32, 95% CI 1.07-1.62); (OR=1.01, 95% CI 0.84-1.23), respectively] and heterogeneity between the studies. The results suggested that the GSTM1 null genotype and GSTT1null genotype, but not the GSTP1 polymorphism, might be a potential risk factors for acute leukemia. Further well-designed studies are needed to confirm our findings."
97ec61d9f3a421c209ad63765cded1a159ec878f,
00f5f8dbffe49c935b19f1c2f7718de1cf558bcc,
39324ac363ef118c4045c1c8bee09feb8d8d3dd1,
4950585b7b7fe5a7cdabbdcf95cf10334643b07a,
930dadddbc41568f506bfb66e9941f05f69bebf6,"Thermal performance of a porous medium solar pond is studied by the laboratory and outdoor experiments. The laboratory experiments are carried out to test the heat storage ability of varies porous materials. The outdoor contrast experiment between two mini solar ponds with a surface area of 2.4m×2.4m, bottom area of 1m×1m are conducted to analyze the thermal performance of the porous medium solar pond. Influences of the porous medium as well as a cover on the temperature distribution are discussed. Results show that the maximum temperature of solar pond increase by about 6∘C due to the introduction of porous medium. Inspired by the principle of the porous medium solar pond, a tide-driven reservoir with heat collection pond and circulating storage pond is proposed to provide fresh seawater with moderate temperature for seaside marine park in winter."
26a458ab23df327428403140266c3733dd9edb6c,"It is difficult to obtain a precise mathematical model of free-floating space robot for the uncertain factors, such as current measurement technology and external disturbance. Hence, a suitable solution would be an adaptive robust control method based on neural network is proposed for free-floating space robot. The dynamic model of free-floating space robot is established; a computed torque controller based on exact model is designed, and the controller can guarantee the stability of the system. However, in practice, the mathematical model of the system cannot be accurately obtained. Therefore, a neural network controller is proposed to approximate the unknown model in the system, so that the controller avoids dependence on mathematical models. The adaptive learning laws of weights are designed to realize online real-time adjustment. The adaptive robust controller is designed to suppress the external disturbance and compensate the approximation error and improve the robustness and control precision of the system. The stability of closed-loop system is proved based on Lyapunov theory. Simulations tests verify the effectiveness of the proposed control method and are of great significance to free-floating space robot."
84f4603e72f572144e5497cd5bd648ea6f10bea4,
b0a8c7ee065431f7203fa2dab275ae58534dbd5b,
e9ba5037b4f22c74d9e656d45e880630b301daf9,"A two-dimensional unsteady large eddy simulation model of a trapezoidal solar pond is carried out on the platform of Fluent17.2. On the basis of verifying the validity of the model, the characteristics of temperature change, velocity and vorticity distribution, and salt concentration change at the interface are analyzed. The result shows that the overall temperature of the solar pond increases over the running time, and the maximum temperature (314 K) appears in the lower convective zone after 10 days of operation. The velocity field and vorticity field are mainly distributed in the lower convective zone (LCZ), and the velocity remans at the order of 10−4 m/s. The salt concentration at the upper and lower interface decreases obviously under the influence of salt diffusion."
e765ba22af07720cf700dfe9072332e486293801,"Objective 
To investigate the effects of numerous re-planning strategies on the anatomic and dosimetric outcomes of target volume and organs at risk (OARs) in patients with head and neck cancer receiving fractionated radiotherapy. 
 
 
Methods 
From 2015 to 2016, 28 patients with head and neck cancer were enrolled in this study with Shandong Cancer Hospital, consisting of 19 patients with nasopharyngeal carcinoma, 4 patients with laryngocarcinoma, and 5 patients with carcinoma of the maxillary sinus. All of them received conventionally fractionated radiotherapy. Each patient had six weekly cone-beam CT (CBCT) scans, which were performed on the first day of every week, to obtain reference images. A virtual CT image was generated by registration of planning CT and each weekly CBCT image. The four re-planning strategies were used for the reconstruction of re-planned dose, while the initial planning was used as a reference. The weekly doses calculated using virtual CT were summed together to obtain the actual dose. The actual and initial planned doses were evaluated. The nonparametric Friedman test was used to evaluate the differences between multiple groups, and the differences between any two groups were analyzed by paired t test. 
 
 
Results 
The sizes of planning target volume, clinical target volume, and left/right parotid glands (PGs) changed significantly within the six weeks (P=0.041, 0.046, 0.024, and 0.017, respectively). For these four re-planning strategies, there were significant differences between the actual dose and the initial planned dose to the PGs (all P<0.05), with average values decreased by 5.02%, 11.17%, 12.08%, and 13.19%, respectively, compared with that in the reference strategy. 
 
 
Conclusions 
Re-planning during treatment course could ensure the sparing of OARs and allow for sufficient dose to the target volume. The higher the number of re-planning strategies, the more the actual dose is close to the initial planed dose; the efficiency of two re-planning strategies is the highest. 
 
 
Key words: 
Head-neck neoplasms/adaptive radiotherapy; Head-neck neoplasms/intensity modulated radiotherapy; Dosimetry"
0f745dc663c70441f3a4a84788ba80d39295ad49,
401f813980a1ee711e13091e1c7922702565aec0,
710669c7d0b6e96e4025e6ea58c4d8eab3c90b48,"Recently the application of fractional order PI control has been widely researched. However, the order in PIλ controller is constant. In order to improve the dynamic performance and robustness of the second-order control system, the variable-order fractional PI controller was designed. Through the performance analysis the of fractional order PI controller with different orders, the optimal variation rules of the order was obtained. Finally, the fuzzy inference rules were established with the velocity error as the input and order as the output, and the fuzzy logic based variable-order fractional PI controller was designed. The numerical simulation results show that the proposed method has better performance than the conventional constant order fractional PI method."
7e7fdf269ef752578c8699931f500d9831ed7223,"In the article, Fe3O4/attapulgite/polyvinylalcohol composites were successfully prepared by the method of coprecipitation. SEM, XRD, and FTIR were used to make characterizations of properties. The composites were flake form. The composites had the ability to treat methyl orange effectively in neutral condition. The best dosage of polyvinyl alcohol was 0.7715 g. With the increase of H2O2 and the prolonging of the laying time, the absorbance decreased. When the dosage of H2O2 was 15 mL and the laying time was 24 h, the efficiency of treatment was 99.99%. © 2015 American Institute of Chemical Engineers Environ Prog, 35: 715–718, 2016"
9bdf3a6f9f4dbb945792d3a23fe9fd023b325eca,
a1f353c171c235ba690959af13d09eca45258aa6,
b1bf0909f5d6516c8d6105747d4fdb6125ab7d85,
cc2687e8ef034e8d5126da80d8669da86509bfa5,
d41061182632c0002a7991f5028ca41c9fb7c32d,
e56f22ab73878189dbba9e8810aca14ee4a1738e,
ec5c3daaafb78dee256743ab99d7d62abeea787d,
f28f4d8268fb90a5c0332f4eb451309862067b96,
5e6e6011491625ab5aed7682a60b50ffdf26ee8c,"Silicene, a monolayer of silicon atoms arranged in a honeycomb lattice, has been undergoing rapid development in recent years due to its superior electronic properties and its compatibility with mature silicon-based semiconductor technology. The successful synthesis of silicene on several substrates provides a solid foundation for the use of silicene in future microelectronic devices. In this review, we discuss the growth mechanism of silicene on an Ag (111) surface, which is crucial for achieving high quality silicene. Several critical issues related to the electronic properties of silicene are also summarized, including the point defect effect, substrate effect, intercalation of alkali metal, and alloying with transition metals."
b80ea04c8cb87eadd475f181c4052f45d7fb0cdc,
486fcec43361aa0da1321439cb1dad553c1a2acf,"For cost savings and environmental benefits, recycled materials known as recycled asphalt pavement (RAP) and recycled asphalt shingles (RAS) have been widely used by the asphalt paving industry in recent years. To maximize the benefit of using RAS, the properties of the recycled materials, especially the asphalt binder properties, should be characterized before the mix design. In most cases, the solvent-based asphalt binder extraction and recovery is unavoidable for binder characterization. However, most of the RAS binder is very stiff, which raises concerns about the asphalt binder recovery method, for example, whether or not the solvent is left in the recovered asphalt binder and if the recovered asphalt binder is stiffened due to over-cooking for removing the solvent. In this paper, the authors compared both rheological properties and chemical components of the original binder before recovery with the same binder after recovery. The rheological properties of the asphalt binder were evaluated using the dynamic shear rheometer (DSR) and the bending beam rheometer (BBR), and its chemical property was measured with Fourier transform infrared spectroscopy (FTIR). The results indicate that the extraction and recovery process did not change properties of the RAS binder. Therefore, the asphalt extraction and recovery process are valid for the RAS asphalt binder evaluation."
4d805314cbf6b989c5e015d0f398104947f79bd2,
9ea3ceb7422410984b71a4c5024f50ae8705ea29,"The utility model discloses a circulating fluidized bed. The circulating fluidized bed comprises a circulating plate type material conveying system, a sedimentation chamber, enclosing baffles, a flue gas outlet, bellows, wind distribution pipes, ash hoppers, a coal material inlet and a coal material outlet, wherein the circulating plate type material conveying system consists of conveying plates, a double-track conveying chain, chain wheels and a variable-frequency motor; and the sedimentation chamber is arranged above the material conveying system, the top of the sedimentation chamber is connected with the flue gas outlet, the enclosing baffles are arranged at the two sides above the material conveying system so as to form a closed space, the bellows are arranged at the middle part of the material conveying system and are connected with the wind distribution pipes, the ash hoppers are arranged below the material conveying system, the coal material inlet is formed in one side of the sedimentation chamber, and the coal material outlet is formed in one side of the material conveying system. The circulating fluidized bed has the advantages that the structure is simple, the safety is good, the applicability is strong, the heat exchange efficiency is high, the consumption of coking melted coal gas of a coke oven is reduced, the energy for production is saved, the production cost is reduced, the economic benefit is increased, the emission of waste gas and waste water of the coke oven is reduced, and the pollution to the environment caused by waste flue gas is reduced."
c8b2c97b17a97c2998d7d90b88b9fa71018c06bf,"This paper aims at the situation of the random losses of detection data that happens in engineering applications under the nonlinear circumstance, and proposes a detection method of data loss based on the particle filter (PF) based on organizational evolution particle swarm optimization (OEPSO-PF). The evolutional operations are acted on organizations directly in the algorithm. The algorithm carries out an iterative optimization for the system state value through the cooperation and competition among samples, it not only guarantees the diversity of solutions in populations, but also has a strong search capability. The simulation result shows that the proposed algorithm in this paper improves the accuracy rate of data loss detection. It has high application value due to its better tracking performance with high data lose rate under the complex condition."
f671de9e88aabfe900cb85fdb4f19af517ab1c28,"Based on the mathematical model and state equation of the permanent magnet synchronous motor (PMSM), a composite nonlinear integral sliding surface which contains integer order integral and fractional order integral was proposed, and the corresponding sliding mode controller was designed in order to improve the dynamic performance and robustness of the PMSM position control system. For further improving the robustness to load disturbance and eliminate the system chattering, the fractional order integral to the sliding surface was considered as a feedforward compensation in the control output. The stability of the proposed method was proved by the Lyapunov stability theory. The simulation results show the dynamic performance and robustness of the proposed method."
2602b6cff935af1607ab3e0da3aadb35f66601c4,"Objective To understand the capability of Chinese food inspection laboratories for detecting genetically modified component in soybeans. Methods China National Accreditation service for Conformity Assessment(CNAS) organized the proficiency test(PT) for detecting genetically modified component in soybeans. 37 laboratories from 15 provinces/cities/municipals took part in the PT. In this paper, the implementation process of the PT was described, including project design, sample preparation, homogeneity and stability test, results statistic and analysis, etc. Results There were 94.60% laboratories and 98.90% samples showing satisfactory results. Conclusion Most laboratories taking part in the PT had good competence in analyzing genetically modified component in food with molecular biology."
2b51d0ebedda1ec104d6048f2ad57d019c62a73b,"To improve the performance of the PMSM, a GSA based fractional oder QFT(FOQFT) control scheme for the PMSM is proposed. Firstly, the composite pseudo linear system, which is composed of the ANN-Inversion system and the controlled PMSM, is equivalent to a linear system with disturbance. Then, a FOQFT control scheme is designed based on the QFT method and fractional calculus, and the parameters of the fractional PID are designed by gravitational search algorithm (GSA). Finally, case study is fulfilled on a PMSM system and results show the effectiveness of the proposed control scheme."
301141687846d62758435ad2ba73e715189faa5e,"This paper takes the improved threshold formula and cluster radios formulas to choose cluster-heads by considering energy-saving, based on typical clustering routing protocol and optimal cluster-head selection formula. In the forming stage of cluster, the proportionality principle is used to make the distribution of cluster even more reasonable and during the stable stage of cluster, the member nodes in cluster use TDMA to communicate with the cluster-head node, and cluster-head nodes communicate with base station BS via multi-hop interrupt communication manner. Then it proposed the realization of target tracking based on the energy- saving routing algorithm. Finally, it can be seen in the simulation results that on the behalf of the network lifetime and average energy consumption, energy-saving routing algorithm is more reasonable. DOI:  http://dx.doi.org/10.11591/telkomnika.v11i2.2032"
7e02ba28bf479bcd25cb238bf50e88a4c4962eee,
8dfc8e485099c4ec7ab6b646537657257b0156a5,"In the paper,design the automatic control system of transporter car using FX2N-32MR as controller.Design the hardware and software of the system.Simulation results show that this system can effectively control transporter car reaching scheduled position and feeding and unloading automatically.And realize unmanned operation."
b65427d0387cf1638af39abd7ffee0c7dd2c03a5,"Objective To investigate the clinical features of hepatitis B virus (HBV) reactivation after precise radiotherapy in patients with primary liver cancer (PLC) and analyze the risk factors for HBV reactivation.Methods A retrospective analysis was performed on the clinical data of 69 hepatitis B surface antigen (HBsAg)-positive patients with PLC,some of whom had HBV reactivation after precise radiotherapy.Before radiotherapy,all patients underwent baseline examinations,including blood routine,liver function test,renal function test,and quantifications of serum alpha-fetoprotein (AFP),serum HBV markers,and serum HBV DNA.During radiotherapy and within 12 weeks after radiotherapy,blood routine was performed biweekly,and liver function test,renal function test,and quantifications of serum AFP,serum HBV markers,and serum HBV DNA were performed once every four weeks.Logistic regression analysis was used to evaluate the association of the indices with HBV reactivation.Results Of the 69 patients,12 (17％) had radiation-induced liver disease,17 (25 ％) had HBV reactivation,and 15 (22％)developed hepatitis due to HBV reactivation.The logistic regression analysis showed that baseline serum HBV DNA level was the risk factor for HBV reactivation after precise radiotherapy.Conclusions HBV reactivation may occur after precise radiotherapy in patients with PLC,and baseline serum HBV DNA level is the independent risk factor for HBV reactivation.The patients who develop hepatitis due to HBV reactivation have poor prognosis even if they receive antiviral therapy in time. 
 
Key words: 
Liver neoplasm/three-dimensional conformal radiotherapy;  Radiation-induced liver disease;  Hepatitis B virus"
d5d1fe8666600826afa39c3cbcf8c7d8c0b227d6,"In order to provide a basis for anti-freezing design of reservoir revetments in cold regions,ice limit pressure of extruded ice plates was studied using the fracture mechanics theories with the consideration of failure characteristics and inner defects of the ice plates.Then,a calculation method of the ice pressure was developed.The calculated results show that the ice plate extrusion failure demonstrates brittle characteristics,and ice pressures increase with the increase of the ice plate thickness.With the same ice thickness,calculated values of the ice pressure are in good agreement with its standard values and measured values.The calculation method of the ice pressure can reflect the physical nature of ice pressure forming process,and is accord with the facts."
f2c4e6b5397a869edf525e7a5b34e6a8072ef242,"The background of this paper is the warehouse target localization and tracking system which is composed of a number of wireless sensor nodes. Firstly this paper established a model of warehouse target localization and tracking system, then a model of multi-sensor data preprocessing and data fusion was established, and self-adaptive linear recursive method was used to eliminate outliers of the original measured data. Then least squares fitting filter was used to do filtering and denoising for the measured data. In the end, the data which were measured by multi-sensor can be fused by Kalman Filtering algorithm. Data simulation analysis shows that the use of kalman filtering algorithm for the fusion of the data measured by multi-sensor is to obtain more accurate warehouse target location data, so as to increase the positioning and tracking accuracy of the warehouse target localization and tracking system. Key Words: Wireless Sensor Network,Data Fusion,Kalman Filtering DOI:  http://dx.doi.org/10.11591/telkomnika.v11i3.2195"
025abd977d221b831cc6bee61af3862982eb943e,"The scale deposit on the water heating coil in stem dipping machine will deteriorate heating performance,therefore the coil-heating was replaced by direct steam ejecting.After improvement,the time for raising temperature from 25 ℃ to 60 ℃ reduced from 21-28 to 14 minutes.When water temperature approached to lower control limit,steam heating system raised it timely.The constant water temperature during stem processing was guaranteed."
05f396270635a5a9d6a4a4a4f6338eb3c0ea452c,"In order to explore the difference in medicinal value of cultured mycelia and fruit bodies of Inonotus obliquus,organic solvent cold soaking and ultrasonic reflux were adopted to extract triterpenoids from fruit bodies and mycelia of Inonotus obliquus.The triterpenoids extracted from both sources were determined to be the same or similar homologues by thin layer chromatography(TLC) and Fourier transform infrared(FT-IR) spectral analysis.Both triterpenoid extracts could inhibit the growth of cancer cells and bacteria as determined by tetrazolium salt assay(MTT) and cylinder plate method.The highest inhibitory rate of triterpenoids to human gastric cancer cell line MGC-803 reached up to 38.27%.Therefore cultured mycelia of Inonotus obliquus have the same medicinal value as fruit bodies."
0fbfd6f0107bb6ab803c11e3a5f2fe2002f36843,"To meet the demands on real time performance and accuracy of micro machining,a sectional control method and the coordinated controller by embedded microcontroller for two-axis piezo platform is proposed.According to the displacement error,the control procedure was divided into two sections,constant voltage control and error eliminating control.With constant voltage control,the control voltage is calculated using the average curve of the main loop of the hysteresis,and an integration control unit is used in error eliminating control.Then,the controller for micro machining with above method was achieved with an ARM Cortex microcontroller,STM32F103.System tests indicated that about the step response,the steady state error is less than 0.1 μm,the settling time is about 1.2 ms and the overshoot is about 5.6%;About the sinusoidal response,the error is ranged from-0.343 μm to 0.245 μm;the steady contour error is ranged from-0.084 μm to 0.079 μm,and the contour error is ranged from-0.41 μm to 0.131 μm include system overshoot.It is indicated that with the controller built,the precise,real-time control for micro machining can be achieved."
13a14837c902d849466cb0f0b4f8e5f20b989f85,
39601fe0df53b8fb60c43a714d9ca1f67500998a,
7ae979fa6c0b40bc35b39bc1616bc2b5059cd904,"In order to research the benthic algae within the coverage of offshore 50m wide of Gouqi Island,on the one hand we acquired echo samples around Gouqi Island by BioSoics DT-X.On the other hand,we predicted the benthic algae coverage by inverse distance weight(IDW) interpolation for non-sampled area.This method which combines echo sounding and interpolation is economic and high-efficient.The results of the study show that benthic algae around Gouqi Island are distributed patchily.Interpolation makes up for the shortcoming that echo sampling cannot cover to reveal the total resources in the entire research range.While,""Closer in Space,more Similar in Coverage Degree"" is the specific character of IDW interpolation,this fits well with the patchily growing benthic algae to predict benthic algae resource.When given a perfect power exponent,the IDW interpolation can predict and assess the distribution of benthic algae effectively,in this research the most suitable power exponent is p=5.Now,the applications of Echo Sounding and Interpolation in assessing benthic algae resource are at the very beginning,even more,most of them are conducted in calm lakes and rivers and only a few similar studies have been done in the ocean environment.At the present stage,deeper research is much needed.This paper digs and develops the application characteristics while conducting Echo Sounding and Interpolation Method to assess the benthic algae resources in complicated ocean environment."
842f0b77591c28079eae63ce613d0f011c138333,"A sliding mode observer and fractional-order phase-locked loop (FO-PLL) method is proposed for the sensorless speed control of a permanent magnet synchronous motor (PMSM). The saturation function is adopted in order to reduce the chattering phenomenon caused by the sliding mode observer. In this proposed FO-PLL method, a regulable fractional order r is involved, which means that the FO-PLL provides an extra degree of freedom, and by selecting a proper fractional order r a better performance may be achieved. In fact, the conventional phase-locked loop (PLL) applied in sensorless PMSM control can be seen as a special case of the proposed FO-PLL. The computer simulation results demonstrate the effectiveness of the proposed method."
973cb025361357c496539b3e9c02840fbe9263fa,"To improve the performance of the terminal voltage and power angle of the turbogenerator, an online learning and active disturbance rejection control (ADRC) based ANN-Inversion(ANNI) robust control scheme is proposed. Firstly, the composite pseudo linear system, which is composed of the ANNI system and the controlled excitation and valve system, is equivalent to a linear system with disturbance. Then, an ESO is designed based on the ADRC method to estimate the states and the disturbance of the composite pseudo linear system online, thus resolves the difficulty of online acquisition of the training samples for online learning of ANN inversion, and the pseudo control input with disturbance compensation is designed for the composite pseudo-linear system. Furthermore, the convergence of the ESO is proved by the linear system theory and an integral order PID controller and a fractional order PID controller are designed for the the composite pseudo linear excitation and valve system. Meanwhile, an online learning algorithm of the ANNI is proposed with online gradient descent method based on offline training, and the convergence of the online learning algorithm of the ANNI is proved according to the Lyapunov stability principles. Finally, case study is fulfilled on a typical two-area four-machine power system and results compared with the conventional AVR/PSS and the offline trained based ANNI control scheme show that the proposed control scheme can greatly improve the transient performance."
c174ff3e5fbe3b47295f2e3184ce5774868a50bb,"In this paper, a robust Fractional order sliding mode controller (FOSMC) is proposed for the speed control of a permanent magnet synchronous motor (PMSM). The sliding mode controller (SMC), which is insensitive to uncertainties, is studied widely in the application of PMSM drive. Furthermore, it is remarkable to see the increasing number of studies related to the theory and application of fractional order controller (FOC), especially PIλDμ controller, in many areas of science and engineering. Research activities are focused on developing new analysis and design methods for fractional order controllers as an extension of classical control theory. In this study, with the exponent reaching law, a fractional order sliding surface is introduced and a FOSMC is designed and applied to a speed control of PMSM, which is typical nonlinear multivariable coupled system. The performance and robustness of the proposed method are tested for nonlinear load torque disturbance, and simulation results show the effectiveness of the proposed algorithm."
fe4bcc2266a1baadd63635e44560503e3b24ea61,"Avian influenza virus(AIV) nonstructural 1(NS1) gene was amplified by real-time polymerse chain reac- tion(RT-PCR) and inserted into pET28a, then transformed into E. coli BL21(DE3) competent cell. With the induction of isopropyl-β-D-thiogalactoside(IPTG) and the purification of Ni-NTA column, we finally obtained purified NS1 protein. T7-phage display system was used to screen the proteins that interacted with NS1 from lung cell cDNA li- brary. The selected positive clones were identified by DNA sequencing and analyzed by BLAST program in Gene- Bank. Two proteins were obtained as NS1 binding proteins, Homo sapiens nucleolar and coiled-body phosphoprotein 1(NOLC1) and Homo sapiens similar to colon cancer-associated antigen. By co-immunoprecipitation and other me- thods, Homo sapiens NOLC1 was found to interact with the NS1 protein, the results would provide the basis for fur- ther studying biological function of NS1 protein. Keywords T7-phage display; Nonstructural 1(NS1) protein; Interacting protein Article ID 1005-9040(2012)-01-103-05"
1a47736573afb162fc2c737674403ed18a1cb73e,"The present paper investigated interface and foam properties of a foaming agent system by hanging drop and air-blowing methods and examined the influence of foaming agent(DWS) and hydrolyzed polyacrylamide(HPAM) concentrations on interfacial tension,surface dilatational modulus and foam properties.The experiment result shows that ultra-low interfacial tension can be formed when the mass fraction of DWS is 0.1%~0.4% and HPAM with a concentration below 1 500 mg/kg in a foaming agent system mixed together with Daqing crude oils.Interfacial tension tends to increase when the HPAM concentration in a foaming agent system is high.The surface dilatational modulus of DWS appears to increase with its concentration and reach to the maximum at a lower concentration of DWS.Adding HPAM in a foaming agent system is helpful in enhancing the surface dilatational modulus of DWS.The foamability and foam stability of a foaming agent system increase with the increase of DWS concentration,the increase of the HPAM concentration is helpful to the enhancement of foam stability but reduces foamability correspondingly."
658f896448c6b25db7512ba7033bf42addcd253f,"In this paper,we investigate the diagnostic value of 11C-methionine(MET) positron emission tomography/computed tomography(PET/CT) for brain gliomas,and compare the results to 18F-fluorodeoxyglucose.Forty-four patients with suspected gliomas were examined with 11C-MET and 18F-FDG PET/CT.18F-FDG and 11C-MET PET/CT images were compared and evaluated by visual and semiquantitative analysis.The accuracy of 11C-MET and 18F-FDG PET/CT for detecting gliomas were 88.6% and 65.9%,respectively.Semiquantitative analysis showed that the 26 gliomas had higher mean ± SD T/NGmax ratio on 11C-MET PET/CT than on 18F-FDG PET/CT(1.95±0.52 vs.0.90±0.27,t=9.101,P0.001).11C-MET had a higher sensitivity than 18F-FDG(83.3% vs.33.3%,χ2 =4.16,P0.05) for low-grade gliomas,but it did not differ significantly from 18F-FDG in the sensitivity for high-grade gliomas(100% vs.64.3%,χ2=3.20,P0.05).The difference was no significant,too,between high-and low-grade gliomas,compared by 11C-MET T/NGmax ratio(2.07±0.51 vs.1.81±0.52,t=1.302,P=0.205).18F-FDG T/NGmax ratio in high-grade gliomas was significantly higher than that in low-grade gliomas(1.03±0.30 vs.0.75±0.11,t=3.198,P=0.004).It is concluded that 11C-MET PET/CT is more accurate than 18F-FDG PET/CT for detecting and delineating gliomas,especially for low-grade gliomas,and it can play a complement role to 18F-FDG in tumor grading."
ac008735b4666085e6b8f35ac5413e82757da1a6,"In order to deeply study on biological function of NS1 protein,learn about distribution and subcellular localization of NS1 protein in mammalian cells.NS1 gene of avian influenza virus was amplified by RT-PCR,and cloned into vector pEGFP-N1.Recombinant plasmid was identified by enzyme digestion and sequencing,the recombinant plasmid pEGFP-N1-NS1 was transfected into 293T cells and Hela cells by liposome,distribution and subcellular localization of NS1 protein was observed by the fluorescence microscope and the confocal laser scanning microscope.Eukaryotic recombinant expression plasmid pEGFP-N1-NS1 was successfully constructed,it had high levels of expression.in 293T cells and Hela cells.NS1 green fluorescent fusion protein is mainly distributed in the nucleus,is slightly distributed in the cytoplasm.The study successfully expressed NS1 protein in mammalian cells and analyzed subcellular localization of NS1protein.It is a solid basis for related function research on influenza virus."
f9454911c360d72d2848117ec62d7d90446f3827,"Objective To investigate the CT,MRI and PET/CT manifestations of renal lymphoma.Materials and Methods Imaging and clinical data of 18 patients with renal lymphoma proved by pathology were analyzed retrospectively.17 cases received CT scan,1 case with plain scan only,other 16 cases received plain and enhanced scan.3 cases received PET/CT scan.1 case received MRI plain and enhanced scan.Results In all of the 18 patients,6 patients were presented as multiple masses pattern;3 patients showed solitary masses pattern;5 cases were presented as contiguous retroperitoneal masses extension pattern;3 patients demonstrated perinephric pattern and 1 patient with infiltration pattern.Conclusion CT,MRI and PET/CT can clearly demonstrate the lesion of renal lymphoma.The general features of renal lymphoma are multiple masses,solitary masses or contiguous retroperitoneal masses extension with homogeneous attenuation and mild enhancement.PET can sensitively detect occult lesions in other ogans."
05f026d13303a022338c23958508a28d44035e65,"Critical marketing environment is the restraint condition of enterprise business activities, which has extremely important influence on the existence and development of enterprises. Meanwhile, critical marketing environment is one of the important factors influencing consumer decision making, too. On the basis of studying the factors influencing consumer decision making and the macro marketing environment of enterprises, this paper explored how the critical marketing environment influences the consumer decision making. In addition, taking Chinese automobile purchase decision making as an example, it studied how the macro marketing environment influences the Chinese automobile consumer decision making."
147c5e933b09a1d5546a5d8b4d7bb2afaad70cd1,"Hypertension is classified to fire-heat constitution,yang deficiency constitution,phlegm-wet constitution and yin deficiency causing predominant yang constitution on base of consititutional theory. The therapeutic method of TCM is to modify the unbalance of constitution of hypertension patient."
3f618c118013cdf89e328ba7c791c994dbc9e0f0,
4fa85b5988cc9331f1f753debc54d2df8ef2feb9,"Objective To evaluate the effects of different vector mosquito control measures for malaria in Yongcheng through field tests.Methods Vector monitoring was conducted in villages using different vector prevention and control measures,where mosquitoes of anopheles were captured every ten days using 50 mosquito nets indoors in early morning and using human baits outdoors at night.Results A total of 79 anopheles mosquitoes were captured by nets in early morning,accounting for 33.33%,with other captives of 158 accounting for 66.67%.The average anopheles density was 0.18/net per capture in the control group,and 0.03,0.03,0.05 and 0.06/net per capture in the impregnated-net group,residual spraying group,impregnated-net plus larviciding group and residual spraying plus larviciding group,respectively.Albeit effective in reducing anopheles density(F=4.553,P0.01),these measures were not deemed different in the effects.The average man-biting rate was 0.10 mosquito/(person·night) in the control group,and 0.02,0.01,0.03 and 0.03 mosquito/(person·night) in the impregnated-net,residual spraying,impregnated-net plus biological larviciding and residual spraying plus larviciding groups,respectively.Though effective in reducing man-biting rates(F=5.261,P0.01),the measures were not considered different in the effects.At different monitoring sites,183 anopheles mosquitoes were captured outdoors by human baits at night,accounting for 15.69%,with other captives of 983 accounting for 84.31%.The average anopheles density was 2.48/h in the control group,and 0.76,2.12,0.24,1.08 and 0.64/h in the impregnated-net,residual spraying,biological larviciding,impregnated-net plus larviciding and residual spraying plus larviciding groups,respectively.The mosquito density in the impregnated-net,biological larviciding,impregnated-net plus larviciding and residual spraying plus larviciding groups was lower than that in the control group,and the density in the residual spraying group was higher than those in the impregnated-net,biological larviciding and residual spraying plus larviciding groups.The average man-biting rate was 12.40 mosquitoes/(person·night) in the control group,and 3.80,10.60,1.20,5.40 and 3.20 mosquitoes/(person·night) in the impregnated-net,residual spraying,biological larviciding,impregnated-net plus larviciding and residual spraying plus larviciding groups,respectively.The man-biting rates in the impregnated-net,biological larviciding,impregnated-net plus larviciding and residual spraying plus larviciding groups were lower than that in the control group,while the rate in the residual spraying group was higher than those in the impregnated net,biological larviciding and impregnated-net plus larviciding groups.The Pearson correlation analysis revealed a positive correlation between the outdoor anopheles density in October and the incidence of malaria in November(r=0.945,P0.01),and a positive correlation between the corrected man-biting rate in October and the incidence of malaria in November(r=0.927,P0.05).Conclusion The above prevention and control measures were effective in reducing both indoor and outdoor anopheles density as well as man-biting rates in Yongcheng city;however,the advantages and disadvantages of specific measure should be further studied."
6916131da722f28665bc7f413ec2d115c590865e,The thermal decomposition of the soluble soybean polysaccharides (SSPS)from soybean dregs has been studied by thermogravimetry (TG)and differential scanning calorimetry (DSC).The results indicate that SSPS had the similar thermal characteristic from 30 ℃ to 105 ℃ no matter what were extracted by hot water or microwave.
858046d02dbc7b3db16dd6efdf9fc9f78fdae270,"In order to increase localization coverage and restrain error cumulation,a new distributed localization algorithm was proposed based on Euclidean ranging.The algorithm utilized multi-hop ranging approach to increase localization coverage.It set believable factor threshold and used combination trilateration method to obtain the set of coordinate estimates,and then weighted average method was employed to improve localization accuracy.Simulation results showed that the algorithm was efficient to prevent localization errors from spreading through the networks,and it was fit for the large scale sensor networks."
88705db9b08aa011264e8ca3d7af6ef97ff2487f,"A three-dimension numerical model to simulate the distribution of salt with a mobile boundary in the Yangtze River estuary is established in this paper. The model is based on an international three-dimension coastal ocean circulation model (ECOM model).Some complex topography is taken into account,including the deep waterway and the artificially enclosed tideland. The computed results match well with the observed ones,which imply that the model is capable of describing the temporal and spatial distribution of salt current in the Yangtze River estuary. The model imitates the 3D characteristics of saltwater intrusion,salt in the surface layer is lower and salt in the bottom layer is higher. From measurements during two tidal cycles by this model in February 1-2,7-8,2003,the sectional salt fluxes from the North Branch and the South Branch were calculated. The result shows that salt changes obviously during spring-neap tidal cycle. The sea transports a huge amount of salt into Yangtze River estuary during spring tidal cycle. This research provides the necessary data for prediction of saltwater intrusion and aproposed project for avoiding saltwater intrusion in the Yangtze River estuary."
ce5aca12b3f11cd1463b5508a7aa359c51d3ae1d,"Objective To survey the psychology of patients with sexually transmitted diseases (STD) and to find the effect of STDs on the psychology in patients. Methods The psychology of 263 cases with STD was evaluated by Hamilton Anxiety Scale (HAS) and Hamilton Depression Rating Scale (HDS), which was compared with that elicited from 223 controls. Results The scores for anxiety and depression in STD group were significantly higher that those in controls. The patients often scored higher if they aged from 18 to 39 years old, not be treated regularly, struggled with the disease for a long time,unmarried, had no child, or suffered from syphilis, genital warts, or genital herpes. Conclusion Anxiety and depression are common in patients with STD. Clinicians should pay more attention to the psychology in these patients and treatment should be given in time."
f731b65685898e0aaeb4fa2009e1e84157d359c8,"Objective To study levels of expression of the proteins HSP60,70,and 90α in colorectal cancer and to analyze the relationship between the expression of HSP60,70,and 90α and the pathology of and prognosis for colorectal cancer.Methods Collection of colorectal cancer samples: fresh colorectal cancer tissues and adjacent normal tissues 5 cm from the tumorous tissue were collected from 49 patients.This was done to detect the level of expression of HSP60,HSP70,and HSP90α proteins in colorectal cancer using Western blot.Results Western blot revealed that the expression of HSP60,HSP70,and HSP90α in tumor tissues compared to that in adjacent non-tumorous tissues was as follows: HSP60/β-actin(1.409±0.899 versus 0.434±0.122),HSP70/β-actin(0.955±0.247 versus 0.405±0.125),and HSP90α/β-actin(0.873±0.199 versus 0.364±0.108).The relative level of expression increased significantly in tumor tissues and adjacent non-tumorous tissues for all the three HSP family genes.Conclusion Results showed that HSP60,HSP70,and HSP90α proteins in colorectal cancer were significantly up-regulated in tumor tissues compared to their expression in adjacent non-tumorous tissue.The high expression of these proteins in colorectal cancer tissues may indicate that they play an important role in the pathogenesis of colorectal cancer."
02bf980ee4946004a91e1f3b1868bbedf773b641,"Objective To develop a method of culturing vascular smooth muscle cells(VSMC) from rat thoracic aorta and observe their growth chararcterise.Methods 0.2% 1-type collagenase and 0.2% 2-type collagenase digested the vascular media of rat thoracic aorta.Then observe the growth of cells and the cultured cells were identified through α-actin immunohistochemistry stain.Results The cells begin to adhere to wall within 24 hours and were successfully passaged in one week.The passaged smooth muscle cells grow with typical ‘hill and valley'pattern under an inverted microscope and the purity of third generation smooth muscle cell assessed by α-actin immuneostaining was 96%.ConculsionsOur co-collagenase method has the advantage of simple , short period and high yield of pure SMC of primary culture"
039f6ca9729de00131e7c723b139ef2399d5ff97,"Objective To analyze the characteristics of TCM syndromes of narcolepsy. Methods The 88 narcolepsy patients were analyzed with the unified questionnaire for observing the characteristics of TCM syndromes. Results The core symptoms of narcolepsy included sleep attacks (88 cases),amyasthenia (73 cases),hyponagogic hallucination (29 cases) and sleep paralysis (27 cases). The main syndromes of narcolepsy included retention of damp due to spleen deficiency (53 cases),the damp heat in the spleen (9 cases),spleen deficiency and liver hyperactivity (8 cases),spleen and kidney Yang deficiency (4 cases),turbid phlegm disturbing brain (6 cases),blood stasis in the brain (4 cases),and phlegm-fire disturbing gallbladder (5 cases). Conclusion The core symptoms of narcolepsy included sleep attacks,amyasthenia,hyponagogic hallucination and sleep paralysis which appear singly or in combination. Its main syndrome is retention of damp due to spleen deficiency."
21076bf29f4c56455ba19e0807d9d6d99d371cd4,"Objective To investigate correlation between thymidylate synthase enhancer region(TSER) polymorphism and the antitumor activities of 5-fluorouracil(5-Fu) determined by ATP sensitivity assay in vitro.Methods The carcinoma cells isolated from fresh gastric tissue in 45 patients were cultured in vitro.The sensitivity of these cells to 5-Fu were tested using ATP-TCA method.TSER genotypes were detected by PCR and the correlation between different of TSER genotypes and its chemosensitivity to 5-Fu was analyzed in this study.Results In all cases,the frequency distribution of TSER 2R/2R、2R/3R and 3R/3R genotypes were 6.7%(3/45)、31.1%(14/45) and 62.2%(28/45),respectively.The overall effective rate of 3R/3R group to 5-Fu was 33.3%(14/42).In 2R/2R and 2R/3R genotype group its effective rate to 5-Fu was 56.3%(9/16),which was significantly higher than that of 3R/3R genotype group(19.2%,P0.05).Conclusions The results in the present study suggest that the polymorphism of TSER was associated with the antitumor activities of 5-fluorouracil,indicating that analysis of TSER polymorphisms might be useful to direct 5-fluorourcil-based antitumor chemotherapy."
491e1c913e8feb56ecc49ecf28bf222134e5f893,"Snails were controlled by environmental modification in the Town of Duandian,Ezhou City in 2007.The snail situation after environmental modification in 2008 was compared to the situation prior to modification in 2007,indicating that snail prevalence declined 37.14%.The average density of snails positive for schistosomiasis decreased 76.77% and that for snails negative for schistosomiasis decreased 86.11%.In comparison to 2008,snail prevalence in 2009 declined further,decreasing 13.23%.The average density of snails positive for schistosomiasis decreased 37.40% and that for snails negative for schistosomiasis decreased 8.00%.Environmental modification was remarkably effective at snail control."
56d11735b29f55b451eacb5a1cf5b9ad22ccf147,This paper presents a design method of tuning parameters for a fractional order controller based on a gain margin and phase margin and well as various digital implementation ways.The simulation results illustrate that the fractional order controller is provided with better control performance and robustness for system parameters in comparison with the conventional PID controller.
69a08a705f817820d1caee8813eaa7546ea29dee,"Objective: To verify and standardize the traditional Chinese medicine(TCM) terms about vascular mild cognitive impairment.Methods: Prospective observation was adopted.155 Leukoaraiosis(LA) with mild cognitive impairment patients were selected to verify the reliability and validity of core symptoms questionnaire of vascular mild cognitive impairment.Results: 24 core symptoms were verifi ed and nominated in traditional Chinese phrases.For example,the English meanings were: when talking one can’t remember his beginning and often stop;forgetting something soon;after saying out something,one suddenly forget it and can’t remember;inability to concentrate;bad at calculation;be in two minds;lazy to talk;talk too slowly;low interest in every thing;slow in mind;retarded thinking;retarded reaction;feelings of sadness;physically clumsy;walking slowly and heavily,et al.Conclusion: The 24 terms could describe the clinical features of vascular mild cognitive impairment generally.The terms of vascular mild cognitive impairment were different from those of dementia."
93825ce00c58781dc5a5c8981fa7c38149e8e2f1,"The safety training is a basic task of safety production.It is the exemplification to implement the principle of ""safety first,prevention first,comprehensive management"",the important measure to establish long-term mechanism,the fundamental measure to reduce safety incidents and to improve safety production.Under the new situation,a rigid teaching methods can not meet the training needs of staff.Only to constantly updated the teaching content,to Innovate the training methods,can we guarantee the quality of training and reached the required training purposes."
9994a7d8d19e2d5a48f66fbee1510d2f81985910,"A key obstacle to high accuracy location is the None Line of Sight (NLOS) transmission of signal in wireless localization. It adds positive error to distance measurements. An effective location algorithm based on nonlinear constraint optimization is proposed to mitigate NLOS errors. The proposed method forms distance error optimization constraint using the geometric relationship between the base stations and mobile station. It improves the location accuracy by calibrating the distance measurements and modifying the objective function. Simulation results show that this new method can efficiently reduce the effect of the NLOS error, and is robust to different NLOS error."
a0ed7a913d8a3a4296411b958748e75ab3be1a36,"PET/CT scan was performed for 30 newly diagnosed esophageal carcinoma patients within the week before operation and radio-and/or chemo-therapy.The diagnosis of esophageal carcinoma was established according to the histopathology examination.In 15 patients with operation,the findings of PET/CT of 130 regional lymph nodes were compared with the histopathology examination.The distant metastasis of all patients and the regional lymph node metastasis of 15 patients without operation were diagnosed by the pathological examination,other imaging findings or/and clinical follow up for over 6 months.The results show that all the primary lesions had very intense 18F-FDG uptake.The sensitivity of PET/CT was 100%.The lymph nodes metastasis was proved in 20 patients (66.7%).PET/CT was true positive in 18patients,false positive in 2 patients.The sensitivity,specificity and accuracy of PET/CT were 90.0%,80.0% and 86.7%,respectively,based on the case study.In 130 excised lymph nodes of 15 patients with the operation,23 of 33 lymph nodes metastasis were detected by PET/CT.The sensitivity,specificity and accuracy of PET/CT were 69.7%,97.9% and 90.8%,respectively,based on the lesion study.PET/CT detected the distant metastasis in the liver,lung and bone in 4 cases.After PET/CT imaging,the staging of tumor in 16 patients was improved,1 was lowered,and 13 was concord with pre-staging.18F-FDG PET/CT plays an important role in the diagnosis and staging for esophageal cancer."
a59bacd1a9253586f6e68edda8da2b78dd1ac862,"Objective To investigate sanitary quality of drinking water in rural areas of Heyuan.Methods Using stratified random sampling principle to investigate and analysis the water.Results Heyuan rural drinking water mainly to the underground water which accounted for 79.47% .It is mainly a non-centralized way of supply , accounted for 77.71%.The percentages of drinking from water supply with complete treatment, partial treatment, tap water without any treatment are 26.92% , 15.39% and 57.69% , respectively.In the 150 water samples, the water quality of rural drinking water above grade II have 82, accounting for 54.67%.Underground water is better than surface water, with a pass rate(χ2=3.96, P0.05).The total bacteria, total coliform count of the centralized water supply is superior to the decentralized water supply, with a significant difference in the passing rate(χ2=4.32、5.58, P0.05).Conclusion The main factor of Heyuan unsafe drinking water in rural areas is the microbial contamination.Strengthen the awareness of rural drinking water hygiene, water well site selection, protection and water purification and disinfection are important to prevent all kinds of water-mediated diseases, and the protection of people's health."
bca8448062f4b132249c9bff740fb7e12c40f818,"Because of restricted resources, wireless sensor networks need to consider synthetically the price of networks, and costs of communication and computing and so on. Aiming at the localization problem for mobile beacon node in wireless sensor network, this paper uses the ant colony algorithm’s characteristics which is self-organized, adaptive, dynamic optimization, integrating the hop number into the pheromone and supporting multiple paths, and proposed a kind of method for getting the mobile beacon node optimal path based on ant colony algorithm. Furthermore, various factors are analyzed for getting optimal path. Experimental results show that the proposed method is efficient."
da83ef1bb7bef1b19af68d8e03ae5dedffe30912,"Field experiments were conducted to study residue and degradation of metalaxyl-M in tomato. Metalaxyl-M residues were quantified by GC-NPD. In Trial I,the metalaxyl-M degradation was detected after tomato was treated at the recommended dose level. In Trial II,effects of application rate and application times on the metalaxyl-M degradation in tomato. Metalaxyl-M degraded rapidly from tomato pulp after applied to the tomato plants,and its half-life was 1.74 d. When tomato plants were sprayed 5 times with the metalaxyl-M at a 2 fold level of the dose the manufacturer recommended,the residue in the toma-to pulp was 0.074 mg/kg after 2 days of the last spray. This indicated that the safe interval of 3 days before the last spray of metalaxyl-M onto tomato,which is recommended by the manufacturer,is acceptable."
e75ad983ab610f5454517011bee61110e6d4064d,"Appropriate node management of networks in the limited resources of the wireless sensor network for effectively control the energy of networks is one of the key points in Research on wireless sensor networks. Using of ant colony algorithm’s characteristics, self-organized, adaptive and dynamic optimization, this paper proposed a new power management which can reduce the node power and re-establish of a new node routing links, to enhance the viability of the entire network, improve the life of the network. Simulation results show that the ant colony algorithm can be applied to node power management of wireless sensor network a good way with obvious energy-saving"
e9678e371ecce0e046e8e231ae046e42bf52f257,
2e041689e9e330c4ecd6254fc67a3d24ec8b7fb6,
7baa70d90b3726618e8b4969cdbfe0097433260f,"Seedlings-height growth regularity of 1-year-old seedlings of Camellia crepnelliana Tutch.was studied,and the growth model of seedlings-height was fitted with Logistic equation.The study showed that the fitted seedlings-height growth curve for C.crepnelliana has high anastomosis with the actual growth curve.It can predict the height growth during growth period and provide a reasonable basis for making the rational and scientific culture technology."
c66ab243608850f8e51ff2a7dd3e256246866556,"Due to extensive mining for 40 years in YangShuling mining areas,the ground-water level is declined,the earth sur- face is subsided in variable degrees and the river is polluted,which induce multi-geological disasters,such as water-loss,soil-e- rosion,karst collapse;and earth surface's cracking,etc..From the viewpoint of effective and negative in coal mining,the prob- lems and their mechanisms caused by coal mining are discussed in detail,in which protections and solutions are given according to different environmental problems."
ea142cedccc0a2928bd602ca5c0bdb64976c9835,"Protein from mycelia and fruit bodies of lnonotus obliquus was assessed in terms of six nutritional quality indices-Amino Acid Score (AAS),Chemical Score (CS),Essential Amino Acid Index (EAAI), Biological Value (BV),Nutritional Index (NI) and Amino Acid Ratio Coefficient Score (AARCS).AAS, CS,EAAI,BV,NI and AARCS values of I.obliquus mycelia were 75.0,44.5,92.2,88.8,24.7 and 52.0,respectively,compared with corresponding values for fruit bodies of 47.1,20.3,99.4,96.6,2.94 and 33.0."
1dbfb95d7cd8de41058749fd296029ecd352006e,
2da35b9dac93080d29ae622ed938d6c2dd3da5e4,"This article describes the basic principle of multi-sensor data fusion; In view of the great span roof structure strain detection system characteristics, we has put forward the self-adaptive sensor weighted fusion algorithm, which may enhance the strain examination system effectively antijamming ability, and guarantee the strain exceptionally examines reliability and accuracy."
4b30286920f54edebc5c81d83c064200ef1cb26a,"In this paper, we present a design of new type of water meter with nonmagnetic and low power. Some kinds of flow measurements are analyzed, and we discuss the principle of the nonmagnetic sensor used by LC oscillating circuit measuring water flux in detail. The MSP430FW427 is used as MCU with the function realized by the SCAN IF module without IC circuit external, which improves the precision and sensitivity of flow measurement. It realizes the fluid flow measurement and in this paper the correlative design of hardware and software are introduced."
708f96708c5aa6f3fa79f0f1c9c9aca95fdde585,"With rapid development of computer technology,in recent years it is remarkable the increasing number of studies related with the application of fractional order controllers(FOC),specially PIλDμcontroller,in many areas of science and engineering.Further research activities are running in order to develop new analysis and design method for fractional order controllers based on an extension of classical PID control theory.After brief introduction of fractional order calculus,the main developments about FOC,specially PIλDμcontroller,were introduced.Possible future researches were introduced."
abcd9ae6fa6edf20066a5b24fe037579360f32d1,"Comus wilsoniana is a native biomass-energy trees species and edible oil trees species in south of Jiangxi. Its seed pre-processing, determining main parameter of seed, seedling phenological, seedling roots grow, seedling growth rhythm were studyed in this paper. Thus the practical and comparatively systematic techniques of seedling-raising wer obtained."
b29bd297861362f841e8b88c8f4d447be54f27de,
bdf0f0d14f650477e8fc1fba40de215e01493c41,"In CNC machine tool, precise contouring feeding movement with transmission machinery is carried out along interpolated position command.Each axis tracking error in the system is controlled independently.The contouring motion error affected by distinguishing feature of each axis was studied.Two errors, the circular arc radius error due to the finite bandwidth of the servo system and the elliptic error due to performance mismatch between motion axes, were analyzed.Conclusions discussed were confirmed by simulation."
cab4a38c199635959f6f5b5dab854cc402535091,"Optimal conditions for extraction of polysaccharide from Inonotus obliquus fruit bodies(IOP)were determined as follows:extraction time,70 min;temperature,80 ℃;ratio of aqueous solvent to fruit body,12∶1;and ultrasound frequency,20 kHz.IOP at a concentration of 1.44 μg/mL inhibited cancer cell growth by~65% as measured using a tetrazolium-based(MTT)assay to evaluate the anti-proliferative effect of the polysaccharide on hepatocarcinoma HEPG2 cells.A 27% and 35% reduction in tumor weight was also observed in S-180 tumor-bearing mice administered low(50 mL/kg·d,8 μg/mL)and high(100 mL/kg·d,16 μg/mL)doses of IOP,respectively."
e8040a28c2ab0e3b2295b2ca588efe11cb895961,"The extraction and purification technnologies of taxol from planted yew leaves,roots and branches are studied,and two extraction methods of ultrasonic heating and refluxing back are compared.After solid-phase extraction and liquid-liquid extraction,precipitation of hexane,silica-gel chromatography purification of paclitaxel,ultraviolet detection is used to detect the content in the leaves, roots and baranches.They are 13.89 μg/g,12.67 μg/g,39.33 μg/g;the purities are 83%,81.7%,90%,respectively.It indicates that taxus branch is the best raw material for extracting taxol and the refluxing extraction method is better."
ee2fa3efa8875181051c3d013c94b86ba9fa941a,"It is very important to remotely measure and monitor hydrology information real-time so as to improve modern management of water resource. In this paper, a new hydrology telemetric system is introduced. The system mainly consists of remote terminal units (RTUs) and monitoring center. The RTU based on ARM is mainly designed to collect, store and transmit the water information to the host computer in the monitoring center. The host computer receives the data from the RTUs and makes correct decision after analyzing, which provides the water conservancy with immediate and exact water information. The remote communication between RTUs and the monitoring center is realized over GPRS. The basic principle, the system composition, and the researches on key technological problems are presented in the paper. And the main idea about improving information security in remote monitoring would also be given. The practical results showed the system is stable and reliable."
2d7eab01e80570fe8194859178a14fe9f2ac51bb,"In this paper, the relation between CCC (cross-coupled control) controller and the closed loop transfer function of each axis is discussed. Based on the analysis of CETF(contouring error transfer function), T-S Fuzzy CCC scheme is proposed to further reduce the contour error. Compared to common CCC controller, the proposed T-S CCC is more effective in reducing the contour error significantly while the controlled axes are not mismatch."
5a6f2e3dfe3b9520471b6b94ff6d1d8506454836,This paper describes the design of cement raw material prepared digital control system by PROFIBUS.It uses the digital technology to implement the digital control system for raw material prepared of the cement factory.This system improves the communication between the industrial system and locale instrument devices.It applies digital communication to replace ttle 4-20 mA or 24VDC signal between locale lever device and controller.
5e571596ec23a8e47e3bb0763388ff8c6364b114,"A crude polysaccharide fraction (PIO) from fruit bodies of the medicinal mushroom, Inonotus obliquus, exhibited strong ·OH scavenging activity as measured by the salicylic acid method. PIO also inhibited endogenous and Fe~ 2+ -Cys-induced lipid peroxidation, and mitochondrial swelling induced by the Fe~ 2+ - vitamin C system. It is proposed that the underlying anti-oxidation mechanism involves the initial elimination of ·OH radicals, which in turn suppresses peroxidation and protects membrane systems."
6e0f3f0fc672c5fef8822fa3ddefcc640321fee1,"The design orthogonal experiment inspects differently withdraws the plan to determine the result the influence.The result indicated, 50 ℃,20 kHz ultrasonic wave power,the 80% methyl alcohol solution 52 mL backflow withdraws 3 times,each time 130 min best withdraws the plan twists the stock blue total glucoside in the observation to the old age rat's antioxidation partial experiments in,after determination to medicine rat red blood cell hyperoxide mutase(SOD) content, proved twists the stock blue total glucoside to have reduces the free radical vigor,the enhancement old age big mouse organism antioxidation ability function."
6fc629a55c804821fd26989b6ba066de7c3da190,"The L9(34) orthogonal design was applied to the study of the microbe proportion, fermentation time and temperature for a fermented jujube juice, with a view of finding out the optimum processing technological parameters. The results indicated that the optimal proportion was saccharomycete : lactic acid bacteria = 2 : 3,the fermentation time was 48h,and the fermentation temperature was 30℃ or 40℃ in different processes."
aef7e5dbbef325ed955724f409a7728d8f2240e5,"The automatic weight adjusting principle, hardware and software design of the weight adjusting signal process are introduced in detail. In the end, a weight adjusting experiment is carried out. The machining efficiency could be improved greatly with high quality machining."
bee495c01bcdaf312d662941666c0e1ae511211e,"Glutamine Synthetase(GS) is a biological enzyme which has been widely used.The optimum fermentation conditions which produce glutamine synthetase in different temperatures and pH about LNU 0165 was studied.LNU 0165 is a high-producing strain by determining activity of GS.Moreover its phylogenetic tree was studied genomically by its 16S rRNA sequence and homology.According to 99% of sequence of homology of 16S rRNA and database in GenBank,finally,LNU 0165 strain was identified as Arthrobacter globiformis."
cb7a25c87e17e3eaf0111460550a33b6033b7199,"Glutamine Synthetase (GS) is a biological enzyme which has been used widely. It was studied on the optimum fermentation conditions for producing glutamine synthetase in different temperatures and pH is about LNU 0165, LNU 0066 and LNU 0254 as an experimantal bacteria. The results show that LNU 0165 is a high-producing strain by determining activity of GS. Optimum temperature is 60 ℃ and pH is about 7.5 for GS activity."
f460e430c14dd47e1d23e4274941a05e3b4966b3,"Cardiovascular diseases, such as vessel embolism,cerebral thrombosis and acute myocardial infarction,are one of main serious diseases harming people's health in the society nowadays. With a very strong fibrinolytic activity and the advantages of high safety,rapid action,low cost and being able to be produced by bacterial fermentation, nattokinase has an extremely bight future in being used in a new generation of health drinks. The advances of study on nattokinase are reviewed,and its use in health drinks forecasted."
5eee4045e16c727af6f2c97b93a668513d971a95,"The performances of servo have a great influence upon the precision of CNC machine tool. The indexes of servo vary according to the different working conditions. This paper presents the idea that the servo can be tested and optimized under the equal state before connected with the machine tool, on the basis the test system developed is introduced in detail."
603f7b43e6c0d9d220d5321a4692e1be22b4e0cb,"Inhibitory test in vitro,antibacterial activity test in vivo,treatment test for pigs infected with Pasteurella multocida,acute toxicity test and clinical treatment test were carried out to evaluate the antibacterial activity and curative effect of alizarin.The inhibitory test in vitro showed that alizarin had(remarkable) inhibitory effect on Staphylococcus aureus,Proteus,Escherichia coli,Pseudomonas aeruginosa,Salmonella and Klebsiella in vitro,and had stronger inhibitory effect than pyroline on E.coli,(P.aeruginosa),Salmonella and Klebsiella.The antibacterial activity test indicated that alizarin had remarkable antibacterial activity against E.coli in mice in vivo.The treatment test in pigs with pasteurellosis showed that alizarin had curative effect on piglets infected with P.multocida.The acute toxicity test and clinical treatment test showed that LD_(50) of mice injected with alizarin was 336mg/kg,and the rate of recovery and total validity to clinical type of dairy cow's mastitis were 87.5% and 100% respectively."
7a369e5d0d99e9c8e3f7309ee33ee963539e59a0,"初步建立一种较简单而有效从市售酸奶中分离乳酸菌来发酵酸奶的分离方法.采用MRS乳酸菌选择性培养基培养.经革兰染色、葡萄糖产酸、接触酶和石蕊牛奶试验分离鉴定出乳酸菌.以正交法优化厌氧培养采用L9(33)正交试验,得出最佳葡萄糖、蔗糖和维生素B2组合的改良MRS配方(葡萄糖 8%,蔗糖 1.5%,维生素B2 0.005%),并用来活化、扩大培养菌种,发酵凝固型酸奶."
8916f19f1561a18108409069bd49c86f50622301,
fabf655373c524f86246a1fd48c82c20837c3bb2,"This paper introduces a real-time oil well monitoring system based on network computing and neural network (NN) technologies. In the system, some enterprise computing techniques such as browser/server Web mode, JMS, message-oriented middleware (MOM) and Java Applet are employed. In addition, GPRS wireless communication is used to achieve remote transmission of oil well data. This scheme makes adopts Java Applet that operates at client side (Web browser) to receive messages ""pushed"" by server through JMS (Java Message Service). In this way, server and client are able to communicate in time so that client side can reflect the real-time oil well data and the fault diagnosis result. In remote monitoring center, the fault diagnosis station takes the responsibility for the fault detection and diagnosis of oil pumping units by means of neural networks and evolutionary computation. This solution accomplishes network share of oil well information, improves the efficiency of system development. The system has already been applied successfully to an oil field, and has got the anticipated results"
05bb95f4352171e634447aee65deeb16dc2a070f,"Hilbert-Huang transform (HHT) is a new method for non-stationary and nonlinear data analysis. It has been applied with great success soon after it was first introduced in 1998. Most researches at present are mainly focused on the algorithm itself and its software implementation. However higher processing speed is necessary for realtime signal processing or quasi-realtime signal processing. In our efforts, a hardware-accelerated approach of HHT method based on PC, DSP and PCI technologies is developed. With this approach, the processing time can be reduced to less than one tenth compared with ""pure software"" ones. In this paper, the basic principle of HHT is briefly reviewed, the hardware and software design considerations are discussed, and some preliminary experimental results are given."
41a6519c33868778190a68072302b7b53b217530,"Separate layer fracturing technology is main technical means of Ⅱ?Ⅲ reservoir stimulation.Single packer fracturing,gravel input fracturing,ball-off fracturing,bridge plug fracturing,limited entry fracturing and their combination have been used previously,but it can't ensure to treat all planned target zone.Sometimes,because ball number is difficult to detemine,success ratio of fracturing treatment has been influenced.In order to solve this problem,separate layer fracturing technology with dual packer is developed.New fracturing string consists of hydraulic anchor,sand blower,safety joint and packer.Field test has used borate crosslinking fracture fluid.This novel fracturing technology has some advantages,such as terget zone can be fully treated,operation is safe and treatment effect is very good.Field test confirms that fracturing success ratio is 100% and production of single well increases by 7.8t and cumulative incremental oil has reached to 2324.7t."
fa0427e367595b6f9f97a6f88571cba7889b755b,"Because of limitation of computer software running speed, CNC system using reference-pulse interpolation algorithm have low least input increment and rapid feed velocity. Higher function index can be obtained by sampled-data interpolation algorithm. But the interpolating results are generally suitable for closed-loop servo system. The paper discussed a new realization method that rough interpolation is realized by sampled-data interpolation algorithm, and fine interpolation can be gained by CPLD electronic device. The scheme can be used in open-loop step motor and All-digital AC servo system. It can improve the performance of CNC system greatly and is practical."
27562bf8c478ce68671232759c79ed144d002406,"对塞曼效应、氘灯校正及自吸校正三种扣背景方式的性能进行分析比较,测定了不同扣背景实验方法的灵敏度、精确度及准确度等技术指标.分析结果表明:不同样品和不同元素受背景干扰的程度不同,用自吸法扣背景会使测定的灵敏度降低,适用于灵敏度较高的元素分析;氘灯法虽能使灵敏度提高,但适用范围较窄(波长＜430 nm);塞曼方式是理想的扣背景方式,测定结果稳定可靠."
867e55bc88ed32eabb5561749e6ea7da88bfc463,"Taking advantage of direct curve interpolation, CNC machine tool can achieve higher precision and machining quality. The paper discusses a Sampled-Data interpolation algorithm for CNC analytical function curve that is of general significance."
aafb9d0b598c95bf47bfd8ddc8e9d3047c1fe0d0,"Objective To evaluate segmental resection of involved portal vein (PV) in the surgical treatment of advanced pancreatic carcinoma (PC). MethodsIn our 22 advanced PC patients involving PV and/or superior mesenteric vein (SMV) extended pancreaticoduodenectomy or distal pancreatectomy plus extensive regional lymph node clearance were performed and cancer involved PV and/or SMV segment were resected enbloc. Results Among 22 cases, 6 underwent wedge resection and repairment of PV or SMV. Eight underwent segmental resection of PV or SMV followed by end-to-end anastomosis in 5,autotransplantation of great saphenous vein in 2, mesocaval shunt in one. The mean operation time was 7 5?h, the average blood transfusion was 600?ml, and there was no mortality in all cases. All 22 patients were followed up with a postoperative survival of 6 mos in one, 12 mos in 3, 18 mos in 6 cases, 24 mos in 8 and 36 mos in 4 cases. ConclusionsIt is rationale to resect the advanced PC en bloc with the cancer involved segment of PV and/or SMV in terms of low morbidity and long postoperative survival."
b11ff0e78411ed03bd8343e785935a9d9b8244f3,"Combined with ASP/ADO of Web server and sampled with an oilfield remote monitor system, the main technical features and wide prospect of enterprise remote monitor system based on Intranet are introduced in this paper.It also discusses a monitor system scheme oriented to the middle-type and mini-type enterprise."
cf7f24a6e26faa4cd48d6dd66ff88c7369adfe98,The paper discusses S3D CNC machine tool simulation software distinguishing feature that is based on the PC computer and OpenGL graphic interface. The software design method can be used for reference in Open Architecture Control System and other CNC machine tool training software.
8a21a4140847caab24aabe3e9ec1132bc09f4847,"To reduce the size,weight and power consumption of strapdown inertial navigation systems(SINS),a navigation computer featured with closecoupled masterslave dual processors is developed,which employs a digital signal processor(DSP) as its navigation algorithm processor and a microcontroller as its I/O interface processor.The two processors are connected with a high speed parallel interface and their softwares are based on a protocol of masterslave pipeline. So the navigation computer can work fluently and rapidly,and the requirements for some SINS on navigation computers' I/O , data processing capability,size,weight,power consumption,etc,are met."
42ffe916eb8c0d48dfb64bfbb997bb97c7bd9c39,"This paper proposes a new diagnostic system framework about Internet-based remote diagnosis system (IRDS). The IRDS's meaning, specific properties, basic functions, system composition and work models are introduced. A computing model based on a three-layer structure is presented also. In the model, the diagnostic network is realized with the embedded Web nets, and the remote diagnostic reasoning engine (RDE) is constructed by two methods: knowledge-based RDE and Java Applet-based BNN RDE agent. An IRDS prototype has been constructed to realize the framework effectively."
4de0135a5311bdfdbdb186a9cfbfda229df98f70,"A direct-transform approach of sampling computing for wavelet analysis and relevant key techniques are presented, and a Web-based remote wavelet signal analysis system model and analysis tool would be introduced also in this paper. It is simple and easy for engineers or browsers to comprehend the transform algorithm, and to apply the tool. Because of the advantages of Internet/intranet environment, it is appropriate and important to apply the method and embed the tool to construct new types of signal analysis systems, especially in the field of Internet-based remote diagnosis systems."
e569bbd9ee8942dbb1ec1fbe82808ad22595d820,
f89efe6954f46490475b05e8f239d8a7435a3bcd,A formula for calculating bending moment acting on the dam slope caused by ice cover during lowering of pool level is established. In this formula the gravity of ice plate is regarded as distributed load and the restrain conditions between dam and ice cover as well as the characteristics of ice are taken into consideration.
332d5ddfff20316f4b0c8dfc5db25398c6aba1aa,"OBJECTIVES
The aim of this paper is to describe the clinical characteristics, diagnostic procedure and operative management of Mirrizi syndrome.


METHODS
Sixteen cases of Mirrizi syndrome were selected and reviewed from 1987 to 1997.


RESULTS
In the 16 cases, 6 cases were male, 10 cases were female, the average age was 62.7 years old. Ten cases were diagnosed to be Mirrizi syndrome preoperatively (62.5%); 3 cases were considered to bile duct tumor, the other 3 cases were emergency, they were confirmed the diagnosis after the operation.


CONCLUSIONS
Ultrasound is recommended as the first choice of screening method, while ERCP may confirm the diagnosis. Surgical approach is considered to be the choice and technical procedures are suggested to prevent intraoperative injury and to repair defects of the common bile duct."
59d1e044e570eda7a74c871698bc5b1cbf3061a5,
6ffbf0bb1bbedf78bddf26cc23aa2e832a4ce754,
bf32106eb31841487312ecae5e3982cca5d7fbc6,
e0f98307096ac8bff1c3302096d5c0771b9a7a9f,"Smart traffic lights in intelligent transportation systems (ITSs) are envisioned to greatly increase traffic efficiency and reduce congestion. Deep reinforcement learning (DRL) is a promising approach to adaptively control traffic lights based on the real-time traffic situation in a road network. However, conventional methods may suffer from poor scalability. In this paper, we investigate deep reinforcement learning to control traffic lights, and both theoretical analysis and numerical experiments show that the intelligent behavior ``greenwave""(i.e., a vehicle will see a progressive cascade of green lights, and not have to brake at any intersection) emerges naturally a grid road network, which is proved to be the optimal policy in an avenue with multiple cross streets. As a first step, we use two DRL algorithms for the traffic light control problems in two scenarios. In a single road intersection, we verify that the deep Q-network (DQN) algorithm delivers a thresholding policy; and in a grid road network, we adopt the deep deterministic policy gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN algorithm delivers the optimal control, and the DDPG algorithm with passive observations has the capability to produce on its own a high-level intelligent behavior in a grid road network, namely, the ``greenwave""policy emerges. We also verify the ``greenwave""patterns in a $5 \times 10$ grid road network. Thirdly, the ``greenwave""patterns demonstrate that DRL algorithms produce favorable solutions since the ``greenwave""policy shown in experiment results is proved to be optimal in a specified traffic model (an avenue with multiple cross streets). The delivered policies both in a single road intersection and a grid road network demonstrate the scalability of DRL algorithms."
01487754025376e4692e126f2d64ebbe44bbe2a1,"Consider a service system where incoming tasks are instantaneously dispatched to one out of many heterogeneous server pools. Associated with each server pool is a concave utility function that depends on the class of the server pool and its current occupancy. We derive an upper bound for the mean normalized aggregate utility in stationarity and introduce two load balancing policies that achieve this upper bound in a large-scale regime. Furthermore, the transient and stationary behavior of these asymptotically optimal load balancing policies is characterized on the scale of the number of server pools in the same large-scale regime. Funding: This work was supported by the Netherlands Organization for Scientific Research (NWO) through [Gravitation Grant NETWORKS-024.002.003] and [Gravitation Grant Vici 202.068]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/stsy.2022.0103 ."
1174d6569da5f4a2c652a66e8f1486c4d763fbb7,"In this paper we establish a necessary and sufficient stability condition for a stochastic ring network. Such networks naturally appear in a variety of applications within communication, computer, and road traffic systems. They typically involve multiple customer types and some form of priority structure to decide which customer receives service. These two system features tend to complicate the issue of identifying a stability condition, but we demonstrate how the ring topology can be leveraged to solve the problem."
cd878bf9fb45bae80bd80b5fdb809caa357f81e2,
de34e324b43008dd5206ae4ec80d59f44ff6d851,"Modern service systems, like cloud computing platforms or data center environments, commonly face a high degree of heterogeneity. This heterogeneity is not only caused by different server speeds but also, by binding task-server relations that must be taken into account when assigning incoming tasks. Unfortunately, there are hardly any theoretical performance guarantees as these systems do not fall within the typical supermarket modeling framework which heavily relies on strong symmetry and homogeneity assumptions. In “Heavy-traffic universality of redundancy systems with assignment constraints,” Cardinaels, Borst, and van Leeuwaarden provide insight in the performance of these systems operating under redundancy scheduling policies. Surprisingly, when experiencing high demand, these systems exhibit state space collapse and can achieve a similar level of resource pooling and performance as a fully flexible system, even subject to quite strict task-server constraints."
ec112fe0ffc0a77163a74fc44ed2f1ef34bd02ac,"We consider a large-scale service system where incoming tasks have to be instantaneously dispatched to one out of many parallel server pools. The user-perceived performance degrades with the number of concurrent tasks and the dispatcher aims at maximizing the overall quality of service by balancing the load through a simple threshold policy. We demonstrate that such a policy is optimal on the fluid and diffusion scales, while only involving a small communication overhead, which is crucial for large-scale deployments. In order to set the threshold optimally, it is important, however, to learn the load of the system, which may be unknown. For that purpose, we design a control rule for tuning the threshold in an online manner. We derive conditions that guarantee that this adaptive threshold settles at the optimal value, along with estimates for the time until this happens. In addition, we provide numerical experiments that support the theoretical results and further indicate that our policy copes effectively with time-varying demand patterns. Summary of Contribution: Data centers and cloud computing platforms are the digital factories of the world, and managing resources and workloads in these systems involves operations research challenges of an unprecedented scale. Due to the massive size, complex dynamics, and wide range of time scales, the design and implementation of optimal resource-allocation strategies is prohibitively demanding from a computation and communication perspective. These resource-allocation strategies are essential for certain interactive applications, for which the available computing resources need to be distributed optimally among users in order to provide the best overall experienced performance. This is the subject of the present article, which considers the problem of distributing tasks among the various server pools of a large-scale service system, with the objective of optimizing the overall quality of service provided to users. A solution to this load-balancing problem cannot rely on maintaining complete state information at the gateway of the system, since this is computationally unfeasible, due to the magnitude and complexity of modern data centers and cloud computing platforms. Therefore, we examine a computationally light load-balancing algorithm that is yet asymptotically optimal in a regime where the size of the system approaches infinity. The analysis is based on a Markovian stochastic model, which is studied through fluid and diffusion limits in the aforementioned large-scale regime. The article analyzes the load-balancing algorithm theoretically and provides numerical experiments that support and extend the theoretical results."
0ea51e4a5dad5efedf63127957775bbbed766b30,
20b69a3352ab01b9e72b37605dd58043ef86f631,
26f3fa6d8cdabb8655f2563da45b07189132a6c2,"The use of drone base stations offers an agile mechanism to safeguard coverage and provide capacity relief when cellular networks are under stress. Such stress conditions can occur for example in case of special events with massive crowds or network outages. In this paper we focus on a disaster scenario with emergence of a hotspot, and analyze the impact of the drone position (altitude, horizontal position) and selection bias on the network performance. We determine the optimal settings of these control parameters as a function of the hotspot location, and demonstrate that the optimized values can drastically reduce the fraction of failed calls."
30486fc7e3f476f95d19dc7d99b8919bd47628e4,
3ca3bbb2c48ae118b8ce1aa4f7c8fe9e478669ec,"Drone base stations can help safeguard coverage and provide capacity relief when cellular networks are under stress. Examples of such stress scenarios are events with massive crowds or network outages. In this paper we focus on a disaster scenario with emergence of a traffic hotspot, where agile drone positioning and load management is a critical issue. In order to address this challenge, we propose and assess a data-driven algorithm which leverages real-time measurements to dynamically optimize the 3D position of the drone as well as a cell selection bias tuned for optimized load management. We compare the performance with three benchmark scenarios: i) no drone; ii) a drone positioned above the failing site; and iii) a drone with a statically optimized position and cell selection bias. The results demonstrate that the proposed algorithm significantly improves the call success rate and achieves close to optimal performance."
48e9caa1349e7a2c51253d958b6e79874a455db5,"We investigate the achievable stability region for redundancy systems and a quite general workload model with different job types and heterogeneous servers, reflecting job-server affinity relations which may arise from data locality issues and soft compatibility constraints. Under the assumption that job types are known beforehand we establish for New-Better-than-Used (NBU) distributed speed variations that no replication gives a strictly larger stability region than replication. Strikingly, this does not depend on the underlying distribution of the intrinsic job sizes, but observing the job types is essential for this statement to hold. In case of non-observable job types we show that for New-Worse-than-Used (NWU) distributed speed variations full replication gives a larger stability region than no replication."
d542354e737f3a923aa17374c3b0c76a08fa2c36,"In classical power-of-two load balancing any server pair is sampled with equal probability. This does not cover practical settings with assignment constraints which force non-uniform server sampling. While intuition suggests that non-uniform sampling adversely impacts performance, this was only supported through simulations, and rigorous statements have remained elusive. Building on product-form distributions for redundancy systems, we prove the stochastic dominance of uniform sampling for a four-server system as well as arbitrary-size systems in light traffic."
11190d1a39503d7c1e9dca453be0280220bb5f3c,"We consider a system with N~parallel servers where incoming jobs are immediately replicated to, say, d~servers. Each of the N servers has its own queue and follows a FCFS discipline. As soon as the first job replica is completed, the remaining replicas are abandoned. We investigate the achievable stability region for a quite general workload model with different job types and heterogeneous servers, reflecting job-server affinity relations which may arise from data locality issues and soft compatibility constraints. Under the assumption that job types are known beforehand we show for New-Better-than-Used (NBU) distributed speed variations that no replication $(d=1)$ gives a strictly larger stability region than replication $(d>1)$. Strikingly, this does not depend on the underlying distribution of the intrinsic job sizes, but observing the job types is essential for this statement to hold. In case of non-observable job types we show that for New-Worse-than-Used (NWU) distributed speed variations full replication ($d=N$) gives a larger stability region than no replication $(d=1)$."
1d709d085fd740318248542fc4848656e442a603,"Redundancy scheduling is a popular concept to improve performance in parallel-server systems. In the baseline scenario any job can be handled equally well by any server, and is replicated to a fixed number of servers selected uniformly at random. Quite often however, there may be heterogeneity in job characteristics or server capabilities, and jobs can only be replicated to specific servers because of affinity relations or compatibility constraints. In order to capture such situations, we consider a scenario where jobs of various types are replicated to different subsets of servers as prescribed by a general compatibility graph. We exploit a product-form stationary distribution and weak local stability conditions to establish a state space collapse in heavy traffic. In this limiting regime, the parallel-server system with graph-based redundancy scheduling operates as a multi-class single-server system, achieving full resource pooling and exhibiting strong insensitivity to the underlying compatibility constraints."
3e79155ee28a14a34e200aa1236c553bc3dea1a2,"We consider random-access networks where each node represents a server with a queue. Each node can be either active or inactive. A node deactivates at unit rate, while activates a rate that depends on its queue, provided none of its neighbors is active. We consider arbitrary bipartite graphs in the limit as the queues become large. We identify the transition time between the two states where one half of the network is active and the other half is inactive. We decompose the transition into a succession of transitions on complete bipartite subgraphs. We formulate a greedy algorithm that takes the graph as input and gives as output the set of transition paths the system is most likely to follow. Along each path we determine the mean transition time and its law on the scale of its mean. Depending on the activation rate functions, we identify three regimes of behavior."
63fd7455fc04633096057d1f6b739d667d7b855a,
7c85b74de41aecd3a88ec4d405256956df92b1d7,
85917a5d7ca03746e0bfe822fc9741c43f397f0b,"We investigate the stability condition for redundancy-d systems where each of the servers follows a processor-sharing (PS) discipline. We allow for generally distributed job sizes, with possible dependence among the d replica sizes being governed by an arbitrary joint distribution. We establish that the stability condition for the associated fluid-limit model is characterized by the expectation of the minimum of d replica sizes being less than the mean interarrival time per server. In the special case of identical replicas, the stability condition is insensitive to the job size distribution given its mean, and the stability condition is inversely proportional to the number of replicas. In the special case of i.i.d. replicas, the stability threshold decreases (increases) in the number of replicas for job size distributions that are NBU (NWU). We also discuss extensions to scenarios with heterogeneous servers."
9cba1e55aedae93fb98b1f4ea0c78fac029607ea,"Consider a system of identical server pools where tasks with exponentially distributed service times arrive as a time-inhomogenenous Poisson process. An admission threshold is used in an inner control loop to assign incoming tasks to server pools while, in an outer control loop, a learning scheme adjusts this threshold over time to keep it aligned with the unknown offered load of the system. In a many-server regime, we prove that the learning scheme reaches an equilibrium along intervals of time where the normalized offered load per server pool is suitably bounded, and that this results in a balanced distribution of the load. Furthermore, we establish a similar result when tasks with Coxian distributed service times arrive at a constant rate and the threshold is adjusted using only the total number of tasks in the system. The novel proof technique developed in this paper, which differs from a traditional fluid limit analysis, allows to handle rapid variations of the first learning scheme, triggered by excursions of the occupancy process that have vanishing size. Moreover, our approach allows to characterize the asymptotic behavior of the system with Coxian distributed service times without relying on a fluid limit of a detailed state descriptor."
a76e9d92d64c3d24ae9ce8571f83dac31d59f088,
aedba10b85af6ad15a042cc90ee6d5ca1a092995,"A key option to further increase mobile network capacity is to deploy dense cellular networks (DCNs). This densification of cellular networks raises challenging issues though, as it likely increases the spatial variation and temporal fluctuations in load. To harness the full potential of DCNs, cell selection algorithms must take these varying load conditions into account. In this paper we study the optimal user association in DCNs based on a Linear Program (LP). Since several system parameters tend to be unknown and time-varying in practice, we develop a dynamic, self-organizing, and load-aware cell selection algorithm: the Shadow Price Assignment (SPA) algorithm. Our algorithm realizes an optimal user association without explicit knowledge of the system parameters by using a parsimonious set of dynamically adapted control parameters. We establish convergence of the control parameters under suitable assumptions. For larger systems the convergence may be slower, and we propose a local clustering approach to further improve the user-perceived performance in systems with many APs. Extensive simulations confirm that the SPA algorithm substantially outperforms conventional approaches."
bbc513dc8514d9c2f86d12ca6d2403dccadec408,
d4f664142b65442175b6ee4c28e502f7346f5278,"Emerging 5G networks will play a critical role in enabling a wide range of novel applications through Ultra-Reliable Low-Latency Communication (URLLC) capabilities besides offering even higher throughput to enhanced Mobile Broadband (eMBB) services. Supporting an increasingly heterogeneous set of performance requirements raises a strong need for innovative resource allocation mechanisms which achieve yet greater spectral efficiency and allow for highly agile transmissions, e.g. through puncturing of mini-slots. Motivated by these challenges, we introduce and analyze various joint scheduling schemes which aim to optimize throughput utility for eMBB flows while satisfying the delay requirements of URLLC flows and specifically accounting for channel variations over time and across frequencies. We show how the throughput dynamics of eMBB users can be described in the presence of puncturing of mini-slots for URLLC transmissions, and make several comparisons in terms of overall throughput performance and implementation complexity."
ed81ff51c3a894a5e84064e71a2f5298d81427ae,"A Network Inventory Manager (NIM) is a software solution that scans, processes and records data about all devices in a network. We consider the problem faced by a NIM that can send out a limited number of probes to track changes in a large, dynamic network. The underlying change rate for the Network Elements (NEs) is unknown and may be highly non-uniform. The NIM should concentrate its probe budget on the NEs that change most frequently with the ultimate goal of minimizing the weighted Fraction of Stale Time (wFOST) of the inventory. However, the NIM cannot discover the change rate of a NE unless the NE is repeatedly probed.We develop and analyze two algorithms based on Reinforcement Learning to solve this exploration-vs-exploitation problem. The first is motivated by the Thompson Sampling method and the second is derived from the Robbins-Monro stochastic learning paradigm. We show that for a fixed probe budget, both of these algorithms produce a potentially unbounded improvement in terms of wFOST compared to the baseline algorithm that divides the probe budget equally between all NEs. Our simulations of practical scenarios show optimal performance in minimizing wFOST while discovering the change rate of the NEs."
f20f3234716cc5b3f022d36ad586edbbd7df15f2,"We consider a system with several job types and two parallel server pools. Within the pools the servers are homogeneous, but across pools possibly not in the sense that the service speed of a job may depend on its type as well as the server pool. Immediately upon arrival, jobs are assigned to a server pool, possibly based on (partial) knowledge of their type. In case such knowledge is not available upon arrival, it can however be obtained while the job is in service; as the service progresses, the likelihood that the service speed of this job type is low increases, creating an incentive to execute the job on different, possibly faster, server(s). Two policies are considered: reroute the job to the other server pool, or replicate it there.We determine the effective load per server under both the rerouting and replication policy for completely unknown as well as partly known job types. We also examine the impact of these policies on the stability bound, which is defined as the maximum arrival rate of jobs for which the effective load per server is smaller than one. We demonstrate that the uncertainty in job types may significantly reduce the stability bound, and that for (highly) unbalanced service speeds full replication achieves the largest stability bound. Finally, we discuss how the use of threshold-based policies can help improve the expected latency for completely or partly unknown job types."
fea4cdb3b32c10c8c52eeb7d554bf3c0ab0e1091,"Consider a system of n parallel server pools where tasks arrive as a time-varying Poisson process. The system aims at balancing the load by using an inner control loop with an admission threshold to assign incoming tasks to server pools; as an outer control loop, a learning scheme adjusts this threshold over time in steps of ∆ units, to keep it aligned with the time-varying overall load. If the fluctuations in the normalized load are smaller than ∆, then we prove that the threshold settles for all large enough n and balances the load when ∆ = 1. Our model captures a tradeoff between optimality and stability, since for higher ∆ the degree of balance decreases, but the threshold remains constant under larger load fluctuations. The analysis of this model is mathematically challenging, particularly since the learning scheme relies on subtle variations in the occupancy state of the system which vanish on the fluid scale; the methodology developed in this paper overcomes this hurdle by leveraging the tractability of the specific system dynamics. Strong approximations are used to prove certain dynamical properties which are then used to characterize the behavior of the system, without relying on a traditional fluid-limit analysis."
12bedba011f6973f6876a6dc9d1607eff65b1cee,We consider a system of N servers inter-connected by some underlying graph topology GN . Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate λ. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in GN .
1ce1e740c097b2e17c889421e1aeb05bb66937e8,"In this paper we study the performance of a bipartite network in which customers arrive at the nodes of the network, but not all nodes are able to serve their customers at all times. Each node can be either active or inactive, and two nodes connected by a bond cannot be active simultaneously. This situation arises in wireless random-access networks where, due to destructive interference, stations that are close to each other cannot use the same frequency band. 
We consider a model where the network is bipartite, the active nodes switch themselves off at rate 1, and the inactive nodes switch themselves on at a rate that depends on time and on which half of the bipartite network they are in. An inactive node cannot become active when one of the nodes it is connected to by a bond is active. The switching protocol allows the nodes to share activity among each other. In the limit as the activation rate becomes large, we compute the crossover time between the two states where one half of the network is active and the other half is inactive. This allows us to assess the overall activity of the network depending on the switching protocol. Our results make use of the metastability analysis for hard-core interacting particle models on bipartite graphs derived in an earlier paper. They are valid for a large class of bipartite networks, subject to certain assumptions. Proofs rely on a comparison with switching protocols that are not time-varying, through coupling techniques."
34ddd80a9184f30191a1a9cc06bc2d60fe157aeb,"We examine a canonical scenario where several wireless data sources generate sporadic delay-sensitive messages that need to be transmitted to a common access point. The access point operates in a time-slotted fashion, and can instruct the various sources in each slot with what probability to transmit a message, if they have any. When several sources transmit simultaneously, the access point can detect a collision, but is unable to infer the identities of the sources involved. While the access point can use the channel activity observations to obtain estimates of the queue states at the various sources, it does not have any explicit queue length information otherwise."
3a97c186757c0530d0ab99b4875d6e0af6f56ebf,"Load balancing algorithms play a vital role in enhancing performance in data centers and cloud networks. Due to the massive size of these systems, scalability challenges, and especially the communication overhead associated with load balancing mechanisms, have emerged as major concerns. Motivated by these issues, we introduce and analyze a novel class of load balancing schemes where the various servers provide occasional queue updates to guide the load assignment. We show that the proposed schemes strongly outperform JSQ(d) strategies with comparable communication overhead per job, and can achieve a vanishing waiting time in the many-server limit with just one message per job, just like the popular JIQ scheme. The proposed schemes are particularly geared however towards the sparse feedback regime with less than one message per job, where they outperform corresponding sparsified JIQ versions. We investigate fluid limits for synchronous updates as well as asynchronous exponential update intervals. The fixed point of the fluid limit is identified in the latter case, and used to derive the queue length distribution. We also demonstrate that in the ultra-low feedback regime the mean stationary waiting time tends to a constant in the synchronous case, but grows without bound in the asynchronous case."
420f7eb5e54727c7f84d4d05b7f6ba0cc2ba3feb,
5ec62e251c060b9b0ce70bc374a0ed64dd0bbe37,We consider the problem of provisioning slices in 5G cellular networks and study two MAC scheduling methods for achieving slice rate and resource share constraints. The first uses an additive adjustment to the standard Proportional Fair scheduler weight and optimizes utility subject to the slice constraints. The second uses a multiplicative adjustment and achieves fairness between the users within a slice in addition to satisfying the slice constraints. We provide a comprehensive analysis of the tradeoffs between utility and fairness for the simple case of constant channel rates and use simulations to illustrate that similar effects hold for typical 3gpp channel models.
6a77f442c357acdc37fa498267856097ae39d896,
6ef092a4ff388bb036512a6d1d4e590fee75c848,"Fuelled by the proliferation of smartphones, wireless traffic has experienced huge growth, which will continue and exacerbate the capacity strain in cellular networks. Network densification has emerged as a powerful paradigm to boost spectral efficiency and accommodate the continual rise in demand for wireless capacity. These dense networks also make resource allocation more challenging though, as they result in more irregular cells with possibly overlapping coverage areas and greater variability in traffic loads. To deal with these load imbalances, there are typically two methods to dynamically match capacity and demand: ""bring users to capacity"" (user association) and ""bring capacity to the users"" (frequency/spectrum allocation). In this paper we study the joint operation of load-aware dynamic user association and frequency allocation algorithms in dense cellular networks. Motivated by a joint load-balancing optimization problem, we consider load-aware algorithms that operate using load measurements at the access points (APs) and can react to changing load conditions without knowledge of difficult-to-obtain system parameters like arrival patterns of users. We present extensive simulation results for various parameter settings, allowing for the user association and frequency allocation algorithms to operate on different time scales. The simulation results show that the joint operation of these algorithms leads to excellent performance without the need to provide an initialized and optimized frequency allocation."
756e54cab813a3348af5733e3ea8cc94610b9258,"We examine a queue-based random-access algorithm where activation and deactivation rates are adapted as functions of queue lengths. We establish its heavy traffic behavior on a complete interference graph, which turns out to be nonstandard in two respects: (1) the scaling depends on some parameter of the algorithm and is not the N/N2 scaling usually found in functional central limit theorems; (2) the heavy traffic limit is deterministic. We discuss how this nonstandard behavior arises from the idleness induced by the distributed nature of the algorithm. In order to prove our main result, we develop a new method for obtaining a fully coupled stochastic averaging principle."
78f422ced8d7f2d487280e833dc5a2e440d1994a,
84ce0a437ad9576ee3653066c89cc96f1d3de011,"Load balancing algorithms play a vital role in enhancing performance in data centers and cloud networks. Due to the massive size of these systems, scalability challenges, and especially the communication overhead associated with load balancing mechanisms, have emerged as major concerns. Motivated by these issues, we introduce and analyze a novel class of load balancing schemes where the various servers provide occasional queue updates to guide the load assignment. We show that the proposed schemes strongly outperform JSQ( d ) strategies with comparable communication overhead per job, and can achieve a vanishing waiting time in the many-server limit with just one message per job, just like the popular JIQ scheme. The proposed schemes are particularly geared however towards the sparse feedback regime with less than one message per job, where they outperform corresponding sparsified JIQ versions. We investigate fluid limits for synchronous updates as well as asynchronous exponential update intervals. The fixed point of the fluid limit is identified in the latter case, and used to derive the queue length distribution. We also demonstrate that in the ultra-low feedback regime the mean stationary waiting time tends to a constant in the synchronous case, but grows without bound in the asynchronous case."
9fe39720ee04279ab828894d089a13ecea0b255e,"Network densification has emerged as a powerful paradigm to boost spectral efficiency and accommodate the continual rise in demand for wireless capacity. In dense cell deployments however, overlapping coverage areas may cause highly varying interference conditions among different cells. Moreover, denser networks experience more temporal load fluctuations due to daily and hourly changing usage patterns. The currently applied universal reuse frequency allocation is not suitable to deal with these issues, and needs to be tailored to dense cell deployments to ensure adequate performance in such scenarios. In this paper we present a dynamic, load aware and self-adapting frequency allocation scheme designed for dense cellular networks: the DyCRA scheme (Dynamic Cost/Reward based Allocation). The scheme makes decisions based on cost-reward trade-offs: rewards arise in the form of capacity, and costs arise in the form of interference (under spatial reuse). We quantify these costs and rewards based on SINR, and use periodic load estimates to determine if access points are in need of extra frequencies, or can spare them, and the cost/reward structure is used to determine which frequencies are allocated or released. Extensive simulation results show that the DyCRA scheme provides efficient resource allocations that adapt to changing traffic conditions and yields significant performance gains in scenarios with nonstationary traffic demands."
b87d9b95a67ade7ca78755815aac0e6e0ca5dbf0,"Load balancing algorithms play a vital role in enhancing performance in data centers and cloud networks. Due to the massive size of these systems, scalability challenges, and especially the communication overhead associated with load balancing mechanisms, have emerged as major concerns. Motivated by these issues, we introduce and analyze a novel class of load balancing schemes where the various servers provide occasional queue updates to guide the load assignment. We show that the proposed schemes strongly outperform JSQ(d) strategies with comparable communication overhead per job, and can achieve a vanishing waiting time in the many-server limit with just one message per job, just like the popular JIQ scheme. The proposed schemes are particularly geared however towards the sparse feedback regime with less than one message per job, where they outperform corresponding sparsified JIQ versions. We investigate fluid limits for synchronous updates as well as asynchronous exponential update intervals. The fixed point of the fluid limit is identified in the latter case, and used to derive the queue length distribution. We also demonstrate that in the ultralow feedback regime the mean stationary waiting time tends to a constant in the synchronous case, but grows without bound in the asynchronous case."
e1bf7f1bcaaedd64e79d62503700df1125072bc7,"Network slicing provides a key functionality in emerging 5G networks, and offers flexibility in creating customized virtual networks and supporting different services on a common physical infrastructure. This capability critically relies on a MAC scheduler to deliver performance targets in terms of aggregate rates or resource shares for the various slices.A crucial challenge is to enforce such guarantees and performance isolation while allowing flexible sharing to avoid resource fragmentation and fully harness channel variations. In the present paper we propose a MAC scheduler which meets these objectives and preserves the basic structure of utility-based schedulers such as the Proportional Fair algorithm in terms of per-user scheduling metrics. Specifically, the proposed scheme involves counters tracking the aggregate rate or resource allocations for the various slices against pre-specified targets, and computes offsets to the scheduling metrics accordingly. This design provides transparency with respect to other scheduling modules, such as link adaptation and beam-forming.We analytically establish that the proposed scheme achieves optimal overall throughput utility subject to the various slicing constraints. In addition, extensive 3GPP-compliant simulation experiments are conducted to assess the impact on best-effort applications and demonstrate substantial gains in overall throughput utility over baseline approaches."
109a6a208709d5fe214b1b40a1cc2a93e68e7b16,"The basic load balancing scenario involves a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. We discuss recent advances on scalable load balancing schemes which provide favorable delay performance when $N$ grows large, and yet only require minimal implementation overhead. 
Join-the-Shortest-Queue (JSQ) yields vanishing delays as $N$ grows large, as in a centralized queueing arrangement, but involves a prohibitive communication burden. In contrast, power-of-$d$ or JSQ($d$) schemes that assign an incoming task to a server with the shortest queue among $d$ servers selected uniformly at random require little communication, but lead to constant delays. In order to examine this fundamental trade-off between delay performance and implementation overhead, we consider JSQ($d(N)$) schemes where the diversity parameter $d(N)$ depends on $N$ and investigate what growth rate of $d(N)$ is required to asymptotically match the optimal JSQ performance on fluid and diffusion scale. 
Stochastic coupling techniques and stochastic-process limits play an instrumental role in establishing the asymptotic optimality. We demonstrate how this methodology carries over to infinite-server settings, finite buffers, multiple dispatchers, servers arranged on graph topologies, and token-based load balancing including the popular Join-the-Idle-Queue (JIQ) scheme. In this way we provide a broad overview of the many recent advances in the field. This survey extends the short review presented at ICM 2018 (arXiv:1712.08555)."
2122f90042fe496eb13a03f1fc633a7a1ef3989b,"The development of 5G wireless networking is flourishing, introducing major paradigm shifts and key new technologies such as Radio- over-Fiber (RoF). A fundamental concept for achieving the 5G design objectives is dynamically allocating frequencies, sub-bands and wavelengths. While these allocation problems have been extensively studied in wireless and optical domains in isolation, the combination has received little attention so far. However, with the advances in software-defined radio access networking and RoF technologies, there is increasing need and scope for joint optimization across the two domains, in order to harness the full potential of these networks. Motivated by these observations, we introduce a model for jointly optimal sub-band and wavelength allocation in pico-cell networks where virtual access points (APs) transmit via remote radio heads (RRHs) connected through RoF technology. We specifically account for the optical network topology, which introduces a distinct set of constraints in both the optical and wireless domain. Since the resulting load balancing problem is NP-hard, we introduce a heuristic for obtaining a stabilizing allocation which provides all RRHs with sufficient spectrum capacity to deal with their load. Numerical experiments demonstrate that the heuristic provides near-optimal solutions."
349d727ad60e742478111d0ed65b6f02700efb3c,"We consider a system of N ~servers inter-connected by some underlying graph topology~G N . Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate lambda. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in G N . The above model arises in the context of load balancing in large-scale cloud networks and data centers, and has been extensively investigated in the case G N is a clique. Since the servers are exchangeable in that case, mean-field limits apply, and in particular it has been proved that for any lambda < 1, the fraction of servers with two or more tasks vanishes in the limit as N -> ınfty. For an arbitrary graph G N , mean-field techniques break down, complicating the analysis, and the queue length process tends to be worse than for a clique. Accordingly, a graph G N is said to be N -optimal or ∞N-optimal when the queue length process on G N is equivalent to that on a clique on an N -scale or ∞N-scale, respectively. We prove that if G N is an Erdos-Rényi random graph with average degree d(N), then with high probability it is N -optimal and ∞N-optimal if d(N) -> ınfty$ and d(N) / (∞N łog(N)) -> ınfty as N -> ınfty, respectively. This demonstrates that optimality can be maintained at N -scale and ∞N-scale while reducing the number of connections by nearly a factor N and ∞N/ łog(N) compared to a clique, provided the topology is suitably random. It is further shown that if G N contains Θ(N) bounded-degree nodes, then it cannot be N -optimal. In addition, we establish that an arbitrary graph G N is N -optimal when its minimum degree is N - o(N), and may not be N -optimal even when its minimum degree is c N + o(N) for any 0 < c < 1/2. Simulation experiments are conducted for various scenarios to corroborate the asymptotic results."
461d6b0f7aaf32856e620db234d6ac0d00756031,"As the Internet-of-Things (IoT) emerges, connecting immense numbers of sensors and devices, the continual growth in wireless communications increasingly manifests itself in terms of a larger and denser population of nodes with intermittent traffic patterns. A crucial issue that arises in these conditions is how to set the activation rates as a function of the network density and traffic intensity. Depending on the scaling of the activation rates, dense node populations may either result in excessive activations and potential collisions, or long delays that may increase with the number of nodes, even at low load. Motivated by the above issues, we examine optimal activation rate scalings in ultra-dense networks with intermittent traffic sources. We establish stability conditions, and provide closed-form expressions which indicate that the mean delay is roughly inversely proportional to the nominal activation rate. We also discuss a multi-scale mean-field limit, and use the associated fixed point to determine the buffer content and delay distributions. The results provide insight in the scalings that minimize the delay while preventing excessive activation attempts. Extensive simulation experiments demonstrate that the mean-field asymptotics yield highly accurate approximations, even when the number of nodes is moderate."
47827c4e429733fcb7e95a8f4dc89daeb1d34432,
47e70e05fcf0dc0d26fa3f4eadeb5339af8ed2bf,"We examine a canonical scenario where several wireless data sources generate sporadic delay-sensitive messages that need to be transmitted to a common access point. The access point operates in a time-slotted fashion, and can instruct the various sources in each slot with what probability to transmit a message, if they have any. When several sources transmit simultaneously, the access point can detect a collision, but is unable to infer the identities of the sources involved. While the access point can use the channel activity observations to obtain estimates of the queue states at the various sources, it does not have any explicit queue length information otherwise. We explore the achievable delay performance in a regime where the number of sources n grows large while the relative load remains fixed. This scaling is particularly pertinent in Internet of Things (IoT) scenarios, where a key challenge is to achieve low delay when the overall traffic activity is dispersed across massive numbers of highly intermittent sources. We establish that, under any medium access algorithm without queue state information, the average delay must be at least of the order of n slots when the load exceeds some threshold λ* < 1$. This demonstrates that bounded delay can only be achieved if a positive fraction of the system capacity is sacrificed. Furthermore, we introduce a scalable Two-Phase algorithm for low-delay IoT applications which achieves a delay upper bounded uniformly in n when the load is below e-1, and a delay of the order of n slots when the load is between e-1 and 1. Additionally, this algorithm provides robustness against correlated source activity, which is also prevalent in IoT scenarios."
4989ea55b24a7d6630c2ea273441f7d248711c33,"Network densification has emerged as a powerful paradigm to boost spectral efficiency and accommodate the continual rise in demand for wireless capacity. The corresponding reduction in cell sizes also results however in greater spatial and temporal uncertainty and variation in traffic patterns and more extreme and unpredictable interference conditions. These features create unprecedented challenges for efficient allocation of spectral resources compared to conventional cellular networks. As a further challenge, the allocation of spectral resources needs to be jointly optimized with the assignment of wavelengths in the optical backhaul of Radio-over-Fiber (RoF) networks, which are increasingly used in dense deployments and indoor environments. Motivated by these issues, we develop online algorithms for joint radio frequency and optical wavelength assignment in RoF networks. The proposed algorithms rely on load measurements at the various access points, and involve configurable thresholds for triggering (re)assignment of spectral resources. We provide a detailed specification of a system implementation, and conduct extensive simulation experiments to examine the behaviour in various scenarios and assess the impact of key parameters. The results in particular demonstrate that the proposed algorithms are capable of maintaining adequate load levels for spatially heterogeneous and time-varying traffic conditions, while providing favourable throughput performance."
6642b3e1742280edb2275ce60bc9f703534e9a1c,"We consider job dispatching in systems with N parallel servers, where jobs arrive according to a Poisson process of rate λ. In redundancy-d policies, replicas of an arriving job are assigned to d≤N servers selected uniformly at random (without replacement) with the objective to reduce the delay. We introduce a quite general workload model, in which job sizes have some probability distribution while the speeds (slowdown factors) of the various servers for a given job are allowed to be inter-dependent and non-identically distributed. This allows not only for inherent speed differences among different servers, but also for affinity relations. We further propose two novel redundancy policies, so-called deltaprobe- d policies, where d probes of a fixed, small, size ­ are created for each incoming job, and assigned to d servers selected uniformly at random. As soon as the first of these d probe tasks finishes, the actual job is assigned for execution with the same speed - to the corresponding server and the other probe tasks are abandoned. We also consider a delta-probe-d policy in which the probes receive preemptiveresume priority over regular jobs. The aim of these policies is to retain the benefits of redundancy-d policies while accounting for systematic speed differences and mitigating the risks of running replicas of the full job simultaneously for long periods of time."
6fce9d7fdc75796f7afe38c33dddc760757211dc,
703d10a6f9b0c87116f7b08e926bcf30d49ca707,"Intelligent Transportation Systems (ITSs) are envisioned to play a critical role in improving traffic flow and reducing congestion, which is a pervasive issue impacting urban areas around the globe. Rapidly advancing vehicular communication and edge cloud computation technologies provide key enablers for smart traffic management. However, operating viable real-time actuation mechanisms on a practically relevant scale involves formidable challenges, e.g., policy iteration and conventional Reinforcement Learning (RL) techniques suffer from poor scalability due to state space explosion. Motivated by these issues, we explore the potential for Deep Q-Networks (DQN) to optimize traffic light control policies. As an initial benchmark, we establish that the DQN algorithms yield the ""thresholding"" policy in a single-intersection. Next, we examine the scalability properties of DQN algorithms and their performance in a linear network topology with several intersections along a main artery. We demonstrate that DQN algorithms produce intelligent behavior, such as the emergence of ""greenwave"" patterns, reflecting their ability to learn favorable traffic light actuations."
87fb0e9ae1f5c7daaf7354eafa93e78f183a3269,"Random-access algorithms such as the CSMA protocol provide a popular mechanism for distributed medium access control in wireless networks. In saturated-buffer scenarios the joint activity process in such random-access networks has a product-form stationary distribution which provides useful throughput estimates for persistent traffic flows. However, these results do not capture the relevant performance metrics in unsaturated-buffer scenarios, which in particular arise in an IoT context with highly intermittent traffic sources. Mean-field analysis has emerged as a powerful approach to obtain tractable performance estimates in such situations, and is not only mathematically convenient, but also relevant as wireless networks grow larger and denser with the emergence of IoT applications. A crucial requirement for the classical mean-field framework to apply however is that the node population can be partitioned into a finite number of classes of statistically indistinguishable nodes. The latter condition is a severe restriction since nodes typically have different locations and hence experience different interference constraints. Motivated by the above observations, we develop in the present paper a novel mean-field methodology which does not rely on any exchangeability property. Since the spatiotemporal evolution of the network can no longer be described through a finite-dimensional population process, we adopt a measure-valued state description, and prove that the latter converges to a deterministic limit as the network grows large and dense. The limit process is characterized in terms of a system of partial-differential equations, which exhibit a striking local-global-interaction and time scale separation property. Specifically, the queueing dynamics at any given node are only affected by the global network state through a single parsimonious quantity. The latter quantity corresponds to the fraction of time that no activity occurs within the interference range of that particular node in case of a certain static spatial activation measure. Extensive simulation experiments demonstrate that the solution of the partial-differential equations yields remarkably accurate approximations for the queue length distributions and delay metrics, even when the number of nodes is fairly moderate."
94209ac2d3a71d9ef4b8d9f52fa80426bf4b622b,
99ba1acf7d1e524c5a86266640e1a9108773af9e,
bdf9bcf01779c5b0853dae5a9bb8764db3ca323a,"We consider load balancing in service systems with affinity relations between jobs and servers. Specifically, an arriving job can be allocated to a fast, primary server from a particular selection associated with this job or to a secondary server to be processed at a slower rate. Such job-server affinity relations can model network topologies based on geographical proximity, or data locality in cloud scenarios. We introduce load balancing schemes that allocate jobs to primary servers if available, and otherwise to secondary servers. A novel coupling construction is developed to obtain stability conditions and performance bounds using a coupling technique. We also conduct a fluid limit analysis for symmetric model instances, which reveals a delicate interplay between the model parameters and load balancing performance."
e21d88158dde576ec45a220055caf912c6adb7d0,"The Blockchain paradigm provides a popular mechanism for establishing trust and consensus in distributed environments. While Blockchain technology is currently primarily deployed in crypto-currency systems like Bitcoin, the concept is also expected to emerge as a key component of the Internet-of-Things (IoT), enabling novel applications in digital health, smart energy, asset tracking and smart transportation. As Blockchain networks evolve to industrial deployments with large numbers of geographically distributed nodes, the block transfer and processing delays arise as a critical issue which may create greater potential for forks and vulnerability to adversarial attacks. Motivated by these issues, we develop stochastic network models to capture the Blockchain evolution and dynamics and analyze the impact of the block dissemination delay and hashing power of the member nodes on Blockchain performance in terms of the overall block generation rate and required computational power for launching a successful attack. The results provide useful insight in crucial design issues, e.g., how to adjust the ‘difficulty-of-work’ in the presence of delay so as to achieve a target block generation rate or appropriate level of immunity from adversarial attacks. We employ a combination of analytical calculations and simulation experiments to investigate both stationary and transient performance features, and demonstrate close agreement with measurements on a wide-area network testbed running the Ethereum protocol."
eb47e0091f570230c31a553c2585792effb41b6b,"We examine a canonical scenario where several wireless data sources generate sporadic delay-sensitive messages that need to be transmitted to a common access point. The access point operates in a time-slotted fashion, and can instruct the various sources in each slot with what probability to transmit a message, if they have any. When several sources transmit simultaneously, the access point can detect a collision, but is unable to infer the identities of the sources involved. While the access point can use the channel activity observations to obtain estimates of the queue states at the various sources, it does not have any explicit queue length information otherwise. We explore the achievable delay performance in a regime where the number of sources n grows large while the relative load remains fixed. We establish that, under any medium access algorithm without queue state information, the average delay must be at least of the order of n slots when the load exceeds some threshold lambda* < 1. This demonstrates that bounded delay can only be achieved if a positive fraction of the system capacity is sacrificed. Furthermore, we introduce a scalable Two-Phase algorithm which achieves a delay upper bounded uniformly in n when the load is below e -1 , and a delay of the order of n slots when the load is between e -1 and 1. Additionally, this algorithm provides robustness against correlated source activity."
f34486e06dde2dd2b5f3d993c215f047f92d7eec,
fee4de0eb08c4bd206dd2e3715844ac523ab75ce,
2a448bd403b25089a7fb0f07655f740f7275cecc,"A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server utilization and limit energy consumption, while providing excellent user-perceived performance in the presence of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically adjusting service capacity in response to demand while meeting performance targets, and queue-driven auto-scaling techniques have been widely investigated in the literature. In typical data center architectures and cloud environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation burden, or may not even be viable at all. Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not require any global queue length information or explicit knowledge of system parameters, and yet provides provably near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do take full advantage of a centralized queue."
45d2e6c24c77a66ad5b55fada65e1fa9fe18661c,"Most load balancing techniques implemented in current data centers tend to rely on a mapping from packets to server IP addresses through a hash value calculated from the flow five-tuple. The hash calculation allows extremely fast packet forwarding and provides flow `stickiness', meaning that all packets belonging to the same flow get dispatched to the same server. Unfortunately, such static hashing may not yield an optimal degree of load balancing, e.g. due to variations in server processing speeds or traffic patterns. On the other hand, dynamic schemes, such as the Join-the-Shortest-Queue (JSQ) scheme, provide a natural way to mitigate load imbalances, but at the expense of stickiness violation.In the present paper we examine the fundamental trade-off between stickiness violation and packet-level latency performance in large-scale data centers. We establish that stringent flow stickiness carries a significant performance penalty in terms of packet-level delay. Moreover, relaxing the stickiness requirement by a minuscule amount is highly effective in clipping the tail of the latency distribution. We further propose a bin-based load balancing scheme that achieves a good balance among scalability, stickiness violation and packet-level delay performance. Extensive simulation experiments corroborate the analytical results and validate the effectiveness of the bin-based load balancing scheme."
625507bf2049e987a3416acd7c3e84722f752c89,"Cascading failure models are typically used to capture the phenomenon where failures possibly trigger further failures in succession, causing knock-on effects. In many networks this ultimately leads to a disintegrated network where the failure propagation continues independently across the various components. In order to gain insight in the impact of network splitting on cascading failure processes, we extend a well-established cascading failure model for which the number of failures obeys a power-law distribution. We assume that a single line failure immediately splits the network in two components, and examine its effect on the power-law exponent. The results provide valuable qualitative insights that are crucial first steps towards understanding more complex network splitting scenarios."
7155522183c51d7a9178ee1d5b20351c3220ebd2,"Load balancing algorithms play a crucial role in delivering robust application performance in data centers and cloud networks. Recently, strong interest has emerged in Join-the-Idle-Queue (JIQ) algorithms, which rely on tokens issued by idle servers in dispatching tasks and outperform power-of-d policies. Specifically, JiQ strategies involve minimal information exchange, and yet achieve zero blocking and wait in the many-server limit. The latter property prevails in a multiple-dispatcher scenario when the loads are strictly equal among dispatchers. For various reasons it is not uncommon however for skewed load patterns to occur. We leverage product-form representations and fluid limits to establish that the blocking and wait then no longer vanish, even for arbitrarily low overall load. Remarkably, it is the least-loaded dispatcher that throttles tokens and leaves idle servers stranded, thus acting as bottleneck. Motivated by the above issues, we introduce two enhancements of the ordinary JIQ scheme where tokens are either distributed non-uniformly or occasionally exchanged among the various dispatchers. We prove that these extensions can achieve zero blocking and wait in the many-server limit, for any subcritical overall load and arbitrarily skewed load profiles. Extensive simulation experiments demonstrate that the asymptotic results are highly accurate, even for moderately sized systems."
83504752f7b1d8591a0d2b7feae5c3f275d8a6e2,"A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server utilization and limit energy consumption, while providing excellent user-perceived performance in the presence of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically adjusting service capacity in response to demand while meeting performance targets, and queue-driven auto-scaling techniques have been widely investigated in the literature. In typical data center architectures and cloud environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation burden, or may not even be viable at all. Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not require any global queue length information or explicit knowledge of system parameters, and yet provides provably near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do take full advantage of a centralized queue."
8b52a1fff81cd20ea4801779fd029cf8ef7946f0,"We present an overview of scalable load balancing algorithms which provide favorable delay performance in large-scale systems, and yet only require minimal implementation overhead. Aimed at a broad audience, the paper starts with an introduction to the basic load balancing scenario, consisting of a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. 
A popular class of load balancing algorithms are so-called power-of-$d$ or JSQ($d$) policies, where an incoming task is assigned to a server with the shortest queue among $d$ servers selected uniformly at random. This class includes the Join-the-Shortest-Queue (JSQ) policy as a special case ($d = N$), which has strong stochastic optimality properties and yields a mean waiting time that vanishes as $N$ grows large for any fixed subcritical load. However, a nominal implementation of the JSQ policy involves a prohibitive communication burden in large-scale deployments. In contrast, a random assignment policy ($d = 1$) does not entail any communication overhead, but the mean waiting time remains constant as $N$ grows large for any fixed positive load. 
In order to examine the fundamental trade-off between performance and implementation overhead, we consider an asymptotic regime where $d(N)$ depends on $N$. We investigate what growth rate of $d(N)$ is required to match the performance of the JSQ policy on fluid and diffusion scale. The results demonstrate that the asymptotics for the JSQ($d(N)$) policy are insensitive to the exact growth rate of $d(N)$, as long as the latter is sufficiently fast, implying that the optimality of the JSQ policy can asymptotically be preserved while dramatically reducing the communication overhead. We additionally show how the communication overhead can be reduced yet further by the so-called Join-the-Idle-Queue scheme, leveraging memory at the dispatcher."
978f78c363fbe62e7dfbf70b06c0be14a1acc8d5,"Fuelled by the proliferation of smartphones, wireless traffic has experienced huge growth, which will continue with the emergence of ultra-broadband 5G applications, and exacerbate the capacity strain in cellular networks. Deployment of pico access points, reducing cell sizes and allowing more efficient reuse of limited radio spectrum, provides a powerful approach to cope with traffic hot spots and bring capacity relief. This network densification makes cell planning more challenging though, and tends to result in more irregular cells with possibly overlapping coverage areas and greater variability in traffic loads. This raises a critical need for more intelligent cell selection algorithms, which not only take signal strength values into account, but also load conditions in order to harness the full potential of the pico-cells. In the present paper we analyse online cell selection algorithms that use a parsimonious set of load-driven control parameters to determine an optimal user association in a measurement-based manner, without requiring explicit knowledge of the system parameters. We exploit stochastic approximation techniques to establish the convergence of the control parameters to the optimal values. Extensive simulation experiments for scenarios with many pico access points confirm that the algorithms are quite effective in optimally balancing the traffic loads in hot spot areas, and further demonstrate that they substantially outperform conventional approaches in terms of service denials and low throughput percentiles. We consider several implementation options and evaluate the relative benefits and potential tradeoffs."
b9ab688b5efb12057b8965082cae1190e5699bbb,"We consider a system of N servers inter-connected by some underlying graph topology GN. Tasks with unit-mean exponential processing times arrive at the various servers as independent Poisson processes of rate λ. Each incoming task is irrevocably assigned to whichever server has the smallest number of tasks among the one where it appears and its neighbors in GN. The above model arises in the context of load balancing in large-scale cloud networks and data centers, and has been extensively investigated in the case GN is a clique. Since the servers are exchangeable in that case, mean-field limits apply, and in particular it has been proved that for any λ < 1, the fraction of servers with two or more tasks vanishes in the limit as N → ∞. For an arbitrary graph GN, mean-field techniques break down, complicating the analysis, and the queue length process tends to be worse than for a clique. Accordingly, a graph GN is said to be N-optimal or √N-optimal when the queue length process on GN is equivalent to that on a clique on an N-scale or √N-scale, respectively. We prove that if GN is an Erdöo s-Rényi random graph with average degree d(N), then with high probability it is N-optimal and √N-optimal if d(N) → ∞ and d(N)/√Nlog(N)) → ∞ as N → ∞, respectively. This demonstrates that optimality can be maintained at N-scale and √N-scale while reducing the number of connections by nearly a factor N and √N/log(N) compared to a clique, provided the topology is suitably random. It is further shown that if GN contains Θ(N) bounded-degree nodes, then it cannot be N-optimal. In addition, we establish that an arbitrary graph GN is N-optimal when its minimum degree is N - o(N), and may not be N-optimal even when its minimum degree is cN + o(N) for any 0 < c < 1/2. Simulation experiments are conducted for various scenarios to corroborate the asymptotic results."
bc8a6035cbba208ef369c86091f06ab774736b13,"Random-access algorithms such as the CSMA protocol provide a popular mechanism for distributed medium access control in large-scale wireless networks. Mean-field analysis has emerged as a convenient approach to obtain tractable performance estimates in such networks, but a critical limitation of the classical set-up is that all nodes are assumed to belong to a finite number of classes. We consider spatial mean-field limits which do not involve such a requirement, characterized in terms of a set of partial-differential equations, and in particular examine the fixed points of these equations for some specific network configurations. We discuss how the fixed points can be used to obtain estimates for key performance metrics, and present simulation experiments to demonstrate the accuracy of these estimates."
bffb58bfd7283e420135dc256a4d7828090851bd,"As electric transmission networks continue to increase in complexity and volatility, there is a growing potential for cascading failure effects to cause major blackouts. Understanding these effects and assessing the risks involved is of critical importance in operating the electric grid and maintaining high reliability. Analysis of empirical data suggests that blackout sizes obey a power-law with exponents that vary across data sets. For a particular macroscopic cascading failure model, such power-law behavior was also observed with one specific exponent. Motivated by the variation in the exponents revealed by empirical blackout data, we extend this cascading failure model with a network splitting mechanism. We demonstrate the impact of the latter feature on the power-law exponent of the blackout size. Moreover, we identify the most likely scenario for a severe blackout to occur. These insights provide crucial steps towards a deeper understanding of more complex network splitting scenarios."
d7924d463dac217faefd704c91f707619e3d8319,"Load balancing mechanisms and scheduling algorithms play a critical role in achieving efficient server utilization and providing robust delay performance in a wide range of networked systems. We will review some celebrated schemes and optimality results which typically assume that detailed state information, e.g. exact knowledge of queue lengths, is available in assigning jobs to queues or allocating a shared resource among competing users. In practice, however, obtaining such state information is non-trivial, and usually involves a significant communication overhead or delay, which is particularly a concern in large-scale networked systems with massive numbers of queues. These scalability issues have prompted increasing attention for the implementation complexity of load balancing and scheduling algorithms as a crucial design criterion, besides the traditional performance metrics. In this talk we examine the delay performance in such networks for various load balancing and scheduling algorithms, in conjunction with the associated implementation overhead. In the first part of the talk we focus on a scenario with a single dispatcher where jobs arrive that need to be assigned to one of several parallel queues. In the second part of the talk we turn to a system with a single resource, e.g. a shared wireless transmission medium, which is to be allocated among several nodes. We will specifically explore the delay scaling properties in a mean-field framework where the total load and service capacity grow large in proportion. The mean-field regime not only offers analytical tractability, but is also highly relevant given the immense numbers of servers in data centers and cloud networks, and dense populations of wireless devices and sensors in Internet-of-Things (IoT) applications. Time permitting, we will also discuss the impact of the underlying network structure and a few open research challenges."
e8764a88c39d00e99ccbec45f8cbc0823a2abd06,"We present an overview of scalable load balancing algorithms which provide favorable delay performance in large-scale systems, and yet only require minimal implementation overhead. Aimed at a broad audience, the paper starts with an introduction to the basic load balancing scenario – referred to as the supermarket model – consisting of a single dispatcher where tasks arrive that must immediately be forwarded to one of N single-server queues. The supermarket model is a dynamic counterpart of the classical balls-and-bins setup where balls must be sequentially distributed across bins. A popular class of load balancing algorithms are so-called power-of-d or JSQ(d) policies, where an incoming task is assigned to a server with the shortest queue among d servers selected uniformly at random. As the name reflects, this class includes the celebrated Join-the-Shortest-Queue (JSQ) policy as a special case (d = N ), which has strong stochastic optimality properties and yields a mean waiting time that vanishes asN grows large for any fixed subcritical load. However, a nominal implementation of the JSQ policy involves a prohibitive communication burden in large-scale deployments. In contrast, a simple random assignment policy (d = 1) does not entail any communication overhead, but the mean waiting time remains constant as N grows large for any fixed positive load. In order to examine the fundamental trade-off between delay performance and implementation overhead, we consider an asymptotic regime where the diversity parameter d(N) depends on N . We investigate what growth rate of d(N) is required to match the optimal performance of the JSQ policy on fluid and diffusion scale, and achieve a vanishing waiting time in the limit. The results demonstrate that the asymptotics for the JSQ(d(N)) policy are insensitive to the exact growth rate of d(N), as long as the latter is sufficiently fast, implying that the optimality of the JSQ policy can asymptotically be preserved while dramatically reducing the communication overhead. Stochastic coupling techniques play an instrumental role in establishing the asymptotic optimality and universality properties, and augmentations of the coupling constructions allow these properties to be extended to infinite-server settings and network scenarios. We additionally show how the communication overhead can be reduced yet further by the so-called Join-the-Idle-Queue (JIQ) scheme, leveraging memory at the dispatcher to keep track of idle servers. 1 ar X iv :1 71 2. 08 55 5v 1 [ cs .P F] 2 2 D ec 2 01 7"
eccff893f40b618cb6f0abf7b5ab345e35fe2364,"Emerging 5G networks will not only offer higher link rates, but also integrate a variety of Radio Access Technologies (RATs) in order to provide ultra-reliable broadband access to a wide range of applications with high throughput and low latency requirements. SDN-enabled dynamic path selection is of critical importance in exploiting the collective transmission resources in such heterogeneous multi-RAT environments and delivering excellent user performance. In the present paper we propose the ‘best-rate’ path selection algorithm for multi-RAT networks with various types of traffic flows. The best-rate algorithm accounts for the radio conditions and performance requirements of individual flows as well as the load conditions at the various access points. We analytically establish that the rates received by the various flows under the best-rate path selection, in conjunction with local fair resource sharing at the individual access points, are close to globally Proportional Fair. Detailed simulation experiments demonstrate that the best-rate algorithm achieves significant gains in terms of user-perceived throughput performance over various baseline policies."
1d84a197dbfabb45862009e550b1a83b40745965,"Inspired by reliability issues in electric transmission networks, we use a probabilistic approach to study the occurrence of large failures in a stylized cascading failure model. In this model, lines have random capacities that initially meet the load demands imposed on the network. Every single line failure changes the load distribution in the surviving network, possibly causing further lines to become overloaded and trip as well. An initial single line failure can therefore potentially trigger massive cascading effects, and in this paper we measure the risk of such cascading events by the probability that the number of failed lines exceeds a certain large threshold. Under particular critical conditions, the exceedance probability follows a power-law distribution, implying a significant risk of severe failures. We examine the robustness of the power-law behavior by exploring under which assumptions this behavior prevails."
25fa3c01b5469fd7dedf4d112303cfff6cff59e5,
30edb202ce468a6533f555e4adbdda4b04585de9,"We consider a system of <i>N</i> parallel queues with unit exponential service rates and a single dispatcher where tasks arrive as a Poisson process of rate λ(<i>N</i>). When a task arrives, the dispatcher assigns it to a server with the shortest queue among <i>d</i>(<i>N</i>) ≤ <i>N</i> randomly selected servers. This load balancing policy is referred to as a power-of-<i>d</i>(<i>N</i>) or JSQ(<i>d</i>(<i>N</i>)) scheme, and subsumes the Join-the-Shortest Queue (JSQ) policy as a crucial special case for <i>d</i>(<i>N</i>) = <i>N</i>.
 We construct a coupling to bound the difference in the queue length processes between the JSQ policy and an arbitrary value of <i>d</i>(<i>N</i>). We use the coupling to derive the fluid limit in the regime where λ(<i>N</i>)/<i>N</i> → λ < 1 and <i>d</i>(<i>N</i>)→ ∞ as <i>N</i> → ∞, along with the corresponding fixed point. The fluid limit turns out not to depend on the exact growth rate of <i>d</i>(<i>N</i>), and in particular coincides with that for the JSQ policy. We further leverage the coupling to establish that the diffusion limit in the regime where (<i>N</i>--λ(<i>N</i>))/ √<i>N</i> → β > 0 and <i>d</i>(<i>N</i>)/ √ <i>N</i> log<i>N</i> → ∞ as <i>N</i> → ∞ corresponds to that for the JSQ policy. These results indicate that the stochastic optimality of the JSQ policy can be preserved at the fluid-level and diffusion-level while reducing the overhead by nearly a factor O(<i>N</i>) and O(√ <i>N</i>), respectively."
3b780968ea5df9e6be22ce3effc452ac8120dd40,"We consider a system of N identical server pools and a single dispatcher in which tasks with unit-exponential service requirements arrive at rate [Formula: see text]. In order to optimize the experienced performance, the dispatcher aims to evenly distribute the tasks across the various server pools. Specifically, when a task arrives, the dispatcher assigns it to the server pool with the minimum number of tasks among d(N) randomly selected server pools. We construct a stochastic coupling to bound the difference in the system occupancy processes between the join-the-shortest-queue (JSQ) policy and a scheme with an arbitrary value of d(N). We use the coupling to derive the fluid limit in case [Formula: see text] and [Formula: see text] as [Formula: see text] along with the associated fixed point. The fluid limit turns out to be insensitive to the exact growth rate of d(N) and coincides with that for the JSQ policy. We further establish that the diffusion limit corresponds to that for the JSQ policy as well, as long as [Formula: see text], and characterize the common limiting diffusion process. These results indicate that the JSQ optimality can be preserved at the fluid and diffusion levels while reducing the overhead by nearly a factor O(N) and O([Formula: see text]), respectively."
62023f723ce418a5bdc99e3cf87a69a1116aa00d,"The proliferation of smartphones has unleashed a tremendous growth in wireless traffic, which is projected to continue for the foreseeable future, and put yet greater strain on the capacity of cellular networks. Deployment of pico base stations, reducing cell sizes and allowing more efficient reuse of limited radio spectrum, provides a powerful approach to cope with traffic hot spots and bring capacity relief. This network densification raises a critical need for more intelligent cell selection algorithms, which not only take signal strength values into account, but also load conditions in order to harness the full potential of the pico cells. We describe how the problem of optimally balancing traffic loads among pico cells may be formulated as a linear program (LP), and show how the dual version of the LP provides insight in the structure of the optimal user association. Since the LP formulation involves several system parameters that tend to be time-varying and hard to estimate in practice, the optimal user association can not be calculated in a direct way. Hence, we develop various online cell selection algorithms that solve the LP and determine the optimal user association in a measurement-based manner, without requiring explicit knowledge of the system parameters. Extensive simulation experiments confirm that the algorithms are quite effective in optimally balancing the traffic loads, and further demonstrate that they substantially outperform conventional approaches in terms of user-perceived throughput performance."
67b5157a1f290e47ecef1c1916af279aa7004e91,"We develop a gradient algorithm for optimizing the performance of product-form networks through online adjustment of control parameters. The use of standard algorithms for finding optimal parameter settings is hampered by the prohibitive computational burden of calculating the gradient in terms of the stationary probabilities. The proposed approach instead relies on measuring empirical frequencies of the various states through simulation or online operation so as to obtain estimates for the gradient. Besides the reduction in computational effort, a further benefit of the online operation lies in the natural adaptation to slow variations in ambient parameters as commonly occurring in dynamic environments. On the downside, the measurements result in inherently noisy and biased estimates. We exploit mixing time results in order to overcome the impact of the bias and establish sufficient conditions for convergence to a globally optimal solution. We discuss our algorithm in the context of different systems, including queueing networks, loss networks, and wireless networks. We also illustrate how the algorithm can be used in such systems to optimize a service/cost trade-off, to map parameter regions that lead to systems meeting specified constraints, and to achieve target performance measures. For the latter application, we first identify which performance measures can be controlled depending on the set of configurable parameters. We then characterize the achievable region of performance measures in product-form networks, and finally we describe how our algorithm can be used to achieve the target performance in an online, distributed fashion, depending on the application context."
718b1b4d14092cc44fefe12e6d52b135dc683693,"We establish mean-field limits for large-scale random-access networks with buffer dynamics and arbitrary interference graphs. Although saturated buffer scenarios have been widely investigated and yield useful throughput estimates for persistent sessions, they fail to capture the fluctuations in buffer contents over time and provide no insight in the delay performance of flows with intermittent packet arrivals. Motivated by that issue, we explore in the present paper random-access networks with buffer dynamics, where flows with empty buffers refrain from competition for the medium. The occurrence of empty buffers thus results in a complex dynamic interaction between activity states and buffer contents, which severely complicates the performance analysis. Hence, we focus on a many-sources regime where the total number of nodes grows large, which not only offers mathematical tractability but is also highly relevant with the densification of wireless networks as the Internet of Things emerges. We exploit timescale separation properties to prove that the properly scaled buffer occupancy process converges to the solution of a deterministic initial value problem and establish the existence and uniqueness of the associated fixed point. This approach simplifies the performance analysis of networks with huge numbers of nodes to a low-dimensional fixed-point calculation. For the case of a complete interference graph, we demonstrate asymptotic stability, provide a simple closed form expression for the fixed point, and prove interchange of the mean-field and steady-state limits. This yields asymptotically exact approximations for key performance metrics, in particular the stationary buffer content and packet delay distributions."
7b135422f7e5b86ebe2830248854f58c6273a48a,
95cf55ad64db00665b73b883c90ba820b1a5f233,"ABSTRACT Inspired by reliability issues in electric transmission networks, we use a probabilistic approach to study the occurrence of large failures in a stylized cascading line failure model. Such models capture the phenomenon where an initial line failure potentially triggers massive knock-on effects. Under certain critical conditions, the probability that the number of line failures exceeds a large threshold obeys a power-law distribution, a distinctive property observed in empiric blackout data. In this paper, we examine the robustness of the power-law behavior by exploring under which conditions this behavior prevails."
a298d0d4271853cefa632a7ac2f2769ba53476a2,"Abstract We consider a system of N parallel queues with identical exponential service rates and a single dispatcher where tasks arrive as a Poisson process. When a task arrives, the dispatcher always assigns it to an idle server, if there is any, and to a server with the shortest queue among d randomly selected servers otherwise (1≤d≤N). This load balancing scheme subsumes the so-called join-the-idle queue policy (d=1) and the celebrated join-the-shortest queue policy (d=N) as two crucial special cases. We develop a stochastic coupling construction to obtain the diffusion limit of the queue process in the Halfin‒Whitt heavy-traffic regime, and establish that it does not depend on the value of d, implying that assigning tasks to idle servers is sufficient for diffusion level optimality."
beb2fa2717d4630ea08e9e521fa2285f4f539ad5,"With the rapid advance of the Internet of Everything, both the number of devices and the range of applications that rely on wireless connectivity show huge growth. Driven by these pervasive trends, wireless networks grow in size and complexity, supporting immense numbers of nodes and data volumes, with highly diverse traffic profiles and performance requirements. While well-established methods are available for evaluating the throughput of persistent sessions with saturated buffers, these provide no insight in the delay performance of flows with intermittent packet arrivals. The occurrence of empty buffers in the latter scenario results in a complex interaction between activity states and packet queues, which severely complicates the performance analysis. Motivated by these challenges, we develop a mean-field approach to analyze buffer contents and packet delays in wireless networks in a many-sources regime. The mean-field behavior simplifies the analysis of a large-scale network with packet arrivals and buffer dynamics to a low-dimensional fixed-point calculation for a network with saturated buffers. In particular, the analysis yields explicit expressions for the buffer content and packet delay distribution in terms of the fixed-point solution. Extensive simulation experiments demonstrate that these expressions provide highly accurate approximations, even for a fairly moderate number of sources."
c1b0da6a6c7847ce885be1ed5e7b00be629f6ffa,"Optimal path selection is of critical importance in exploiting the collective transmission resources from heterogeneous Radio Access Technologies (RATs) in emerging 5G networks and delivering excellent user performance. In the present paper we propose the `best-rate' path selection algorithm for a multi-RAT scenario with various types of traffic flows. We demonstrate that the rates received by the various flows under the best-rate algorithm, in conjunction with local fair resource sharing at the individual access points, are close to globally Proportional Fair. Extensive simulations confirm that the proposed algorithm outperforms various baseline policies in terms of user-perceived throughput."
d268169e30f997fc7eddde9f3899dd21d38ed20b,"We consider the problem of optimal rate allocation and admission control for adaptive video streaming sessions in wireless networks with user dynamics. The central aim is to achieve an optimal tradeoff between several key objectives: maximizing the average rate utility per user, minimizing the temporal rate variability, and maximizing the number of users supported. We derive sample path upper bounds for the long-term net utility rate in terms of either a linear program or a concave optimization problem, depending on whether the admissible rate set is discrete or continuous. We then show that the upper bounds are asymptotically achievable in large-scale systems by policies which either deny access to a user or assign it a fixed rate for its entire session, without relying on any advance knowledge of the duration. Moreover, the asymptotically optimal policies exhibit a specific structure, which allow them to be characterized through just a single variable, and have the further property that the induced offered load is unity. We exploit the latter insights to devise parsimonious online algorithms for learning and tracking the optimal rate assignments and establish the convergence of these algorithms. Extensive simulation experiments demonstrate that the proposed algorithms perform well, even in relatively small-scale systems."
df5a4934ddd836ec9c34d0e6214a2ad89063f014,"We consider a system of N identical parallel server pools and a single dispatcher where tasks arrive as a Poisson process. Arriving tasks cannot be queued, and must immediately be assigned to one of the server pools to start execution. The execution times are assumed to be exponentially distributed, and do not depend on the number of tasks contending for service. However, the experienced performance (e.g. in terms of received throughput or packet-level delay) does degrade with an increasing number of concurrent tasks at the same server pool. In order to optimize the performance, the dispatcher therefore aims to evenly distribute the tasks across the various server pools, using either a power-of-d or a threshold-based load balancing scheme. In the power-of-d scheme, an arriving task is assigned to the server pool with the minimum number of active tasks among d(N) randomly selected server pools (1 ≤ d(N) ≤ N). In the threshold-based scheme, an incoming task is dispatched to an arbitrary server pool with fewer than L active tasks, if there is any, to an arbitrary server pool with fewer than H > L tasks otherwise, or to a randomly selected server pool if all server pools have H or more tasks. This scheme can be implemented in a server-driven manner, with O(1) communication overhead per task, as opposed to O(d(N)) in the power-of-d scheme. We derive the fluid-level dynamics for the power-of-d scheme with d(N) → ∞ as N → ∞ and the threshold-based scheme, along with the associated fixed points. As it turns out, the fluid limit for the power-of-d scheme does not depend on the exact growth rate of d(N). We also characterize the diffusion-level behavior of the power-of-d scheme with d(N) ≫ √N log(N), and show that it coincides with that of the threshold-based scheme with suitably selected parameters L and H. In particular, the threshold-based scheme can achieve similar performance as the power-of-d scheme with d(N) ≫ √N log(N), and thus diffusion-level optimality, with only O(1) rather than O(N) communication overhead per task."
e2d363946035f1393fe7b9f97be7e52a05b0af15,
e7553f57a2e2b6bb9b13128b031d3ca533e35377,"We consider a system of $N$ parallel single-server queues with unit exponential service rates and a single dispatcher where tasks arrive as a Poisson process of rate $\lambda(N)$. When a task arrives, the dispatcher assigns it to a server with the shortest queue among $d(N)$ randomly selected servers ($1 \leq d(N) \leq N$). This load balancing strategy is referred to as a JSQ($d(N)$) scheme, marking that it subsumes the celebrated Join-the-Shortest Queue (JSQ) policy as a crucial special case for $d(N) = N$. 
We construct a stochastic coupling to bound the difference in the queue length processes between the JSQ policy and a scheme with an arbitrary value of $d(N)$. We use the coupling to derive the fluid limit in the regime where $\lambda(N) / N \to \lambda 0$ as $N \to \infty$ with $d(N)/(\sqrt{N} \log (N))\to\infty$ corresponds to that for the JSQ policy. These results indicate that the optimality of the JSQ policy can be preserved at the fluid-level and diffusion-level while reducing the overhead by nearly a factor O($N$) and O($\sqrt{N}/\log(N)$), respectively."
2f7f1fbecd7e25419cc9ef420546fb6f47c31bd3,
31d191c6d79993a4cea98f5412d90955fb719451,"The deployment of pico cells to cover traffic hot spots within the footprint of a macro cell provides a powerful approach to meet the massive growth in traffic demands fueled by smartphones and bandwidth-hungry applications. Joint optimization of resource allocation and user association is critical to achieve the maximum capacity benefits and performance gains in such heterogeneous network deployments (HetNets). In order to gain insight in the achievable capacity gains, we examine in the present paper the stability and performance of a HetNet system in the presence of flow-level dynamics. The stability condition reveals that in stationary traffic conditions the maximum capacity can be achieved with a static resource split and traffic association rule, provided that these are suitably selected. This suggests that dynamic adaptation on time scales commensurate with the variations in traffic parameters suffices to extract most of the achievable capacity gains. For the case of static cell boundaries and Proportional Fair scheduling, we also present a method for evaluating the flow-level performance in terms of the distribution of the number of active file transfers and expected transfer delay."
517c22ec6b5f16be4c7824ea71f28399d3e4cd6e,"We consider dense wireless random-access networks, modeled as systems of particles with hardcore interaction. The particles represent the network users that try to become active after an exponential back-off time, and stay active for an exponential transmission time. Due to wireless interference, active users prevent other nearby users from simultaneous activity, which we describe as hardcore interaction on a conflict graph. We show that dense networks with aggressive back-off schemes lead to extremely slow transitions between dominant states, and inevitably cause long mixing times and starvation effects."
6d95bb2e3ad4bbef12f5e8b3885960090c20259c,
804a31dde5ee05167e755107a42c56aec898b988,
8aad4b7ebebf1867a4500054aaa742f34b7ab028,"Distributed algorithms such as CSMA provide a popular mechanism for sharing the transmission medium among competing users in large-scale wireless networks. Conventional models for CSMA that are amenable for analysis assume that users always have packets to transmit. In contrast, when users do not compete for medium access when their buffers are empty, a complex interaction arises between the activity states and the buffer contents. We develop a meanfield approach to investigate this dynamic interaction for networks with many users. We identify a time-scale separation between the evolution of the activity states and the buffer contents, and obtain a deterministic dynamical system describing the network dynamics on a macroscopic scale. The fixed point of the dynamical system yields highly accurate approximations for the stationary distribution of the buffer contents and packet delay, even when the number of users is relatively moderate."
b4d65a07e27b17f38b688d1445342ca64c8d8b3f,
b9878659222124ed7cd82b160b43abcdae4c71ca,
c1f5fb87d079a176772216a34390db3666da9d99,"Mobile operators increasingly share their wireless networks to save cost. Some network parameters in the shared environment are operator-specific whereas others apply globally to all operators so their choice may lead to a disagreement between operators. We suggest that this problem can be solved by a voting system where operators cast weighted votes. We compare two weighting methods, namely the Proportional method and the so-called Penrose method for different operators' shares and benefits. We observe that the Penrose method performs especially well in cases where the foreseeable benefit of a decision is rather ambiguous for operators (in contrast, the distribution of operators' shares hardly influences which method is better). Even when operators' benefits of a decision are biased in one or the other direction according to a binomial distribution, the Penrose method performs better than the Proportional method overall. These findings are particularly relevant for multi-operator self-organizing networks (SON)."
c8019b1b66da3d7b8efdf09bcf31e4debb7a9654,"This paper considers the problem of server-side scheduling for jobs composed of multiple pieces with consecutive (progressive) deadlines. One example is server-side scheduling for video service, where clients request flows of content from a server with limited capacity, and any content not delivered by its deadline is lost. We consider the simultaneous goals of 1) minimizing overall loss, and 2) differentiating loss fractions across classes of flows in proportion to relative weights. State-of-the-art policies, like Discriminatory Processor Sharing and Weighted Fair Queueing, use a fixed static proportional allocation of service rate and fail to achieve both goals. The well-known Earliest Deadline First policy minimizes overall loss, but fails to provide proportional loss across flows, because it treats packets as independent jobs. This paper introduces the Earliest Progressive Deadline First (EPDF) class of policies. We prove that all policies in this broad class minimize overall loss. Furthermore, we demonstrate that many EPDF policies accurately differentiate loss fractions in proportion to class weights, satisfying the second goal."
ce69cae3d719154501e16d55fdeafa8dd81badd9,
12d41ee22ba96a122e0ce80c16a6e66061b3c432,"As the use of wireless sensor networks increases, the need for (energy-)efficient and reliable broadcasting algorithms grows. Ideally, a broadcasting algorithm should have the ability to quickly disseminate data, while keeping the number of transmissions low. In this paper we develop a model describing the message count in large-scale wireless sensor networks. We focus our attention on the popular Trickle algorithm, which has been proposed as a suitable communication protocol for code maintenance and propagation in wireless sensor networks. Besides providing a mathematical analysis of the algorithm, we propose a generalized version of Trickle, with an additional parameter defining the length of a listen-only period. This generalization proves to be useful for optimizing the design and usage of the algorithm. For single-cell networks we show how the message count increases with the size of the network and how this depends on the Trickle parameters. Furthermore, we derive distributions of inter-broadcasting times and investigate their asymptotic behavior. Our results prove conjectures made in the literature concerning the effect of a listen-only period. Additionally, we develop an approximation for the expected number of transmissions in multi-cell networks. All results are validated by simulations."
2561d1fcefc801cbd4c4061ec16f3a51fc563abc,
2be2a7030c96d66981a85de4a7bdc06b575013e3,
38de6564c41fc4addb154043066c532548923675,"We consider Markovian many-server systems with admission control operating in a Quality-and-Efficiency-Driven (QED) regime, where the relative utilization approaches unity while the number of servers grows large, providing natural Economies-of-Scale. In order to determine the optimal admission control policy, we adopt a revenue maximization framework, and suppose that the revenue rate attains a maximum when no customers are waiting and no servers are idling. When the revenue function scales properly with the system size, we show that a nondegenerate optimization problem arises in the limit. Detailed analysis demonstrates that the revenue is maximized by nontrivial policies that bar customers from entering when the queue length exceeds a certain threshold of the order of the typical square-root level variation in the system occupancy. We identify a fundamental equation characterizing the optimal threshold, which we extensively leverage to provide broadly applicable upper/lower bounds for the optimal thresh..."
5e3ef2c14a15bbdefde72b2102c04049c3cf0868,
893159b65fddcfead0d734553b977c71b36d61d1,"Bandwidth-sharing networks as considered by Roberts and Massoulie [28] (Roberts JW, Massoulie L (1998) Bandwidth sharing and admission control for elastic traffic. Proc. ITC Specialist Seminar, Yokohama, Japan) provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Under mild assumptions, it has been established that a wide family of so-called α-fair bandwidth-sharing strategies achieve stability in such networks provided that no individual link is overloaded. In the present paper we focus on bandwidth-sharing networks where the load on one or several of the links exceeds the capacity. To characterize the overload behavior, we examine the fluid limit, which emerges when the flow dynamics are scaled in both space and time. We derive a functional equation characterizing the fluid limit, and show that any strictly positive solution must be unique, which in particular implies the convergence of the scaled number of flows to the fluid limit for nonz..."
8be347ce9e04e96cdb905989eb90a3924c5abd95,"As the use of wireless sensor networks increases, the need for (energy-)efficient and reliable broadcasting algorithms grows. Ideally, a broadcasting algorithm should have the ability to quickly disseminate data, while keeping the number of transmissions low. In this paper we develop a model describing the message count in large-scale wireless sensor networks. We focus our attention on the popular Trickle algorithm, which has been proposed as a suitable communication protocol for code maintenance and propagation in wireless sensor networks. Besides providing a mathematical analysis of the algorithm, we propose a generalized version of Trickle, with an additional parameter defining the length of a listen-only period. This generalization proves to be useful for optimizing the design and usage of the algorithm. For single-cell networks we show how the message count increases with the size of the network and how this depends on the Trickle parameters. Furthermore, we derive distributions of inter-broadcasting times and investigate their asymptotic behavior. Our results prove conjectures made in the literature concerning the effect of a listen-only period. Additionally, we develop an approximation for the expected number of transmissions in multi-cell networks. All results are validated by simulations."
a09fcaf569e7b2f0e04ffc442119c6307c179e63,"We consider the problem of optimal rate allocation and admission control for adaptive video streaming sessions in wireless networks with user dynamics. The central aim is to achieve an optimal tradeoff between several key objectives: maximizing the average rate utility per user, minimizing the temporal rate variability, and maximizing the number of users supported. We identify the structure of algorithms that achieve asymptotically optimal performance in large-capacity systems, and exploit the insight into this structure to devise parsimonious and robust online algorithms. Extensive simulation experiments demonstrate that the proposed online algorithms perform well, even in systems with relatively small capacity."
a277d238da69232fbc10860e7788cd2a7fb23330,"We consider dense wireless random-access networks, modeled as systems of particles with hard-core interaction. The particles represent the network users that try to become active after an exponential back-off time, and stay active for an exponential transmission time. Due to wireless interference, active users prevent other nearby users from simultaneous activity, which we describe as hard-core interaction on a conflict graph. We show that dense networks with aggressive back-off schemes lead to extremely slow transitions between dominant states, and inevitably cause long mixing times and starvation effects."
a9c07049bb9ba909e02f9bb40a17eb7befd9056c,
c5f098907f998ca3304393addbeac8f8e09aa48b,"Two popular products on the interest rate market are Constant Maturity Swap (CMS) derivatives and CMS spread derivatives. This thesis focusses on the efficient pricing of CMS and CMS spread derivatives, in particular the pricing of CMS and CMS spread options. The notional values for these products are usually quite large, so even small errors when pricing these products can lead to substantial losses. Therefore, the pricing of these products has to be accurate. It is possible to use sophisticated models (e.g. Libor Market Model) to price these products, however the downside is that these models generally have high computational costs; they are not very efficient. To efficiently price CMS options the Terminal Swap Rate (TSR) approach can be used. From this approach TSR models are obtained, we will consider four different TSR models. Two of these TSR models are established in the literature, the other two TSR models are developed in this thesis. The main advantages of a TSR model is that the computational costs are low and that it has good numerical tractability. To price CMS spread options the copula approach is usually used. With the copula approach a pricing formula can be obtained for efficient valuations of CMS spread options. The copula that is considered in this thesis is the Gaussian copula. The TSR models are also a key component in the copula approach, because the marginal distributions are obtained with the help of a TSR model. Furthermore, an alternative approach is considered for the pricing of CMS spread options. The CMS spread options are priced with a relatively simple stochastic volatility model, the displaced diffusion SABR model. The displaced diffusion SABR model is obtained by applying the Markovian projection method to a modification of a two-dimensional version of the well-established SABR model. The calibration of the two-dimensional SABR model is performed with the help of the TSR approach."
ca93a0870bffe95e4b919cac4f09aec28163854c,"Motivated by challenging resource allocation issues arising in large-scale wireless and wireline communication networks, we study distributed network utility maximization problems with a mixture of concave (e.g., best-effort throughputs) and nonconcave (e.g., voice/video streaming rates) utilities. In the first part of the paper, we develop our methodological framework in the context of a locally coupled networked system, where nodes represent agents that control a discrete local state. Each node has a possibly nonconcave local objective function, which depends on the local state of the node and the local states of its neighbors. The goal is to maximize the sum of the local objective functions of all nodes. We devise an iterative randomized algorithm, whose convergence and optimality properties follow from the classical framework of Markov Random Fields and Gibbs Measures via a judiciously selected neighborhood structure. The proposed algorithm is distributed, asynchronous, requires limited computational effort per node/iteration, and yields provable convergence in the limit. In order to demonstrate the scope of the proposed methodological framework, in the second part of the paper we show how the method can be applied to two different problems for which no distributed algorithm with provable convergence and optimality properties is available. Specifically, we describe how the proposed methodology provides a distributed mechanism for solving nonconcave utility maximization problems: 1) arising in OFDMA cellular networks, through power allocation and user assignment; 2) arising in multihop wireline networks, through explicit rate allocation. Several numerical experiments are presented to illustrate the convergence speed and performance of the proposed method."
e013bde4a929bb9d48ed49dfc0ae71d10b81c3d1,"Random access schemes are simple and inherently distributed, yet capable of matching the optimal throughput performance of centralized scheduling algorithms. The throughput optimality however has been established for activation rules that are relatively sluggish, and may yield excessive queues and delays. More aggressive/persistent access schemes have the potential to improve the delay performance, but it is not clear if they can offer any universal throughput optimality guarantees. In this paper, we identify a limit on the aggressiveness of nodes, beyond which instability is bound to occur in a broad class of networks."
37607580f731ba4b5a6d4bcc1caea1ec1249d28d,
4575d7bbcebae12899aef4dafb0a0f40a6092cb3,
5237eb9abff975ea4b78e72e311ce08239df99a7,"We explore the achievable delay performance in wireless random-access networks. While relatively simple and inherently distributed in nature, suitably designed backlog-based random-access schemes provide the striking capability to match the optimal throughput performance of centralized scheduling mechanisms. The specific type of activation rules for which throughput optimality has been established, may however yield excessive backlogs and delays. Motivated by that issue, we examine whether the poor delay performance is inherent to the basic operation of these schemes, or caused by the specific kind of activation rules. We derive delay lower bounds for backlog-based activation rules, which offer fundamental insight in the cause of the excessive delays. For fixed activation rates we obtain lower bounds indicating that delays and mixing times can grow dramatically with the load in certain topologies as well."
5689c07d2d793f4bc241c13a9aa4794194636f86,"The deployment of pico cells to cover traffic hot spots within the footprint of a macro cell provides a powerful approach to meet the massive growth in traffic demands fueled by smartphones and bandwidth-hungry applications. Joint optimization of resource allocation and user association is of critical importance to achieve the maximum capacity benefits in such heterogeneous network deployments (HetNets). We first examine the problem of minimizing the amount of resources required to satisfy given traffic demands. We characterize the structure of the optimal solution, and identify a simple optimality condition in terms of the physical transmission rates of the edge users between the macro cell and the various pico cells. We further demonstrate how these structural properties can be leveraged in designing a distributed online algorithm for achieving a max-min fair throughput allocation across all users. Numerical experiments are presented to illustrate the results."
70bc95e1788b37b535596119d2cc49a61e4d6ca0,
9c3c7eb24539fb4bb9fb06a5d006c185fe7a27fd,"The deployment of pico cells to cover traffic hot spots within the footprint of a macro cell provides a powerful approach to meet the massive growth in traffic demands fueled by smartphones and bandwidth-hungry applications. Joint optimization of resource allocation and user association is of critical importance to achieve the maximal capacity benefits in such heterogeneous network deployments (HetNets). We specifically examine the problem of maximizing the aggregate throughput utility of the various users. We characterize the structure of the optimal solution, and identify a simple optimality condition in terms of the transmission rates of the edge users between the macro cell and the various pico cells. Exploiting the structural properties, we develop distributed online algorithms for the broad class of alpha-fair utility functions, which includes several common fairness notions. Numerical experiments are presented to illustrate the results."
a070cf07de6b85ba04e56819634070b6953ae115,"Recent advances have resulted in queue-based algorithms for medium access control which operate in a distributed fashion, and yet achieve the optimal throughput performance of centralized scheduling algorithms. However, fundamental performance bounds reveal that the “cautious” activation rules involved in establishing throughput optimality tend to produce extremely large delays, typically growing exponentially in $$1/(1{-}\rho )$$1/(1-ρ), with $$\rho $$ρ the load of the system, in contrast to the usual linear growth. Motivated by that issue, we explore to what extent more “aggressive” schemes can improve the delay performance. Our main finding is that aggressive activation rules induce a lingering effect, where individual nodes retain possession of a shared resource for excessive lengths of time even while a majority of other nodes idle. Using central limit theorem type arguments, we prove that the idleness induced by the lingering effect may cause the delays to grow with $$1/(1{-}\rho )$$1/(1-ρ) at a quadratic rate. To the best of our knowledge, these are the first mathematical results illuminating the lingering effect and quantifying the performance impact. In addition extensive simulation experiments are conducted to illustrate and validate the various analytical results."
d7f87cce07de3182a482f97f2bb14bdc96f7ef4d,"Recent advances have resulted in queue-based algorithms for medium access control which operate in a distributed fashion, and yet achieve the optimal throughput performance of centralized scheduling algorithms. However, fundamental performance bounds reveal that the “cautious” activation rules involved in establishing throughput optimality tend to produce extremely large delays, typically growing exponentially in 1/(1−ρ), with ρ the load of the system, in contrast to the usual linear growth. Motivated by that issue, we explore to what extent more “aggressive” schemes can improve the delay performance. Our main finding is that aggressive activation rules induce a lingering effect, where individual nodes retain possession of a shared resource for excessive lengths of time even while a majority of other nodes idle. Using central limit theorem type arguments, we prove that the idleness induced by the lingering effect may cause the delays to grow with 1/(1−ρ) at a quadratic rate. To the best of our knowledge, these are the first mathematical results illuminating the lingering effect and quantifying the performance impact. In addition extensive simulation experiments are conducted to illustrate and validate the various analytical results."
e0cd9bd761af12da5c1231b8c9e306fa2c9122bb,"Cloud providers may operate large-scale data centers in a few locations. We argue that deploying many small-scale data centers at network edge can significantly improve user experience in terms of latency. Small-scale data centers, however, may not be able to provide elastic services. In this paper, we investigate distributed small-scale data centers with load reallocation where jobs that cannot be suitably processed locally will be reallocated to remote data centers. We formulate an optimization problem for load reallocation in distributed data centers, provide performance comparisons among different alternatives and offer insights on handling multiple job types. We develop online optimization algorithms that can be operated in a decentralized and measurement-based fashion to dynamically reallocate load in response to sudden load surges. The experimental results demonstrate that elasticity can be practically provided by small-scale data centers enhanced with effective load reallocation techniques."
e34815c2a25ea6c8d66758e46bdfcc988823c0be,"We explore the achievable delay performance in wireless random-access networks. While relatively simple and inherently distributed in nature, suitably designed queue-based random-access schemes provide the striking capability to match the optimal throughput performance of centralized scheduling mechanisms in a wide range of scenarios. The specific type of activation rules for which throughput optimality has been established, may however yield excessive queues and delays. 
Motivated by that issue, we examine whether the poor delay performance is inherent to the basic operation of these schemes, or caused by the specific kind of activation rules. We derive delay lower bounds for queue-based activation rules, which offer fundamental insight in the cause of the excessive delays. For fixed activation rates we obtain lower bounds indicating that delays and mixing times can grow dramatically with the load in certain topologies as well."
f8af15bb67075d80836620863193ba531aaa96c5,"We use fluid limits to explore the (in)stability properties of wireless networks with queue-based random-access algorithms. Queue-based random-access schemes are simple and inherently distributed in nature, yet provide the capability to match the optimal throughput performance of centralized scheduling mechanisms in a wide range of scenarios. Unfortunately, the type of activation rules for which throughput optimality has been established, may result in excessive queue lengths and delays. The use of more aggressive/persistent access schemes can improve the delay performance, but does not offer any universal maximum-stability guarantees. In order to gain qualitative insight and investigate the (in)stability properties of more aggressive/persistent activation rules, we examine fluid limits where the dynamics are scaled in space and time. In some situations, the fluid limits have smooth deterministic features and maximum stability is maintained, while in other scenarios they exhibit random oscillatory charact..."
25d6d4e478920838960e10322b069d8d34282b3b,
26eb5e8839b33ebeba97d01f3e3b1e5e833d3b9b,
31756e6d73099d5ae6d77e77d8f893c1df3840c1,"We develop an online gradient algorithm for optimizing the performance of product-form networks through online adjustment of control parameters. The use of standard algorithms for finding optimal parameter settings is hampered by the prohibitive computational burden of calculating the gradient in terms of the stationary probabilities. The proposed approach instead relies on measuring empirical frequencies of the various states through simulation or online operation so as to obtain estimates for the gradient. Besides the reduction in computational effort, a further benefit of the online operation lies in the natural adaptation to slow variations in ambient parameters as commonly occurring in dynamic environments. On the downside, the measurements result in inherently noisy and biased estimates. We exploit mixing time results in order to overcome the impact of the bias and establish sufficient conditions for convergence to a globally optimal solution."
31fcd7e268e683088611e3430a7867bce01743f8,
4ace46077dfda816edc33dc81dc1604bda09b656,"When a communication network's capacity increases, it is natural to want the bandwidth allocated to increase to exploit this capacity. But, if the same relative capacity increase occurs at each network resource, it is also natural to want each user to see the same relative benefit, so the bandwidth allocated to each route should remain proportional. We will be interested in bandwidth allocations which scale in this \textit{iso-elastic} manner and, also, maximize a utility function. 
Utility optimizing bandwidth allocations have been frequently studied, and a popular choice of utility function are the weighted $\alpha$-fair utility functions introduced by Mo and Walrand \cite{MoWa00}. Because weighted $\alpha$-fair utility functions possess this iso-elastic property, they are frequently used to form fluid models of bandwidth sharing networks. In this paper, we present results that show, in many settings, the only utility functions which are iso-elastic are weighted $\alpha$-fair utility functions. 
Thus, if bandwidth is allocated according to a network utility function which scales with relative network changes then that utility function must be a weighted $\alpha$-fair utility function, and hence, a control protocol that is robust to the future relative changes in network capacity and usage ought to allocate bandwidth inorder to maximize a weighted $\alpha$-fair utility function."
585369b3eadfd8159d9db696ebfe62d6882e5f62,
9262e1f59f1f2ee1fd8aeb3193ed18807f09bdc5,"We consider a stylized stochastic model for a wireless CSMA network. Experimental results in prior studies indicate that the model provides remarkably accurate throughput estimates for IEEE 802.11 systems. In particular, the model offers an explanation for the severe spatial unfairness in throughputs observed in such networks with asymmetric interference conditions. Even in symmetric scenarios, however, it may take a long time for the activity process to move between dominant states, giving rise to potential starvation issues. In order to gain insight in the transient throughput characteristics and associated starvation effects, we examine in the present paper the behavior of the transition time between dominant activity states. We focus on partite interference graphs, and establish how the magnitude of the transition time scales with the activation rate and the sizes of the various network components. We also prove that in several cases the scaled transition time has an asymptotically exponential distribution as the activation rate grows large, and point out interesting connections with related exponentiality results for rare events and meta-stability phenomena in statistical physics. In addition, we investigate the convergence rate to equilibrium of the activity process in terms of mixing times."
b70abfba2f73f5236196064bb0b38655983545c9,
bb06a0756fdbfa56f1aaf09e8ed243b681e30f3d,"Backlog-based wireless access schemes are simple and inherently distributed, yet provide a striking capability to match the optimal throughput performance of centralized scheduling mechanisms in a wide range of scenarios. Unfortunately, the type of activation rules for which throughput optimality has been established, may result in excessive backlogs and delays. The use of more aggressive/persistent access schemes than these can improve the delay performance, but does not offer any universal maximum-stability guarantees. Motivated by the above issues, we use fluid limits to explore the (in)stability properties of backlog-based random-access algorithms. Such fluid limits have varying qualitative properties, dependent on the specific scenario, ranging from ones with smooth deterministic features, to others which exhibit random oscillatory characteristics. It turns out that more aggressive access schemes continue to provide maximum stability in some networks, e.g. complete interference graphs. As we show however, in other topologies such schemes can drive the system into inefficient states and thus cause instability. Simulation experiments are conducted to illustrate and validate the analytical results."
ce55474aac8728a681d92f4b12df5c62cb8bf07c,"We characterize the achievable range of performance measures in product-form networks where one or more system parameters can be freely set by a network operator. Given a product-form network and a set of configurable parameters, we identify which performance measures can be controlled and which target values can be attained. We also discuss an online optimization algorithm, which allows a network operator to set the system parameters so as to achieve target performance metrics. In some cases, the algorithm can be implemented in a distributed fashion, of which we give several examples. Finally, we give conditions that guarantee convergence of the algorithm, under the assumption that the target performance metrics are within the achievable range."
da924271763bc67c10de5079a89064e08ba8254b,
f1f53c98cd06988b7426fbc21d28b073bf1881ee,
0240695f153c928c63df243f66d8cccfffa59501,"The ability of base stations in wireless access networks to regularly and autonomously self-optimize their parameters has become a key requirement from network operators. A number of specific optimization use cases have been discussed whose federation relies on a separation in groups that are treated consecutively with negligible or no mutual interactions. In this paper, we introduce coordination and separation strategies and discuss their suitability to avoid interaction conflicts. In particular, we discuss a separation by the amount of measurements needed to make a reasonable decision to modify the parameters. For this case, we derive insight into the statistical relation between the tolerances we accept and the number of observations that are necessary to trigger a parameter modification. Recommendations are provided towards a stable holistic autonomous solution for wireless access networks."
19cd6c0e6bcf4cf6613e4ab6a20d6cc80833827a,"Operations Research Letters is a publication for literature on all aspects of operations research and the management and decision sciences. The features distinguishing it from other journals in the field are 
* concise articles, generally limited to 6 journal pages 
* rapid review and fast publication 
* broad coverage of the literature. 
Apart from the page limitation, originality, relevance, quality and clarity are the only criteria for selecting the material to be published. The journal covers continuous and discrete optimization, stochastic models, and situations with multiple decision makers. The subject matter can be theory, methodology, empirical studies, and applications. The mainstream of contributions focuses on new models, theorems, algorithms, and experimental work that the author wants to disseminate rapidly. We will publish theory and methodology with proofs only sketched, provided that the author submits support material that enables us to verify the findings. We will also publish computational and experimental studies that are not necessarily based on new theory or methodology, but are of significant scientific value because they confirm or refute prior results. Similarly, we will publish reports on applications and case studies that demonstrate a novel use of existing techniques or contain significant ideas about data collection and analysis, modelling, or implementation."
3528d51d58d10fb632a673c1ff32934f92f3547d,"In this paper we investigate the number and maximum severity of the ruin excursion of the insurance portfolio reserve process in the Cramér–Lundberg model with and without tax payments. We also provide a relation of the Cramér–Lundberg risk model with the G/G/∞ queue and use it to derive some explicit ruin probability formulae. Finally, the renewal risk model with tax is considered, and an asymptotic identity is derived that in some sense extends the tax identity of the Cramér– Lundberg risk model."
4b0bf65def4993adaf718b4ce6cbcd950ff1aecf,
51049024c839e12f5b9ff8d2539d7238c4ee3fb1,"We explore the achievable delay performance in wireless CSMA networks. While relatively simple and inherently distributed in nature, suitably designed backlog-based CSMA schemes provide the striking capability to match the optimal throughput performance of centralized scheduling mechanisms in a wide range of scenarios. The specific type of activation rules for which throughput optimality has been established, may however yield excessive backlogs and delays. Motivated by that issue, we examine whether the poor delay performance is inherent to the basic CSMA operation of these schemes, or caused by the specific kind of activation rules. We first establish lower bounds for the delay in the case of fixed activation rates. The bounds indicate that the delay can dramatically grow with the load in certain topologies. We also discuss to what extent the bounds apply to backlog-based activation rules. Simulation experiments are conducted to illustrate and validate the analytical results."
6d27ebed8477967352224ccb631da0fca25b1a2f,"This paper considers the scheduling problem for networks with interference constraints and switchover delays, where it takes a nonzero time to reconfigure each service schedule. Switchover delay occurs in many telecommunication applications such as satellite, optical or delay tolerant networks (DTNs). Under zero switchover delay it is well known that the Max-Weight algorithm is throughput-optimal without requiring knowledge of the arrival rates. However, we show that this property of Max-Weight no longer holds when there is a nonzero switchover delay. We propose a class of variable frame based Max-Weight (VFMW) algorithms which employ the Max-Weight schedule corresponding to the beginning of the frame during an interval of duration dependent on the queue sizes. The VFMW algorithms dynamically adapt the frame sizes to the stochastic arrivals and provide throughput-optimality without requiring knowledge of the arrival rates. Numerical results regarding the application of the VFMW algorithms to DTN and optical networks demonstrate a good delay performance."
77ef65eca683ab18529f414e2ed4f5e519bba565,"We explore the spatio-temporal congestion dynamics of wireless networks with backlog-based random-access mechanisms. While relatively simple and inherently distributed in nature, suitably designed backlog-based access schemes provide the striking capability to match the optimal throughput performance of centralized scheduling algorithms in a wide range of scenarios. In the present paper, we show that the specific activity functions for which maximum stability has been established, may however yield excessive queue lengths and delays. The results reveal that more aggressive/persistent access schemes can improve the delay performance, while retaining the maximum stability guarantees in a rich set of scenarios. In order to gain qualitative insights and examine stability properties we will investigate fluid limits where the system dynamics are scaled in space and time. As it turns out, several distinct types of fluid limits can arise, exhibiting various degrees of randomness, depending on the structure of the network, in conjunction with the form of the activity functions. We further demonstrate that, counter to intuition, additional interference may improve the delay performance in certain cases. Simulation experiments are conducted to illustrate and validate the analytical findings."
8390b91255f6466bca317a5095b4c3a6316c01ff,"We consider a wireless OFDMA cellular network and address the problem of jointly allocating power to frequencies (subbands) and assigning users to cells, for a variety of elastic and inelastic services. The goal is to maximize the sum of the users' throughput utility functions subject to various constraints, such as minimum throughput requirements. The problem naturally fits into a Network Utility Maximization (NUM) framework with a mixture of concave (e.g., data rates) and nonconcave (e.g., voice/video streaming) utilities. The hardness of this nonconvex, mixed integer program prohibits the use of standard convex optimization algorithms, or efficient combinatorial approximation techniques. We devise a randomized algorithm for the said NUM problem, whose proof of asymptotic optimality is derived from the classical framework of interacting particle systems, via a judiciously selected neighborhood structure. The proposed algorithm is highly distributed, asynchronous, requires limited computational effort per node/iteration, and yields provable convergence in the limit. Several numerical experiments are presented to illustrate the convergence speed and performance of the proposed method."
8a89976290d6ed76ba286e8eae3fb4243fb9cb03,"MaxWeight scheduling algorithms provide an effective mechanism for achieving queue stability and guaranteeing maximum throughput in a wide variety of scenarios. The maximumstability guarantees however rely on the fundamental premise that the system consists of a fixed set of sessions with stationary ergodic traffic processes. In the present paper we examine a scenario where the population of active sessions varies over time, as sessions eventually end while new sessions occasionally start. We identify a simple necessary and sufficient condition for stability, and show that MaxWeight policies may fail to provide maximum stability. The intuitive explanation is that these policies tend to give preferential treatment to flows with large backlogs, so that the rate variations of flows with smaller backlogs are not fully exploited. In the usual framework with a fixed collection of flows, the latter phenomenon cannot persist since the flows with smaller backlogs will build larger queues and gradually start receiving more service. With a dynamic population of flows, however, MaxWeight policies may constantly get diverted to arriving flows, while neglecting the rate variations of a persistently growing number of flows in progress with relatively small remaining backlogs. We also perform extensive simulation experiments to corroborate the analytical findings."
99417c58c7a44d8d07bac4edcb186db600b1729a,
9b203f5093415f309ce2d5da4c4f984c7076bb5a,"Backlog-based CSMA strategies provide a popular mechanism for distributed medium access control in wireless networks. When suitably designed, such strategies offer the striking capability to match the optimal throughput performance of centralized scheduling algorithms in a wide range of scenarios. Unfortunately, however, the activation rules used in these schemes tend to yield excessive backlogs and delays. More aggressive activation rates can potentially improve the delay performance, but may not allow provable maximum-stability guarantees. In order to gain a fundamental understanding how the shape of the activation function affects the queueing behavior, we focus on a single- node scenario, thus separating the impact of the network topology. We demonstrate that three qualitatively different regimes can arise, depending on how rapidly the activation function increases with the backlog. Simulation experiments are conducted to validate the analytical findings."
b5fd0557e07e353154ab4cb9fb88ba7a7db8b886,"In this paper we investigate the number and maximum severity of the ruin excursion of the insurance portfolio reserve process in the Cramér–Lundberg model with and without tax payments. We also provide a relation of the Cramér–Lundberg risk model with the G/G/∞ queue and use it to derive some explicit ruin probability formulae. Finally, the renewal risk model with tax is considered, and an asymptotic identity is derived that in some sense extends the tax identity of the Cramér– Lundberg risk model."
f2df4630b1bae444fd8cef9a9081f9ec5685ac3d,
f4750a4de3fc5e32891be3a5de22cc805636eb03,"We examine the stability of wireless networks whose users are distributed over a torus. Users arrive at spatially uniform locations with intensity λ and each user has a random number of packets to transmit with mean β. In each time slot, an admissible subset of users is selected uniformly at random to transmit one packet. A subset of users is called admissible when their simultaneous activity obeys the prevailing interference constraints. We consider the SINR model and the protocol model as two canonical models for interference, and denote by μ the maximum number of users in an admissible subset for the model under consideration. We show that the necessary condition λβ < μ is also sufficient for random admissible-set scheduling to achieve stability. Thus random admissible-set scheduling achieves stability, if feasible to do so at all, for a wide range of interference scenarios. The proof relies on a description of the system as a measure-valued process and the identification of a Lyapunov function."
f8d77da73ba8d8555ee5d938e106253be0c22778,"MaxWeight scheduling has gained enormous popularity as a powerful paradigm for achieving queue stability and maximum throughput in a wide variety of scenarios. The maximum-stability guarantees however rely on the fundamental premise that the system consists of a fixed set of flows with stationary ergodic traffic processes. In the present paper we examine networks where the population of active flows varies over time, as flows eventually end while new flows occasionally start. We show that MaxWeight policies may fail to provide maximum stability due to persistent inefficient spatial reuse. The intuitive explanation is that these policies tend to serve flows with large backlogs, even when the resulting spatial reuse is not particularly efficient, and fail to exploit maximum spatial reuse patterns involving flows with smaller backlogs. These results indicate that instability of MaxWeight scheduling can occur due to spatial inefficiency in networks with fixed transmission rates, which is fundamentally different from the inability to fully exploit time-varying rates shown in prior work. We discuss how the potential instability effects can be countered by spatial traffic aggregation, and describe some of the associated challenges and performance trade-offs."
325421f861e6e1b9eff35e4ab4dc78a2874f1090,
4acf0abd43f0b4d48e50873a41e02368daedf635,"We examine the stability of wireless networks whose users are distributed over a compact space. Users arrive at spatially uniform locations with intensity \lambda and each user has a random number of packets to transmit with mean ??\beta. In each time slot, an admissible subset of users is selected uniformly at random to transmit one packet. A subset of users is called admissible when their simultaneous activity obeys the prevailing interference constraints. We consider a wide class of interference constraints, including the SINR model and the protocol model. Denote by \mu?? the maximum number of users in an admissible subset for the model under consideration. We will show that the necessary condition $\lamba \beta <\mu$ is also su??fficient for random admissible-set scheduling to achieve stability. Thus random admissible-set scheduling achieves stability, if feasible to do so at all, for a broad class of interference scenarios. The proof relies on a description of the system as a measure-valued process and the identi??cation of a Lyapunov function."
63b4efad23afb1b885c0fb49ed5726fec80a2050,"CSMA is the predominant distributed access protocol for wireless mesh networks. Originally designed for singlehop settings, in multi-hop networks CSMA can exhibit severe performance problems in terms of stability and end-to-end throughput. To ensure a smoother flow of packets, we examine a new scheme referred to as extra back-off (EB) flow control. In this scheme a node remains silent for a certain extra back-off time (imposed on top of the usual back-off time that is part of CSMA) after it has transmitted a packet, so as to give both the downstream and upstream neighbors the opportunity to transmit. EB flow control entails only a small modification to CSMA, preserving its distributed character, yet considerably improving the network performance."
7b8d5f8518e9aff9f98e4146179f30f1ad305cd2,"The delivery of video content is expected to gain huge momentum, fueled by the popularity of user-generated clips, growth of VoD libraries, and wide-spread deployment of IPTV services with features such as CatchUp/PauseLive TV and NPVR capabilities. The `time-shifted' nature of these personalized applications defies the broadcast paradigm underlying conventional TV networks, and increases the overall bandwidth demands by orders of magnitude. Caching strategies provide an effective mechanism for mitigating these massive bandwidth requirements by replicating the most popular content closer to the network edge, rather than storing it in a central site. The reduction in the traffic load lessens the required transport capacity and capital expense, and alleviates performance bottlenecks. In the present paper, we develop light-weight cooperative cache management algorithms aimed at maximizing the traffic volume served from cache and minimizing the bandwidth cost. As a canonical scenario, we focus on a cluster of distributed caches, either connected directly or via a parent node, and formulate the content placement problem as a linear program in order to benchmark the globally optimal performance. Under certain symmetry assumptions, the optimal solution of the linear program is shown to have a rather simple structure. Besides interesting in its own right, the optimal structure offers valuable guidance for the design of low-complexity cache management and replacement algorithms. We establish that the performance of the proposed algorithms is guaranteed to be within a constant factor from the globally optimal performance, with far more benign worst-case ratios than in prior work, even in asymmetric scenarios. Numerical experiments for typical popularity distributions reveal that the actual performance is far better than the worst-case conditions indicate."
83aea77ea4c141e8b5455eac856b81e75894d1a5,"In this paper we investigate the number and maximum severity of the ruin excursion of the insurance portfolio reserve process in the Cramer-Lundberg model with and without tax payments. We also provide a relation of the Cramer-Lundberg risk model with the G/G/$\infty$ queue and use it to derive some explicit ruin probability formulas. Finally, the renewal risk model with tax is considered, and an asymptotic identity is derived that in some sense extends the tax identity of the Cramer-Lundberg risk model."
bfe4b4dcef6fee470204ba145dd95c82be682b44,
cb02f8bddb809a0cd8875af51af5c4c142d64faa,
cc8f98db4633cc99eee31d6d6da5299c4747fc55,"We examine the stability of wireless networks whose users are distributed over a compact space. A subset of users is called {\it admissible} when their simultaneous activity obeys the prevailing interference constraints and, in each time slot, an admissible subset of users is selected uniformly at random to transmit one packet. We show that, under a mild condition, this random admissible-set scheduling mechanism achieves maximum stability in a broad set of scenarios, and in particular in symmetric cases. The proof relies on a description of the system as a measure-valued process and the identification of a Lyapunov function."
cec6d6bca0e2720c22ef64fa6deba1802d5bf42a,
d31377facef9c761de9fcd4cc8d491ca34506db4,
e3d0b2e547e46a1a2128e2ab002cb7483cfe2476,"Random-access algorithms such as CSMA provide a popular mechanism for distributed medium access control in largescale wireless networks. In recent years, tractable models have been shown to yield accurate throughput estimates for CSMA networks. We consider the saturated model on a general conflict graph, and prove that for each graph, there exists a vector of activation rates (or mean back-off times) that leads to equal throughputs for all users. We describe an algorithm for computing such activation rates, and discuss a few specific conflict graphs that allow for explicit characterization of these fair activation rates."
0420ade175d6cc444c47d372a9a4121307aaac44,"The emergence of new communication services with anytime access to content, such as personalized time-shifted television (TV) programming, requires a combination of unicast and multicast content distribution, and possibly caching, for efficient support of user requests. We present a general framework for the analysis and design of such delivery systems that takes into account network resource constraints, time constraints, content features, user preferences, and service availability requirements. In this framework, the time-shifted content delivery system is modeled via the well-known probabilistic urn model with novel occupancy probabilities reflecting actual content popularity. New approximations to occupancy distributions are derived which enable fast computation of various system performance measures, such as likelihood of access to requested programs. We illustrate the utility of this analytical framework via a prototypical scenario in emerging personalized wireless video services."
184eacb7c5957bea0fa026ce621b18ac862c0940,
191f78bdb450f0cdaa9b832a4181f38d6cdd2189,
2aac531784e8010d1677a5d484e3e194dcbd5b39,
3ea9152ba4d9149afd83d06adb9753ce8a2eff46,"An emerging communication service with significant potential is delivery of personalized content. Such a service typically is provided through either live or time-shifted program multicast. While the former requires adequate bandwidth for a combination of unicast, multicast or broadcast, the latter typically leverages caching. Both modes of distribution have hard resource constraints. Determination of effective tradeoffs between transmission-constrained multicast and storage-constrained caching presents a significant challenge, particularly in a bandwidth-limited wireless environment. We describe a general analytical model for efficient and effective solutions to these multi-dimensional problems in content delivery networks. We illustrate the utility of this analytical framework via a prototypical scenario in personalized wireless video services."
9520e9cdfc8512c799af57b04ddd724bf7c302a7,"The delivery of video content is expected to gain huge momentum, fueled by the popularity of user-generated clips, growth of video-on-demand (VoD) libraries, and widespread deployment of Internet Protocol television (IPTV) services with features such as CatchUp/PauseLive TV and network personal video recorder (NPVR) capabilities. The “time-shifted” nature of these personalized applications defies the broadcast paradigm underlying conventional TV networks and increases the overall bandwidth demands by orders of magnitude. Caching strategies provide an effective mechanism for mitigating these massive bandwidth requirements by storing copies of the most popular content closer to the network edge, rather than keeping it in a central site. The reduction in the traffic load lessens the required transport capacity and capital expense and alleviates performance bottlenecks. In this paper, we develop lightweight cooperative cache management algorithms aimed at maximizing the traffic volume served from cache and minimizing the bandwidth cost. As a canonical scenario, we focus on a cluster of distributed caches and show that under certain symmetry assumptions, the optimal content placement has a rather simple structure. Besides being interesting in its own right, the optimal structure offers valuable guidance for the design of low-complexity cache management and replacement algorithms. We establish that the proposed algorithms are guaranteed to operate within a constant factor from the globally optimal performance, with benign worst-case ratios, even in asymmetric scenarios. Numerical experiments reveal that typical performance is far better than the worst-case conditions indicate."
ad2e0cf31863c52e1749a732606451365ac53417,
b3586eda5b8cff16cdaf1b356f4a8a3df8780375,"The design of scheduling policies for wireless data systems has been driven by a compromise between the objectives of high overall system throughput and the degree of fairness among users, while exploiting multi-user diversity, i.e., fast-fading variations. These policies have been thoroughly investigated in the absence of user mobility, i.e., without slow fading variations. In the present paper, we examine the impact of intra- and inter-cell user mobility on the trade-off between throughput and fairness, and on the suitable choice of alpha-fair scheduling policies. We consider a dynamic setting where users come and go over time as governed by random finite-size data transfers, and explicitly allow for users to roam around. It is demonstrated that the overall performance improves as the fairness parameter alpha is reduced, and in particular, that proportional fair scheduling may yield relatively poor performance, in sharp contrast to the standard scenario with only fast fading. Since a lower alpha tends to affect short-term fairness, we explore how to set the fairness parameter so as to strike the right balance between overall performance and short-term fairness. It is further established that mobility tends to improve the performance, even when the network operates under a local fair scheduling policy as opposed to a globally optimal strategy. We present extensive simulation results to confirm and illustrate the analytical findings."
dec29944eb3211490d4144b1464331cd26eff08a,
e85797d169ec3fd2b727fad558596eebc1717972,
f922ff4a975c6c8bf39098d32732b4e7c9e42cd0,
ff6ca99c699b8835b58ae60e79dffecb112907e7,"MaxWeight scheduling algorithms provide an effective mechanism for achieving queue stability and guaranteeing maximum throughput in a wide variety of scenarios. The maximum-stability guarantees however rely on the fundamental premise that the system consists of a fixed set of sessions with stationary ergodic traffic processes. In the present paper we examine a scenario where the population of active sessions varies over time, as sessions eventually end while new sessions occasionally start. We identify a simple necessary and sufficient condition for stability, and show that MaxWeight policies may fail to provide maximum stability. The intuitive explanation is that these policies tend to give preferential treatment to flows with large backlogs, so that the rate variations of flows with smaller backlogs are not fully exploited. In the usual framework with a fixed collection of flows, the latter phenomenon cannot persist since the flows with smaller backlogs will build larger queues and gradually start receiving more service. With a dynamic population of flows, however, MaxWeight policies may constantly get diverted to arriving flows, while neglecting the rate variations of a persistently growing number of flows in progress with relatively small remaining backlogs. We also perform extensive simulation experiments to corroborate the analytical findings."
0554be4607e7460fc0a4c41b3fc37a2347892bea,
16a76cea7049fac7e52d25706284c7fa6f86f6eb,"Bandwidth-sharing networks as considered by Massoulie & Roberts provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Under mild assumptions, it has been established that a wide family of so-called alpha-fair bandwidth-sharing strategies achieve stability in such networks provided that no individual link is overloaded. In the present paper we focus on alpha-fair bandwidth-sharing networks where the load on one or several of the links exceeds the capacity. Evidently, a well-engineered network should not experience overload, or even approach overload, in normal operating conditions. Yet, even in an adequately provisioned system with a low nominal load, the actual traffic volume may significantly fluctuate over time and exhibit temporary surges. Furthermore, gaining insight in the overload behavior is crucial in analyzing the performance in terms of long delays or low throughputs as caused by large queue build-ups. The way in which such rare events tend to occur, commonly involves a scenario where the system temporarily behaves as if it experiences overload. In order to characterize the overload behavior, we examine the fluid limit, which emerges from a suitably scaled version of the number of flows of the various classes. Focusing on linear solutions to the fluid-limit equation, we derive a fixed-point equation for the corresponding asymptotic growth rates. It is proved that a fixed-point solution is also a solution to a related strictly concave optimization problem, and hence exists and is unique. The results are illustrated for linear topologies and star networks as two important special cases."
19d291e752ca013739b34a79b21f3e726cce116b,"One of the key questions in dimensioning a hybrid P2P content distribution system is that of the required infrastructure support in terms of server bandwidth. In this paper, we develop and propose simple mathematical models for analyzing and dimensioning hybrid peer-to-peer content distribution networks. We first use a deterministic fluid model to capture the essential peer and server dynamics within a single swarm, and subsequently derive a stochastic fluid model to capture the dynamics in the case of multiple swarms, i.e., concurrent swarms of a number of content objects. Based on the models, we derive solutions for estimating the server capacity required to support a single swarm as well as a number of concurrent file swarms at a given level of service quality. Numerical results demonstrate how a hybrid P2P approach can yield substantial performance gains and capacity savings compared to a pure client/server system, with churn rate and upload bandwidth being critical factors. Compared to a pure peer-to-peer scenario, the hybrid approach can dramatically boost the performance and improve reliability."
2e850ecaf1eb997f4bff70b90979da1640ea7ee9,"Performance results are presented for digital subscriber line (DSL) precoders, also known as dynamic spectrum management (DSM) level 3, in terms of improvement of signal-to-noise ratio (SNR) and achievable bit rates under the very high speed digital subscriber line 2 (VDSL2) standard. Simulations were conducted using channel coefficients obtained from both empirical models and line measurements. The performance impact of various key factors is examined, including the number of lines, line lengths, frequency range, precoder design, channel measurement accuracy, and implementation accuracy (quantization errors). The results provide estimates of the achievable rate gains and offer guidance for the design and implementation of DSL precoders."
3bf93a3e72da07a08474bd468f15436ddd084e7f,"IEEE 802.11s is the task group in the IEEE that is in the process of standardizing wireless mesh networks. A hot topic in this standardization effort concerns the need for additional medium access functionality beyond the basic IEEE 802.11 carrier sense multiple access with collision avoidance (CSMA/CA). In this paper, we discuss the connection between CSMA/CA and Dijkstra's classical Philosophers’ problem, and its implications for the debate inside IEEE 802.11s. In an alternative view of this paper, we state some new mathematical models, theorems and conjectures related to the Philosophers’ problem."
3d6afb63e174e8c08e2ae441ea171e73e73e6145,"Bandwidth-sharing networks as considered by Massoulie & Roberts provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Under mild assumptions, it has been established that a wide family of so-called alpha-fair bandwidth-sharing strategies achieve stability in such networks provided that no individual link is overloaded. In the present paper we focus on alpha-fair bandwidth-sharing networks where the load on one or several of the links exceeds the capacity. Evidently, a well-engineered network should not experience overload, or even approach overload, in normal operating conditions. Yet, even in an adequately provisioned system with a low nominal load, the actual traffic volume may significantly fluctuate over time and exhibit temporary surges. Furthermore, gaining insight in the overload behavior is crucial in analyzing the performance in terms of long delays or low throughputs as caused by large queue build-ups. The way in which such rare events tend to occur, commonly involves a scenario where the system temporarily behaves as if it experiences overload. In order to characterize the overload behavior, we examine the fluid limit, which emerges from a suitably scaled version of the number of flows of the various classes. Focusing on linear solutions to the fluid-limit equation, we derive a fixed-point equation for the corresponding asymptotic growth rates. It is proved that a fixed-point solution is also a solution to a related strictly concave optimization problem, and hence exists and is unique. The results are illustrated for linear topologies and star networks as two important special cases."
513e1de4e06e6e0c38d2d50d0437c56bfb361db6,
7b2baa5c86402ea9a986377ea31873e454fd01f4,"Content distribution networks are experiencing tremendous growth, in terms of traffic volume, scope, and diversity, fueled by several technological advances and competing paradigms. Traditional client/server architectures as deployed in the majority of today's commercial networks provide high reliability and superior level of service quality but involve significant investment in network infrastructure. In contrast, peer-to-peer swarming technologies such as BitTorrent have emerged as a popular mechanism for cost-effective and scalable content dissemination, accounting for a huge portion of today's Internet traffic, but stand or fall with the availability of abundant, free bandwidth, rendering performance inherently unreliable. A hybrid approach offers a promising concept to strike the right balance and combine the highly reliable performance of traditional client/server systems with the scalability and cost-effectiveness of peer-to-peer mechanisms. In order to examine the potential performance gains and capacity savings from a hybrid approach, we develop simple mathematical models for peer-assisted content distribution networks. We use these models to investigate the impact of several key parameters, such as churn rate, object size, and upload bandwidth, and to compare various resource management strategies. The results demonstrate that a peer-assisted approach can yield substantial performance gains and capacity savings compared to a pure client/server system, with churn rate and upload bandwidth being critical factors. Compared to a pure peer-to-peer scenario, the hybrid approach can dramatically boost the performance and improve the reliability."
a727b9f03731485386c2f14d96f6fb90bedc6e75,"In bandwidth-sharing networks, users of various classes require service from different subsets of shared resources simultaneously. These networks have been proposed to analyze the performance of wired and wireless networks. For general arrival and service processes, we give sufficient conditions in order to compare sample-path wise the workload and the number of users under different policies in a linear bandwidth-sharing network. This allows us to compare the performance of the system under various policies in terms of stability, the mean overall delay and the weighted mean number of users. 
 
For the important family of weighted α-fair policies, we derive stability results and establish monotonicity of the weighted mean number of users with respect to the fairness parameter α and the relative weights. In order to broaden the comparison results, we investigate a heavy-traffic regime and perform numerical experiments."
bc18ae4a67f6003b8eca61660388424ac44f6537,
cd427ef3be74bb9169d91cd3f2f0f96da02bdc13,"Channel conditions in wireless networks exhibit huge variations across space and time, giving rise to vast fluctuations in the transmission rates. Channel-aware scheduling strategies provide an effective mechanism for improving throughput performance by exploiting such rate variations, and these have been extensively examined at the packet level for a static user configuration. In this paper, we discuss the performance implications at the flow level for a dynamic user population, taking into account variations on a slower time scale and wide-range user mobility as well. First of all, we present simple necessary conditions for flow-level stability and prove that these are in fact (near) sufficient for a wide family of utility-based scheduling strategies. It is further shown how the flow-level performance of the proportional fair scheduling strategy may be evaluated by means of a processor-sharing model with a state-dependent service rate. In addition, we examine the impact of variations on a slower time scale, and establish that the so-called fluid and quasi-stationary regimes yield explicit, insensitive performance bounds. Finally, we turn our attention to a network of several base stations (BSs) with handoffs of active sessions governed by wide-range user mobility. It is demonstrated that mobility increases the capacity, not only in the case of globally optimal scheduling but also when each of the BSs adheres to a local fair-sharing discipline."
da22c79e96983e5c79fcfa68dea5a63b440e8420,"Recent changes in the production and use of multimedia content have fostered development of new ways to distribute this material to consumers. In particular, the increasing demand for this content has led to the design and deployment of new content distribution networks. The latest examples of these networks take advantage of new technological underpinnings. For example, they typically make use of larger, less expensive storage, and many incorporate peer-to-peer distribution technologies. The key challenges in designing this new generation of content distribution networks involve responding to changes in not only memory and network resources, but also content availability and popularity. This letter discusses the key design constraints for modern content distribution networks and some of the ways that these constraints can be met by innovative combinations of resource control algorithms. It also describes how a simulation environment is used to analyze some of these techniques in the context of video distribution requirements."
f5f744e2f431318f5589eadf576e6e3f5cd8e342,"A multiple-input, multiple-output (MIMO) communication system is configured to perform user scheduling with reduced channel station information. The system includes multiple terminals and at least one base station configured to communicate with the terminals. The base station is operative to obtain channel vector magnitudes for respective ones of the terminals, to identify a subset of the terminals based on the channel vector magnitudes, to obtain channel vector phase information for the identified subset of terminals, and to utilize the channel vector phase information to control transmission to the identified subset of terminals. The system may be, for example, a multi-user MIMO system in which the multiple terminals comprise autonomous sin le-antenna terminals."
2a5a2423f4a50ae85f847da5834028efdb7c07cd,
3da26c19c9e637b07d026d96ccec3c61edbea8de,
43264bae943695b80189a57093e3a7f40dc34a57,"Bandwidth-sharing networks as considered by Massoulie & Roberts provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Although valuable stability results have been obtained, crucial performance metrics such as flow-level delays and throughputs in these models have remained intractable in all but a few special cases. In particular, it is not well understood to what extent flow-level delays and throughputs achieved by standard bandwidth-sharing mechanisms such as alpha-fair strategies leave potential room for improvement. In order to gain a better understanding of the latter issue, we set out to determine the scheduling policies that minimize the mean delay in some simple linear bandwidth-sharing networks. While admittedly simple, linear networks provide a useful model for flows that traverse several links and experience bandwidth contention from independent cross-traffic. Even for linear topologies it is rarely possible however to explicitly identify optimal policies except in a few limited cases with exponentially distributed flow sizes. Rather than aiming for strictly optimal policies, we therefore focus on a class of relatively simple priority-type strategies that only separate large flows from small ones. To benchmark the performance of these strategies, we compare them with proportional fair as the prototypical alpha-fair policy, and establish that the mean delay may be reduced by an arbitrarily large factor when the load is sufficiently high. In addition, we show the above strategies to be asymptotically optimal for flow size distributions with bounded support. Numerical experiments reveal that even at fairly moderate load values the performance gains can be significant."
8e974260d33663a32d49a913c6f724cacffe506e,"Valiant's concept of randomized load balancing (RLB), also promoted under the name 'two-phase routing', has previously been shown to provide a cost-effective way of implementing overlay networks that are robust to dynamically changing demand patterns. RLB is accomplished in two steps; in the first step, traffic is randomly distributed across the network, and in the second step traffic is routed to the final destination. One of the benefits of RLB is that packets experience only a single stage of routing, thus reducing queueing delays associated with multi-hop architectures. In this paper, we study the queuing performance of RLB, both through analytical methods and packet-level simulations using ns2 on three representative carrier networks. We show that purely random traffic splitting in the randomization step of RLB leads to higher queuing delays than pseudo-random splitting using, e.g., a round-robin schedule. Furthermore, we show that, for pseudo-random scheduling, queuing delays depend significantly on the degree of uniformity of the offered demand patterns, with uniform demand matrices representing a provably worst-case scenario. These results are independent of whether RLB employs priority mechanisms between traffic from step one over step two. A comparison with multi-hop shortest-path routing reveals that RLB eliminates the occurrence of demand-specific hot spots in the network."
ae11f2d5cfe4b16c8036107f69627ee6fd89d99e,"Channel-aware scheduling strategies have emerged as an effective mechanism for improving the throughput of wireless data users by exploiting rate variations. The improvement in throughput comes however at the expense of an increase in the variability of the service rate received over time. While the larger variability only has a limited impact on delay-tolerant data transfers, it does severely affect delay-sensitive applications. In order to examine the merits of channel-aware scheduling for the latter users, we consider a wireless system supporting a combination of streaming and elastic traffic. We first examine a scenario with rate-adaptive streaming traffic, and analyze the flow-level performance in terms of transfer delays and user throughputs for various canonical resource sharing schemes. Simulation experiments demonstrate that the analytical results yield remarkably accurate estimates, and indicate that channel-aware scheduling achieves significant performance gains. Next we investigate a scenario where the streaming sources have an intrinsic rate profile and stringent delay requirements. In that case, channel-aware scheduling yields only modest performance gains, and may even be harmful."
bb7e9178fbf67a2b47b0df58d3f7769ee3219c2d,
df8ee06f016b5eddf083f449430811e1b388b349,"While the (Egalitarian) Processor-Sharing (PS) discipline offers crucial insights in the performance of fair resource allocation mechanisms, it is inherently limited in analyzing and designing differentiated scheduling algorithms such as Weighted Fair Queueing and Weighted Round-Robin. The Discriminatory Processor-Sharing (DPS) and Generalized Processor-Sharing (GPS) disciplines have emerged as natural generalizations for modeling the performance of such service differentiation mechanisms. A further extension of the ordinary PS policy is the Multilevel Processor-Sharing (MLPS) discipline, which has captured a pivotal role in the analysis, design and implementation of size-based scheduling strategies. We review various key results for DPS, GPS and MLPS models, highlighting to what extent these disciplines inherit desirable properties from ordinary PS or are capable of delivering service differentiation."
e1a922f1acb9cbb89db2cd15cb3dcf67e2579ad3,
08781ed878a5a8b92a187445d18179509fd36b38,"• A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. • The final author version and the galley proof are versions of the publication after peer review. • The final published version features the final layout of the paper including the volume, issue and page numbers."
1028b8a31158a281a33969b12eae4901659fe895,"We consider a queuing system with a workload-dependent service rate. We specifically assume that the service rate is first increasing and then decreasing as a function of the amount of work. The latter qualitative behavior is quite common in practical situations, such as production systems. The admission of work into the system is controlled by a policy for accepting or rejecting jobs, depending on the state of the system. We seek an admission control policy that maximizes the long-run throughput. Under certain conditions, we show that a threshold policy is optimal, and we derive a criterion for determining the optimal threshold value."
1690bf3bfe7a645e8dac2c6b27444dc0d8ac57f4,"The capacity region of the Gaussian multi-antenna broadcast channel was characterized recently in [19]. It was shown that a scheme based on Dirty Paper Coding [2] achieves the full capacity region when the transmitter has perfect channel state information. However, this scheme potentially involves considerable amounts of feedback and complex algorithms for coding and user selection. This has led to a quest for practical transmission schemes and ways to reduce the amount of channel state information required. In particular, it has been shown that when the total number of users is large, the sum capacity can be closely approached by transmitting to a small subset of near-orthogonal users. In order to further quantify the latter observation, we study a Gaussian broadcast channel with two transmit antennas and K statistically identical, independent users each with a single receive antenna. We obtain an exact asymptotic characterization of the gap between the full sum capacity and the rate that can be achieved by transmitting to a suitably selected pair of users. Specifically, we consider various simple schemes for user-pair selection that take into account the channel norms as well as the relative orientation of the channel vectors. We conclude that a scheme that picks the strongest user and selects a second user to form the best pair, is asymptotically optimal, while also being attractive in terms of feedback and operational complexity."
1e59cfc2d7ada99adaf96ed558051764002986f9,
207f66e366e5c7d28eafe1d71f88812faffcb350,
2339c1740ffa858ef4c7fb24d8af7ee02b11f1c4,
3db45516bf686ae97583f6e6b6b29e10403254dc,"Over the past few years, the processor-sharing discipline has emerged as a useful paradigm for evaluating the flowlevel performance of elastic data transfers competing for bandwidth on a single bottle-neck link. Bandwidth-sharing networks as considered by Massoulie & Roberts [2] provide a natural extension for modeling the dynamic interaction among competing elastic flows that traverse several links. Bonald & Massoulie [1] showed that a wide class of α-fair bandwidth-sharing policies as introduced by Mo & Walrand [3] achieve stability in such networks under the simple (and necessary) condition that no individual link is overloaded. While stability is arguably the most fundamental performance criterion, flow-level delays and throughputs are obviously crucial metrics too. Although useful approximations and bounds have been obtained, the latter performance metrics have largely remained intractable in all but a few special cases. In particular, it is not well understood to what extent"
442e28a9913b8269a58350e88688182da3f5bba0,"Channel-aware scheduling strategies provide an effective mechanism for improving the throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mainly been examined at the packet level for a static user population, often assuming infinite backlogs. Recently, some studies have also explored the flow-level performance in a scenario with user dynamics governed by the arrival and completion of random service demands over time. Although in certain cases the performance may be evaluated by means of a Processor-Sharing model, in general the flow-level behavior has remained largely intractable, even basic stability properties. In the present paper we derive simple necessary stability conditions, and show that these are also sufficient for a wide class of utility-based scheduling policies. This contrasts with the fact that the latter class of strategies generally fail to provide maximum-throughput guarantees at the packet level."
47145dd9e804fa880032d9e14bf1ba51f4841f82,"We consider a system with two service classes with heterogeneous traffic characteristics and Quality-of-Service requirements. The available bandwidth is shared between the two traffic classes in accordance with the Generalized Processor Sharing (GPS) discipline. GPS-based scheduling algorithms, such as Weighted Fair Queueing, provide a popular mechanism for service differentiation among heterogeneous traffic classes. While the performance of GPS for given weights has been thoroughly examined, the problem of selecting weight values that maximize the traffic-carrying capacity, has only received limited attention so far. In the present paper, we address the latter problem for the case of general Gaussian traffic sources. Gaussian models cover a wide variety of both long-range dependent and short-range dependent processes, and are especially suitable at relatively high levels of aggregation. In particular, we determine the realizable region, i.e., the combinations of traffic sources that can be supported for given Quality-of-Service requirements in terms of loss and delay metrics. The results yield the remarkable observation that simple priority scheduling strategies achieve nearly the full realizable region. 1."
6a643248fac248435bce76b7c8e7b7ccf416a6bd,"We consider the flow-level performance of a linear network supporting elastic traffic, where the service capacity is shared among the various classes of users according to a weighted alpha-fair policy. Assuming Poisson arrivals and exponentially distributed service requirements for each class, the dynamics of the user population may be described by a Markov process. While valuable stability results have been established for the family of alpha-fair policies, the distribution of the number of active users has remained intractable in all but a few special cases. In order to gain further insight in the flow-level performance in more general scenarios, we develop approximations for the mean number of users based on the assumption that one or two of the nodes experience heavy-traffic conditions.In case of just a single 'bottleneck' node, we exploit the fact that this node approximately behaves as a two-class Discriminatory Processor-Sharing model. In the case that there are two nodes critically loaded, we rely on the observation that the joint workload process at these nodes is asymptotically independent of the fairness coefficient alpha, provided all classes have equal weights. In particular, the distribution of the joint workload process is roughly equal to that for an unweighted Proportional Fair policy, which is exactly known. In both cases, the numbers of users at non-bottleneck nodes can be approximated by that in an M/M/1 queue with reduced service capacity. Extensive numerical experiments indicate that the resulting approximations tend to be reasonably accurate across a wide range of parameters, even at relatively moderate load values. The approximations for the mean number of users also provide useful estimates for the mean transfer delays and user throughputs."
7a93810c6084b0c0a0414ce3260b10eafedd610f,"Bandwidth-sharing networks as considered by Massoulie & Roberts provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Although valuable stability results have been obtained, crucial performance metrics such as flow-level delays and throughputs in these models have remained intractable in all but a few special cases. In particular, it is not well understood to what extent flow-level delays and throughputs achieved by standard bandwidth-sharing mechanisms such as alpha-fair strategies leave potential room for improvement. In order to gain a better understanding of the latter issue, we set out to determine the scheduling policies that minimize the mean delay in some simple linear bandwidth-sharing networks. We compare the performance of the optimal policy with that of various alpha-fair strategies so as to assess the efficacy of the latter and gauge the potential room for improvement. The results indicate that the optimal policy achieves only modest improvements, even when the value of a is simply fixed, provided it is not too small."
7ce0ea1536d541e51977ab5d5670409930a712e9,
861479d52bbfa29ea416c3f4f29da767e9e95e16,
8c8ac36d472d8ebd4c0efec036dcb3f01000d6ac,"We examine the stability of multi-class queueing systems with the special feature that the service rates of the various classes depend on the number of users present of each of the classes. As a result, the various classes interact in a complex dynamic fashion. Such models arise in several contexts, especially in wireless networks, as resource sharing algorithms become increasingly elaborate, giving rise to scaling efficiencies and complicated interdependencies among traffic classes. Under certain monotonicity assumptions we provide an exact characterization of stability region. We also discuss how some of the results extend to weaker notions of monotonicity. The results are illustrated for simple examples of wireless networks with two or three interfering base stations."
9d91250ef2d2fa89f0eacdd8664e37e3dbc84284,
9ee2e9c6eac32241b3f964117576dfcbd5ad6179,
db018f057723acaaffea8cc4dc69d3d02b36fc01,"Abstract—The performance of wireless data systems has been thoroughly studied in the context of a single base station. In the present paper we analyze networks with several interacting base stations, and specifically examine the capacity impact of intraand inter-cell mobility. We consider a dynamic setting where users come and go over time as governed by random finite-size data transfers, and explicitly allow for users to roam around over the course of their service. We show that mobility tends to increase the capacity, not only in case of globally optimal scheduling, but also when each of the base stations operates according to a fair sharing policy. The latter approach offers the advantages that it avoids complex centralized control, and grants each user a fair share of the resources, preventing the potential starvation that may occur under a globally optimal strategy. An important implication is that a simple, conservative capacity estimate is obtained by ‘ignoring’ mobility, and assuming that users remain stationary for the duration of their service. We further demonstrate that the capacity region for globally optimal scheduling is in general strictly larger than the stability region for a fair sharing discipline. However, if the users distribute themselves so as to maximize their individual throughputs, thus enabling some implicit coordination, then a fair sharing policy is in fact guaranteed to achieve stability whenever a globally optimal strategy is able to do so."
de4ba36bbf0b9fecaf3a350f949370263727b82e,"We study the problem of efficiently scheduling users in a Gaussian broadcast channel with M transmit antennas and K independent receivers, each with a single antenna. We first focus on a scenario with two transmit antennas and statistically identical users, and analyze the gap between the full sum capacity and the rate that can be achieved by transmitting to a suitably selected pair of users. In particular, we consider a scheme that picks the user with the largest channel gain, and selects a second user from the next L - 1 strongest ones to form the best pair, taking channel orientations into account as well. We prove that the expected rate gap converges to 1/(L- 1) nats/symbol when the total number of users K tends to infinity. Allowing L to increase with K, it may be deduced that transmitting to a properly chosen pair of users is asymptotically optimal, while considerably reducing the feedback overhead and scheduling complexity. Next, we tackle the problem of maximizing a weighted sum rate in a scenario with heterogeneous user characteristics. We establish a novel upper bound for the weighted sum capacity, which we then use to show that the maximum expected weighted sum rate can be asymptotically achieved by transmitting to a suitably selected subset of at most MC  users, where C denotes the number of distinct user classes. Numerical experiments indicate that the asymptotic results are remarkably accurate and that the proposed schemes operate close to absolute performance bounds, even for a moderate number of users."
f0b1f296d1908bcfdcb0519820db22d020942823,"Over the past few years, the design and performance of channel-aware scheduling strategies have attracted huge interest. In the present paper, we examine a somewhat different notion of scheduling, namely coordination of transmissions among base stations, which has received little attention so far. The inter-cell coordination comprises two key elements: (i) interference avoidance and (ii) load balancing. The interference avoidance involves coordinating the activity phases of interfering base stations so as to increase transmission rates. The load balancing aims at diverting traffic from heavily loaded cells to lightly loaded cells. Numerical experiments demonstrate that inter-cell scheduling may provide significant capacity gains."
130e7ed735f5fc713306b11fa540de80af6c8254,
1a7742b9c00cde7b24d8b19d23d83a82c9ce75e9,
1e02550c778f0d48edc2b314de6e9296d134529b,"With multiple air-interface support capabilities and higher cell densities, future cellular networks will offer a diverse spectrum of user services. The resulting dynamics in traffic load and resource demand will challenge present control loop algorithms. In addition, frequent upgrades in the network infrastructure will substantially increase the network operation costs if done using current optimization methodology. This motivates the development of dynamic control algorithms that can automatically adjust the network to changes in both traffic and network conditions and autonomously adapt when new cells are added to the system. Bell Labs is pursuing efforts to realize such algorithms with research on near-term approaches that benefit present third-generation (3G) systems and the development of control features for future networks that perform dynamic parameter adjustment across protocol layers. In this paper, we describe the development of conceptual approaches, algorithms, modeling, simulation, and real-time measurements that provide the foundation for future dynamic network optimization techniques."
23773fcf77684d03361fc7e9de305a93540b5f6b,"We consider a fluid queue fed by several heterogeneous M/G/∞ input processes with regularly varying session lengths. Under fairly mild assumptions, we derive the exact asymptotic behavior of the stationary workload distribution. In addition, we obtain several asymptotic results for the transient workload distribution, which are applied to obtain a conditional limit theorem for the most probable time to overflow. 
 
The results are strongly inspired by the large-deviations idea that overflow is typically due to some minimal combination of extremely long concurrent sessions causing positive drift. The typical configuration of long sessions is identified through a simple integer program, paving the way for the exact computation of the asymptotic workload behavior. The calculations provide crucial insight in the typical overflow scenario."
2a41b7ea16e65fd85a7412710c8903eb6ea2796c,"Over the past few years, the design and performance of channel-aware scheduling strategies have attracted huge interest. In the present paper we examine a different notion of scheduling, namely coordination of transmissions among base stations, which has received little attention so far. The inter-cell coordination comprises two key elements: (i) interference avoidance; and (ii) load balancing. The interference avoidance involves coordinating the activity phases of interfering base stations so as to increase transmission rates. The load balancing aims at diverting traffic from heavily-loaded cells to lightly-loaded cells. We consider a dynamic scenario where users come and go over time as governed by the arrival and completion of random data transfers, and evaluate the potential capacity gains from inter-cell coordination in terms of the maximum amount of traffic that can be supported for a given spatial traffic pattern. Numerical experiments demonstrate that inter-cell scheduling may provide significant capacity gains, the relative contribution from interference avoidance vs. load balancing depending on the configuration and the degree of load imbalance in the network."
39f3f562a81f01d40645e88e01ef91f7b553fdd4,
7bb0780207e51c5d15fccfbbb34029b64154c135,
7c97cd351acced8a45b59c4b8839a0fbb2a1be22,
9a0d1032c95d5438c2917d7dc2b8b3e7845e0f05,
b2068f5908af0631534ff3809ff8027ff52c8e5f,"We consider a multi-class queueing system operating under the discriminatory processor-sharing (DPS) discipline. The DPS discipline provides a natural approach for modeling the flow-level performance of differentiated bandwidth-sharing mechanisms. Motivated by the extreme diversity in flow sizes observed in the Internet, we examine the system performance in an asymptotic regime where the flow dynamics of the various classes occur on separate time scales. Specifically, from the perspective of a given class, the arrival and service completions of some of the competing classes (called mice) evolve on an extremely fast time scale. In contrast, the flow dynamics of the remaining classes (referred to as elephants) occur on a comparatively slow time scale. Assuming a strict separation of time scales, we obtain simple explicit expressions for various performance measures of interest, such as the distribution of the numbers of flows, mean delays, and flow throughputs. In particular, the latter performance measures are insensitive, in the sense that they only depend on the service requirement distributions through their first moments. Numerical experiments show that the limiting results provide remarkably accurate approximations in certain cases."
b21a915577dfa5b391d2b70bb01773a8fe19161f,"Channel-aware scheduling strategies, such as the Proportional Fair algorithm for the CDMA 1xEV-DO system, provide an effective mechanism for improving throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mostly been explored at the packet level for a static user population, often assuming infinite backlogs. In the present paper, we focus on the performance at the flow level in a dynamic setting with random finite-size service demands. We show that in certain cases the user-level performance may be evaluated by means of a multiclass Processor-Sharing model where the total service rate varies with the total number of users. The latter model provides explicit formulas for the distribution of the number of active users of the various classes, the mean response times, the blocking probabilities, and the throughput. In addition we show that, in the presence of channel variations, greedy, myopic strategies which maximize throughput in a static scenario, may result in sub-optimal throughput performance for a dynamic user configuration and cause potential instability effects."
f344cfb02a8971da72cfd8408c647763815fee80,
fff4d998fe51df217e03f39f1a215fa3975977b5,
00a697f3dd8b30a1b8f4fa1d836b4c4f490e9624,"We consider a fluid queue fed by multiple On–Off flows with heavy-tailed (regularly varying) On periods. Under fairly mild assumptions, we prove that the workload distribution is asymptotically equivalent to that in a reduced system. The reduced system consists of a “dominant” subset of the flows, with the original service rate subtracted by the mean rate of the other flows. We describe how a dominant set may be determined from a simple knapsack formulation. 
 
The dominant set consists of a “minimally critical” set of On–Off flows with regularly varying On periods. In case the dominant set contains just a single On–Off flow, the exact asymptotics for the reduced system follow from known results. For the case of several On–Off flows, we exploit a powerful intuitive argument to obtain the exact asymptotics. Combined with the reduced-load equivalence, the results for the reduced system provide a characterization of the tail of the workload distribution for a wide range of traffic scenarios."
1c54487daecadbbbc513b5f3535c28f76c23e7f5,"We consider a fixed number of streaming sessions sharing a bottleneck link with a dynamic population of elastic flows. We assume that the sizes of the elastic flows exhibit heavy-tailed characteristics. The elastic flows are TCP-controlled, while the transmission rates of the streaming applications are governed by a so-called TCP-friendly rate control protocol.
 Adopting the Processor-Sharing (PS) discipline to model the bandwidth sharing, we investigate the tail distribution of the deficit in service received by the streaming sessions compared to a nominal service target. The latter metric provides an indication for the quality experienced by the streaming applications. The results yield valuable qualitative insight into the occurrence of persistent quality disruption for the streaming users. We also examine the delay performance of the elastic flows."
50fd9578d2d6c6ef77324e839b59f010dbfaa022,"The potential for exploiting rate variations to increase the capacity of wireless systems by opportunistic scheduling has been extensively studied at packet level. In the present paper, we examine how slower, mobility-induced rate variations impact performance at flow level, accounting for the random number of flows sharing the transmission resource. We identify two limit regimes, termed fluid and quasistationary, where the rate variations occur on an infinitely fast and an infinitely slow time scale, respectively. Using stochastic comparison techniques, we show that these limit regimes provide simple performance bounds that only depend on easily calculated load factors. Additionally, we prove that for a broad class of fading processes, performance varies monotically with the speed of the rate variations. These results are illustrated through numerical experiments, showing that the fluid and quasistationary bounds are remarkably tight in certain usual cases"
522f6cdca3538a37e6d70974d3ad760e0ae6c2c2,
57925223cc480c52b580856f21ed7db6b85fbe99,
5b23eeacb1ae81bcb8fcca480d26584f04e1f5ac,"Channel-aware scheduling strategies, such as the Proportional Fair algorithm for the CDMA 1xEV-DO system, provide an effective mechanism for improving throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channelaware scheduling algorithms has mostly been explored at the packet level for a static user population, sometimes including packet-scale dynamics, but often assuming infinite backlogs. The assumption of a static user population is a reasonable modeling convention because scheduling algorithms tend to operate on a much finer time scale than the flow-level dynamics. However, when examining throughput performance, and comparing the throughput allocation among elastic traffic streams under various strategies, it is not satisfactory to assume that the user population is independent of the throughput characteristics and the parameter settings of the scheduling algorithm. In order to capture the interdependence between the scheduling algorithm and the user population, we move away from a static scenario with a fixed ensemble of persistent users, and consider a dynamic setting where elastic traffic flows come and go over time as governed by the arrival and completion of random service demands. We show that in certain cases the flow-level performance may be evaluated by means of a multi-class Processor-Sharing model where the total service rate varies with the total number of flows. The latter model provides explicit formulas for the distribution of the number of active flows of the various classes, the mean response times, the blocking probabilities, and the flow throughput. In the presence of varying channel conditions, it further turns out that it is not so much maximizing the instantaneous throughput in an absolute sense that determines stability, but rather taking maximum advantage of the relative channel variations. In particular, we show that greedy, myopic strategies that maximize throughput in a static scenario, may result in sub-optimal throughput performance for a dynamic user configuration and cause potential instability effects."
5c92e8b3a37581a50c1c8baca2adec0b737c359f,"The potential for exploiting rate variations to improve the performance of wireless data networks by opportunistic scheduling has been extensively studied at the packet level. In the present paper, we examine how slower, mobility-induced rate variations impact the performance at the flow level, accounting for the dynamic number of users sharing the transmission resource. We identify two limit regimes, termed fluid regime and quasi-stationary regime, where the rate variations occur on an infinitely fast and an infinitely slow time scale, respectively. Using stochastic comparison techniques, we show that these limit regimes provide simple, insensitive performance bounds that only depend on easily calculated load factors. Additionally, we prove that for a broad class of Markov-type fading processes, the performance varies monotonically with the time scale of the rate variations. The results are illustrated through numerical experiments, showing that the fluid and quasi-stationary bounds are remarkably sharp in certain typical cases."
79c4bb12948b7b80c76a9c82eba3bc92223aa06c,
9188b4e20a7d656d0a7be8408875a4441801c7a9,
ba813f1f942915afa3c5566bf3649717499f2670,
be8d06e94da2a7b546514394f2320284737ec21d,
c35813aaa0065811a8c52b795dec0a194472091e,
da660d3021bbec4e6320f88114c336e97a20e458,"The performance of wireless data systems has been extensively studied in the context of a single base station. In the present paper we investigate the flow-level performance in networks with multiple base stations. We specifically examine the complex, dynamic interaction of the number of active flows in the various cells introduced by the strong impact of interference between neighboring base stations. For the downlink data transmissions that we consider, lower service rates caused by increased interference from neighboring base stations result in longer delays and thus a higher number of active flows. This in turn results in a longer duration of interference on surrounding base stations, causing a strong correlation between the activity states of the base stations. Such a system can be modelled as a network of multi-class processor-sharing queues, where the service rates for the various classes at each queue vary over time as governed by the activity state of the other queues. The complex interaction between the various queues renders an exact analysis intractable in general. A simplified network with only one class per queue reduces to a coupled-processors model, for which there are few results, even in the case of two queues. We thus derive bounds and approximations for key performance metrics like the number of active flows, transfer delays, and flow throughputs in the various cells. Importantly, these bounds and approximations are insensitive, yielding simple expressions, that render the detailed statistical characteristics of the system largely irrelevant."
e4e60973e2e3a81a42dadaf6314a0e8327b845cc,"We develop a stochastic mean-value method for the derivation of delay asymptotics in Processor-Sharing (PS) type systems with heavy-tailed service requirements. In order to demonstrate the strength of the approach, we apply the method to obtain the sojourn time asymptotics for a multi-class G/G/1 queue operating under the Discriminatory Processor-Sharing (DPS) discipline. Besides interesting from a queueing-theoretic perspective, DPS is also of practical relevance as it provides a useful paradigm for modelling the flow-level performance of differentiated resource-sharing mechanisms. Unlike for ordinary PS, however, the queue length for DPS does not have a simple distribution, and there are no manageable transform results available for the sojourn time. These circumstances seriously complicate the derivation of delay asymptotics using existing proof methods, and render DPS as a good ‘test case’ for judging the merits of alternative approaches. We use the stochastic mean-value method to show that under certain assumptions, the service requirement and sojourn time of a given class have similar tail behaviour, independent of the specific values of the DPS weights. The results suggest that DPS offers a useful instrument for effectuating preferential treatment to smaller service demands without inflicting excessive delays on larger requests. We also briefly discuss the potential applicability of the method for deriving the delay asymptotics under a broader class of resource-sharing strategies."
f597748ee47bbfffe1331ddc5bd39e95cac462e4,
fb02a4837ff3cece42e2934253963441bd024628,"We study the joint queue length distribution of the Discriminatory Processor Sharing model, assuming all classes have phase-type service requirement distributions. We show that the moments of the joint queue length distribution can be obtained by solving linear equations. We use this to study the system in two asymptotic regimes. In the first regime, the different user classes operate on strictly separated time scales. Then we study the system in heavy traffic."
05d98d04fae37d5e10f4246ad55d385b23f4475c,"We consider a queue fed by a mixture of light-tailed and heavy-tailed traffic. The two traffic flows are served in accordance with the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing (WFQ), have emerged as an important mechanism for achieving service differentiation in integrated networks. We derive the asymptotic workload behaviour of the light-tailed traffic flow under the assumption that its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed flow served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is, in fact, asymptotically equivalent to that in the isolated system, multiplied by a certain prefactor, which accounts for the interaction with the heavy-tailed flow. Specifically, the prefactor represents the probability that the heavy-tailed flow is backlogged long enough for the light-tailed flow to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario."
117d5f964d8376bcf8d8ae4c049c9bd6d4467050,"The relative delay tolerance of data applications, together with bursty traffic characteristics, opens up the possibility for scheduling transmissions so as to optimize throughput. A particularly attractive approach in fading environments is to exploit the variations in the channel conditions and transmit to the user with the current ""best"" channel. We show that the ""best"" user may be identified as the maximum-rate user when feasible rates are weighted with some appropriately determined coefficients. Interpreting the coefficients as shadow prices, or reward values, the optimal strategy may thus be viewed as a revenue-based policy, which always assigns the transmission slot to the user yielding the maximum revenue. Calculating the optimal-revenue vector directly is a formidable task, requiring detailed information on the channel statistics. Instead, we present adaptive algorithms for determining the optimal-revenue vector online in an iterative fashion, without the need for explicit knowledge of the channel behavior. Starting from an arbitrary initial vector, the algorithms iteratively adjust the reward values to compensate for observed deviations from the target throughput rates. The algorithms are validated through extensive numerical experiments. Besides verifying long-run convergence, we also examine the transient performance, in particular the rate of convergence to the optimal-revenue vector. The results show that the target throughput ratios are tightly maintained and that the algorithms are well able to track sudden changes in channel conditions or throughput targets."
19a50405d34739f20981b644cb11c38f53cd09f6,"We consider a queue fed by a mixture of light-tailed and heavy-tailed traffic. The two traffic flows are served in accordance with the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing, have emerged as an important mechanism for achieving service differentiation in integrated networks. We derive the asymptotic workload behavior of the light-tailed traffic flow under the assumption that its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed flow served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is in fact asymptotically equivalent to that in the isolated system, multiplied with a certain pre-factor, which accounts for the interaction with the heavy-tailed flow. Specifically, the pre-factor represents the probability that the heavy-tailed flow is backlogged long enough for the light-tailed flow to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario."
511dae566542a90a76d951781bc7885236919dee,
58965750433a4c4eb1b77d490521e581cc1db986,"We determine the exact large-buffer asymptotics for a mixture of light-tailed and heavy-tailed input flows. Earlier studies have found a ‘reduced-load equivalence’ in situations where the peak rate of the heavy-tailed flows plus the mean rate of the light-tailed flows is larger than the service rate. In that case, the workload is asymptotically equivalent to that in a reduced system, which consists of a certain ‘dominant’ subset of the heavy-tailed flows, with the service rate reduced by the mean rate of all other flows. In the present paper, we focus on the opposite case where the peak rate of the heavy-tailed flows plus the mean rate of the light-tailed flows is smaller than the service rate. Under mild assumptions, we prove that the workload distribution is asymptotically equivalent to that in a somewhat ‘dual’ reduced system, multiplied by a certain prefactor. The reduced system now consists of only the light-tailed flows, with the service rate reduced by the peak rate of the heavy-tailed flows. The prefactor represents the probability that the heavy-tailed flows have sent at their peak rate for more than a certain amount of time, which may be interpreted as the ‘time to overflow’ for the light-tailed flows in the reduced system. The results provide crucial insight into the typical overflow scenario."
5ba123064b4419b7892741460b677995c7f14098,
66c40aad9c3fdc28e9da15805bc819bc323b105a,"We investigate the tail asymptotics of the supremumof X(t)+Y(t)−ct,where X = {X(t),t≥ 0} and Y = {Y(t),t≥ 0} are two independentstochastic processes. We assume that the process Y has subexponen-tial characteristics and that the process X is more regular in a certainsense than Y. A key issue examined in earlier studies is under whatconditions the process X contributes to large values of the supremumonly through its average behavior. The present paper studies variousscenarios where the latter is not the case, and the process X showssome form of “atypical” behavior as well. In particular, we considera ﬂuid model fed by a Gaussian process X and an (integrated) On-Oﬀ process Y. We show that, depending on the model parameters,the Gaussian process may contribute to the tail asymptotics by itsmoderate deviations, large deviations, or oscillatory behavior."
72cfb2a28b22ae6e45cafae50d4fbb19a8cff83b,
78edafdc5e1931e0242374b47177835c02bab904,
8396b71874cc8111b6197fc4fbc7ff20d0ab323d,"Third-generation (3G) wireless systems such as 3G1X, 1×EV-DO, and 1xEV-DV provide support for a variety of high-speed data applications. The success of these services critically relies on the capability to ensure an adequate quality of service (QoS) experience to users at an affordable price. With wireless bandwidth at a premium, traffic engineering and network planning play a vital role in addressing these challenges. We present models and techniques that we have developed for quantifying the QoS perception of 1×EV-DO users generating file transfer protocol (FTP) or Web browsing sessions. We show how user-level QoS measures may be evaluated by means of a Processor-Sharing model that explicitly accounts for the throughput gains from multi-user scheduling. The model provides simple analytical formulas for key performance metrics such as response times, blocking probabilities, and throughput. Analytical models are especially useful for network deployment and in-service tuning purposes due to the intrinsic difficulties associated with simulation-based optimization approaches. © 2003 Lucent Technologies Inc."
b8ba8491a4783a769f4729005448d9d548deb077,
bd0546b95f552135f5412c2099eb153bcc18e6af,
bd9aff53bb863f8a953ef810f17ffb25d92c2f6f,"The Medium Access Control (MAC) scheme proposed by DAVIC/DVB, IEEE 802.14 and DOCSIS for the upstream channel of Hybrid Fiber Coaxial (HFC) access networks is based on a mixable contention-based/contention-less time slot assignment. Contention-less slots are assigned by the head end to end stations according to a reservation scheme. Contention-based slots are randomly accessed by active terminals without any preliminary allocation, so that collisions may occur. To resolve contention, the contention tree algorithm has been widely accepted by the DVB/DAVIC, IEEE 802.14 and DOCSIS standards for MAC because of higher throughput and lower access delay. In this paper we propose a novel contention resolution mechanism and compare its performance with that of existing procedures. The proposed procedure is termed as static arrival slot mechanism. In this mechanism, one slot in each frame is exclusively reserved for new arrivals that wish to access the channel using contention resolution, and at least one slot is reserved for resolving their contention if there was one in the arrival slot. The performance of the proposed mechanism is evaluated through analysis and simulation. The results show that the proposed mechanism outperforms existing contention resolution procedures under heavy traffic."
c9d4b7c5cecb4d47c5b2c3c250ac2325fc5c49b1,"Channel-aware scheduling strategies, such as the Proportional Fair algorithm for the CDMA 1xEV-DO system, provide an effective mechanism for improving throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mostly been explored at the packet level for a static user population, often assuming infinite backlogs. In the present paper, we focus on the performance at the flow level in a dynamic setting with random finite-size service demands. We show that in certain cases the user-level performance may be evaluated by means of a multiclass Processor-Sharing model where the total service rate varies with the total number of users. The latter model provides explicit formulas for the distribution of the number of active users of the various classes, the mean response times, the blocking probabilities, and the mean throughput. In addition we show that, in the presence of channel variations, greedy, myopic strategies which maximize throughput in a static scenario, may result in sub-optimal throughput performance for a dynamic user configuration and cause potential instability effects."
cd0db1e590dcfe54db16ffae0c6f093d4fb429fc,
cedb08171c83de1a82e80a2ac4377206d7d5ca07,
e73ae52c8c568cdf22330ce6d088f44afb6d0e39,
1e01272576103d55368a1f2a9f42ffa7fb5f4c5e,
3e996c8af739af0e9aae794ef3d93003134a016d,
5237e8e74a4a4502ecebd8827d004654db677f74,
583cd47fe87b6586246c43c91c21e6f29d3f1ab0,
67b5ae569a77f53c712dbc250c80f8f23ef67f08,
7835b3c906772b3e8031223b230c09204b0a9cbe,
9fea4ed51062a8c2dd1e37253c7d3ff122b1dfaa,"A walled structure of cast concrete is provided having a vertical sheet of flexible corrosion-resistant sheet material formed to the desired wall contour and having an exposed face surface, a series of vertical ribs on the sheet opposite the face surface, a series of spaced support members engaged in a base support having a U-shaped receptacle on the top thereof receiving the bottom edge of said sheet to hold the bottom edge in a desired selected position, a plurality of spaced vertical standards engaged in said base support and extending to a point adjacent the top edge of the vertical sheet and engaged with the ribs on said sheet and a continuous cast concrete layer surrounding the vertical sheet opposite the face surface from a point adjacent the top edge to the base support and extending beneath and engaging a portion of the lower edge of said sheet."
aca0c5e0df907acdb1530346fdd47720c19465e1,
c293a7fa28e2b1325789d318569846beb341fab4,"We consider a queue fed by a mixture of light-tailed and heavy-tailed traffic. The two traffic classes are served in accordance with the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing (WFQ), have emerged as an important mechanism for achieving service differentiation in integrated networks. We derive the asymptotic workload behavior of the light-tailed class for the situation where its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed class served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is in fact asymptotically equivalent to that in the isolated system, multiplied with a certain pre-factor, which accounts for the interaction with the heavy-tailed class. Specifically, the pre-factor represents the probability that the heavy-tailed class is backlogged long enough for the light-tailed class to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario."
d3c86fcb6432eb815df26aff9ace987546a1d7fc,
eaa843bc798d1b4f1d41f0514f22cdc138ee9670,
2161b9da44c965afed41f40f41eb4974fcd30c16,
2b8bb00a390c17440460990e274f6bf0abe91fd5,"We consider a fluid queue fed by several heterogeneous M / G / 00 input processes with regularly varying session lengths. Under fairly mild assumptions, we derive the exact asymptotic behavior of the stationary workload distribution. As a by-product, we obtain asymptotically tight bounds for the transient workload distribution. The results are strongly inspired by the large-deviations idea that overflow is typically due to some minimal combination of extremely long concurrent sessions causing positive drift. The typical configuration of long sessions is identified through a simple integer program, paving the way for the exact computation of the asymptotic workload behavior. The calculations provide crucial insight in the typical overflow scenario. 2000 Mathematics Subject Classification: 60K25 (primary), 60FIO, 90B18, 90B22 (secondary)."
4f033191f603ced63176df952a5f970bac591979,"We consider a system with two heterogeneous traffic classes, one having light-tailed characteristics, the other one exhibiting heavy-tailed properties. The two traffic classes are served in accordance with the Generalized Processor Sharing (GPS) discipline. GPS-based scheduling algorithms, such as Weighted Fair Queueing (WFQ), have emerged as an important mechanism for achieving service differentiation in integrated-services networks.We determine the workload asymptotics of the light-tailed class for the situation where its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed class served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is in fact asymptotically equivalent to that in the isolated system, multiplied with a certain pre-factor, which accounts for the interaction with the heavy-tailed class. Specifically, the pre-factor represents the probability that the heavy-tailed class is backlogged long enough for the light-tailed class to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario."
640d850c6111cbe07d08e68a5cca16b006ed7a16,"We consider a fluid queue fed by multiple on-off flows with heavy-tailed (regularly varying) on-periods. Under fairly mild assumptions, we prove that the workload distribution is asymptotically equivalent to that in a reduced system. The reduced system consists of a dominant subset of the flows, with the original service rate subtracted by the mean rate of the other flows. We describe how a dominant set may be determined from a simple knapsack formulation. We exploit a powerful intuitive argument to obtain the exact asymptotics for the reduced system. Combined with the reduced-load equivalence, the results for the reduced system provide an asymptotic characterization of the buffer behavior."
af0c42939d03c7cac0fd9e6bc1c9ec0468245d75,"To improve efficiency, we allow a carrier in a TDMA (Time Division Multiple Access) network to be shared by adjacent cells. This sharing of time slots is seriously hampered by the lack of synchronization in distinct cells. We study packing algorithms that overcome this obstacle by clustering calls. The results suggest that even simple greedy algorithms are nearly optimal, and that little extra performance can be gained either by allowing the rejection of calls or by repacking."
bbd610c716f0c9d7fed632e27eb451ef47229f52,"We consider networks where traffic is served according to the generalised processor sharing (GPS) principle. GPS-based scheduling algorithms are considered important for providing differentiated quality of service in integrated-services networks. We are interested in the workload of a particular flow i at the bottleneck node on its path. Flow i is assumed to have long-tailed traffic characteristics. We distinguish between two traffic scenarios, (i) flow i generates instantaneous traffic bursts and (ii) flow i generates traffic according to an on/off process. In addition, we consider two configurations of feedforward networks. First we focus on the situation where other flows join the path of flow i. Then we extend the model by adding flows which may branch off at any node, with cross traffic as a special case. We prove that under certain conditions the tail behaviour of the workload distribution of flow i is equivalent to that in a two-node tandem network where flow i is served in isolation at constant rates. These rates only depend on the traffic characteristics of the other flows through their average rates. This means that the results do not rely on any specific assumptions regarding the traffic processes of the other flows. In particular, flow i is not affected by excessive activity of flows with 'heavier-tailed' traffic characteristics. This confirms that GPS has the potential to protect individual flows against extreme behaviour of other flows, while obtaining substantial multiplexing gains."
ecf6e27d0dac90d02938dd0f45560a5b42e6d8cd,"The relative delay tolerance of data applications, together with the bursty traffic characteristics, opens up the possibility for scheduling transmissions so as to optimize throughput. A particularly attractive approach, in fading environments, is to exploit the variations in the channel conditions, and transmit to the user with the currently 'best' channel. We show that the 'best' user may be identified as the maximum-rate user when the feasible rates are weighed with some appropriately determined coefficients. Interpreting the coefficients as shadow prices, or reward values, the optimal strategy may thus be viewed as a revenue-based policy. Calculating the optimal revenue vector directly is a formidable task, requiring detailed information on the channel statistics. Instead, we present adaptive algorithms for determining the optimal revenue vector on-line in an iterative fashion, without the need for explicit knowledge of the channel behavior. Starting from an arbitrary initial vector, the algorithms iteratively adjust the reward values to compensate for observed deviations from the target throughput ratios. The algorithms are validated through extensive numerical experiments. Besides verifying long-run convergence, we also examine the transient performance, in particular the rate of convergence to the optimal revenue vector. The results show that the target throughput ratios are tightly maintained, and that the algorithms are well able to track changes in the channel conditions or throughput targets."
11c3a89efa4d9399f9f80da6bf6f28ae722c28d8,"We analyze the asymptotic behavior of long-tailed traffic sources under the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing, have emerged as an important mechanism for achieving differentiated quality-of-service in integrated-services networks. Under certain conditions, we prove that in an asymptotic sense an individual source with long-tailed traffic characteristics is effectively served at a constant rate, which may be interpreted as the maximum feasible average rate for that source to be stable. Thus, asymptotically, the source is only affected by the traffic characteristics of the other sources through their average rate. In particular, the source is essentially immune from excessive activity of sources with 'heavier'-tailed traffic characteristics. This suggests that GPS-based scheduling algorithms provide an effective mechanism for extracting high multiplexing gains, while protecting individual connections."
37827e8f1c0c74ac56f7b4280eefe97ee2ad97d0,
67010d83f27d44199814194e296b496bbcb6200c,"textabstractWe develop a framework for asymptotic optimization of a queueing system. The motivation is the staffing problem of call centers with 100's of agents (or more). Such a call center is modeled as an M/M/N queue, where the number of agents~$N$ is large. Within our framework, we determine the asymptotically optimal staffing level~$N^*$ that trades off agents' costs with service quality: the higher the latter, the more expensive is the former. As an alternative to this optimization, we also develop a constraint satisfaction approach where one chooses the least~$N^*$ that adheres to a given constraint on waiting cost. Either way, the analysis gives rise to three regimes of operation: quality-driven, where the focus is on service quality; efficiency-driven, which emphasizes agents' costs; and a rationalized regime that balances, and in fact unifies, the other two. Numerical experiments reveal remarkable accuracy of our asymptotic approximations: over a wide range of parameters, from the very small to the extremely large, $N^*$ is {em exactly/ optimal, or it is accurate to within a single agent. We demonstrate the utility of our approach by revisiting the square-root safety staffing principle, which is a long-existing rule-of-thumb for staffing the M/M/N queue. In its simplest form, our rule is as follows: if $c$ is the hourly cost of an agent, and $a$ is the hourly cost of customers' delay, then $N^* = R + y^*({a over c) sqrt R$, where $R$ is the offered load, and $y^*(cdot)$ is a function that is easily computable."
81de698ef1e4c7d18bf178c0bd84b211d1ea2b46,
9368e756ec019d3f4ee0f7c973583ccf243165e9,
a3eb79cf6a2c66a484d8b2d430aca9ef6dcf03bb,"Consider two M/G/1 queues that are coupled in the following way. Whenever both queues are non-empty, each server serves its own queue at unit speed. However, if server 2 has no work in its own queue, then it assists server 1, resulting in an increased service speed r/sub 1//sup *//spl ges/1 in the first queue. This kind of coupling is related to generalized processor sharing. We assume that the service request distributions at both queues are regularly varying at infinity of index -v/sub 1/ and -v/sub 2/, namely, they are heavy-tailed. Under this assumption, we present a detailed analysis of the tail behaviour of the workload distribution at each queue. If the guaranteed unit speed of server 1 is already sufficient to handle its offered traffic, then the workload distribution at the first queue is shown to be regularly varying at infinity of index 1-v/sub 1/. But if it is not sufficient, then the workload distribution at the first queue is shown to be regularly varying at infinity of index 1-min(v/sub 1/,v/sub 2/). In particular, traffic at server 1 is then no longer protected from worse-behaved (heavier-tailed) traffic at server 2."
af62e7498308efd31c5493c27525f10e21998aa9,"We analyze the queueing behavior of long-tailed traffic flows under the Generalized Processor Sharing (GPS) discipline. GPS-based scheduling algorithms, such as Weighted Fair Queueing, play a major role in achieving differentiated quality-of-service in integrated-services networks. We prove that, in certain scenarios, a flow may be strongly affected by the activity of `heavier''-tailed flows, and may inherit their traffic characteristics, causing induced burstiness. This phenomenon contrasts with previous results which show that, under certain conditions, an individual flow with long-tailed traffic characteristics is effectively served at a constant rate. In particular, the flow is then essentially immune from excessive activity of flows with `heavier''-tailed traffic characteristics. The sharp dichotomy in qualitative behavior illustrates the crucial importance of the weight parameters in protecting individual flows."
d7fa5bb9f97ee11234e21bdae2a9f4ab10404ed5,"We consider a fluid queue fed by the superposition of n homogeneous on-off sources with generally distributed on and off periods. The buffer space B and link rate C are scaled by n, so that we get nb and nc, respectively. Then we let n grow large. In this regime, the overflow probability decays exponentially in the number of sources n. We specifically examine the scenario where b is also large. We obtain explicit asymptotics for the case where the on periods have a subexponential distribution, e.g., Pareto, Lognormal, or Weibull. The results show a sharp dichotomy in the qualitative behavior, depending on the shape of the function v(t) := - logP(A* > t) for large t, A* representing the residual on period. If v(.) is regularly varying of index 0 (e.g., Pareto, Lognormal), then, during the path to overflow, the input rate will only slightly exceed the link rate. Consequently, the buffer will fill ‘slowly’, and the typical time to overflow will be ‘more than linear’ in the buffer size. In contrast, if v(.) is regularly varying of index strictly between 0 and 1 (e.g., Weibull), then the input rate will significantly exceed the link rate, and the time to overflow is roughly proportional to the buffer size. In both cases there is a substantial fraction of the sources that remain in the on state during the entire path to overflow, while the others contribute at their mean rates. These observations lead to approximations for the overflow probability. The approximations may be extended to the case of heterogeneous sources. The results provide further insight into the so-called reduced-load approximation."
ddb810d68888ac681ea10ad004dc0c0b551d6ac9,"We introduce a reward paradigm to derive novel bounds for the performance of dynamic channel assignment (DCA) schemes. In the case of uniform reuse, our bounds closely approach the performance of maximum packing (MP), which is an idealized DCA scheme. This suggests not only that the bounds are extremely tight, but also that no DCA scheme, however sophisticated, will be able to achieve significant capacity gains beyond those obtained from MP. Our bounds extend to varying reuse scenarios which may arise in the case of reuse partitioning techniques, measurement-based DCA schemes, or micro-cellular environments. In these cases, the bounds slightly diverge from the performance of MP, which inflicts higher blocking on outer calls than inner calls, but not to the extent required to maximize carried traffic. This reflects the inherent tradeoff that arises in the case of varying reuse between efficiency and fairness. Asymptotic analysis confirms that schemes which minimize blocking intrinsically favor inner calls over outer calls, whereas schemes which do not discriminate among calls inevitably produce higher network-average blocking. Comparisons also indicate that DCA schemes are crucial in fully extracting the potential capacity gains from tighter reuse."
08de3b66f05e4d2e48a371f47786ac464244f7f4,"Template-driven scheduling mechanisms provide a simple and robust scheme for sharing a resource among several traffic classes. The performance of such scheduling mechanisms critically relies on the regularity properties of the template. An important application, which motivated our study, arises in the context of weighted round-robin cell scheduling algorithms used for (de)multiplexing the various traffic streams in high-speed switches. In that setting, regularity of the template significantly reduces cell delay and delay variation (jitter), and equally important, the burstiness of the outgoing stream. 
 
 
 
We propose a measure for quantifying the regularity in terms of the spacing of the slots assigned to the various classes. The proposed criterion is consistent with common regularity notions, while the structural form allows for the computation of an optimal template using a quadratic assignment formulation. We also establish the asymptotic optimality of simple and fast heuristic algorithms in a scenario with a large number of traffic classes. Numerical experiments demonstrate that the heuristic procedures yield excellent results, even if the regime of asymptotic optimality does not strictly apply. Copyright © 1999 John Wiley & Sons, Ltd."
53b3b38cd5a1de0db276064eadad44daee251872,"We are pleased to present the program of the SIGMETRICS 2005 conference. The program covers a wide spectrum of performance-related topics, including peer-to-peer networks, traffic measurement and classification, bandwidth sharing and scheduling policies, network performance measurements, caching and file systems, traffic estimation and topology inference, wireless networks, network and server performance evaluation. Overall, the program features a broad mix of methodological contributions and modeling studies, balanced with a diverse range of applications of performance evaluation models and methods. 
 
The paper selection process was highly competitive. A total of 237 submissions were received, of which 31 papers were accepted for presentation at the conference, while an additional 20 papers were selected for poster presentation. The papers were selected after a rigorous review process based on their originality, technical quality and relevance to the conference. For all papers, at least three independent reviews were sought from the Program Committee, whose ranks were filled with 57 established experts representing 12 countries and a variety of backgrounds in academia, industry and government institutions. For many papers, additional reviews were obtained from external specialists, resulting in a total of over 800 reviews. The final paper selection took place at the Program Committee meeting which was held at Columbia University on January 21 and 22, 2005. Throughout the entire process, every effort was made to avoid any (potential) conflict-of-interest situations, in particular during the discussions at the Program Committee meeting. 
 
A special subcommittee decided on the following two awards: ""Coupon Replication Systems"" by Laurent Massoulie, Milan Vojnovic (Microsoft Research) as the conference's best paper, and ""A Network Service Curve Approach for the Stochastic Analysis of Networks"" by Florin Ciucu, Almut Burchard, Jorg Liebeherr (University of Virginia) as the best student paper. 
 
Jussara Almeida and Thomas Bonald did an excellent job publicizing the conference. We thank Martin Arlitt, Anirban Mahanti, and Camille Sinanan for their highly capable work on finance, proceedings, and registration/local arrangements, respectively. Leana Golubchik, SIGMETRICS Chair, gave valuable advice and support at crucial moments. We received able and professional assistance from ACM Headquarters staff members, including Caryn Chan, Irene Frawley, Adrienne Griscti, Maritza Nichols, and Jessica Wilmers. Lisa Tolles, the Proceedings Coordinator at Sheridan Printing, provided dedicated and careful work on the proceedings production. Sue Williamson designed conference promotional material, with photos courtesy of the Banff/Lake Louise Tourism Bureau. The Banff Park Lodge personnel, especially Valerie Hunter-Prenger and Ann Haagaard, assisted with conference planning, and our student volunteers helped make it all happen. 
 
The main credit for the program obviously goes to the authors for contributing their finest work, as well as the tutorial speakers and the workshop organizers for their excellent contributions. We also wish to express our deep gratitude to Urs Hoelzle from Google for agreeing to give the keynote address. The Program Committee members and external reviewers made further vital contributions, lending their expertise, spending many diligent hours reviewing papers, and braving a January blizzard to attend the Program Committee meeting in New York City. We sincerely thank them for their tremendous dedication and efforts in ensuring a high-quality program. We particularly owe the members of the awards committee Thomas Bonald, Ed Coffman (chair), Arif Merchant, Philippe Nain, and Y.C. Tay for their selfless exertions in performing such a demanding task. We further thank the General Co-Chairs Derek Eager and Carey Williamson for their constant guidance, support and confidence. We are grateful to Leana Golubchik (Chair of SIGMETRICS) and several previous Program Chairs for sharing their experiences and insights. We are indebted to Sophie Majewski, Vishal Misra, Erich Nahum, and Dan Rubenstein for making the logistic arrangements for the Program Committee meeting. We applaud the Tutorial Co-Chairs Kimberly Keeton and Vishal Misra for setting up an excellent tutorial program, the Proceedings Chair Anirban Mahanti for assembling the proceedings, and the Publicity Co-Chairs Thomas Bonald and Jussara Almeida for their great job in publicizing the conference. Special thanks to Matthew Andrews, Bruce Shepherd, and Phil Whiting for contributing code to the paper assignment algorithm. Finally, big thanks to H.M. Law who served as webmaster for the conference website and administered the submission management system. Without his tireless efforts, it would have been impossible to handle hundreds of papers and reviews."
a3be9dfc2c6d76a6584af0be8e73f23dea9f0f7a,"textabstractWe consider a fluid queue fed by a superposition of $n$ homogeneous on-off sources with generally distributed on- and off-periods. We scale buffer space $B$ and link rate $C$ by $n$, such that we get $nb$ and $nc$, respectively. Then we let $n$ grow large. In this regime, the overflow probability decays exponentially in the number of sources $n$; we specifically examine the situation in which also $b$ is large. We explicitly compute asymptotics for the case in which the on-periods have a subexponential distribution, e.g., Pareto, Lognormal, or Weibull. We provide a detailed interpretation of our results. Crucial is the shape of the function $v(t) := - log Pr( A^* > t )$ for large~$t$, $A^*$ being the residual on-period. If $v(.)$ is slowly varying (e.g., Pareto, Lognormal), then, during the trajectory to overflow, the input rate will only slightly exceed the link rate. Consequently, the buffer will fill `slowly', and the typical time to overflow will be `more than linear' in the buffer size. In contrast, if $v(.)$ is regularly varying of index strictly between 0 and 1 (e.g., Weibull), then the input rate will significantly exceed the link rate, and the time to overflow is essentially proportional to the buffer size. In both cases there is a substantial fraction of the sources that remain in the on-state during the entire path to overflow, while the other sources contribute at mean rate. These observations lead to straightforward approximations for the overflow probability. These approximations can be extended to the case of heterogeneous sources; the results provide further insight into the so-called reduced-load approximation."
fd7effbea39dcfd7f4c64b37d47f821077208b02,"We identify properties of optimal scheduling schemes for downlink traffic in a code division multiple access (CDMA) data-only network. Under idealised assumptions, we show that it is optimal for each base station to transmit to at most one delay-tolerant user at a time. Moreover we prove that a base station, when on, should transmit at maximum power for optimality. For a linear network, we characterise the optimal schedule as the solution to a linear program. As a by-product, our analysis yields bounds on throughput gains obtainable from downlink scheduling."
0cb76e92380c0458537b37016a91744a0ed111a9,"We describe the simulation of a new dynamic channel assignment algorithm in FDMA/TDMA wireless networks. The algorithm relies on periodic interference measurements by each of the base stations on the inactive frequencies, so as to identify appropriate candidate channels. The adaptive nature provides automatic configuration at the time of system initialization and adaptation to system expansion and traffic patterns with spatial or temporal variations. By eliminating the manual frequency planning process inherent to today's fixed channel assignment procedures, the self-organizing capability guarantees ease of operation for service providers, while increasing both capacity and voice quality. Our simulation experiments demonstrate stability of the algorithm and confirm its self-organizing capability. They also indicate a significant decrease of call blocking and dropping and other quality-of-service improvements."
76a0335ccbe578f51843ad269b47e2d51c274259,
7bc89babb20919519f1fa0b7059bc1f6eba5e75f,
7bd6d9c05b3b996b121f25dc497fcf096f2fb4de,"This note develops two-moment approximations for blocking probabilities in overflow systems with repacking. The approximations are based on fixed point schemes, iteratively relating the arrival rate at the primary channels to the mean number of calls at the overflow channels. The analysis focuses on layered cellular mobile communications networks, but can also be applied to multiple retrial queues sharing a common (finite) orbit. Numerical results show that our approximation is fairly accurate for systems in which the mean repacking rate does not exceed the call arrival rate."
7f687cb083ae0f69aadc6d8afcea332f8b9aa73a,"We consider virtual partitioning (VP), which is a scheme for sharing a resource among several traffic classes in an efficient, fair, and robust manner. In the preliminary design stage, each traffic class is allocated a nominal capacity, which is based on expected offered traffic and required quality of service. During operations, if the current capacity usage by a class exceeds its nominal allocation, then it is declared to be in ""overload,"" and a state-dependent trunk reservation mechanism gives it lower priority in the admission of new calls. We develop efficient computational algorithms for the case of heterogeneous traffic, and perform extensive numerical experiments to demonstrate the accuracy of the various approximations. The performance of VP is examined and compared to that of complete sharing and complete partitioning. Particular weight is placed on robustness, meaning that traffic classes with arrival rates conforming to the design continue to receive the required quality of service, despite the presence of misbehaving classes with excessive arrival rates. We adopt a reward-penalty paradigm as a combined measure for efficiency and fairness, and show that not only is the revenue generated by VP extremely close to the maximum achievable value, but that the structural form of the optimal policy also closely resembles that of VP. The numerical results confirm that the scheme is efficient, fair, and very robust."
80815a7949aa05c065b544362e18bf8cc0f50210,"The primary objective in the present paper is to gain fundamental understanding of the performance achievable in ATM networks as a function of the various system characteristics. We derive limit theorems that characterize the achievable performance in terms of the offered traffic, the admissible region, and the revenue measure. The insights obtained allow for substantial simplifications in the design of real-time connection admission control algorithms. In particular, we describe how the boundaries of admissible regions with convex complements may be linearized-thus reducing the admissible region-so as to obtain a convenient loss network representation. The asymptotic results for the achievable performance suggest that the potential reduction in revenue is immaterial in high-capacity networks. Numerical experiments confirm that the actual reduction is typically negligible, even in networks of moderate capacity."
b627034c87fe7ab38a86eab172c4d00dd9978ad3,
c12a51c68e9b662a85b0c527b97ddc45addbd0ee,"Next-generation wireless networks are expected to support a wide range of high-speed data services, with Web browsing as one of the major applications. Although high data rates have been shown feasible in a single-user setting, the resource allocation issues that arise in a multiple-user context remain extremely challenging. Compared with voice, data traffic is typically more bursty, while the users are less sensitive to delay. These characteristics require resource allocation strategies to operate in a fundamentally different manner if the spectrum is to be used efficiently. In this paper we propose several algorithms for scheduling the efficient transmission of data to multiple users. As a new feature, the various schemes exploit knowledge of the buffer contents to achieve high throughput, while maintaining fairness by providing quality of service (QoS) to individual users. The proposed algorithms are backward compatible with existing cellular and personal communications services (PCS) standards such as IS-136. They provide a powerful approach to improving spectrum efficiency in forthcoming high-speed data cellular services. The extensive simulation experiments we present in this paper demonstrate that the algorithms significantly outperform conventional schemes."
d21561617712c5e64e3b7c836af643f22a078b9f,"The primary objective in the present paper is to gain fundamental understanding of the performance achievable in ATM networks as a function of the various system characteristics. We derive limit theorems that characterize the achievable performance in terms of the offered traffic, the admissible region, and the revenue measure. The insights obtained allow for substantial simplifications in the design of real-time connection admission control algorithms. In particular, we describe how the boundaries of admissible regions with convex complements may be linearized - thus reducing the admissible region - so as to obtain a convenient loss network representation. The asymptotic results for the achievable performance suggest that the potential reduction in revenue is immaterial in high-capacity networks. Numerical experiments confirm that the actual reduction is typically negligible, even in networks of moderate capacity."
d3434b9be2aaa3eb1415ff5b549108fa3170d0aa,"We derive bounds for the performance of dynamic channel assignment (DCA) schemes which strengthen the existing Erlang bound. The construction of the bounds is based on a reward paradigm as an intuitively appealing way of characterizing the achievable carried traffic region. In one-dimensional networks, our bounds closely approach the performance of maximum packing (MP), which is an idealized DCA scheme. This suggests not only that the bounds are extremely tight, but also that no DCA scheme, however sophisticated, can be expected to outperform MP in any significant manner, if at all. Our bounds extend to scenarios with varying re-use which may arise in the case of dynamic re-use partitioning or measurement-based DCA schemes. In these cases, the bounds slightly diverge from the performance of MP, which inflicts higher blocking on outer calls than inner calls, but not to the extent required to maximize carried traffic. This reflects the trade-off that arises in the case of varying re-use between efficiency and fairness. Asymptotic analysis confirms that schemes which minimize blocking intrinsically favor inner calls over outer calls, whereas schemes which do not discriminate among calls inevitably produce higher network-average blocking."
7b2980442cde9d1a65ee1bee895ec45ca331f4d8,"We consider two different single-server cyclic polling models: (i) a model with zero switchover times, and (ii) a model with nonzero switchover times, in which the server keeps cycling when the system is empty. For both models we relate the steady-state queue length distribution at a queue to the queue length distributions at server visit beginning and visit completion instants at that queue; as a by-product we obtain a short proof of the Fuhrmann-Cooper decomposition. For the large class of polling systems that allow a multitype branching process interpretation, we expose a strong relation between the queue length, as well as waiting-time, distributions in the two models. The results enable a very efficient numerical computation of the waiting-time moments under different switchover time scenarios."
893b59ed10f5c67435a743681c561984acfcd2bf,"Simple optimal policies are known for the problem of scheduling jobs to minimize expected makespan on two parallel machines when the job running-time distribution has a monotone hazard rate. But no such policy appears to be known in general. We investigate the general problem by adopting two-point running-time distributions, the simplest discrete distributions not having monotone hazard rates. We derive a policy that gives an explicit, compact solution to this problem and prove its optimality. We also comment briefly on first-order extensions of the model, but each of these seems to be markedly more difficult to analyze."
9cc75b8a29b9e549b2bfe21d39bf55e368d14472,"Wireless service providers are continually looking for new features and products to improve quality of service, increase system capacity, and reduce administrative overhead. The simulation tool W provides a flexible platform for the exploration of a broad range of system-level design and performance issues in wireless networks. Investigation of network issues using W gives Lucent Technologies the ability to design new features and products more effectively by reducing development intervals and implementing a one-pass design on field-quality products. We describe the scope of the simulation platform and illustrate the specific use of the W tool for examination of a next-generation channel assignment algorithm currently under development. The algorithm's adaptive nature provides automatic configuration at system initialization, as well as adaptation to system expansion and traffic patterns with spatial or temporal variations, thus ensuring ease of operation for service providers. Initial simulation experiments confirm its self-organizing capability and suggest a significant improvement in quality of service in terms of call blocking and dropping."
6a2fb8fe9fe354f572ad560a5679f8a54402d6e4,
0b62fe51d5d5ef4f1185be2474b4ad40b19488e9,"Time limits are the major mechanisms used for controlling a large variety of multistation single-medium computer-communication systems like the FDDI network and the IEEE 802.4 Token Bus. The proper use of these mechanisms is still not understood and rules for efficient system operation are not available. The authors' objective is the derivation of such rules. They use a cyclic polling model with different service limits (k-limited service) at the different queues, thus emulating time limits. They are interested in determining these k-limit values so as to minimize the mean waiting cost of messages in the system. A simple approximative approach is proposed for two major problems: one in which a limit is set on the token rotation time and one in which no limits are imposed. The approach is tested for a variety of cases and is shown to be very effective. >"
10f3957fd46270733dc94e1802bb8f4b8c4e852d,
14119e39923f2033d16f00b1643e9fad75730223,"We study a globally gated polling system with a dormant server, which makes a halt at its home base when there are no customers present in the system. We derive an explicit expression for the cycle time distribution as well as for the waiting-time distribution at each of the queues. As a justification of the dormant server policy, we show the waiting time at each of the queues to be smaller (in the increasing-convex-ordering sense) than in the ordinary nondormant server case."
2d30756aa5c7ac0294a11c096a8332926f10f6de,
9d3ca2706a3b483f8149a1a568b98e73e5670ae0,The model under consideration consists of n customer types attended by m parallel non-identical servers. Customers are allocated to the servers in a probabilistic manner; upon arrival customers are sent to one of the servers according to an m &times; n matrix of routing probabilities. We consider the problem of finding an allocation that minimizes a weighted sum of the mean waiting times. We expose the structure of an optimal allocation and describe for some special cases in detail how the structure may be exploited in actually determining an optimal allocation.
c6b678194ca61d0206f305e05ae6c790041747ad,
968cf67ee6e3a166e2e5269066a0ca0e66e43320,
ae80c05c41591a5d2af88a6298d8c3745f7f6728,
d02d41e056f2bab41e00bc7b6d8d28cfaf701507,"We consider a polling model with multiple servers, each of which visits the queues according to its own service order table. In general, such a model is not tractable by means of analytical techniques. In this paper, we show how the model can be analyzed by the power-series algorithm (PSA), a tool for the numerical evaluation and optimization of the performance of a broad class of multi-queue models. Various numerical experiments with the PSA are performed, providing new insights into the behavior of multiple-server polling systems"
f1cd0ed7019e778cc0da06f2cf03a4d9ca582c52,
952a8f5291942d1258e4198e0f3fe85707fba0cc,"This paper analyses a variant of the M/G/1 queue in which the service times of arriving customers depend on the length of the interval between their arrival and the previous arrival. The dependence structure under consideration arises when individual customers arrive according to a Poisson process, while customer collectors are sent out according to a Poisson process to collect the customers and to bring them to the service facility. In this case collected numbers of customers, and hence total collected service requests, are positively correlated with the corresponding collect intervals. Viewing a batch of collected customers as one (super) customer gives rise to an M/G/1 queue with a positive correlation between service times of such customers and their interarrival times. Both for individual customers and for supercustomers we derive the transforms of the sojourn time, waiting time and queue length distributions. We also compare over results with those for the ordinary M/ G/:1 queue with dependence."
c69516cbd248abaa06b53a7b357654b924f5ea52,
2c41227382fa4958827904edbe7bcb593d4ae304,"This paper analyses a variant of the ikl/G/l queue in which the service times of arriving customers depend on the length of the interval between their arrival and the previous arrival. The dependence structure corresponds to the case of customers arriving according to an ordinary Poisson arrival process, and customer collectors being sent out according to a Poisson process to collect the customers and to bring them to the service facility. In this case collected numbers of customers , and hence total collected service requests, are positively correlated with the corresponding collect intervals. Viewing a batch of collected customers as one (super) customer gives rise to an itf/G/l queue with a positive correlation between service times of such customers and their interarrival times. Both for individual customers and for batches of collected customers we derive the transforms of the sojourn time and waiting time distributions. We also compare our results with those for the ordinary M/G/l queue without dependence. 1 Introduction Consider the following situation. Customers arrive at pickup points according to independent Poisson processes. At these pickup points they wait for a bus to *This author wassupportedby NFI. o Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of tha Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. bring them to a single-server service facility (e.g., the check-in count er of a hotel). Busses with unlimited customer capacity move according to a fixed route along the pickup points, with fixed speed, collecting all waiting customers that they encounter and finally delivering all collected customers at the service facility. The intervals between the starts of successive bus tours are exponentially distributed. Because of the fixed tour length, the arrival process of busses at the service facility is a Pois-son process. Viewing the batch of customers brought to the service facility by a bus as one supercustomer, the service facility very closely resembles an ordinary M/G/l queue; the only difference is that the service time of a supeTcustomeT depends on the pTevious inteT-aTTival time. Indeed, if two consecutive busses arrive at a relatively long (short) interval, then the second bus is likely to …"
2caea8c80cddfd23be1a33069b3677cec876b8c7,"Langaris (1987) presents a busy period analysis of a queue with positively correlated arrival and service processes. We would like to point out that his results are valid for the first busy period only, and not, as is claimed by Langaris, for busy periods in steady state. Langaris considers a single-server queue in which the interarrival time and service time of a customer have a bivariate density function g(t, s), with negative-exponential marginal density functions. A considerable part of his analysis is devoted to f,,(t, s), the probability density function of the following event: the busy period length is t and the number of customers served during this busy period is n, given that the first service of that busy period has length s < t. After obtaining the Laplace transform of f,,(t, s), Langaris derives the Laplace transform of b~(t), the probability density function of the following event: the busy period has length t and exactly n customers are served during this busy period. For this derivation we use the relation"
af452b57a846a91a253dcc1cc32b76d6c649b7c3,
e4561b8be74d8466a82d77a2d86bce2cda9aff99,
ee9a143527045c7c987da752d92b98bbeca6b95d,"Bandwidth-sharing networks as considered by Massoulié & Roberts provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Although valuable stability results have been obtained, crucial performance metrics such as flow-level delays and throughputs in these models have remained intractable in all but a few special cases. In particular, it is not well understood to what extent flow-level delays and throughputs achieved by standard bandwidth-sharing mechanisms such as -fair strategies leave potential room for improvement. In order to gain a better understanding of the latter issue, we set out to determine the scheduling policies that minimize the mean delay in some simple linear bandwidth-sharing networks. We compare the performance of the optimal policy with that of various -fair strategies so as to assess the efficacy of the latter and gauge the potential room for improvement. The results indicate that the optimal policy achieves only modest improvements, even when the value of is simply fixed, provided it is not too small."
3aad5bb2c6468b0a7b638376433bed9651290bd1,"Abstract The study analyzed the situational characteristics of 112 incidents where police used firearms to handle high threat situations. Most shooting incidents emanated from usually uneventful tasks, e.g., handling burglaries or disturbances. The assailants were commonly armed with firearms (26%), sharp (27%) or blunt objects (10%). The incidents were regularly short-lasting (in 39% were shots fired ≤3 s from threat emerged) and occurred at short distances (in 42% at distances ≤3 m). Predominantly, the first responders had to address the situation and did so with warning shots or, equally common, with fire-for-effect shots (40%) or a combination thereof. Psychological stress was manifested as feelings of panic at some point and as motor skill alterations, e.g., firing without using sights and with one hand only. Analysis of these incidents shows that all field duty police officers should receive training in handling potentially life-threatening, sudden, close-range attacks."
e733f6518206acf74b32fea6a5898bcbb25566ce,"Abstract Pepper spray (OC) is a policing tool aimed to prevent or stop aggressive behavior by quickly and temporarily incapacitate without injuring. To date, few studies have investigated OC’s operational usefulness and limitations. OC reduced violent behavior in 93% of the 936 incidents investigated. However, the operative range was often <2 m and it took between 3 and 5 s of spraying before obtaining effect, partly owing to the difficulties of hitting a small, sometimes erratically moving target. Collateral hits were noted in 24% of the incidents, whereof 90% were other officers. Noteworthy, in 21% of incidents officers put themselves at large personal risk by using OC at close range against people armed with lethal weapons. Hence, OC emerges as a suitable tool for handling low threat situations but lacks key traits to ensure safe and efficient policing of high threat situations, e.g., handling armed assailants."
6c51f983c90a88496ad4f62818965f1f9e92bec3,
cfc7460a3ac33bbb591da0dca85becef06668b15,
48d51101c401036eb43b7f96005fc2789c3151cd,"A docking system (1) according to the invention comprises a) at least one self-propelled working-tool (3), preferably intended for attendance of ground or floor, such as grass-cutting, moss-scratching, watering, vacuum-cleaning, polishing, transportation etc., having a body (16) and b) at least one docking station (2) for the at least one working tool (3), c) wherein the docking station and the tool can by way of emitted signals cstablish contact with each other, so that the tool can drive up to the docking station, d) wherein the docking station is provided with at least one first transmission part (5, 6; 5', 6') and the working tool is provided with at least one cooperating second transmission part (7, 8) for transmission of energy and/or information between the docking station and the working tool, e) wherein the docking station is provided with at least one rising part (10, 11, 12, 13), of which at least one part is used for mounting of the first transmission part(s). f) wherein the tool's second transmission part(s) (7, 8) is/are located on the upper side of the body."
66af8a23f468a4e6d39ebe00aaf9c27b5c22da95,
7b22b47029322844bb2531ab83754567fa930e8c,
ae1ec379c45c17602dbaf517fb1c4285c68ffbfa,
eaf6c834fa170e80e2a501af2f2e735ba45b686b,"L'invention concerne un systeme d'arrimage (1) comprenant principalement au moins une station d'arrimage (2) destinee a au moins un outil motorise (3) et l'outil (3) en question destine, de preference, a l'entretien du sol comme la tonte du gazon, le grattage de la mousse, l'arrosage, le nettoyage a l'aspirateur, le cirage etc. La station d'arrimage et l'outil peuvent etablir le contact l'un avec l'autre a l'aide de signaux emis de telle facon que l'outil puisse monter jusqu'a la station d'arrimage. La station d'arrimage est dotee au moins d'un premier element de transmission (5, 6; 5', 6') destine a la transmission d'energie et/ou d'informations entre la station d'arrimage et l'outil, lequel est dote au moins d'un second element de transmission (7, 8) cooperant. La station d'arrimage est concue sous la forme d'une plaque de base (9) destinee a etre placee sur le sol et dotee au moins d'un element en saillie (10, 11, 12, 13) dont au moins un element sert a la fixation d'element(s) de transmission, et la station d'arrimage est adaptee de telle facon que l'outil dote de n'importe quel element, tel que une/des roue(s) (15), ou un corps (16). puisse etre souleve jusqu'a la station d'arrimage."
fc5521642a0002330fbfdb77c557c096edc2ea04,
955f6c7ba515dee35ef4da0eedcf68f3cefdfd85,
03110cfca0eeab3f72551aa038ed0a28a270f968,
7652b5f97e3cd82083163c134e4dce1862c5025c,
772d15323e35a9e1973ccc7a557b6109a11f65eb,
32ea8cdc7f1b1c7ac359f1063030c50ad6c45a39,
016f55c5cae7ddc3ad894a26d635cf6b0e337381,"Abstract The need to reform research assessment processes related to career advancement at research institutions has become increasingly recognized in recent years, especially to better foster open and responsible research practices. Current assessment criteria are believed to focus too heavily on inappropriate criteria related to productivity and quantity as opposed to quality, collaborative open research practices, and the socioeconomic impact of research. Evidence of the extent of these issues is urgently needed to inform actions for reform, however. We analyze current practices as revealed by documentation on institutional review, promotion, and tenure (RPT) processes in seven countries (Austria, Brazil, Germany, India, Portugal, the United Kingdom and the United States). Through systematic coding and analysis of 143 RPT policy documents from 107 institutions for the prevalence of 17 criteria (including those related to qualitative or quantitative assessment of research, service to the institution or profession, and open and responsible research practices), we compare assessment practices across a range of international institutions to significantly broaden this evidence base. Although the prevalence of indicators varies considerably between countries, overall we find that currently open and responsible research practices are minimally rewarded and problematic practices of quantification continue to dominate."
056e1e2f924f30d9358e66b94ae747c94a3f2fbe,"Background: Open Access aims at improving the discovery, access and re-use of research not only within the scientific community, but also within broader society, for instance to promote innovation in industry. Yet, the extent to which openly available scientific work impacts technological inventions remains largely unknown. Methods: We combine publicly available data sources about patents and scholarly publications to explore the extent to which Open Access scientific literature is cited in patents. Results: Investigating over 22 million patent families indexed in Google Patents between 2010 and 2020, we found that around one third referenced non-patent literature. However, the number of references per patent family can vary considerably across technological sectors and inventor countries. Based on a sample of 215,962 scientific non-patent references published between 2008 and 2020, we determined the Open Access status using Unpaywall, Europe PubMed Central and arXiv.  The proportion of Open Access citations grew over the years, with nearly half of cited articles being openly available. Discussion: In line with research on both technology-science linkage and Open Access, we found considerable country- and subject- specific variations.  In particular, patents representing inventions from the US and the UK cited Open Access work disproportionately more often, although it is challenging to link these observations to specific science policies and incentives. We recommend that follow-up research and monitoring exercise take advantage of a growing evidence base associated with patent citations and Open Access evidence."
061f6b2331c50b8251361bb8b8c1e583304b92d2,"—One of the most common problems preventing the application of prediction models in the real world is lack of generalization: The accuracy of models, measured in the benchmark does repeat itself on future data, e.g. in the settings of real business. There is relatively little methods exist that estimate the conﬁdence of prediction models. In this paper, we propose novel methods that, given a neural network classiﬁcation model, estimate uncertainty of particular predictions generated by this model. Furthermore, we propose a method that, given a model and a conﬁdence level, calculates a threshold that separates prediction generated by this model into two subsets, one of them meets the given conﬁdence level. In contrast to other methods, the proposed methods do not require any changes on existing neural networks, because they simply build on the output logit layer of a common neural network. In particular, the methods infer the conﬁdence of a particular prediction based on the distribution of the logit values corresponding to this prediction. The proposed methods constitute a tool that is recommended for ﬁltering predictions in the process of knowledge extraction, e.g. based on web scrapping, where predictions subsets are identiﬁed that maximize the precision on cost of the recall, which is less important due to the availability of data. The method has been tested on different tasks including relation extraction, named entity recognition and image classiﬁcation to show the signiﬁcant increase of accuracy achieved."
09498b9776a90301c7dec87d382511c4a489098e,
4352706cd2bc1eb8d705aa6e0eb718719afab3e0,"Machine learning research, particularly in genomics, is often based on wide shaped datasets, i.e. datasets having a large number of features, but a small number of samples. Such configurations raise the possibility of chance influence (the increase of measured accuracy due to chance correlations) on the learning process and the evaluation results. Prior research underlined the problem of generalization of models obtained based on such data. In this paper, we investigate the influence of chance on prediction and show its significant effects on wide shaped datasets. First, we empirically demonstrate how significant the influence of chance in such datasets is by showing that prediction models trained on thousands of randomly generated datasets can achieve high accuracy. This is the case even when using cross-validation. We then provide a formal analysis of chance influence and design formal chance influence estimators based on the dataset parameters, namely its sample size, the number of features, the number of classes and the class distribution. Finally, we provide an in-depth discussion of the formal analysis including applications of the findings and recommendations on chance influence mitigation."
6064edd4f3fe9fbd433c2dd2d65f748ed09d35bd,"We present a novel dataset which enables quantitative analysis of the relationship between institutional support for Open Science and research performance. We analysed promotion, review, and tenure policies (PRT) from institutions originating from seven countries and combined them with bibliographical data from the outputs generated by each institution. The data were normalised and evaluated against Open Science and Responsible Research and Innovation (RRI) indicators, to enable comparisons and easy machine readable access. The significance of this dataset lies in its potential to answer a range of questions that are key to the understanding of what motivates academics with regards to their research practices and publishing behaviors, using various indicators, including the MoRRI. To our knowledge, this collection constitutes one of the first efforts in delivering a large machine readable dataset enabling quantitative analysis on these aspects, as much work in this area has been carried out only through surveys and qualitative analysis."
7dbc3d8d54c7de746ba12a45f8aba48dc9771641,"We investigate the effect of varying citation context window sizes on model performance in citation intent classification. Prior studies have been limited to the application of fixed-size contiguous citation contexts or the use of manually curated citation contexts. We introduce a new automated unsupervised approach for the selection of a dynamic-size and potentially non-contiguous citation context, which utilises the transformer-based document representations and embedding similarities. Our experiments show that the addition of non-contiguous citing sentences improves performance beyond previous results. Evalu- ating on the (1) domain-specific (ACL-ARC) and (2) the multi-disciplinary (SDP-ACT) dataset demonstrates that the inclusion of additional context beyond the citing sentence significantly improves the citation classifi- cation model’s performance, irrespective of the dataset’s domain. We release the datasets and the source code used for the experiments at: https://github.com/oacore/dynamic_citation_context"
81356818dd2aeb24ba9b135acf12e521b79bedbd,
9250bd18d653cec89d98c764e7736fe174e2b5c0,
96ed54a7cf9c590b2d8c0a869672eb782a021c4b,"Classifying citations according to their purpose and importance is a challenging task that has gained considerable interest in recent years. This interest has been primarily driven by the need to create more transparent, efficient, merit-based reward systems in academia; a system that goes beyond simple bibliometric measures and considers the semantics of citations. Such systems that quantify and classify the influence of citations can act as edges that link knowledge nodes to a graph and enable efficient knowledge discovery. While a number of researchers have experimented with a variety of models, these experiments are typically limited to single-domain applications and the resulting models are hardly comparable. Recently, two Citation Context Classification (3C) shared tasks (at WOSP2020 and SDP2021) created the first benchmark enabling direct comparison of citation classification approaches, revealing the crucial impact of supplementary data on the performance of models. Reflecting from the findings of these shared tasks, we are releasing a new multi-disciplinary dataset, ACT2, an extended SDP 3C shared task dataset. This modified corpus has annotations for both citation function and importance classes newly enriched with supplementary contextual and non-contextual feature sets the selection of which follows from the lists of features used by the more successful teams in these shared tasks. Additionally, we include contextual features for cited papers (e.g. Abstract of the cited paper), which most existing datasets lack, but which have a lot of potential to improve results. We describe the methodology used for feature extraction and the challenges involved in the process. The feature enriched ACT2 dataset is available at https://github.com/oacore/ACT2."
9843d4771366606368bc7e01065763af86f0c1aa,"With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 3rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https://sdproc.org/2022/). The SDP workshop consisted of a research track, three invited talks and five Shared Tasks: 1) MSLR22: Multi-Document Summarization for Literature Reviews, 2) DAGPap22: Detecting automatically generated scientific papers, 3) SV-Ident 2022: Survey Variable Identification in Social Science Publications, 4) SKGG: Scholarly Knowledge Graph Generation, 5) MuP 2022: Multi Perspective Scientific Document Summarization. The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges."
d8e156bd121c0a53ce92a22be863b7ad18043ff0,"medical and physical sciences Units of Assessment (UoAs) and economics, reaching 42% above the baseline (72% overall) in the best case. This is based on 1000 bibliometric inputs and half of the articles used for training in each UoA. Prediction accuracies above the baseline for the social science, mathematics, engineering, arts, and humanities UoAs were much lower or close to zero. The Random Forest Classifier (standard or ordinal) and Extreme Gradient Boosting Classifier algorithms performed best from the 32 tested. Accuracy was lower if UoAs were merged or replaced by Scopus broad categories. We increased accuracy with an active learning strategy and by selecting articles with higher prediction probabilities, as estimated by the algorithms, but this substantially reduced the number of scores predicted."
dab01f1c3647722808135908418bf8ee5dbbddc0,"Background: Open Access aims at improving the discovery, access and re-use of research not only within the scientific community, but also within broader society, for instance to promote innovation in industry. Yet, the extent to which openly available scientific work impacts technological inventions remains largely unknown. Methods: We combine publicly available data sources about patents and scholarly publications to explore the extent to which Open Access scientific literature is cited in patents. Results: Investigating over 22 million patent families indexed in Google Patents between 2010 and 2020, we found that around one third referenced non-patent literature. However, the number of references per patent family can vary considerably across technological sectors and inventor countries. Based on a sample of 215,962 scientific non-patent references published between 2008 and 2020, we determined the Open Access status using Unpaywall, Europe PubMed Central and arXiv. The proportion of Open Access citations grew over the years, with nearly half of cited articles being openly available. Discussion: In line with research on both technology-science linkage and Open Access, we found considerable country- and subject-specific variations. In particular, patents representing inventions from the US and the UK cited Open Access work disproportionately more often, although it is challenging to link these observations to specific science policies and incentives. We recommend that follow-up research and monitoring exercise take advantage of a growing evidence base associated with patent citations and Open Access evidence."
dd9e10ce452aaae707835a97b5dcd1decae1825f,"We present a new gold-standard dataset and a benchmark for the Research Theme Identification task, a sub-task of the Scholarly Knowledge Graph Generation shared task, at the 3rd Workshop on Scholarly Document Processing. The objective of the shared task was to label given research papers with research themes from a total of 36 themes. The benchmark was compiled using data drawn from the largest overall assessment of university research output ever undertaken globally (the Research Excellence Framework - 2014). We provide a performance comparison of a transformer-based ensemble, which obtains multiple predictions for a research paper, given its multiple textual fields (e.g. title, abstract, reference), with traditional machine learning models. The ensemble involves enriching the initial data with additional information from open-access digital libraries and Argumentative Zoning techniques (CITATION). It uses a weighted sum aggregation for the multiple predictions to obtain a final single prediction for the given research paper. Both data and the ensemble are publicly available on https://www.kaggle.com/competitions/sdp2022-scholarly-knowledge-graph-generation/data?select=task1_test_no_label.csv and https://github.com/ProjectDoSSIER/sdp2022, respectively."
7e1f4a2e5882e9e2c9bd41cfeccd5542888b89c1,"The Cranfield Paradigm is a widely adopted and the de-facto standard approach to the evaluation of IR systems. However, this approach does not inherently support situations in which the user is acquiring knowledge (is learning) during an information seeking session consisting of the submission of a sequence of queries into an information retrieval system. More specifically, during a situation in which the retrieval of a particular document at the beginning of a session can be considered not relevant (due to the user’s lack of knowledge), while it can be considered relevant at a later point in the session (once the user acquired all required prerequisite knowledge). In this position paper, we reflect on the limitations of the Cranfield Paradigm in the context of knowledge acquisition tasks and propose several alternatives. These alternatives are based on the notion of evaluating a session consisting of a sequence of individual queries created to address a specific information need as part of a knowledge acquisition task."
91cc8738ec5ee3ffd4c335ddb3acdfb3724235fc,
a5fa1f41698bf5edb6f65052f13f4bfa16e13f75,"This paper provides an overview of the 2021 3C Citation Context Classification shared task. The second edition of the shared task was organised as part of the 2nd Workshop on Scholarly Document Processing (SDP 2021). The task is composed of two subtasks: classifying citations based on their (Subtask A) purpose and (Subtask B) influence. As in the previous year, both tasks were hosted on Kaggle and used a portion of the new ACT dataset. A total of 22 teams participated in Subtask A, and 19 teams competed in Subtask B. All the participated systems were ranked based on their achieved macro f-score. The highest scores of 0.26973 and 0.60025 were reported for subtask A and B, respectively."
ba7269b4591bd2bcf6c37d52cb9c97b5edfdbcb6,"Abstract The aim of this literature review is to examine the current state of the art in the area of citation classification. In particular, we investigate the approaches for characterizing citations based on their semantic type. We conduct this literature review as a meta-analysis covering 60 scholarly articles in this domain. Although we included some of the manual pioneering works in this review, more emphasis is placed on the later automated methods, which use Machine Learning and Natural Language Processing (NLP) for analyzing the fine-grained linguistic features in the surrounding text of citations. The sections are organized based on the steps involved in the pipeline for citation classification. Specifically, we explore the existing classification schemes, data sets, preprocessing methods, extraction of contextual and noncontextual features, and the different types of classifiers and evaluation approaches. The review highlights the importance of identifying the citation types for research evaluation, the challenges faced by the researchers in the process, and the existing research gaps in this field."
cb4755866a31a40adc128b4c3df9ffcb1e6eadc5,"The 3C Citation Context Classification task is the first shared task addressing citation context classification. The two subtasks, A and B, associated with this shared task, involves the classification of citations based on their purpose and influence, respectively. Both tasks use a portion of the new ACT dataset, developed by the researchers at The Open University, UK. The tasks were hosted on Kaggle, and the participated systems were evaluated using the macro f-score. Three teams participated in subtask A and four teams participated in subtask B. The best performing systems obtained an overall score of 0.2056 for subtask A and 0.5556 for subtask B, outperforming the simple majority class baseline models, which scored 0.11489 and 0.32249, respectively. In this paper we provide a report specifying the shared task, the dataset used, a short description of the participating systems and the final results obtained by the teams based on the evaluation criteria. The shared task has been organised as part of the 8th International Workshop on Mining Scientific Publications (WOSP 2020) workshop."
f25a978da0f34c98db85c8c6f1613570b9c8a841,"With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks and three Shared Tasks (LongSumm 2021, SCIVER and 3C). The program was geared towards NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges. 1 Workshop description Over the past several years and at various venues, the Joint Workshop on Bibliometric-enhanced IR and NLP for Digital Libraries (BIRNDL1) (Cabanac et al., 2020; Mayr et al., 2018), the Allen Institute for AI, USA IBM Research AI, Haifa Research Lab, Israel SRI International, USA ÚFAL, Charles University, Czech Republic Google AI, USA Oak Ridge National Laboratory, USA The Open University, UK GESIS – Leibniz Institute for the Social Sciences, Germany Elsevier, USA Microsoft Research, USA CL-SciSumm Shared Task, and the International Workshop on Mining Scientific Publications (WOSP2) (Knoth et al., 2020) have established themselves as the principal venues for research in scholarly document processing (SDP). However, as these venues are collocated with conferences that are not focused on NLP, current solutions in this domain lag behind modern techniques generated by the greater NLP community. In 2020, the first SciNLP workshop3 was held online at the AKBC 2020 conference; the workshop brought together interested parties in a talk series focused on various aspects of scientific NLP. The first Scholarly Document Processing (SDP) workshop then took place in co-location with the EMNLP 2020 conference as an online workshop (see overview in Chandrasekaran et al. (2020)), and provided a dedicated venue for those working on SDP to submit and discuss their research. Following these successes and the clear appetite for venues to foster discussions around scholarly NLP, SDP 2021 again aimed to connect researchers and practitioners from different communities working with scientific literature and data and created a premier meeting point to facilitate discussions on open problems in SDP. We believe that ACL events are the most appropriate venue for the SDP workshop for three reasons. First, ACL events are the premier venues for the confluence of NLP and ML, and most of the cornerstone tasks in processing scholarly documents are NLP tasks. Improving machine understanding of scholarly semantics embedded in research papers is essential to furthering many tasks and applications in scholarly document processing. Second, the clear practical importance https://philippmayr.github.io/BIRNDL-WS/ https://wosp.core.ac.uk/ https://scinlp.org/"
08c3cc23c9ff8c8ae3aedbd2f59577967acd4b8f,"We investigate to what extent Open Access publications were cited in patents. Drawing on publicly available big data sources, including Google Patents and Unpaywall, we present a patent citation analysis dedicated to the Open Access status of scientific non-patent references. Our findings suggest an Open Access citation advantage in patents, but show variation across technology sectors and countries."
2272b80233173760ed9f6cdb5499912e283fdc50,"Each year the number of Open Access (OA) papers is gradually increasing. We carried out a study investigating 400 universities from 8 countries to examine: i) the total number of OA papers per country, ii) proportion of OA papers published by representative universities in each country classified into three tiers of research quality: high, middle and low, iii) how universities within the same country compare to each other and iv) the growth of OA papers in countries per year. We conclude that among the analysed countries the UK and USA rank first and second respectively, while Russia and India are positioned towards the bottom of the list. We observe no link between the proportion of OA papers published by authors at a university and the university ranking, with some universities in the middle university rank tier having a larger proportion of OA papers than those in the high tier."
243824b59ddc3a2610c6e14e442b2216dd0d307b,"Each year the number of Open Access (OA) papers is gradually increasing. We carried out a study investigating 400 universities from 8 countries to examine: i) the total number of OA papers per country, ii) proportion of OA papers published by representative universities in each country classified into three tiers of research quality: high, middle and low, iii) how universities within the same country compare to each other and iv) the growth of OA papers in countries per year. We conclude that among the analysed countries the UK and USA rank first and second respectively, while Russia and India are positioned towards the bottom of the list. We observe no link between the proportion of OA papers published by authors at a university and the university ranking, with some universities in the middle university rank tier having a larger proportion of OA papers than those in the high tier."
273e7a33ec437dab8c1c4640f54891dcfe8d5fab,"Deduplication is the task of identifying near and exact duplicate data items in a collection. In this paper, we present a novel method for deduplication of scholarly documents. We develop a hybrid model which uses structural similarity (locality sensitive hashing) and meaning representation (word embeddings) of document texts to determine (near) duplicates. Our collection constitutes a subset of multidisciplinary scholarly documents aggregated from research repositories. We identify several issues causing data inaccuracies in such collections and motivate the need for deduplication. In lack of existing dataset suitable for study of deduplication of scholarly documents, we create a ground truth dataset of 100K scholarly documents and conduct a series of experiments to empirically establish optimal values for the parameters of our deduplication method. Experimental evaluation shows that our method achieves a macro F1-score of 0.90. We productionise our method as a publicly accessible web API service serving deduplication of scholarly documents in real time."
40f449cb0078a50c7088422b8b1f13f81307a0e3,"In this paper, we build on the idea of Citation Proximity Analysis (CPA), originally introduced in [1], by developing a step by step scalable approach for building CPA-based recommender systems. As part of this approach, we introduce three new proximity functions, extending the basic assumption of co-citation analysis (stating that the more often two articles are co-cited in a document, the more likely they are related) to take the distance between the co-cited documents into account. Asking the question of whether CPA can outperform co-citation analysis in recommender systems, we have built a CPA based recommender system from a corpus of 368,385 full-texts articles and conducted a user survey to perform an initial evaluation. Two of our three proximity functions used within CPA outperform co-citations on our evaluation dataset."
65e736e281f648358800128b2886ef6ab6c9b3ba,"The ability to understand not only that a piece of research has been cited, but why it has been cited has wide-ranging applications in the areas of research evaluation, in tracking the dissemination of new ideas and in better understanding research impact. There have been several studies that have collated datasets of citations annotated according to type using a class schema. These have favoured annotation by independent annotators and the datasets produced have been fairly small. We argue that authors themselves are in a primary position to answer the question of why something was cited. No previous study has, to our knowledge, undertaken such a large-scale survey of authors to ascertain their own personal reasons for citation. In this work, we introduce a new methodology for annotating citations and a significant new dataset of 11,233 citations annotated by 883 authors. This is the largest dataset of its type compiled to date, the first truly multi-disciplinary dataset and the only dataset annotated by authors. We also demonstrate the scalability of our data collection approach and perform a comparison between this new dataset and those gathered by two previous studies."
6a4012d3d290f173aed9d8be9f47dfdc26bee976,"The entire body of research literature is currently estimated at 100-150 million publications with an annual increase of around 1.5 million. Systematically reading and analysing the full body of knowledge is now beyond the capacities of any human being. Consequently, it is important to better understand how we can leverage Natural Language Processing/Text Mining techniques to aid knowledge creation and improve the process by which research is being done. This workshop aims to bring together people from different backgrounds who: (a) have experience with analysing and mining databases of scientific publications, (b) develop systems that enable such analysis and mining of scientific databases (especially those who manage publication databases) or (c) who develop novel technologies that improve the way research is being done."
d9456d5731009ea5d0172ad65dde0cb68eedfa21,
e155847c202b5d9f8adf5cb52101058d977c9cd0,"The 3C Citation Context Classification task is the first shared task addressing citation context classification. The two subtasks, A and B, associated with this shared task, involves the classification of citations based on their purpose and influence, respectively. Both tasks use a portion of the new ACT dataset, developed by the researchers at The Open University, UK. The tasks were hosted on Kaggle, and the participated systems were evaluated using the macro f-score. Three teams participated in subtask A and four teams participated in subtask B. The best performing systems obtained an overall score of 0.2056 for subtask A and 0.5556 for subtask B, outperforming the simple majority class baseline models, which scored 0.11489 and 0.32249, respectively. In this paper we provide a report specifying the shared task, the dataset used, a short description of the participating systems and the final results obtained by the teams based on the evaluation criteria. The shared task has been organised as part of the 8th International Workshop on Mining Scientific Publications (WOSP 2020) workshop."
e70c02386e61a25b31c2cefb308d8ae532eb56cc,"The Community of Open Scholarship Grassroots Networks (COSGN), includes 120 grassroots networks, representing virtually every region of the world and every research discipline. These networks communicate and coordinate on topics of common interest. We propose, using an NSF 19-501 Full-Scale implementation grant, to formalize governance and coordination of the networks to maximize impact and establish standard practices for sustainability. In the project period, we will increase the capacity of COSGN to advance the research and community goals of the participating networks individually and collectively, and establish governance, succession planning, shared resources, andcommunication pathways to ensure an active, community-sustained network of networks. By the end of the project period, we will have established a self-sustaining network of networks that leverages disciplinary and regional diversity, actively collaborates across networks for grassroots organizing, and shares resources for maximum impact on culture change for open scholarship."
19d043eebbe1e60bd9e70e4968fdac46b4b86a84,"Recent years have seen fast growth in the number of policies mandating Open Access (OA) to research outputs. We conduct a large-scale analysis of over 800 thousand papers from repositories around the world published over a period of 5 years to investigate: a) if the time lag between the date of publication and date of deposit in a repository can be effectively tracked across thousands of repositories globally, and b) if introducing deposit deadlines is associated with a reduction of time from acceptance to public availability of research outputs. We show that after the introduction of the UK REF 2021 OA policy, this time lag has decreased significantly in the UK and that the policy introduction might have accelerated the UK's move towards immediate OA compared to other countries. This supports the argument for the inclusion of a time-limited deposit requirement in OA policies."
40f1816c1f50218614b2b1c87dbc278ca89ba838,"CORE offers seamless, unrestricted access to millions of research papers from around the world to everyone for free. CORE believes that knowledge can be used for the public good, and the more widely this knowledge can be distributed the better, for everyone."
5bc11f44d92d67f334c771daa182f34aedfdb126,"Recent years have seen fast growth in the number of policies mandating Open Access (OA) to research outputs. We conduct a largescale analysis of over 800 thousand papers from repositories around the world published over a period of 5 years to investigate: a) if the time lag between the date of publication and date of deposit in a repository can be effectively tracked across thousands of repositories globally, and b) if introducing deposit deadlines is associated with a reduction of time from acceptance to open public availability of research outputs. We show that after the introduction of the UK REF 2021 OA policy, this time lag has decreased significantly in the UK and that the policy introduction might have accelerated the UK’s move towards immediate OA1 compared to other countries. This supports the argument for the inclusion of a time-limited deposit requirement in OA policies."
694a359199f4e453bb0e9fab2ab59443f381b332,"Type • Presentation Abstract In this work, we conduct experiments comparing OAI-PMH with ResourceSync along a set of dimensions, scenarios and implementation setups. Our results are the first to disclose that OAI-PMH performance varies significantly across platforms, explaining why it is difficult for aggregators to stay synchronised with data providers. We also provide evidence showing that ResourceSync is, if applied in suitable synchronisation mode, substantially faster and more effective in synchronisation of resources and puts also lower load on the data providers than OAI-PMH. By comparing the performance of OAI-PMH with ResourceSync on the task of metadata harvesting (small files), i.e. the most challenging scenario for ResourceSync, we demonstrate that ResourceSync performs up to about 10x faster in the Batch mode than an"
753080d0aa15eea8de1a114d8ae9eeac4efa894e,"To foster responsible research and innovation, research communities, institutions, and funders are shifting their practices and requirements towards Open Science. Open Science skills are becoming increasingly essential for researchers. Indeed general awareness of Open Science has grown among EU researchers, but the practical adoption can be further improved. Recognizing a gap between the needed and the provided training offer, the FOSTER project offers practical guidance and training to help researchers learn how to open up their research within a particular domain or research environment. Aiming for a sustainable approach, FOSTER focused on strengthening the Open Science training capacity by establishing and supporting a community of trainers. The creation of an Open Science training handbook was a first step towards bringing together trainers to share their experiences and to create an open and living knowledge resource. A subsequent series of train-the-trainer bootcamps helped trainers to find inspiration, improve their skills and to intensify exchange within a peer group. Four trainers, who attended one of the bootcamps, contributed a case study on their experiences and how they rolled out Open Science training within their own institutions. On its platform the project provides a range of online courses and resources to learn about key Open Science topics. FOSTER awards users gamification badges when completing courses in order to provide incentives and rewards, and to spur them on to even greater achievements in learning. The paper at hand describes FOSTER Plus’ training strategies, shares the lessons learnt and provides guidance on how to reuse the project’s materials and training approaches."
8a8e42d560fba1dec499015063cb4ca07ef4ec99,"This paper describes the methods used in the submission of Knowledge Media institute (KMI), The Open University to the NTCIR-9 Cross-Lingual Link Discovery (CLLD) task entitled CrossLink. KMI submitted four runs for link discovery from English to Chinese; however, the developed methods, which utilise Explicit Semantic Analysis (ESA), are applicable also to other language combinations. Three of the runs are based on exploiting the existing cross-lingual mapping between different versions of Wikipedia articles. In the fourth run, we assume information about the mapping is not available. Our methods achieved encouraging results and we describe in detail how their performance can be further improved. Finally, we discuss two important issues in link discovery: the evaluation methodology and the applicability of the developed methods across different textual collections."
8adeeb9779b3a61003628f22eb4622ec3d4204b0,
9ceb1b2e77bb359b8868d65c1c864b541079979e,"Over the recent years, there has been a growing interest in developing new research evaluation methods that could go beyond the traditional citation-based metrics. This interest is motivated on one side by the wider availability or even emergence of new information evidencing research performance, such as article downloads, views and Twitter mentions, and on the other side by the continued frustrations and problems surrounding the application of purely citationbased metrics to evaluate research performance in practice. Semantometrics are a new class of research evaluation metrics which build on the premise that full-text is needed to assess the value of a publication. This paper reports on the analysis carried out with the aim to investigate the properties of the semantometric contribution measure [1], which uses semantic similarity of publications to estimate research contribution, and provides a comparative study of the contribution measure with traditional bibliometric measures based on citation counting."
e4993b6c405f129c10641d837bb392fbfbacb108,"In this paper we introduce the Academic Citation Typing (ACT) Platform, a highly scalable online tool that takes as its input any full text research paper and which then enables rapid annotation and classi cation of in-text citations according to purpose and in uence. In contrast to previous work, we employ rst authors as annotators. Our evaluation shows that these authors are able to quickly classify the citations within their own papers. Over 200 authors have thus far annotated their papers using the ACT platform. This approach has already enabled the collection of the largest dataset of citations annotated according to their purposes and in uence on the citing paper. Furthermore, this process is ongoing and the dataset will continue to expand following this initial phase."
f3e86b940f9364e14f3be36e1cf17e33e7c26284,"In this paper we show that citation counts and Mendeley readership are poor indicators of research excellence. Our experimental design builds on the assumption that a good evaluation metric should be able to distinguish publications that have changed a research field from those that have not. The experiment has been conducted on a new dataset for bibliometric research which we call TrueImpactDataset. TrueImpactDataset is a collection of research publications of two types – research papers which are considered seminal work in their area and papers which provide a survey (a literature review) of a research area. The dataset also contains related metadata, which include DOIs, titles, authors and abstracts. We describe how the dataset was built and provide overview statistics of the dataset. We propose to use the dataset for validating research evaluation metrics. By using this data, we show that widely used research metrics only poorly distinguish excellent research."
0163b6bfa5533bfcb8aeac60eaad4197d6372429,"Cross-Lingual Link Discovery (CLLD) aims to automatically find links between documents written in different languages. In this paper, we first present a relatively simple yet effective methods for CLLD in Wiki collections, explaining the findings that motivated their design. Our methods (team KMI) achieved in the NTCIR-10 CrossLink-2 evaluation the best overall results in the English to Chinese, Japanese and Korean (E2CJK) task and were the top performers in the Chinese, Japanese, Korean to English task (CJK2E) [Tang et al.,2013]. Though tested on these language combinations, the methods are language agnostic and can be easily applied to any other language combination with sufficient corpora and available pre-processing tools. In the second part of the paper, we provide an in-depth analysis of the nature of the task, the evaluation metrics and the impact of the system components on the overall CLLD performance. We believe a good understanding of these aspects is the key to improving CLLD systems in the future."
04eef3bc57d74b6d23c19139eab18c22dbbea9d4,"Most Performance-based Research Funding Systems (PRFS) draw on peer review and bibliometric indicators, two different methodologies which are sometimes combined. A common argument against the use of indicators in such research evaluation exercises is their low correlation at the article level with peer review judgments. In this study, we analyse 191,000 papers from 154 higher education institutes which were peer reviewed in a national research evaluation exercise. We combine these data with 6.95 million citations to the original papers. We show that when citation-based indicators are applied at the institutional or departmental level, rather than at the level of individual papers, surprisingly large correlations with peer review judgments can be observed, up to r <= 0.802, n = 37, p < 0.001 for some disciplines. In our evaluation of ranking prediction performance based on citation data, we show we can reduce the mean rank prediction error by 25% compared to previous work. This suggests that citation-based indicators are sufficiently aligned with peer review results at the institutional level to be used to lessen the overall burden of peer review on national evaluation exercises leading to considerable cost savings."
249f49c80ea990e11b6efe272a29407260da7d3f,"This work was presented in COAR 2018 Annual Meeting and General Assembly, Hamburg (Germany)"
316277d0fd755a0422e3f383e208ff0e8e5d0bc4,
39aecfca010e596c4eafd0491a091ad5a24db3b0,"Studying citation patterns of scholarly articles has been of interest to many researchers from various disciplines. While the relationship of citations and scientific impact has been widely studied in the literature, in this paper we develop the idea of analyzing the semantic distance of scholarly articles in a citation network (citation-distance network) to uncover patterns that reflect scientific impact. More specifically, we compare two types of publications in terms of their citation-distance patterns, seminal publications and literature reviews, and focus on their referencing patterns as well as on publications which cite them. We show that seminal publications are associated with a larger semantic distance, measured using the content of the articles, between their references and the citing publications, while literature reviews tend to cite publications from a wider range of topics. Our motivation is to understand and utilize this information to create new research evaluation metrics which would better reflect scientific impact."
3f5557e837dc21d94aa3640c66c0ee5a9ea4df30,
4672368e66e3243bb8ed8eaa2cc6c49c2ae6e4ce,
728f5a8538384477e6fdb8a0ef544716c41429cb,"Patterns of scientific collaboration and their effect on scientific production have been the subject of many studies. In this paper we analyze the nature of ties between co-authors and study collaboration patterns in science from the perspective of semantic similarity of authors who wrote a paper together and the strength of ties between these authors (i.e. how much have they previously collaborated together). These two views of scientific collaboration are used to analyze publications in the TrueImpactDataset [11], a new dataset containing two types of publications - publications regarded as seminal and publications regarded as literature reviews by field experts. We show there are distinct differences between seminal publications and literature reviews in terms of author similarity and the strength of ties between their authors. In particular, we find that seminal publications tend to be written by authors who have previously worked on dissimilar problems (i.e. authors from different fields or even disciplines), and by authors who are not frequent collaborators. On the other hand, literature reviews in our dataset tend to be the result of an established collaboration within a discipline. This demonstrates that our method provides meaningful information about potential future impacts of a publication which does not require citation information."
79d260d9da44e1696f7eb95415edc2b341ea55bf,
8e0cdce3122be8e6bb591f67bd4520ec53660621,"The OpenMinTeD platform aims to bring full text Open Access scholarly content from a wide range of providers together with Text and Data Mining (TDM) tools from various Natural Language Processing frameworks and TDM developers in an integrated environment. In this way, it supports users who want to mine scientific literature with easy access to relevant content and allows running scalable TDM workflows in the cloud."
998f41a753194c6798c787846aa8a81171ee2de9,"Patterns of scientific collaboration and their effect on scientific production have been the subject of many studies. In this paper, we analyze the nature of ties between co-authors and study collaboration patterns in science from the perspective of semantic similarity of authors who wrote a paper together and the strength of ties between these authors (i.e. how frequently have they previously collaborated together). These two views of scientific collaboration are used to analyze publications in the TrueImpactDataset (Herrmannova et al., 2017) (Herrmannova et al., 2017), a new dataset containing two types of publications – publications regarded as seminal and publications regarded as literature reviews by field experts. We show there are distinct differences between seminal publications and literature reviews in terms of author similarity and the strength of ties between their authors. In particular, we find that seminal publications tend to be written by authors who have previously worked on dissimilar problems (i.e. authors from different fields or even disciplines), and by authors who are not frequent collaborators. On the other hand, literature reviews in our dataset tend to be the result of an established collaboration within a discipline. This demonstrates that our method provides meaningful information about potential future impacts of a publication which does not require citation information."
bf96e171b48ad9561cd8790e17415c31f115d39f,"Recent works in the area of academic recommender systems have demonstrated the effectiveness of co-citation and citation closeness in related-document recommendations. However, documents recommended from such systems may drift away from the main theme of the query document. In this work, we investigate whether incorporating the textual information in close proximity to a citation as well as the citation position could reduce such drifting and further increase the performance of the recommender system. To investigate this, we run experiments with several recommendation methods on a newly created and now publicly available dataset containing 53 million unique citation-based records. We then conduct a user-based evaluation with domain-knowledgeable participants. Our results show that a new method based on the combination of Citation Proximity Analysis (CPA), topic modelling and word embeddings achieves more than 20% improvement in Normalised Discounted Cumulative Gain (nDCG) compared to CPA."
db29ae38ae23ff4811c823513f1c2b2717d0258c,"Text and data mining offers an opportunity to improve the way we access and analyse the outputs of academic research. But the technical infrastructure of the current scholarly communication system is not yet ready to support TDM to its full potential, even for open access outputs. To address this problem, Petr Knoth, Nancy Pontika and Lucas Anastasiou have developed the CORE Publisher Connector, a toolkit service designed to assist text miners in accessing content though a single machine interface. The Connector aims to solve the heterogeneity among publisher APIs and assist text miners with data collection, provide a centralised point of access to all openly available scientific publications, and provide a high-performance, constantly updated access interface."
e2b27c1c0c5de79dd93690e624259da72520d46e,
e6cf010535ce582141fd822a236eee6cb1b27bb0,"Studying citation patterns of scholarly articles has been of interest to many researchers from various disciplines. While the relationship of citations and scientific impact has been widely studied in the literature, in this paper we develop the idea of analyzing the semantic distance of scholarly articles in a citation network (citation-distance network) to uncover patterns that reflect scientific impact. More specifically, we compare two types of publications in terms of their citation-distance patterns, seminal publications and literature reviews, and focus on their referencing patterns as well as on publications which cite them. We show that seminal publications are associated with a larger semantic distance, measured using the content of the articles, between their references and the citing publications, while literature reviews tend to cite publications from a wider range of topics. Our motivation is to understand and utilize this information to create new research evaluation metrics which would better reflect scientific impact."
f30dd024e583a0af0cdfdbd886d101b69ddf00ad,"Patterns of scientific collaboration and their effect on scientific production have been the subject of many studies. In this paper we analyze the nature of ties between co-authors and study collaboration patterns in science from the perspective of semantic similarity of authors who wrote a paper together and the strength of ties between these authors (i.e. how much have they previously collaborated together). These two views of scientific collaboration are used to analyze publications in the TrueImpactDataset [11], a new dataset containing two types of publications – publications regarded as seminal and publications regarded as literature reviews by field experts. We show there are distinct differences between seminal publications and literature reviews in terms of author similarity and the strength of ties between their authors. In particular, we find that seminal publications tend to be written by authors who have previously worked on dissimilar problems (i.e. authors from different fields or even disciplines), and by authors who are not frequent collaborators. On the other hand, literature reviews in our dataset tend to be the result of an established collaboration within a discipline. This demonstrates that our method provides meaningful information about potential future impacts of a publication which does not require citation information."
fef74ae390f942a7d77a7befa178b37304f5a646,"In this paper, we argue why and how the integration of recommender systems for research can enhance the functionality and user experience in repositories. We present the latest technical innovations in the CORE Recommender, which provides research article recommendations across the global network of repositories and journals. The CORE Recommender has been recently redeveloped and released into production in the CORE system and has also been deployed in several third-party repositories. We explain the design choices of this unique system and the evaluation processes we have in place to continue raising the quality of the provided recommendations. By drawing on our experience, we discuss the main challenges in offering a state-of-the-art recommender solution for repositories. We highlight two of the key limitations of the current repository infrastructure with respect to developing research recommender systems: 1) the lack of a standardised protocol and capabilities for exposing anonymised user-interaction logs, which represent critically important input data for recommender systems based on collaborative filtering and 2) the lack of a voluntary global sign-on capability in repositories, which would enable the creation of personalised recommendation and notification solutions based on past user interactions."
02d637557c226f90304cdeafee7c4dfacbc0b2ee,"This work looks in depth at several studies that have attempted to automate the process of citation importance classification based on the publications’ full text. We offer a comparison of their individual similarities, strengths and weaknesses. We analyse a range of features that have been previously used in this task. Our experimental results confirm that the number of in-text references are highly predictive of influence. Contrary to the work of Valenzuela et al. (2015), we find abstract similarity one of the most predictive features. Overall, we show that many of the features previously described in literature have been either reported as not particularly predictive, cannot be reproduced based on their existing descriptions or should not be used due to their reliance on external changing evidence. Additionally, we find significant variance in the results provided by the PDF extraction tools used in the pre-processing stages of citation extraction. This has a direct and significant impact on the classification features that rely on this extraction process. Consequently, we discuss challenges and potential improvements in the classification pipeline, provide a critical review of the performance of individual features and address the importance of constructing a large-scale gold-standard reference dataset."
082d1b18b8cf6dfcedd29f53fe5a35bcdcbcc1b4,"In this poster, we outline the technical difficulties and present how we succeeded in harvesting metadata records and full text content of millions of OA articles from publisher APIs. We also show how we have managed to provide an interoperable layer over these data using ResourceSync. 
 
To achieve this we have created a publisher connector, which harvests the open access scientific papers from publishers and exposes the content in a standardised API. Our contribution can be summarised as: a) creation of a seamless layer for accessing content from across publishers, b) offering of a generic integrated access point to these data via ResourceSync and c) provision of a high performance access interface, which will be constantly updated. This is first service to provide a harmonised access layer over non-standardised publisher APIs for retrieving gold and hybrid gold scholarly content as well as the first implementation of ResourceSync scaling to millions of documents with the potential for fast real-time updates."
0de4ad868dec25a2c91a85e9d7bf64d4bca7f1fb,"In this paper, we build on the idea of Citation Proximity Analysis (CPA), originally introduced in [1], by developing a step by step scalable approach for building CPA-based recommender systems. As part of this approach, we introduce three new proximity functions, extending the basic assumption of co-citation analysis (stating that the more often two articles are co-cited in a document, the more likely they are related) to take the distance between the co-cited documents into account. Asking the question of whether CPA can outperform co-citation analysis in recommender systems, we have built a CPA based recommender system from a corpus of 368,385 full-texts articles and conducted a user survey to perform an initial evaluation. Two of our three proximity functions used within CPA outperform co-citations on our evaluation dataset."
29f99e16605f7dcde37d5fe124fc5e70dec696ff,
41fd9c2d5a806722a011e94b5f98a353a2251624,
686bdce91d7799ebb758a5c5d332b4516501d702,"In this paper we show that citation counts and Mendeley readership are poor indicators of research excellence. Our experimental design builds on the assumption that a good evaluation metric should be able to distinguish publications that have changed a research field from those that have not. The experiment has been conducted on a new dataset for bibliometric research which we call TrueImpactDataset. TrueImpactDataset is a collection of research publications of two types -- research papers which are considered seminal work in their area and papers which provide a survey (a literature review) of a research area. The dataset also contains related metadata, which include DOIs, titles, authors and abstracts. We describe how the dataset was built and provide overview statistics of the dataset. We propose to use the dataset for validating research evaluation metrics. By using this data, we show that widely used research metrics only poorly distinguish excellent research."
763d5477494a2ce1080e8788678e060eecf5d1c7,
8423ffc02e09b0c12e0a25e1f38558ba6408e5ca,"In this paper, we argue why and how the integration of recommender systems for research can enhance the functionality and user experience in repositories. We present the latest technical innovations in the CORE Recommender, which provides research article recommendations across the global network of repositories and journals. The CORE Recommender has been recently redeveloped and released into production in the CORE system and has also been deployed in several third-party repositories. We explain the design choices of this unique system and the evaluation processes we have in place to continue raising the quality of the provided recommendations. By drawing on our experience, we discuss the main challenges in offering a state-of-the-art recommender solution for repositories. We highlight two of the key limitations of the current repository infrastructure with respect to developing research recommender systems: 1) the lack of a standardised protocol and capabilities for exposing anonymised user-interaction logs, which represent critically important input data for recommender systems based on collaborative filtering and 2) the lack of a voluntary global sign-on capability in repositories, which would enable the creation of personalised recommendation and notification solutions based on past user interactions."
8f81a287182432a6ce818fe22a111595a8af5c08,"In this paper, we argue why and how the integration of recommender systems for research can enhance the functionality and user experience in repositories. We present the latest technical innovations in the CORE Recommender, which provides research article recommendations across the global network of repositories and journals. The CORE Recommender has been recently redeveloped and released into production in the CORE system and has also been deployed in several third-party repositories. We explain the design choices of this unique system and the evaluation processes we have in place to continue raising the quality of the provided recommendations. By drawing on our experience, we discuss the main challenges in offering a state-of-the-art recommender solution for repositories. We highlight two of the key limitations of the current repository infrastructure with respect to developing research recommender systems: 1) the lack of a standardised protocol and capabilities for exposing anonymised user-interaction logs, which represent critically important input data for recommender systems based on collaborative filtering and 2) the lack of a voluntary global sign-on capability in repositories, which would enable the creation of personalised recommendation and notification solutions based on past user interactions."
9700e0e062e8306078d9befddeddf334ed4418c7,"In this paper, we build on the idea of Citation Proximity Analysis (CPA), originally introduced in [1], by developing a step by step scalable approach for building CPA-based recommender systems. As part of this approach, we introduce three new proximity functions, extending the basic assumption of co-citation analysis (stating that the more often two articles are co-cited in a document, the more likely they are related) to take the distance between the co-cited documents into account. Ask- ing the question of whether CPA can outperform co-citation analysis in recommender systems, we have built a CPA based recommender system from a corpus of 368,385 full-texts articles and conducted a user survey to perform an initial evaluation. Two of our three proximity functions used within CPA outperform co-citations on our evaluation dataset."
a59e586538e4b937d9f112e651a616cd50ffd6b2,"The depth and breadth of research now being published is overwhelming for an individual researcher to keep track of let alone consume. Recommender systems have been developed to make it easier for researchers to discover relevant content. However, these have predominately taken the form of item-to-item recommendations using citation network features or text similarity features.
 This paper details how the Mendeley Suggest recommender system has been designed and developed. We show how implicit user feedback (based on activity data from the reference manager) and collaborative filtering (CF) are used to generate the recommendations for Mendeley Suggest. Because collaborative filtering suffers from the cold start problem (the inability to serve recommendations to new users), we developed additional recommendation methods based on user-defined attributes, such as discipline and research interests.
 Our off-line evaluation shows that where possible, recommendations based on collaborative filtering perform best, followed by recommendations based on recent activity. However, for cold users (for whom collaborative filtering was not possible) recommendations based on discipline performed best. Additionally, when we segmented users by career stages, we found that among senior academics, content-based recommendations from recent activity had comparable performance to collaborative filtering. This justifies our approach of developing a variety of recommendation methods, in order to serve a range of users across the academic spectrum."
aba8541fe936d9ac7b2c04d5c70b80e3dd06a411,
b35db59606cceb26d90184d55691b55ba74022cf,
c8f2a59d13db0149ac3514b0e2176a00bc5a5f76,"Researchers increasingly report their results through online publications, from research papers, data and software to experiments, observations and ideas. Immense amount of research-related data is available on the web on interlinked pages, in repositories, databases, social networking sites, etc. Consequently, researchers rely on online sources, often through search engines, to perform literature searches for their research âĂŤ to search for papers, topics, people etc. to be able to produce new research. However, these publications can be used not only for traditional literature searches, but also as a source for discovering popular and emerging research topics, key publications and people or evaluating research excellence. To aid research, it is important to leverage the potential of data mining technologies to improve the process of how research is being done. This workshop aims to bring together people from different backgrounds who are interested in analysing and mining scholarly data available via web and social media sources using various approaches such as query log mining, graph analysis, text mining, etc., and/or who develop systems that enable such analysis and mining. The topics of this workshop include, but are not limited to, the following areas:"
e0f896ba97f9ee37e2e852f39f08b7314b73a32e,"The purpose of the recommender is to improve the discoverability of research outputs by providing suggestions for similar research papers both within the collection of the hosting repository and the CORE collection. Repository managers can install the recommender to advance the accessibility of other scientific papers and outreach to other scientific communities, since the CORE Recommender acts as a gate to millions of open access research papers."
0bbbcb0c83d727dd152a1252277fc711f241c1ce,This paper presents a method for facilitating cross-language retrieval and machine translation in domain specific collections. The method is based on a semi-automatic adaption of a multilingual domain ontology and it is particularly suitable for the eLearning domain. The presented approach has been integrated into a real-world system supporting cross-language retrieval and machine translation of large amounts of learning resources in nine European languages. The system was built in the context of a European Commission Supported project Eurogene and it is now being used as a European reference portal for teaching human genetics.
174293856f93ff0a67f009aae52544cf5f2085a6,"In recent years the amount of unstructured data stored on the Internet and other digital sources has increased significantly. These data contain often valuable, but hardly retrievable information. The term unstructured data refers mainly to data that does not have a data structure. As a result of this, the unstructured data is not easily readable by machines. In this work, we present a simple method for automatic extraction of semantic relations that can be used to precisely locate valuable pieces of information."
308d09de544b948d4d991573a92a5ead2f42364b,"The current system for disseminating research, which is dominated by commercial publishers, is far from ideal. In an economic sense, prices for both subscriptions and APCs are over-inflated and will likely continue to rise at unacceptable rates. Additionally, there are significant inequalities in the international publishing system both in terms of access and participation. The incentives built into the system, which oblige researchers to publish in traditional publishing venues, perpetuate these problems and greatly stifle our ability to evolve and innovate. 
 
The Next Generation Repositories offers an alternative vision, “to position repositories as the foundation for a distributed, globally networked infrastructure for scholarly communication, on top of which layers of value-added services will be deployed, thereby transforming the system, making it more research-centric, open to and supportive of innovation, while also collectively managed by the scholarly community.” An important component of this vision is that repositories will provide access to a wide variety of research outputs, creating the conditions whereby a greater diversity of contributions to the scholarly record will be accessible, and also formally recognized in research assessment processes. This is very much is aligned with others strategic thinking, such as MIT’s Future of Libraries Report and Lorcan Dempsey’s notion of the “inside-out” library, that are defining a new role of libraries in the 21st century. 
 
This future involves a shift away from libraries purchasing content for their local users, towards libraries curating and sharing with the rest of the world the research outputs produced at their institution. However, to achieve this vision, we need to adopt new functionalities and technologies in repositories and build additional services such as standardized usage metrics, peer review and social networking on top of them. The Next Generation Repositories report provides recommendations for these new behaviours and technologies to move the vision forward. There are already several groups actively working on the adoption of these technologies and services, including OpenAIRE, National Institute of Informatics (Japan) and a US Implementers Group facilitated led by COAR. 
 
This paper will present the vision for next generation repositories, provide an overview of the conceptual model including the role of libraries and hubs that will offer services to the network of repositories. In addition, we will provide an overview of current activities to put the next generation repositories infrastructure and services in place."
4db6e256005c172bf4f63afabbc2498a2916d606,"Over the recent years, there has been a growing interest in developing new research evaluation methods that could go beyond the traditional citation-based metrics. This interest is motivated on one side by the wider availability or even emergence of new information evidencing research performance, such as article downloads, views and Twitter mentions, and on the other side by the continued frustrations and problems surrounding the application of purely citation-based metrics to evaluate research performance in practice. Semantometrics are a new class of research evaluation metrics which build on the premise that full-text is needed to assess the value of a publication. This paper reports on the analysis carried out with the aim to investigate the properties of the semantometric contribution measure [1], which uses semantic similarity of publications to estimate research contribution, and provides a comparative study of the contribution measure with traditional bibliometric measures based on citation counting."
4e03536d30e6d46844dcc1a9dc8246ca57869373,"Open science refers to all things open in research and scholarly communication: from publications and research data to code, models and methods as well as quality evaluation based on open peer review. However, getting started with implementing open science might not be as straightforward for all stakeholders. For example, what do research funders expect in terms of open access to publications and/or research data? Where and how to publish research data? How to ensure that research results are reproducible? These are all legitimate questions and, in particular, early career researchers may benefit from additional guidance and training. In this paper we review the activities of the European-funded FOSTER project which organized and supported a wide range of targeted trainings for open science, based on face-to-face events and on a growing suite of e-learning courses. This article reviews the approach and experiences gained from the first two years of the project."
572444d416484b6fed8098bde563bc1c78fdc67c,"Digital libraries that store scientific publications are becoming increasingly central to the research process. They are not only used for traditional tasks, such as finding and storing research outputs, but also as a source for discovering new research trends or evaluating research excellence. With the current growth of scientific publications deposited in digital libraries, it is no longer sufficient to provide only access to content. To aid research, it is especially important to leverage the potential of text and data mining technologies to improve the process of how research is being done."
57faaa6f9ef93a2b144e6392cf140e36a4a790ed,
72138af2afbf9713af6b56c441340ce398ea418b,"Over the recent years, there has been a growing interest in developing new scientometric measures that could go beyond the traditional citation-based bibliometric measures. This interest is motivated on one side by the wider availability or even emergence of new information evidencing research performance, such as article downloads, views and Twitter mentions, and on the other side by the continued frustrations and problems surrounding the application of citation-based metrics to evaluate research performance in practice. 
 
This document reports on the analysis carried out with the aim to investigate the properties of the semantometric contribution measure (Knoth & Herrmannova, 2014). In particular, we provide a comparative evaluation of the semantometric contribution measure alongside citation counting. Our analysis also focuses on the potential application of semantometric measures in large databases of research papers, for example the Open Access research articles aggregation system CORE."
764afd77ed4edf234dd7370693294a9ed5c0a0a2,"In this paper we analyse a new dataset of scholarly publications, the Microsoft Academic Graph (MAG). The MAG is a heterogeneous graph comprised of over 120 million publication entities and related authors, institutions, venues and fields of study. It is also the largest publicly available dataset of citation data. As such, it is an important resource for scholarly communications research. As the dataset is assembled using automatic methods, it is important to understand its strengths and limitations, especially whether there is any noise or bias in the data, before applying it to a particular task. This article studies the characteristics of the dataset and provides a correlation analysis with other publicly available research publication datasets to answer these questions. Our results show that the citation data and publication metadata correlate well with external datasets. The MAG also has very good coverage across different domains with a slight bias towards technical disciplines. On the other hand, there are certain limitations to completeness. Only 30 million papers out of 127 million have some citation data. While these papers have a good mean total citation count that is consistent with expectations, there is some level of noise when extreme values are considered. Other current limitations of MAG are the availability of affiliation information, as only 22 million papers have these data, and the normalisation of institution names."
7d79dc2ca8313d6bbfcebc0786af9e5834e0a9b5,"The amount of open access content stored in repositories has increased dramatically, which has created new technical and organisational challenges for bringing this content together. The COnnecting REpositories (CORE) project has been dealing with these challenges by aggregating and enriching content from hundreds of open access repositories, increasing the discoverability and reusability of millions of open access manuscripts. As repository managers and library directors often wish to know the details of the content harvested from their repositories and keep a certain level of control over it, CORE is now facing the challenge of how to enable content providers to manage their content in the aggregation and control the harvesting process. In order to improve the quality and transparency of the aggregation process and create a two-way collaboration between the CORE project and the content providers, we propose the CORE Dashboard."
82a372d760affb1b3cd5711f04007339919e98d4,"In the current technology dominated world, interoperability of systems managed by different organisations is an essential property enabling the provision of services at a global scale. In the Text and Data Mining field (TDM), interoperability of systems offering access to text corpora offers the opportunity of increasing the uptake and impact of TDM applications. The global corpus of all research papers, i.e. the collection of human knowledge so large no one can ever read in their lifetime, represents one of the most exciting opportunities for TDM. Although the Open Access movement, which has been advocating for free availability and reuse rights to TDM from research papers, has achieved some major successes on the legal front, the technical interoperability of systems offering free access to research papers continues to be a challenge. COnnecting REpositories (CORE) (Knoth and Zdrahal, 2012) aggregates the world’s open access full-text scientific manuscripts from repositories, journals and publisher systems. One of the main goals of CORE is to harmonise and pre-process these data to lower the barrier for TDM. In this paper, we report on the preliminary results of an interoperability survey of systems provided by journal publishers, both open access and toll access. This helps us to assess the current level of systems’ interoperability and suggest ways forward."
afd36a15a9f46ca059b3e50d1afc209a2e4e6071,
b0ab97041e1e1b23a8e71aac631a47f4c07dc89c,
dd0e247616f74419bc00116a366848b2e1abb29f,"With the growing amount of published research, automatic evaluation of scholarly publications is becoming an important task. In this paper we address this problem and present a simple and transparent approach for evaluating the importance of scholarly publications. Our method has been ranked among the top performers in the WSDM Cup 2016 Challenge. The first part of this paper describes our method. In the second part we present potential improvements to the method and analyse the evaluation setup which was provided during the challenge. Finally, we discuss future challenges in automatic evaluation of papers including the use of full-texts based evaluation methods."
f2c05c6d42345462991c730cfe057498fc6bd681,"In the current technology dominated world, interoperability of systems managed by different organisations is an essential property enabling the provision of services at a global scale. In the Text and Data Mining field (TDM), interoperability of systems offering access to text corpora offers the opportunity of increasing the uptake and impact of TDM applications. The global corpus of all research papers, i.e. the collection of human knowledge so large no one can ever read in their lifetime, represents one of the most exciting opportunities for TDM. Although the Open Access movement, which has been advocating for free availability and reuse rights to TDM from research papers, has achieved some major successes on the legal front, the technical interoperability of systems offering free access to research papers continues to be a challenge. COnnecting REpositories (CORE) (Knoth and Zdrahal, 2012) aggregates the world’s open access full-text scientific manuscripts from repositories, journals and publisher systems. One of the main goals of CORE is to harmonise and pre-process these data to lower the barrier for TDM. In this paper, we report on the preliminary results of an interoperability survey of systems provided by journal publishers, both open access and toll access. This helps us to assess the current level of systems’ interoperability and suggest ways forward."
0e52f1c8d3569254079c57c8bfa489af1d3f9dec,"To date, many studies of scientific citation, collaboration and coauthorship networks have focused on the concept of cross-community ties. In this article we explore how Semantometrics can help to characterise the types of research collaboration in scholarly publication networks and the nature of the cross-community ties, and how this information can be utilised in aiding research evaluation. In contrast to the existing research evaluation metrics such as Bibliometrics, Altmetrics or Webometrics, which are based on measuring the number of interactions in the scholarly network, Semantometrics build on the premise that fulltext is needed to understand the value of publications. Using the CORE dataset as a case study, this paper looks at the relation between the semantic distance of authors and their research endogamy value. We identify four potential types of collaboration in a coauthorship network. The results suggest similar measures can be used to provide meaningful information about the nature of collaboration in scholarly publication networks."
5ba649aa1f4d67af06612a934b997179ef84327c,
65f3b49f9d2f664202fd94e74d0d2655d6a4978f,The Open Science Taxonomy can be found online at the Facilitate Open Science Training for European Research (FOSTER) portal.
7fff062253edf975905332dcb1558c481c2c07d9,"The essential compliment of skills beyond research excellence, rigor and method are traditionally described as “soft skills”. This includes how to formulate an argument, how to construct a scientific publication, how to communicate such publications to non-experts, place them in context of societal challenges and relevant policies, how to write a competitive proposal and “market” one’s research idea to build a research group around an interesting research topic."
8368e0d9409876495dc84ee98200fc9fa890336c,"The term ""Open Science"" is recently widely used, but it is still unclear to many research stakeholders - funders, policy makers, researchers, administrators, librarians and repository managers - how Open Science can be achieved. FOSTER (Facilitate Open Science Training for European Research) is a European Commission funded project, which is developing an e-learning portal to support the training of a wide range of stakeholders in Open Science and related areas. In 2014 the FOSTER project co-funded 28 training activities in Open Science, which include more than 110 events, while in 2015 the project has supported 24 community training events in 18 countries. In this paper, we describe the FOSTER approach in structuring the Open Science domain for educational purposes, present the functionality of the FOSTER training portal and discuss its use and potential for training the key stakeholders using self-learning and blended-learning methods."
89ae656046763b81b31b8209da2b03d5786216a9,"The aim of this article is to demonstrate some of the possible uses of a novel set of metrics called Semantometrics in relation to the role of ""bridges"" in scholarly publication networks. In contrast to the existing metrics such as Bibliometrics, Altmetrics or Webometrics, which are based on measuring the number of interactions in the scholarly network, Semantometrics build on the premise that full-text is needed to understand scholarly publication networks and the value of publications."
baedf002dd39ec383bc4406fcf5dc317ae4a3d70,"A vast amount of information is today stored in the form of textual documents, many of which are available online. These documents come from different sources and are of different types. They include newspaper articles, books, corporate reports, encyclopedia entries and research papers. At a semantic level, these documents contain knowledge, which was created by explicitly connecting information and expressing it in the form of a natural language. However, a significant amount of knowledge is not explicitly stated in a single document, yet can be derived or discovered by researching, i.e. accessing, comparing, contrasting and analysing, information from multiple documents. Carrying out this work using traditional search interfaces is tedious due to information overload and the difficulty of formulating queries that would help us to discover information we are not aware of. 
 
In order to support this exploratory process, we need to be able to effectively navigate between related pieces of information across documents. While information can be connected using manually curated cross-document links, this approach not only does not scale, but cannot systematically assist us in the discovery of sometimes non-obvious (hidden) relationships. Consequently, there is a need for automatic approaches to link discovery. 
 
This work studies how people link content, investigates the properties of different link types, presents new methods for automatic link discovery and designs a system in which link discovery is applied on a collection of millions of documents to improve access to public knowledge."
0f70ba8a14b5c79fb0a087145ca73df53d99e1d4,"Usage statistics are frequently used by repositories to justify their value to the management who decide about the funding to support the repository infrastructure. Another reason for collecting usage statistics at repositories is the increased use of webometrics in the process of assessing the impact of publications and researchers. Consequently, one of the worries repositories sometimes have about their content being aggregated is that they feel aggregations have a detrimental effect on the accuracy of statistics they collect. They believe that this potential decrease in reported usage can negatively influence the funding provided by their own institutions. This raises the fundamental question of whether repositories should allow aggregators to harvest their metadata and content. In this paper, we discuss the benefits of allowing content aggregations harvest repository content and investigate how to overcome the drawbacks."
62d304c5a3fe0b91473f8f98400985cd066db607,"We propose Semantometrics, a new class of metrics for evaluating research. As opposed to existing Bibliometrics,Webometrics, Altmetrics, etc., Semantometrics are not based on measuring the number of interactions in the scholarly communication network, but build on the premise that full-text is needed to assess the value of a publication. This paper presents the first Semantometric measure, which estimates the research contribution. We measure semantic similarity of publications connected in a citation network and use a simple formula to assess their contribution. We carry out a pilot study in which we test our approach on a small dataset and discuss the challenges in carrying out the analysis on existing citation datasets. The results suggest that semantic similarity measures can be utilised to provide meaningful information about the contribution of research papers that is not captured by traditional impact measures based purely on citations."
692b9ed7bafa05302e7a4dfaef853cf8fe366542,
7b740078d6dd02bcb1238510d62be53aaa18ece2,
931e540f7ee62f928a3201fc1c761c6369dfc3ee,"In this paper, we present the overview of Europeana Cloud system, which is a new undertaking of Europeana Foundation and partnering institutions aimed to provide shared, cloud-based infrastructure for aggregation and exchange of cultural heritage metadata and content for European institutions."
a3a952dca1772454822705ed941a19af211b998d,
da27934a34ade643b3a70c43cf3c27f714ce2756,"While openness is well applied to software development and exploitation (open sources), and successfully applied to new business models (open innovation), fundamental and applied research seems to lag behind.
Even after decades of advocacy, in 2011 only 50% of the public-funded research was freely available and accessible (Archambault et al., 2013). The current research workflows, stemming from a pre-internet age, result in loss of opportunity not only for the researchers themselves (cf. extensive literature on topic at Open Access citation project, http://opcit.eprints.org/), but also slows down innovation and application of research results (Houghton & Swan, 2011).
Recent studies continue to suggest that lack of awareness among researchers, rather than lack of e-infrastructure and methodology, is a key reason for this loss of opportunity (Graziotin 2014).
The session will focus on why Open Science is ideally suited to achieving tenure-relevant researcher impact in a “Publish or Perish” reality. Open Science encapsulates tools and approaches for each step along the research cycle: from
Open Notebook Science to Open Data, Open Access, all setting up researchers for capitalising on social media in order to promote and discuss, and establish unexpected collaborations.
Incorporating these new approaches into a updated personal research workflow is of strategic beneficial for young researchers, and will prepare them for expected long term funder trends towards greater openness and demand for greater return on investment (ROI) for public funds.
* All authors offer equal contribution. 323"
e05f2ac2a018237090b9eb60abbe7c7eb4f244f2,
451a403b6676c593e8cee1fb4b71ef04f3679bf2,"Cross-Lingual Link Discovery (CLLD) aims to automatically find links between documents written in different languages. In this paper, we first present a relatively simple yet effective methods for CLLD in Wiki collections, explaining the fndings that motivated their design. Our methods (team KMI) achieved in the NTCIR-10 CrossLink-2 evaluation the best overall results in the English to Chinese, Japanese and Korean (E2CJK) task and were the top performers in the Chinese, Japanese, Korean to English task (CJK2E)1 [Tang et al.,2013]. Though tested on these language combinations, the methods are language agnostic and can be easily applied to any other language combination with sufficient corpora and available pre-processing tools. In the second part of the paper, we provide an in depth analysis of the nature of the task, the evaluation metrics and the impact of the system components on the overall CLLD performance. We believe a good understanding of these aspects is the key to improving CLLD systems in the future."
6a4c744635563aa2b2d14400051f510865565f1f,"Cross-Lingual Link Discovery (CLLD) aims to automatically find links between documents written in different languages. In this paper, we first present a relatively simple yet effective methods for CLLD in Wiki collections, explaining the findings that motivated their design. Our methods (team KMI) achieved in the NTCIR-10 CrossLink-2 evaluation the best overall results in the English to Chinese, Japanese and Korean (E2CJK) task and were the top performers in the Chinese, Japanese, Korean to English task (CJK2E) [Tang et al.,2013]. Though tested on these language combinations, the methods are language agnostic and can be easily applied to any other language combination with sufficient corpora and available pre-processing tools. In the second part of the paper, we provide an in-depth analysis of the nature of the task, the evaluation metrics and the impact of the system components on the overall CLLD performance. We believe a good understanding of these aspects is the key to improving CLLD systems in the future."
8b45f8635188a29fd786d6cb3b4eae6115664bbe,The paper describes the Eurogene portal for sharing and reusing multilingual multimedia educational resources in human genetics. The content is annotated using concepts of two ontologies and a topic hierarchy. The ontology annotation is used to guide search and for calculating semantically similar content. Educational resources can be aggregated into learning packages. The system is in routine use since 2009.
9bab7441bfadb0f587fc62368d9ded0bde7e7920,"An essential goal of the open access (OA) movement is free availability of research outputs on the Internet. One of the recommended ways to achieve this is through open access repositories (BOAI, 2002). Given the growing number of repositories and the significant proportion of research outputs already available as OA (Laakso & Bjork, 2012), it might come as a surprise that OA content is not necessarily easily discoverable on the Internet (Morrisson, 2012; Konkiel, 2012), more precisely, it is available, but often difficult to find. If OA content in repositories cannot be discovered, there is little incentive to make it available on the Internet in the first place. Therefore, not trying hard enough to increase the visibility of OA content would be a lost opportunity for achieving the main OA goals, including also the reuse potential of OA content. In this paper, we build on our experience in finding and aggregating open access content (not just metadata) from repositories, discussing the main issues and summarizing the lessons learned into two principles that, if adopted, will dramatically increase the discoverability of OA content on the Internet and will improve the possibilities of OA content reuse."
acd8910f491949313088e25a626aa3175a4f1df3,"The push for free online availability of research outputs promoted by the Open Access (OA) movement is undoubtedly transforming the publishing industry. However, the mere availability of research outputs is insufficient. To exploit the full potential of OA, it must be possible to search, discover, mine, analyse, etc. this content. To achieve this, it is essential to improve the existing OA technical infrastructure to effectively support these functionalities. Many of the vital benefits of OA are expected to come with the ability to reuse OA content in unanticipated ways. Access to the OA content must therefore be flexible, yet practical, content-based and not just metadata based. In this demonstration, we present the CORE system, which aggregates millions of OA resources from hundreds of OA repositories and journals. We discuss the use cases aggregations should support and demonstrate how the CORE system addresses them, including searching, discovering, mining and analyzing content. We also show how aggregated OA content can be reused to build new applications on top of CORE's functionality."
f80f00495d56e3e8c08c5527a699acf8f4cba254,
03248c8ad8268b176c80f2148111f3304ad58fc8,"The paper argues that automatic link generation and typing methods are needed to find and maintain crossdocument links in large and growing textual collections. Such links are important to organise information and to support search and navigation. We present an experimental study on mining cross-document links from a collection of 5000 documents. We identify a set of link types and show that the value of semantic similarity is a good distinguishing indicator. Keywords-text mining, automatic link generation and typing, semantic similarity, digital libraries"
0fa4a87397119b71ab4df692a044e03a78e9c204,"In recent years the amount of unstructured data stored on the Internet and other digital sources has increased significantly. These data contain often valuable, but hardly retrievable information. The term unstructured data refers mainly to data that does not have a data structure. As a result of this, the unstructured data is not easily readable by machines. In this work, we present a simple method for automatic extraction of semantic relations that can be used to precisely locate valuable pieces of information."
37cb9ba0350bd38e7c00e8da71bbd9f4711745ca,"In recent years a number of new approaches for visualising and browsing document collections have been developed. These approaches try to address the problems associated with the growing amounts of content available and the changing patterns in the way people interact with information. Users now demand better support for exploring document collections to discover connections, compare and contrast information. Although visual search interfaces have the potential to improve the user experience in exploring document collections compared to textual search interfaces, they have not yet become as popular among users. The reasons for this range from the design of such visual interfaces to the way these interfaces are implemented and used. In this paper we study these reasons and determine the factors that contribute to an improved visual browsing experience. Consequently, by taking these factors into account, we propose a novel visual search interface that improves exploratory search and the discovery of document relations. We explain our universal approach, and how it could be applied to any document collection, such as news articles, cultural heritage artifacts or research papers."
523d7bbe4ad0c97b3acbda2aa3b7874c3bd31be3,
de0afa82e9ee35f005aa74243251196b9b47cb38,"The last 10 years have seen a massive increase in the amount of Open Access publications in journals and institutional repositories. The open availability of large volumes of state-of-the-art knowledge online has the potential to provide huge savings and benefits in many fields. However, in order to fully leverage this knowledge, it is necessary to develop systems that (a) make it easy for users to discover and access this knowledge at the level of individual resources, (b) explore and analyse this knowledge at the level of collections of resources and (c) provide infrastructure and access to raw data in order to lower the barriers to the research and development of systems and services on top of this knowledge. In this paper, we argue why these requirements should be satisfied and show that current systems do not meet them. Consequently, we present the CORE (COnnecting REpositories) system, a large-scale Open Access aggregation, outlining its existing functionality and discussing the future technical development. We demonstrate how the system addresses the above needs and how it can be applied to the benefit of the whole ecosystem that includes institutional repositories, individuals, researchers, developers, funding bodies and governments."
1c0ed3237388f959f08a1237ac09067aa42b9835,"Recent developments on the Web are marked by the growing support for the Linked Data initiative, which encourages government and public organisations, as well as private institutions, to expose their data on the Web. This results in a plentitude of multi-lingual document collections where the original resources are published in the language, in which they are available. The challenges of multilingualism present on the Semantic Web are also reflected in the context of services on the Web, characterised by the rapid increase in popularity and use of Web APIs, as indicated by the growing number of available APIs and the applications built on top of them. Web APIs are commonly described in plain-text as part of Web pages, following no particular guidelines and conforming to no standards, despite some initial approaches in the area [1, 2]. Therefore, API providers publish descriptions in any language they see fit, making the service discovery and the subsequent processing of the documentation challenging tasks. In this paper, we present a cross-lingual approach that calculates semantic similarity of text to help classify and annotate Web APIs, based on their textual descriptions. Furthermore, we show how our solution can be implemented as part of SWEET [3], which is a tool that enables the semi-automated creation of semantic Web API descriptions. In addition, we demonstrate how the cross-lingual approach can be adopted to support the language-independent discovery of Web APIs."
391609e857db74c510895a4f741893a6e1fdaeea,"This paper explores how to automatically generate cross language links between resources in large document collections. The paper presents new methods for Cross Lingual Link Discovery(CLLD) based on Explicit Semantic Analysis (ESA). The methods are applicable to any multilingual document collection. In this report, we present their comparative study on the Wikipedia corpus and provide new insights into the evaluation of link discovery systems. In particular, we measure the agreement of human annotators in linking articles in different language versions of Wikipedia, and compare it to the results achieved by the presented methods."
6aac9b15c8fe765771fed2a9375443dcf3461f17,
7667d7f4ce847ca41aa524179749c0d815273187,"This submission reports on the results of the ongoing JISC-funded project CORE (COnnecting REpositories) which aims to facilitate the access and navigation across relevant scientific papers stored in Open Access repositories. This is being achieved by harvesting metadata and full-text content from diverse Open Access repositories, applying text mining techniques to discover semantic relations between the articles and representing and exposing these relations as Linked Data. The information about associations between articles will be made publicly available to enable the emergence of a wide range of applications that can exploit the provided data. Within this project, we will demonstrate the usability of the CORE system on two use-cases: (1) Improving the accessibility of content and the navigation capabilities for digital library users, (2) Enabling more ubiquitous access to digital content through smart phones and tablet devices."
a68e728c3e4adc12c5bec78f5edacc1e997c500b,"This paper describes the methods used in the submission of Knowledge Media institute (KMI), The Open University to the NTCIR-9 Cross-Lingual Link Discovery (CLLD) task entitled CrossLink. KMI submitted four runs for link discovery from English to Chinese; however, the developed methods, which utilise Explicit Semantic Analysis (ESA), are applicable also to other language combinations. Three of the runs are based on exploiting the existing cross-lingual mapping between different versions of Wikipedia articles. In the fourth run, we assume information about the mapping is not available. Our methods achieved encouraging results and we describe in detail how their performance can be further improved. Finally, we discuss two important issues in link discovery: the evaluation methodology and the applicability of the developed methods across different textual collections."
af18323cda5e5cfe09bf756c25f7ac909ad61f2f,The paper argues that automatic link generation and typing methods are needed to ﬁnd and maintain cross document links in large and growing textual collections. Such links are important to organise information and to support search and navigation. We present an experimental study on mining cross document links from a collection of 5000 documents. We identify a set of link types and show that the value of semantic similarity is a good distinguishing indicator.
b4844d029abdb2d64e31a6966a322d69561353a2,"Recent developments on the Web are marked by the growing support for the Linked Data initiative, which encourages government and public organisations, as well as private institutions, to expose their data on the Web. This results in a plentitude of multi-lingual document collections where the original resources are published in the language, in which they are available. The challenges of multilingualism present on the Semantic Web are also reflected in the context of services on the Web, characterised by the rapid increase in popularity and use of Web APIs, as indicated by the growing number of available APIs and the applications built on top of them. Web APIs are commonly described in plain-text as part of Web pages, following no particular guidelines and conforming to no standards, despite some initial approaches in the area [1, 2]. Therefore, API providers publish descriptions in any language they see fit, making the service discovery and the subsequent processing of the documentation challenging tasks. In this paper, we present a cross-lingual approach that calculates semantic similarity of text to help classify and annotate Web APIs, based on their textual descriptions. Furthermore, we show how our solution can be implemented as part of SWEET [3], which is a tool that enables the semi-automated creation of semantic Web API descriptions. In addition, we demonstrate how the cross-lingual approach can be adopted to support the language-independent discovery of Web"
c98b2a5f357c7b8dce094fa540852dddedfd474f,"The paper argues that automatic link generation and typing methods are needed to find and maintain crossdocument links in large and growing textual collections. Such links are important to organise information and to support search and navigation. We present an experimental study on mining cross-document links from a collection of 5000 documents. We identify a set of link types and show that the value of semantic similarity is a good distinguishing indicator. Keywords-text mining, automatic link generation and typing, semantic similarity, digital libraries"
1661938cfc650fa0b8085a16e5a73b4dede924a3,
3318c875d91f7100aa1887cd18445b6d6837a7d8,This paper presents a method for facilitating cross-language retrieval and machine translation in domain specific collections. The method is based on a semi-automatic adaption of a multilingual domain ontology and it is particularly suitable for the eLearning domain. The presented approach has been integrated into a real-world system supporting cross-language retrieval and machine translation of large amounts of learning resources in nine European languages. The system was built in the context of a European Commission Supported project Eurogene and it is now being used as a European reference portal for teaching human genetics.
6a9503fa3a01a1fed5ca6ed3bc2937f7c31b520e,
9d2cd7b59ac7ba68633d552cecc4f9c7d339b4bc,"This work investigates and reviews state-of-the-art approaches to automatic organization of unstructured digital content. Given the review of the current challenges, the research questions address a vital issue of automatic metadata generation with the focus on link generation and link typing based on the analysis of content. The motivation and the potential impact these methods may generate both in the general context and in the context of digital learning repositories are discussed. Finally, a detailed research plan and the current results are presented."
9f04fe908c2d459d92e6fb459ae4e6414b56c26b,"This paper investigates the use and the prediction potential of semantic similarity measures for automatic generation of links across different documents and passages. First, the correlation between the way people link content and the results produced by standard semantic similarity measures is investigated. The relation between semantic similarity and the length of the documents is then also analysed. Based on these findings a new method for link generation is formulated and tested."
3c84bf0026022a219f4f1357ba25b9e705b42172,"This paper describes how semantic annotations in terms of a domain ontology and theme hierarchy can be used for organising and reusing educational resources. A case study is presented in the domain of human genetics. The technology has been developed as a part of the Eurogene project and allows the user to submit, annotate and retrieve multimedia learning resources in nine European languages. We present two use case examples: Query by example and discovering learning pathways."
7ce19a6061c265319306e8ca6fe2a0a07d3fc49f,
8950dbeff89e8c2bdaad08d597c2dea1a52e4d8a,"One of the important tasks in the use of learning resources in e-learning is the necessity to annotate learning objects with appropriate metadata. However, annotating resources by hand is time consuming and difficult. Here we explore the problem of automatic extraction of metadata for description of learning resources. First, theoretical constraints for gathering certain types of metadata important for e-learning systems are discussed. Our approach to annotation is then outlined. This is based on a domain ontology, which allows us to annotate learning resources in a language independent way.We are motivated by the fact that the leading providers of learning content in various domains are often spread across countries speaking different languages. As a result, cross-language annotation can facilitate accessibility, sharing and reuse of learning resources."
fbe5e3b3196081c5a343c491ec0922526c36d666,"Automatic Term Recognition focuses on the extraction of words and multi-word expressions that are significant for a given domain. There is a considerable interest in using ATR for automatic metadata generation, creation of thesauri and terminological glossaries, keyword extraction, ontology building, etc. In this paper, we build upon the work done at the University of Sheffield, where a library with a few algorithms for ATR was recently developed. We enrich this library with new ATR algorithms and tools for evaluation. Our aim is to perform an experimental study comparing the base ATR methods as well as their combinations under various conditions. The results of the study indicate that better precision can be usually reached by combining ATR methods using foreground and ATR methods using background knowledge. The created platform is freely available and prepared for extensions by other researchers."
1bc1fb4248d93fb5fe6950aefb410e53f09a7fcc,"In recent years the amount of unstructured data stored on the Internet and other digital sources has increased significantly. These data contain often valuable, but hardly retrievable information. The term unstructured data refers mainly to data that does not have a data structure. As a result of this, the unstructured data is not easily readable by machines. In this work, we present a simple method for automatic extraction of semantic relations that can be used to precisely locate valuable pieces of information."
2fa28ca0e144e478c12e9b39d25cffd01b8f62f1,"Recently, there has been much effort in making biomedical knowledge, typically stored in scientific articles, more accessible and interoperable. As a matter of fact, the unstructured nature of such texts makes it difficult to apply knowledge discovery and inference techniques. Annotating information units with semantic information in these texts is the first step to make the knowledge machine-analyzable. In this work, we first study methods for automatic information extraction from natural language text. Then we discuss the main benefits and disadvantages of the state-of-art information extraction systems and, as a result of this, we adopt a machine learning approach to automatically learn extraction patterns in our experiments. Unfortunately, machine learning techniques often require a huge amount of training data, which can be sometimes laborious to gather. In order to face up to this tedious problem, we investigate the concept of weakly supervised or bootstrapping techniques. Finally, we show in our experiments that our machine learning methods performed reasonably well and significantly better than the baseline. Moreover, in the weakly supervised learning task we were able to substantially bring down the amount of labeled data needed for training of the extraction system."
8b1cb1823ddfcbcb3a804fa92a23e18306d12692,"In recent years the amount of unstructured data stored on the Internet and other digital sources has increased significantly. These data contain often valuable, but hardly retrievable information. The term unstructured data refers mainly to data that does not have a data structure. As a result of this, the unstructured data is not easily readable by machines. In this work, we present a simple method for automatic extraction of semantic relations that can be used to precisely locate valuable pieces of information."
a1e983ac1b4aef7ea892433c77b322bbf523b645,"We argue that there is a need for globally unique decentralised persistent identifiers (PIDs) for identifying research outputs resolvable to repositories. We propose OAI identifiers as a solution to this problem, explaining how OAI identifiers complement DOIs in the delivery of an open scholarly research graph. We then present the first OAI resolver built on top of the CORE aggregation system that works out-of-the-box for repositories that expose their metadata through OAI-PMH (the vast majority of repositories)."
7a6925e9185ec8dceb16222213ca995b400f249b,"Virtual Reality (VR) is a technology that has been used to provide the Mirror Visual Feedback (MVF) illusion to patients with promising results. In the present work, the goal is to design, develop and test a portable VR-based MVF system that monitors behavioral information about the performance of a simple motor task. The developed application runs in a stand-alone VR system and allows the researcher to select the real and virtual hands used to perform the motor task. The system was evaluated with a group of twenty healthy volunteers (12 men and 8 women) with ages between 18 and 66 years. Participants had to repetitively perform a motor task in four different experimental conditions: two mirror conditions (performing real movements with the dominant and with the non-dominant hand) and two non-mirror conditions. A significant effect of the experimental condition on embodiment score (p < 0.001), response time (p < 0.001), performance time (p < 0.001), trajectory length (p < 0.004) and trajectory maximum horizontal deviation (p < 0.001) was observed. Furthermore, a significant effect of the experimental moment (initial, middle and final parts of the training) on the performance time was observed (p < 0.001). These results show that the monitored parameters provide relevant information to evaluate the participant’s task performance in different experimental conditions."
83b3d13220d1a5ea7598e2909ece174d10f1f4f2,"Virtual reality is a technology that has been integrated in recent years in many motor rehabilitation protocols. It allows the patient to perform the training exercises in a virtual world controlling the movements of an avatar. Although exercises are usually adapted or modified by the therapist depending on the patients’ evolution, VR systems do not usually include analytics to evaluate the performance of the exercises. In the present work, the goal is to design and develop a VR-based first-person perspective motor rehabilitation system that registers relevant performance measures during the training procedure. The developed application runs on a PC with an HTC Vive Pro virtual reality headset and Valve Index controllers. It has been developed using Unity 3D with OpenVR Unity integration. The exercise consists of moving an object from an initial to a final position in specific moments (iterations). The application registers information about the behavior and the performance of the participant during each iteration. The behavioral data that is monitored and saved during each training session will be analyzed and a new version of the application will be developed to provide the therapist with an analytics platform that can be considered to decide new steps of the training protocol."
88f60e36878b4b1d6bcf611aec4602a4d17f180e,
c51588485b20fd7d7e3a1417c6d1dab99c5af301,
38452eab5d65df92c140913f631692ea617030e4,"The neurovisceral integration model proposes a neuronal network that is related to heart rate activity and cognitive performance. The aim of this study was to determine whether heart rate variability (HRV) and variability in electroencephalographic (EEG) functional connectivity in the resting state are related to cognitive flexibility. Thirty-eight right-handed students completed the CAMBIOS test, and their heart and EEG activity was recorded during 6 min in the resting state with their eyes open. We calculated correlations, partial correlations and multiple linear regressions among HRV indices, functional brain connectivity variability and CAMBIOS scores. Furthermore, the sample was divided into groups according to CAMBIOS performance, and one-way ANOVA was applied to evaluate group differences. Our results show direct and inverse correlations among cognitive flexibility, connectivity (positive and negative task networks) and heartbeat variability. Partial correlations and multiple linear regressions suggest that the relation between HRV and CAMBIOS performance is mediated by neuronal oscillations. ANOVA confirms that HRV and variability in functional brain connectivity is related to cognitive performance. In conclusion, the levels of brain signal variability might predict cognitive flexibility in a cognitive task, while HRV might predict cognitive flexibility only when it is mediated by neuronal oscillations."
5167b3bd33147d8766879fe245ea2c5982251fd4,"Neurofeedback is a form of neuromodulation based on learning to modify some aspects of cortical activity. Sensorimotor rhythm (SMR) oscillation is one of the most used frequency bands in neurofeedback. Several studies have shown that subjects can learn to modulate SMR power to control output devices, but little is known about possible related changes in brain networks. The aim of this study was to investigate the enhanced performance and changes in EEG power spectral density at somatosensory cerebral areas due to a bidirectional modulation-based SMR neurofeedback training. Furthermore, we also analyzed the functional changes in somatosensory areas during resting state induced by the training as exploratory procedure. A six-session neurofeedback protocol based on learning to synchronize and desynchronize (modulate) the SMR was implemented. Moreover, half of the participants were enrolled in two functional magnetic resonance imaging resting-state sessions (before and after the training). At the end of the training, participants showed a successful performance enhancement, an increase in SMR power specific to somatosensory locations, and higher functional connectivity between areas associated with somatosensory activity in resting state. Our research increases the better understanding of the relation between EEG neuromodulation and functional changes and the use of SMR training in clinical practice."
4dc75703282da1be423c9e374a2f6da367e94699,"Neurofeedback is a self-regulation technique that can be applied to learn to voluntarily control cerebral activity in specific brain regions. In this work, a Transcranial Doppler-based configurable neurofeedback system is proposed and described. The hardware configuration is based on the Red Pitaya board, which gives great flexibility and processing power to the system. The parameter to be trained can be selected between several temporal, spectral, or complexity features from the cerebral blood flow velocity signal in different vessels. As previous studies have found alterations in these parameters in chronic pain patients, the system could be applied to help them to voluntarily control these parameters. Two protocols based on different temporal lengths of the training periods have been proposed and tested with six healthy subjects that were randomly assigned to one of the protocols at the beginning of the procedure. For the purposes of the testing, the trained parameter was the mean cerebral blood flow velocity in the aggregated data from the two anterior cerebral arteries. Results show that, using the proposed neurofeedback system, the two groups of healthy volunteers can learn to self-regulate a parameter from their brain activity in a reduced number of training sessions."
c3cf6bea215e869efe6d605a1d9312f297d56cda,"Nowadays, design and manufacturing process of dental prostheses is very handmade, time consuming and has a raised economic cost. Besides that, there is no objective methodology for the fulfillment of its functional design specifications. This paper presents an overview of MIRACLE project whose objective is the development and validation of an intelligent system for the design, simulation and flexible manufacture of implant-supported dental prostheses. The developed system in MIRACLE is a CAD/CAM system which allows to test the functional characteristics of dental prostheses considering mandible-maxilla interaction (called occlusion) using virtual models, contrary to most commercial solutions where this test is performed using expensive anatomical replicas tested with mechanical articulators and evaluated with patients. Another objective of MIRACLE is to develop a parametric finite elements model (FEM) of the whole prosthesis in order to analyze the failure risk of dental implants and prostheses before its surgical implantation enabling a re-design process. This paper is focused on the CAD/CAM subsystem developed in order to automatize the process of manufacturing surgical guides using several 3D models of the patient dental anatomy. A summarized version of the image processing step will be also presented. The CAD/CAM subsystem has been clinically validated achieving mean errors less than 5 degrees in the placement of the prosthetic crowns."
d8823fd2f3b0762e9ae058c7d11b5de7e45e611a,"The aim of this study is to characterize in resting-state conditions the cerebral blood flow velocity (CBFV) signals of fibromyalgia patients. The anterior and middle cerebral arteries of both hemispheres from 15 women with fibromyalgia and 15 healthy women were monitored using Transcranial Doppler (TCD) during a 5-minute eyes-closed resting period. Several signal processing methods based on time, information theory, frequency and time-frequency analyses were used in order to extract different features to characterize the CBFV signals in the different vessels. Main results indicated that, in comparison with control subjects, fibromyalgia patients showed a higher complexity of the envelope CBFV and a different distribution of the power spectral density. In addition, it has been observed that complexity and spectral features show correlations with clinical pain parameters and emotional factors. The characterization features were used in a lineal model to discriminate between fibromyalgia patients and healthy controls, providing a high accuracy. These findings indicate that CBFV signals, specifically their complexity and spectral characteristics, contain information that may be relevant for the assessment of fibromyalgia patients in resting-state conditions."
5b9552b4ba30477a7932c653a33bdc1a1b8d2ee6,
9a61f754343cc6c9eb20029d4121263c204efe83,
72965eb6c6ab356f30adf305ef4ee5deb430796c,
7bde9204e24e04f7c907820a830609b13a2c2334,"For generations, man has kept studying and thinking about the communication capacity that could be established between machines and thought. This is an old dream, a fantasy that was not supposed to happen some day: pushing the limits of the human condition. Nowadays, advances in science make this dream almost come true. We always wanted to have any kind of super power, but now it gets really believable or possible. We are the first species to take control of our own evolution, not in the future but right now, thanks to artificial intelligence. Artificial intelligence is a new technology that allows an old fantasy to come true: improving the daily human and build machines that haunt the imagination of science fiction. Technologies of cognitive neuroscience have recently increased considerably and helped to advance the understanding of some mechanisms of the brain and human thought. This advance was made possible thanks to developments in the technology field and the computer science progress."
8a734003daf0ec8cc91fcbaa601d71743c85a070,"People all use more or less adapted strategies to confront adverse emotional situations in their lives without being psychologically affected. The emotional regulation (ER) strategies that we use determine the way in which we feel, express, and behave. Moreover, ER strategies are particularly important in adolescents, a population for which ER strategy deficits can be linked to the appearance of numerous mental health disorders, such as depression or anxiety, or disruptive behaviors. Thus, the early detection of dysfunctional ER strategies and training in adaptive ER strategies can help prevent future occurrences of possible behavioral and psychosocial disorders. In this article, the authors present the GameTeen System (GT-System), a novel instrument based on virtual reality and serious games for the assessment and training of ER strategies in adolescents. The results of their preliminary evaluation suggest that this system can effectively train and evaluate emotional regulation strategies in adolescents."
b532ae389da53e58af4e80d1a2c2bcf8bab39716,
2f74a997f173b21c1bda34033fc9ac0d5e738f85,"Background To date, still images or videos of real animals have been used in functional magnetic resonance imaging protocols to evaluate the brain activations associated with small animals’ phobia. Objective The objective of our study was to evaluate the brain activations associated with small animals’ phobia through the use of virtual environments. This context will have the added benefit of allowing the subject to move and interact with the environment, giving the subject the illusion of being there. Methods We have analyzed the brain activation in a group of phobic people while they navigated in a virtual environment that included the small animals that were the object of their phobia. Results We have found brain activation mainly in the left occipital inferior lobe (P<.05 corrected, cluster size=36), related to the enhanced visual attention to the phobic stimuli; and in the superior frontal gyrus (P<.005 uncorrected, cluster size=13), which is an area that has been previously related to the feeling of self-awareness. Conclusions In our opinion, these results demonstrate that virtual stimulus can enhance brain activations consistent with previous studies with still images, but in an environment closer to the real situation the subject would face in their daily lives."
30d1bd6f975905b1e04ddeeb3623367634f4325e,
d2a4900c11a8028cd8148eb9ed984840581b7cef,"This study was funded by the Ministerio de Educacion y Ciencia Spain, Project Game Teen (TIN2010-20187) and partially 
by projects Consolider-C (SEJ2006-14301/PSIC), ‘CIBER of Physiopathology of Obesity and Nutrition, an initiative 
of ISCIII’, the Excellence Research Program PROMETEO (Generalitat Valenciana. Conselleria de Educacion, 2008-157) 
and the Consolider INGENIO program (CSD2007-00012). The work of Miriam Clemente was supported by the Generalitat 
Valenciana under a VALi+d Grant."
d2ae65c3a371e95a68109e13a1f2834c1a6496e5,
08b6a4aa97cf0df303f4c35c3f92e25ca49c8147,"Up to now, still images or videos of real animals have been used in functional Magnetic Resonance Imaging (fMRI) protocols to evaluate brain activations associated to small animals' phobia. Our aim in the present work is to evaluate the use of virtual environments in this context, which will have the added benefit of allowing the subject to move and interact with the environment, giving the subject the illusion of being there. We have analyzed brain activation in a group of phobic people while they navigated in a virtual environment that included the small animals that are the object of their phobia. We have found activation mainly in the left occipital inferior lobe, related with enhanced visual attention to the phobic stimuli; and in the superior frontal gyrus, related with the feeling of self-awareness. In our opinion, these results demonstrate that virtual stimulus can enhance brain activations coherent with previous studies with still images, but in an environment closer to the real situation they would face in their daily lives."
1ad5a0e5984cf441d7319137946e32304b952a01,"Virtual Environments (VEs) have been used as mood induction procedures. In this context, it is necessary to have instruments to analyze the emotional state during VE exposure. Objective techniques such as EEG should be evaluated for this purpose. The aim in this work was to study the changes in the brain activity with a portable EEG device during a negative mood induction based on a VE. A virtual park was used to induce a negative mood (sadness) in ten participants. Changes in the brain activity of subjects were compared between two moments (before and after emotional induction). Obtained results were in accordance with previous scientific literature regarding frontal EEG asymmetry, which supports the possibility of using the portable EEG as a reliable instrument to measure emotions in VE."
894cad19d0ab91c554d42a03b6f7cc6a90ba83b9,"New electroencephalography (EEG) devices, more portable and cheaper, are appearing on the market. Studying the reliability of these EEG devices for emotional studies would be interesting, as these devices could be more economical and compatible with Virtual Reality (VR) settings. Therefore, the aim in this work was to validate a low-cost EEG device (Emotiv Epoc) to monitor brain activity during a positive emotional induction procedure. Emotional pictures (IAPS) were used to induce a positive mood in sixteen participants. Changes in the brain activity of subjects were compared between positive induction and neutral conditions. Obtained results were in accordance with previous scientific literature regarding frontal EEG asymmetry, which supports the possibility of using this low-cost EEG device in future mood induction studies combined with VR."
bd02e634d016fee56fc5be40c5d2932f2313d4d1,"In the Virtual Reality field, presence refers to the sense of ""being there"" in the virtual world. Our aim in this work is to evaluate the usefulness of the Emotiv EPOC EEG device to measure the brain activations due to the sense of presence during the navigation in a Virtual Environment (VE), using for the analysis the sLORETA tool. We compare between two experimental conditions: free and automatic navigation through a VE. In this preliminary step, we monitored 9 healthy subjects, obtaining significant differences between the free and automatic navigation conditions in the activity of the right insula for the Theta and Alpha bands. The insula activation is related to stimulus attention and selfawareness processes, directly related with the sense of presence."
1554937b09d272753ad15502840049125698e7f4,"In the area of brain activity monitoring, the technique of transcranial Doppler (TCD) is widely used in neuroscience laboratories because it is non-invasive and can be used in a wide variety of environments, which makes it appropriate to conduct practical tests. The objective of this work is to propose a new methodology to analyze changes in blood flow velocity (BFV) in main cerebral vessels measured by TCD during some visual tasks initiated via a multimedia screen. To conduct this experiment, 23 volunteers of different ages (19 to 66 years) and different sexes were selected. The tests involve measuring -using a Doppler device- the brain activity of each person during the execution of the predefined visual tasks. The BFV data are analyzed with a methodology based on the analysis of statistical differences between repose and activation periods."
970a5047a7b0a119f17b96297269301134524a27,"The aim of this paper is to describe GameTeen, a novel instrument for the assessment and training of Emotional Regulation (ER) strategies in adolescent population. These new tools are based on the use of 3D serious games that can be played under different settings. The evolution of ER strategies will be monitored in two ways depending on the setting where the tool is presented. Firstly, in the laboratory, physiological signals and facial expressions of participants will be recorded. Secondly, in real life settings, ecological momentary assessment tools will be used to obtain answers from the subjects using their mobile phone. The goal is to obtain more attractive and reliable tools to evaluate and train ER strategies."
b06fdc8f5d4f2ee4481d3bca744e1a5d056ecdea,"As Virtual Reality (VR) is starting to be used to train emotional regulation strategies, it would be interesting to propose objective techniques to monitor the emotional reactions of participants during the virtual experience. In this work, the main goal is to analyze if portable EEG systems are adequate to monitor brain activity changes caused by the emotional regulation strategies applied by the participants. The EEG signals captured from subjects that navigate through a virtual environment designed to induce a negative mood will be compared between three experimental groups that will receive different instructions about the emotional regulation strategies to apply. The study will allow us to validate the possibilities of portable EEG devices to monitor emotional regulation strategies during VR exposure."
b9516fea27c0c4afb7bc4d9777d4da8ff5b7bbf5,"The aim of this paper is to present digital representations of humans (i.e., avatars) that look like the self, applied to the Mental Health (MH) field. Virtual Representations of the Self (VRS) are in our opinion a tool with a great potential for engaging teenagers in emotional regulation strategies learning and an excellent example of new technology application to the basic concept in psychology field such as Bandura's modeling [1]. VRSs have already demonstrated their potential on human behavior modification (e.g. modification of physical activity; eating habits) in general population [2]. Thus, the same technology can bring in our opinion a lot to the Mental Health field, especially in emotional regulation learning. This paper presents a theoretical background and describes the methodology that we plan to apply in order to validate the efficacy of VRSs in clinical settings. Also, the implications of such technology and future research lines are discussed."
f971c0c30bc040064aff04a61f06865aae3f2615,"The aims of the present study are to examine the reliability and validity of the Heart Rate signal registered using two self-made wireless ECG systems, R-Tips and TipsShirt, and to compare them with another commercial ECG device typically used in psychophysiology studies. An ECG simulator was used to artificially generate signals corresponding to different cardiac frequencies. Results of the reliability study showed that the signal acquisition, signal processing and signal transmission were reliable and valid for R-Tips and TipsShirt. Consequently, these wireless ECG prototypes could be used for studies where the freedom of movements of the participants is fundamental without any loss of quality in the registered signals."
0805292f870894886e855f51c694cca392188eb2,
0a52fc24f1ccb8ac748f8d1c4f1435547d461838,
38be25abac6c9ebf223c950d32159e835e50ca0f,
8476e92d749bd784b79e7b4d771db368c96a86b4,"This work aims to identify the arousal and presence level during an emotional engineering study. During the experimental sessions, a high-immersion Virtual Reality (VR) system, a CAVE-like configuration, will be used. Thirty-six volunteers will navigate through virtual houses that can be customized and that have been designed for emotional induction. Emotional induction will be obtained by stimulating the senses of sight, hearing and smell. For this purpose, the ambient lighting, music and smell will be controlled by the researcher, who will create a comfortable environment for the subject. Several physiological variables - Electrocardiogram (ECG), Respiratory signal and Galvanic Skin Response (GSR) - will be recorded during the sessions. The obtained results will help furniture companies identify the senses that have more influence on emotions and will be the basis for new studies about user needs in the sector of furniture and interior decoration."
85b411db7a4617ed2d6e172adcccd523865e83f6,
cb5d73195f9cc8f9fcc8a1df904830970cfab1e7,
ea11191467be799041c8190d971ead3c4cba90d7,"One of the techniques used to monitor variations in presence during a virtual reality experience is the analysis of breaks in presence (BIPs). Previous studies have monitored peripheral physiological responses during BIPs in order to find a characteristic physiological response. In this work, blood flow velocity (BFV) in middle cerebral arteries (MCAs) has been monitored using transcranial Doppler ultrasound during the exposure to a virtual environment. Two BIPs of different intensity were forced during the virtual reality experience. Variations in BFV during each BIP and during the recovery periods that followed them have been analyzed. A decreasing trend was observed in BFV signal during the most intense BIP in most subjects. However, during the less intense BIP an oscillating behavior was observed. Significant differences have been found between the maximum percentage variations observed in each BIP. During the recovery periods, an increasing trend was observed. The mean response times (time elapsed since the beginning of the period until the maximum percentage variation in the period occured) ranged between 10.116 s and 12.774 s during the BIPs, and between 11.025 s and 13.345 during the recovery periods, depending on the vessel and on the kind of BIP."
312ef075c185a006d7855d9968c9021e6aa22e51,"Transcranial Doppler is a tool to measure blood flow velocity (BFV) in the main arteries of the brain that has been used in previous studies to analyze brain activity during virtual reality (VR) experiences. Increments in BFV were found during the exposure to virtual environments in comparison with baseline periods. However, due to the complexity of VR experiences, there are several factors that can be having an influence in these variations, so it is necessary to separately analyze those different aspects. In this work, we summarize our results related to visual perception. A method based on spectral analysis was used to analyze the magnitude and temporal evolution of the maximum BFV signal. Results showed that, in the presence of visual stimuli, BFV quickly rises to a maximum that is achieved after a few seconds. The contribution of the visual stimuli factor to the observed BFV variations during a VR experience can be estimated from the results of the developed work."
8343163c5e42a163a9c9977302ba6e04dfd68e01,"Virtual reality is one of the most challenging applications of computer graphics and is currently being used in many fields. Participants of immersive virtual environments have unique experiences which were never before possible. Although they know from a cognitive point of view that the virtual environment is not a real place, they act and think as if the virtual environment were real. Virtual environments take advantage of the imaginative ability of people to psychologically transport them to other places. In this chapter, we are going to analyze the two-way relationship between virtual reality and neuroscience. First, it will be described how virtual reality can be a useful tool in neuroscience research, as long as it can be used to create controlled environments where participants can perform tasks while their responses are monitored in order to achieve a more detailed understanding of the associated brain processes. Previous work and research in this field will be detailed and discussed. Secondly, the applications of neuroscience in the virtual reality field will be analyzed. There are aspects of the virtual reality experience such as presence that can be an object of study for neuroscientists (Sanchez-Vives & Slater, 2005). Results from neuroscience studies can help virtual reality researchers to improve their knowledge about the processes that occur in the brain during the exposure to virtual environments and generate more compelling and effective versions of the virtual environments that they develop. At the end of the chapter, some general conclusions and implications that the research in virtual reality and neuroscience may have for future work will be described. The different kinds of studies that will be described in this chapter are listed in Table 1."
9797846da18985124dd531bb6ef68fbef4595513,
d4e8d3bc7919743062519be5d4078941b0304a6f,"Many studies have been developed using brain imaging methods to investigate psychological disorders. On the other hand, there are many studies that make use of virtual reality (VR) to simulate a real condition during psychological treatments. In this research, we plan to analyze brain activity during the exposure to a virtual environment related to phobias. Our first goal is to study the possibility of activating brain areas related to phobias, specifically phobias to small animals (spiders and cockroaches), using virtual reality as stimulus, while the patient is inside a functional magnetic resonance imaging (fMRI) machine. The second goal of the research is to analyze if there are differences in the activated areas after patients have followed a psychological treatment for this specific phobia. That is why two different sessions with fMRI will be performed, before and after an intensive treatment for the phobia. In the fMRI room, participants will wear special glasses to visualize the VR environments in which they have to navigate (using also a joystick adapted to fMRI). They will have to perform some tasks while being exposed to the phobic stimuli. The VR environment used in the fMRI sessions has three different conditions: first, a clean room without spiders or cockroaches; second, the same room, but dirty and disordered (giving the sensation of having small animals, although actually there are none); third, the same dirty room but having spiders and cockroaches. It is our hypothesis that the patients will get anxious in the situation in which it is possible that the animal appears and the patterns of brain activation will be different in this condition."
27a1dc59926c6aad0cec2b5ff847945bfd6be4d1,
5eddd5372d7e2a3add3667216793f9f384a2c070,"In this work, we propose the use of Transcranial Doppler Monitoring (TCD) as a tool to measure brain activity during the exposure to Virtual Environments (VE) used in clinical therapy sessions. The technique is non-invasive, and can be easily integrated with Virtual Reality (VR) settings. Moreover, it provides a high temporal resolution, which grants the possibility to analyze changes in brain activity during the evolution of a clinical session and to correlate them with specific events that may occur in the VE. We have performed two studies combining TCD with VR. Results of these studies show that it is feasible to use this technique in combination with VR settings designed for virtual therapy. It was observed that immersion and navigation modifications in the VE generated changes in brain activity that can be detected using TCD."
7af957b799bd196609157567641f2a504614a3a8,
7b8503997931d29c86b3fa77bdc945c91f99924d,"In this work, we propose the use of Transcranial Doppler Monitoring (TCD) as a tool to measure brain activity during the exposure to Virtual Environments (VE) used in clinical therapy sessions. The technique is non-invasive, and can be easily integrated with Virtual Reality (VR) settings. Moreover, it provides a high temporal resolution, which grants the possibility to analyze changes in brain activity during the evolution of a clinical session and to correlate them with specific events that may occur in the VE. We have performed two studies combining TCD with VR. Results of these studies show that it is feasible to use this technique in combination with VR settings designed for virtual therapy. It was observed that immersion and navigation modifications in the VE generated changes in brain activity that can be detected using TCD."
928bff76e72f99c38f6caed3d89d564815506018,"Virtual reality enables people to behave and feel as if they were present in a virtual environment and therefore is a useful tool in many fields. In order to study the usefulness of virtual environments, the concept of presence is examined. Up to now, the most common method to measure presence has been to use subjective measures based on validated questionnaires about user experience. However, more objective measurements, such as physiological measurements, are now being considered. In this study, transcranial Doppler (TCD) sonography is presented as a brain activity measurement technique that can be used to study presence in virtual environments. Thirty-two subjects navigated in a virtual environment in different immersive conditions while TCD was monitored. The results show that there are changes in blood flow velocity in the subjects during moments associated with different levels of presence."
7a50ca4fdfa4240e2ecba5c59b9d340d4dc2abfb,
cb0847ac9f99de49ada1bed59d2d0b8d76adab29,
516d2018a7a2669b6bd2cd8e292d12be6fcf3921,"Mariano Alcañiz, Rosa Baños, Cristina Botella, Paolo Cottone, Jonathan Freeman, Andrea Gaggioli, Edward Keogh, Fabrizia Mantovani, Giuseppe Mantovani, Javier Montesa, Concepción Perpiñá, Beatriz Rey, Giuseppe Riva, John Waterworth 1 Universidad Politécnica de Valencia, Valencia (SPAIN) Universitat Jaume I, Castellón (SPAIN) 3 Istituto Auxologico Italiano, Milan (ITALY) 4 Goldsmith College, London (UNITED KINGDOM) 5 Interactive Institute, Umea (SWEDEN) 6 Department of General Psychology, University of Padova, Padova (ITALY)"
7590a138beffc5180324a1908abbdf9962ec5c69,"A preliminary study on the use of an adaptive display for treating emotional disorders is presented. This adaptive display (named EMMA) varies the contents that are presented depending on the emotions of the user at each moment. The application has been designed to help in the treatment of Post-Traumatic Stress Disorder (PTSD) and Adjustment Disorder (AD). The specific objective of the present work is to test the effectiveness of this adaptive display, specifically the acceptance of the treatment by patients. EMMA's tools are compared with the standard of care for PTSD and AD. Results showed differences only for the variable aversiveness. Participants in the EMMA condition evaluated the treatment less unpleasant at post treatment, compared to participants in the traditional condition."
9eade63f845b06cbdd0b0b5b85669a84a84553d5,
077e0e695e5cf1262d5301fa0990b5f965f53d03,"This paper presents the technical characteristics of the first prototype that uses Augmented Reality to treat acrophobia. The immersive photographs are the virtual elements that represent the locations that the user fears. A total of 36 different immersive photographs have been included in the system (12 different locations with 3 parallel photographs in each location). At first, the system shows the central photograph. If the user rotates his/her head and stays in the same position, he/she can spin over the immersive photograph, changing his/her point of view inside the photograph. If he/she moves to the left/right (i.e. the physical position) the photograph will change and the related left/right photo will appear."
1007a84a1aaea18b0574560a29a080ff03caef6a,"Augmented reality (AR) refers to the introduction of virtual elements in the real world. That is, the person is seeing an image composed of a visualization of the real world, and a series of virtual elements that, at that same moment, are super-imposed on the real world. The most important aspect of AR is that the virtual elements supply to the person relevant and useful information that is not contained in the real world. AR has notable potential, and has already been used in diverse fields, such as medicine, the army, coaching, engineering, design, and robotics. Until now, AR has never been used in the scope of psychological treatment. Nevertheless, AR presents various advantages. Just like in the classical systems of virtual reality, it is possible to have total control over the virtual elements that are super-imposed on the real world, and how one interacts with those elements. AR could involve additional advantages; on one side it could be less expensive since it also uses the real world (this does not need to be modeled), and it could facilitate the feeling of presence (the sensation of being there), and reality judgment (the fact of judging the experience as real) of the person since the environment he or she is in, and what he or she is seeing is, in fact the ""reality."" In this paper, we present the data of the first case study in which AR has been used for the treatment of a specific phobia, cockroaches phobia. It addresses a system of AR that permits exposure to virtual cockroaches super-imposed on the real world. In order to carry out the exposure, the guidelines of Ost with respect to ""one-session treatment"" were followed. The results are promising. The participant demonstrated notable fear and avoidance in the behavioral avoidance test before the treatment, and not only was an important decrease in the scores of fear and avoidance observed after the treatment, but also the participant was capable of approaching, interacting, and killing live cockroaches immediately following the treatment. The results are maintained in a follow-up conducted 1 month after the termination of the treatment."
14b9d476693e7190a71ee53876bfe3f9af07e640,"As Biocca pointed out, the “two poles model“ of presence has only considered the virtual and pyshical spaces, but not the imaginary spaces. This work is aimed at comparing the sense of presence between virtual and imaginary environments. 100 participants were randomly assigned to one of the two conditions (imagined versus virtual spaces) and the subjective sense of presence was measured in three moments (begining, middle, and end). Results indicate that the participants in “imagery” spaces indicated a decrease of their sense of presence, whereas the opposite occurs in participants in “virtual” spaces. Imagination seems not to be a long-lasting procedure to elicit presence. However, VR helps users to stay there as time goes by. That is, it provides a “physical” context in which the self can be placed."
440e1e972c9f617002844f865ec661becddf8f41,"A preliminary study on the use of an adaptive display for treating emotional disorders is presented. The purpose of this study is twofold: to obtain a virtual environment that adapts to the emotional states of the user at each moment and to analyze the possibility of using it for the treatment of emotional disorders. Until now, different types of adaptive displays have been developed and studied, some of which try to react to affect states of the user (Reynolds et al., 2004). The novelty of our system lies in the use of the adaptive display for the treatment of emotional disorders."
8020e2ce77988cd15e429d2b048fb758439742c4,"In this paper we present an Augmented Reality book for remembering past events and to plan future ones. We have developed this system using Brainstorm eStudio. We have incorporated Augmented Reality options into Brainstorm eStudio using a plugin of ARToolKit. The user can create his own book selecting images, objects and videos from a database. The selection of elements and their inclusion into the book is achieved using a tangible interface."
d20d63c2b10255af86a98b7626537fa14bc700ed,
05b3191314d8be1e34e4cc8b3bb37905805055d3,"New technologies lead us to a series of new applications that we could not imagine just a few years before. Many services have appeared for Internet, the global computer network: FTP, e-mail, World Wide Web. Psychological treatments are one of the multiple applications that can be developed using these tools. Dynamic web pages that include information prepared by the therapist for different patients and that receive information from them can be generated. Other tools such as e-mail or chats can be used to provide a direct communication. Databases can be integrated in web applications for storing data about different patients. Several formats can be used for storing the information, and some of them such as XML provide a promising method of psychological data standardization. Using different development tools, virtual environments can also be generated and integrated in web pages, so new psychological treatments such as virtual environment exposure are also possible from web applications. This entire basis provides the structure that allows that new applications can be imagined and developed. In a few years, new trends will appear, probably one of them will be the use of wireless devices to provide psychological treatment and help at any place and any time."
3e19de492fbbd61ec3c947d42cbf41863ed1f1da,"User interface (UI) design is a critical component of any virtual environment (VE) application, and especially for VE applied to medicine. User interfaces for VE are becoming more diverse. Mice, keyboards, windows, menus, and icons--the standard parts of traditional WIMP interfaces--are still prevalent, but nontraditional devices and interface components are proliferating rapidly. These include spatial input devices such as trackers, 3-D pointing devices, and whole-hand devices allowing gestural input. Three-dimensional, multisensory output technologies--such as stereoscopic projection displays, head-mounted displays (HMDs), spatial audio systems, and haptic devices--are also becoming more common. In this chapter we present a brief overview of 3-D interaction and user interfaces technologies for VE."
751e5e1a5366457ed150586f0367c13a37a9c736,"Ambient Intelligence builds on three recent key technologies: Ubiquitous Computing, Ubiquitous Communication and Intelligent User Interfaces – some of these concepts are barely a decade old and this reflects on the focus of current implementations of AmI. Ubiquitous Computing means integration of microprocessors into everyday objects like furniture, clothing, white goods, toys, even paint. Ubiquitous Communication enables these objects to communicate with each other and the user by means of ad-hoc and wireless networking. An Intelligent User Interface enables the inhabitants of the AmI environment to control and interact with the environment in a natural (voice, gestures) and personalised way (preferences, context)."
86ebb65b13a8b188709a0ce7933f76a6d949db90,
b536106480ebfbae4ff6f51f91942380f78500fc,"A participantssense of ""being there"" in a mediated experience is determined by a variety of characteristics or components. The EMMA Project (IST-2001-39192) is interested in analyzing the relationships between presence and emotions, especially for some Virtual Reality applications, such as mental health (both for promotion and treatment goals). This research will help to understand better the development of some psychopathological phenomena and the development of new correcting experiences and learning to cope with those psychopathological experiences. Mood devices are special hardware and software configurations able to induce different forms of mood enhancement. In this paper, we describe one of the ""mood devices"" that have been developed inside the project: the EMMA room."
f55fb706b5d0508380905cd4004f66ce3b5a1933,"Most definitions on presence have been based on cognitive or environmental aspects. However, we think that presence, like all human experiences, is influenced by emotions. EMMA project (IST-2001-39192) is aimed to study the nature of this relationship between emotions and presence. One of the main hypotheses proposed by EMMA is that emotions may enhance presence. In this line, the main objective of the present paper is to study the differences in presence between “emotional” environments and “neutral” environments. In order to achieve this objective, we have designed a Mood Induction Procedure using VR (VR-MIP, to induce different moods (sadness, joy, anxiety and relax) in experimental subjects. Our results point out that VR-MIPs are able to induce different moods in the users. Regarding the role of mood on the sense of presence, our results show differences between emotional and neutral environments in some presence measurements."
fb4dcbd818e5839f025a6bc247b3bc5632be502f,"The present study is designed to test the role of immersion and media content in the sense of presence. Specifically, we are interested in the affective valence of the virtual environments. This paper describes an experiment that compares three immersive systems (a PC monitor, a rear projected video wall, and a head-mounted display) and two virtual environments, one involving emotional content and the other not. The purpose of the experiment was to test the interactive role of these two media characteristics (form and content). Scores on two self-report presence measurements were compared among six groups of 10 people each. The results suggest that both immersion and affective content have an impact on presence. However, immersion was more relevant for non-emotional environments than for emotional ones."
fc682c228fe1b3a72328ed16442b42a6fa47af0c,"One of the most important measures of Presence is the questionnaire. Different instruments have been introduced; however, they are based on different and partly implicit theoretical assumptions. The MEC model of Spatial Presence has been proposed as a theoretical framework for the unification and simplification of the existing Presence research. Based on a short explication of this model, the definitions and operationalization of selected constructs (i.e. involvement and presence) within the MEC Spatial Presence Questionnaire (MEC SPQ) will be explained and distinguished from their former use in presence research. Finally, we will present some findings from pretest studies that were conducted with 290 students from three different countries (U.S., Portugal, Finland) to demonstrate the differences between presence and involvement. Testing four different media (linear text, hypertext, film, virtual environment) the data not only supported the constructs’ validity but also allowed to create highly consistent and homogeneous scale versions for these constructs."
0fd2a52c1c6a4d7c5f3b4d5283e235c0c9af9ea5,"In this work that is being validated within the VEPSY project, we present a system that allows the patient to continue a psychological virtual reality treatment from his or her home PC as complementary therapy. In the consulting room, we have been using virtual therapy for panic disorder and agoraphobia treatment to expose the patient to several situations. For the complementary therapy, a structured treatment via the Internet has been prepared, which consists of several parts: an assessment protocol; a structured treatment protocol organized in several blocks (such as psychoeducation and exposure); and an outcome protocol. The same situations as in the consulting room have been selected for the exposure, but each of them has been divided into several virtual environments with specific characteristics that limit its difficulty level. The stimuli that are used at each level are controlled automatically by the system. The information of the patient is stored in a database, which is placed in a remote server using XML format and used to control which stages of the treatment he or she can access. The psychologist can limit the evolution of the patient. The virtual environments are installed in the patient's PC, and they are implemented with a mechanism that ensures that they can only be run when the patient connects to the web. The user should not have any special virtual reality hardware at home, so head rotations have been simulated with the navigation system."
225112fc26bd21e11a61085bed6e531b7a803e70,"So far, scientific literature has paid attention to the cognitive and environmental determinants of presence, trying to offer a definition and assessment measures that could seize such an elusive concept. However, the emotional determinants of presence have received less attention. Emotional responses could play a key role in generating and enhancing presence, specially for some Virtual Reality (VR) applications, such as mental health field (both for promotion and treatment goals). The main goal of EMMA project -an European Community funded research project (IST-2001-39192)- is to study the relationships between presence and emotions. In particular, after analyzing the possible emotional impact of high compelling synthetic experiences characterized by a high level of presence, the EMMA project wants to develop ""mood devices"" able to induce different forms of mood enhancement."
d69ede5015d7b088c973e74112e7a72f3d062667,"One of the main questions in the humanities is how cultures and artistic expressions change over time. While a number of researchers have used quantitative computational methods to study historical changes in literature, music, and cinema, our paper offers the first quantitative analysis of historical changes in visual art created by users of a social online network. We propose a number of computational methods for the analysis of temporal development of art images. We then apply these methods to a sample of 270,000 artworks created between 2001 and 2010 by users of the largest social network for art—DeviantArt (www.deviantart.com). We investigate changes in subjects, techniques, sizes, proportions and also selected visual characteristics of images. Because these artworks are classified by their creators into two general categories—Traditional Art and Digital Art—we are also able to investigate if the use of digital tools has had a significant effect on the content and form of artworks. Our analysis reveals a number of gradual and systematic changes over a ten-year period in artworks belonging to both categories."
9bbefa7a3e0c1d39fb27770c5da4d37d7d946dd2,"How can we use computational analysis and visualization of content and interactions on social media network to write histories? Traditionally, historical timelines of social and political upheavals give us only distant views of the events, and singular interpretation of a person constructing the timeline. However, using social media as our source, we can potentially present many thousands of individual views of the events. We can also include representation of the everyday life next to the accounts of the exceptional events. This paper explores these ideas using a particular case study - images shared by people in Kiev on Instagram during 2014 Ukranian Revolution. Using Instagram public API we collected 13208 geo-coded images shared by 6165 Instagram users in the central part of Kiev during February 17-22, 2014. We used open source and our own custom software tools to analyze the images along with upload dates and times, geo locations, and tags, and visualize them in different ways."
026b3531e63cd67a3b5e8ccacfece26b26edb14c,"Humanities and social sciences require new research approaches to deal with and to benefit from the explosion of digital media. Thanks to large scale data storage and processing, close reading of individual items can be supplemented with analysis of broader trends from massive amounts of data. Cultural Analytics (CA) is a recently developed methodology for the exploration of content and visual form in large image and video collections. It has already been applied to a variety of media types, including TV programs, feature films, newspapers, and video games. However, until now these applications did not take into account the social context of media. In many cases, media artifacts under scrutiny are generated or used by people in a social setting, either in forms of (digital) communities, or in terms of social relations, which can be also analyzed as a valuable source of information. Social network analysis (SNA) is a set of methods for the analysis of human networks, including massive online social networks. In this article, we argue that CA and SNA can be combined synergistically, using data and images from deviantArt, the leading online network of user-created art, as a case study."
1e1282e4251b4e84a24b09bf14670e95f6765b38,"In this paper we visually explore the data structure of two different visual platforms: the database behind the social environment of a social networking site, and the intricate infrastructure of a research institute for preservation of deposited datasets. We argue that visual analytics of metadata of collections can be used in multiple ways: for the backend users, to inform the archive about structure and growth of its collection; to foster collection strategies; and to check metadata consistency, for the end-users, to give an overview to the collections, and thus to generate more awareness of the collection and its metadata, to give the enduser extra information to contextualize the entirety of the archive. We conclude with a discussion on how text based search combined with different type of visually enhanced browsing improves data access, navigation, and reuse in these two radically different contexts."
4fa84b123bee4ef9abd46ce3b0b9922dd4446f21,"Dit onderzoek heeft als doel om ter beantwoording van vragen uit de Tweede Kamer informatie te verzamelen ter verklaring van de daling in de (cijfers over de) aangiftebereidheid van burgers, en om op basis van de uitkomsten waar mogelijk aanknopingspunten te benoemen voor beleid dat zich richt op het vergroten van de aangiftebereidheid. De hoofdvraag luidt: hoe kan de (vermeende) daling in de aangiftebereidheid van burgers in de periode 2005 tot 2015 worden verklaard?"
fa23bd20575e813deff563e0fc76fe0592911321,
ba3af23941a7fe754ba888f23eb59abe7003abd8,
2471e9a676f8e1571d391c52d4cdc91f8f4e41b2,
4fa2caf17aa7604ae56f80025694c5a930e86f74,
62bacb3236654906c97ad50e6380233cbfa7449d,
64bf62a43f3aaff19c0dc6f4dc4770e0982f548b,
7bd18c400fb4e14d61241bb4eb447a8ef4b4b3ba,
abf5d69241cd6cb27d6b83fc8a12be8c74c69350,"In 1988, the World Health Assembly launched the Global Polio Eradication Initiative (GPEI). In the year of the announcement, 350 000 cases of paralysis from polio were reported across 125 countries. Today, over 2.5 billion children have since been immunised and, at the time of writing, there were 5 cases of polio in two countries, with only 37 cases reported in all of 2016. Much of the success of global polio eradication efforts has rested on meticulous surveillance in the hardest to reach areas of the globe, and the principle of immunisation of every last child. As a result, between 1988 and 2017, GPEI’s polio immunisation and acute flaccid paralysis (ALP) surveillance programme among children under 15 years of age—the gold standard for detecting poliomyelitis—are estimated to have avoided paralysis for 16 million children. Such success has come at a significant financial cost, with over US$9 billion invested in polio eradication efforts since 1988. However, the economic returns to date are estimated to be $27 billion, with a further US$20 billion anticipated by 2035, and $17 billion from vitamin A supplementation delivered in parallel. Witnessing interruption in transmission of polio virus will be a critical turning point for infectious disease control and surveillance. Yet, how the global community reacts and transitions in the coming years presents a unique opportunity to shape the future of surveillance—with ramifications reaching far beyond the immunisation, detection and management of polio alone. The increasing frequency of outbreaks of novel and existing pathogens adds a further imperative. Middle East respiratory syndrome (MERS) coronavirus, Ebola virus disease, Zika virus, yellow fever and the threat of avian influenza have all presented surveillance and infection control challenges over the past 5 years. For the next generation of financing, governance and implementation of infectious disease surveillance, polio must learn from the mistakes and missed opportunities of the first and only human disease to be eradicated—smallpox—in transitioning assets, and look to the future. Much of the focus of eradication initiatives are on interrupting transmission of the infectious pathogen. And rightfully so. However, considering the significant amount of resources, both human and financial, which are invested in surveillance and eradication initiatives, how the transition of assets of value are planned, prioritised and protected is key. In a global health landscape of limited resources, transition represents as important a goal as eradication. Part of the reason transition will matter more for surveillance going forward is the evolution of the political and economic landscape for health and development. After the interruption of polio transmission and the certification of polio eradication, the GPEI will cease to exist. Sunsetting of the organisation, after first ensuring safe timely handover of the most valuable assets, is central to the success of development initiatives in general, and polio eradication efforts in particular. A successful sunsetting of the partnership and transition of assets will demonstrate a clear message of success for investing in global health, and the case that high impact development initiatives work, are catalytic and are time bound. There is, however, a threat to global investment in surveillance if polio eradication effort is not coupled with an excellent transition plan. This challenge is particularly evident when attempting to raise donor funding from a limited pool of contributors, and to raise domestic funding following successful polio control efforts when so few cases remain visible in comparison to significant other societal challenges. Furthermore, polio eradication alone represents 20% of the current WHO budget. Although, in practice, polio systems have major benefits across health issues and to health systems more broadly, the case to transition valuable polio assets in support of global health governance is critical. Going forward, the need for polio surveillance will diminish but the need for infectious disease surveillance will increase. The value of surveillance to societies, countries and global partnership must be better communicated. Global health, development and surveillance needs good news that transcends scientific, political and geographic boundaries. In the current socio-political and economic climate that is challenging the rationale of official development assistance (ODA), and where contributions to development ED IT O R IA L"
1135fe17c63dc73dfaf5e0f1122d3e831910c96a,"Diarrheal diseases (DD) are leading causes of disease burden, death, and disability, especially in children in low-income settings. DD can also impact a child's potential livelihood through stunted physical growth, cognitive impairment, and other sequelae. As part of the Global Burden of Disease Study, we estimated DD burden, and the burden attributable to specific risk factors and particular etiologies, in the Eastern Mediterranean Region (EMR) between 1990 and 2013. For both sexes and all ages, we calculated disability-adjusted life years (DALYs), which are the sum of years of life lost and years lived with disability. We estimate that over 125,000 deaths (3.6% of total deaths) were due to DD in the EMR in 2013, with a greater burden of DD in low- and middle-income countries. Diarrhea deaths per 100,000 children under 5 years of age ranged from one (95% uncertainty interval [UI] = 0–1) in Bahrain and Oman to 471 (95% UI = 245–763) in Somalia. The pattern for diarrhea DALYs among those under 5 years of age closely followed that for diarrheal deaths. DALYs per 100,000 ranged from 739 (95% UI = 520–989) in Syria to 40,869 (95% UI = 21,540–65,823) in Somalia. Our results highlighted a highly inequitable burden of DD in EMR, mainly driven by the lack of access to proper resources such as water and sanitation. Our findings will guide preventive and treatment interventions which are based on evidence and which follow the ultimate goal of reducing the DD burden."
15213379aac68f8ba1e202954a9b3e656943aca7,
3402f5ed9edf5351cef361b4c14b9767ab0f9ec2,
3f498509e4ef2ecaa3b4fc46395bc5a5feb12fcf,"JRSM Open is an online-only, peer-reviewed, open access companion journal to JRSM. JRSM Open was launched in June 2010 and is rapidly developing into a leading international online resource of clinical papers, reviews and case reports on all aspects of improving patient care. As with other open access journals, authors are required to pay a small fee for published articles to cover the production costs and ensure that their article is freely available to readers. Articles published in JRSM Open are included in PubMed, PubMed Central and Google Scholar. You can find JRSM Open on the Internet at http:// jro.sagepub.com. To submit a clinical paper or case report to JRSM Open, visit http://mc.manuscriptcentral.com/shorts. For submission inquiries, contact Dr Kamran Abbasi, editor of JRSM and JRSM Open, by email at kamran.abbasi@rsm.ac.uk. Articles published in JRSM Open will also be mentioned in the print edition of JRSM. Here are the articles published in JRSM Open in December 2015:"
628507f6301f9a5163126c6a48a7dde14c439d6a,
7edff8a3bd94120147b7045e144bfb14a40acbce,
891dbc4a18ff783416f9f4910135b86bd420a586,"Global health and TB care and prevention have entered a new era. With the adoption of 17 Sustainable Development Goals (SDGs) to succeed the 8 Millennium Development Goals (MDGs), new impetus has been given to integrating health and development.1 The introduction of the SDGs will help shape collaborative efforts to improve health outcomes generally, and TB burden specifically, over the next 15 years.2,3 
 
Health is a cornerstone of sustainable development, in particular among low- and middle-income settings, and an area where substantial resources have been mobilised and significant achievements made over the past 15 years. According to the United Nations (UN) MDG Report,4 the number of people living on less than $1.25 a day was reduced by over 1 billion, with the proportion of undernourished people down from 23.3% in 1992–1993 to 12.9% in 2014–2015. Global number of deaths among children under 5 years of age halved from 12.7 to 6 million per annum in 15 years—an improvement but a long way to go. Official development assistance concurrently increased from $81 billion in 1990 to $135 billion in 2015, although it appears to have plateaued over the last 3–4 years for health.5"
a28399abdaf80845e162989998ccb6e012c34475,
a607cf556de38311a2f68b05df284cc433f8a6f2,
aed7d72651e1d76a99a82329fe21cf12dde532dc,
b2f580c687de1bf32ed2286f058c5851fd42dcfe,"BACKGROUND
In transitioning from the Millennium Development Goal to the Sustainable Development Goal era, it is imperative to comprehensively assess progress toward reducing maternal mortality to identify areas of success, remaining challenges, and frame policy discussions. We aimed to quantify maternal mortality throughout the world by underlying cause and age from 1990 to 2015.


METHODS
We estimated maternal mortality at the global, regional, and national levels from 1990 to 2015 for ages 10-54 years by systematically compiling and processing all available data sources from 186 of 195 countries and territories, 11 of which were analysed at the subnational level. We quantified eight underlying causes of maternal death and four timing categories, improving estimation methods since GBD 2013 for adult all-cause mortality, HIV-related maternal mortality, and late maternal death. Secondary analyses then allowed systematic examination of drivers of trends, including the relation between maternal mortality and coverage of specific reproductive health-care services as well as assessment of observed versus expected maternal mortality as a function of Socio-demographic Index (SDI), a summary indicator derived from measures of income per capita, educational attainment, and fertility.


FINDINGS
Only ten countries achieved MDG 5, but 122 of 195 countries have already met SDG 3.1. Geographical disparities widened between 1990 and 2015 and, in 2015, 24 countries still had a maternal mortality ratio greater than 400. The proportion of all maternal deaths occurring in the bottom two SDI quintiles, where haemorrhage is the dominant cause of maternal death, increased from roughly 68% in 1990 to more than 80% in 2015. The middle SDI quintile improved the most from 1990 to 2015, but also has the most complicated causal profile. Maternal mortality in the highest SDI quintile is mostly due to other direct maternal disorders, indirect maternal disorders, and abortion, ectopic pregnancy, and/or miscarriage. Historical patterns suggest achievement of SDG 3.1 will require 91% coverage of one antenatal care visit, 78% of four antenatal care visits, 81% of in-facility delivery, and 87% of skilled birth attendance.


INTERPRETATION
Several challenges to improving reproductive health lie ahead in the SDG era. Countries should establish or renew systems for collection and timely dissemination of health data; expand coverage and improve quality of family planning services, including access to contraception and safe abortion to address high adolescent fertility; invest in improving health system capacity, including coverage of routine reproductive health care and of more advanced obstetric care-including EmOC; adapt health systems and data collection systems to monitor and reverse the increase in indirect, other direct, and late maternal deaths, especially in high SDI locations; and examine their own performance with respect to their SDI level, using that information to formulate strategies to improve performance and ensure optimum reproductive health of their population.


FUNDING
Bill & Melinda Gates Foundation."
b59b1bf3debd166baae35c58c90089531ea03a0d,
ed93f891fd633b43122ee27638a950814aac4835,
31a4dba65c1ffb15633d1ca4f436da1ab4f4f4a8,
3b40ad23cd73e74e08fed7d81a610aa235e701ae,
40a0240342d08503215bb325fb68c619399cfde7,
871e1926c9387e3e249a458aea1f5e57e145c5a7,
dde06b9ebe8bb853bbf0268df3e960ee51c57b22,"Objectives Pelvic organ prolapse (POP) is a major cause of morbidity in Nepal, particularly affecting women in the rural communities. Women with POP in Nepal may suffer from symptoms for decades. At present, the Government of Nepal advocates surgical intervention but access to surgical care is inadequate. This report evaluated the feasibility of a non-surgical public health programme in rural Nepal, and describes risk factors associated with POP in this setting. Design Prospective monitoring and evaluation study of a new public health programme. Setting Baglung district, rural Nepal. Participants Women with gynaecological symptoms of POP. Main outcome measures Risk factors for disease progression were assessed using Fisher’s exact test, Pearson’s χ2-test and logistic regression analysis. Results Of the 74 women included in this analysis, 70.8% were diagnosed with stage 2 POP or greater. The majority of women did not have any further children following the onset of POP symptoms (63.5%). Duration of symptoms ranged from 2 months to 60 years, with 73.4% of women suffering for over 5 years and 28.4% suffering for over 20 years. Univariate analyses identified age at screening, age at onset of symptoms, the duration of symptoms and an associated rectocele as factors associated with increasing POP severity (p < 0.05). Kegel exercises were taught to 25 (33.8%) women with POP and ring pessaries were offered to 47 (63.5%) women with POP. Conclusions Non-surgical interventions may provide an opportunity to address the significant burden of POP in rural Nepal."
e16bed4fbeabcb522c4209d3b67eaa1d4f6fcb27,"Innovative financing strategies for global health are urgently needed to reinvigorate investment and new tools for impact. Bottleneck areas along the research and development (R&D) pipeline require particular attention, such as the transitions from preclinical discovery to clinical study, and product development to implementation and delivery. Successful organizations mobilizing and disbursing resources through innovating financing mechanisms include UNITAID, the Global Fund, and Gavi, the Vaccine Alliance. Although precise numbers are poorly documented, estimated investment in low-income settings falls seriously short of local need. This commentary discusses the newly established Global Health Investment Fund as a case study to support late-stage global health R&D."
0ca27242e4eca92dca9f601575b1e9e094a2331f,
456feb3e3b04e602a8451bae09646e1d5d3802a9,
7b8c66f9c6e79c7948b0bf0570265a7b4a2d7628,
631f557cb915e764ab641bb74d7afe8a1bc4d8a1,
923da0ad52ab57dc657c3f4b035a5446c47a8488,
a92467fc6f5dc05762199b742869ce130e1710dd,
dd48ef553e2cf8b17b5d91103a903c0a76ec8be5,
edac2dd5be9fd4229cb7fe2ed4178dbe3f3b92a9,
579af6e37120fc10d4516e2be387284364555354,"Medical ethics and law education in the UK is undergoing continuous transformation. In parallel, human rights teaching with respect to health is expanding as a distinct field. Yet a resistance to the inclusion of human rights in the medical ethics and law curriculum persists. In response to Stirrat and colleagues, this article seeks to highlight the mutual benefit that could be derived from an integration of human rights into the already established medical ethics and law teaching in medical schools. It proposes that incorporating human rights into the curriculum would add value to traditional medical ethics and law teaching and provide a promising opportunity to enhance the interest from the student body."
8ec8721de21c4e2ef85ce16da07100a91bfbfc29,"Rights are moral and legal entitlements. Human rights have foundations in the theory of natural law and by definition human rights ‘belong justifiably’ to all persons (6). Several core notions play essential roles in the realisation of human rights, namely the concept of a right, a duty, an entitlement and an obligation. For descriptive purposes, rights are classified as negative rights and positive rights. Negative rights imply freedoms, for instance the right to be free from forced medical experimentation or the right to be free from torture and ill treatment. Positive rights imply entitlements, for instance the right to access essential medicines and vaccines. The philosophical basis for human rights is not restricted to the 20 Century and the United Nations Universal Declaration of Human Rights (7). Core principles shape several religious and ancient legal texts, such as the Babylonian Code of Hammurabi, the Hindu Laws of Manu and the Analects of Confucius. The origins of the modern human rights movement arguably stems from the end of the 18 Century at a time where the relationship between government and the governed was evolving rapidly and redefining itself, highlighted by treatises in political philosophy on the Social Contract. Two influential revolutions in the United States of America, 1776, and in France, 1789, generated the Declaration of Independence and the Declaration of the Rights of Man and the Citizen, respectively, upholding the concept of universal natural rights. However, it is the genocidal atrocities and medical experimentation of an unparalleled evil committed by the Nazis, directed primarily at millions of Jews throughout Europe, from which the Universal Declaration of Human Rights (UDHR) was born. The inhumanity of the Nazi regime and gross disregard for human rights of all human beings left people and their leaders questioning the morality of the human race. Currently, the International Covenant on Economic, Social and Cultural Rights (ICESCR) (8) and the International Covenant on Civil and Political Rights (ICCPR) (9) are legally binding instruments in international human rights law upholding, enshrining and protecting universal human rights. They are part of what is referred to as the Bill of Human Rights, which is composed of the ICESCR, ICCPR and UDHR. These instruments are vital in In the current era of globalisation, the world is diversifying as never before. Inequalities in economic, social, spiritual, political and civil matters characterise daily life. Estimates consider 80% of global disease burden lies in ‘developing’ or low-income countries, based on crude calculations by disability-adjusted life years (DALYs) (1). Moreover, measures do not seem to be in place to redress these inequalities. For instance, the Commission on Health Research Development estimated, albeit several years ago, that 90% of all global research and development expenditure is dedicated to 10% of the world’s disease burden, primarily concentrated in wealthier countries (2). Today, there may be a new climate of awareness maturing. Governments representing ‘developed’ or high-income countries often discuss the urgent need to help the world’s poorest or rescue the ‘‘bottom billion’’ from devastating illness (3,4). However, the optimistic rhetoric is not always matched by foreign policy and international trade agreements [consider TRIPS (5), the World Trade Organization’s Trade-Related Aspects of International Property Rights Agreement consolidating strict patent rules worldwide with significant impact on access to essential medicines]. The following perspective provides a comprehensive overview of the right to health and proposes a human rights-based approach to health as a sustainable framework that transcends borders for justice in healthcare. Linked Comment: Simms. Int J Clin Pract 2011; 65: 233–6. PERSPECT IVE"
c8b2155240471eddcf4270499ac32a41c8d20b01,"In this era of increasing drug resistance among infectious diseases such as tuberculosis (TB), the complex population dynamics of border areas must be monitored more extensively. TB remains a major public health threat; its antimicrobial treatment is long; and the only vaccine licensed in the world, live-attenuated Mycobacterium bovis Bacille Calmette-Guérin (BCG), exhibits varying efficacy. In addition to epidemiological surveillance, the underlying determinants contributing to the health and wellbeing of populations are of key importance. Although it received heightened attention in the past, tuberculosis transmission in the United States-Mexico border area demands renewed interest. Lessons learned should be applied to similar areas around the globe."
09fedb6c3b2628437e1614137059a11adecd5a52,
0d7ad58665936153a6d88220924da9bc1775dc23,
372450cf4a1f8ee8382a485fe0787bfdbebf2573,
947c4c47a0d21c77c0289af0f91dec9d548fb787,
c4bb9226ec84f77e2f173d973eb37c8b73ba86cc,"The community of medical ethics and law educators are commendable for their consistent endeavours to communicate with students regarding their views on subjects addressed, assessments and teaching methods. There is still, however, some division between how and what students want to learn and the reality in practice. The ‘Spotlight on: Health and Human Rights’ conference, held at the Royal Society of Medicine on 31 October 2009, provided medical students and junior doctors with the opportunity to discuss the implications of human rights on health, patient rights, medical ethics and law, and their future role in health care. The conference was supported by the World Health Organization, the Royal Society of Tropical Medicine & Hygiene and the Institute of Medical Ethics, and over 100 delegates shared their views during an intensive one-day programme. Speakers from a variety of backgrounds in medicine, public policy, law and academia commended the young delegates for their commitment and attending on a Saturday. Dr Peter Hall, Chair of Doctors for Human Rights in the UK, opened the conference with an introduction to health and human rights and a poignant account of his experiences in Rwanda. Subsequent sessions covered specific topics, both theoretical and practical. Dr Christoffer Van Tulleken, University College London (UCL) Centre for International Development and Health, shed light onto a human rights-based approach in humanitarian crises, building on his experience in Burma with Cyclone Nargis. Lawyer Susan Wright, Director of Medecins du Monde UK, and her inspiring lecture on undocumented migrants and access to essential health care left a resounding impression on the delegates. Dr David McCoy, also from the UCL Centre for International Development and Health, lectured on health systems and the right to health – speaking memorably of the shift observed in the NHS, where individuals are treated as consumers rather than simply as patients. Support from the Institute of Medical Ethics allowed Gunilla Backman, of the Swedish International Development and Cooperation Agency, to contribute an insightful lecture on the value added by a human rights-based approach to health, drawing on both experience at a local level, such as in Guatemala, and with large international health organizations, such as the GAVI Alliance in Geneva. For the afternoon session, workshops discussing case studies on maternal health, patient rights and access to essential medicines shifted the focus from keynote speakers to the delegates and culminated in a closing panel debate. Led by two speakers from Amnesty International, Naomi McAuliffe and Dr Jim Welsh, discussion focused primarily on what health-care workers can actively do, with Ms McAuliffe reiterating that ‘health professionals are human rights defenders’. Evaluation feedback stressed the need to incorporate human rights into the medical student curriculum and inspired delegates to gain a better understanding of the subject. The day was considered by many delegates to be ‘inspirational’, ‘fascinating’ and with ‘a lot to absorb’. The right to health is a notion that strikes a chord with medical students and appeals to their compassionate, considerate and ethically minded side. As children growing up during the UN Decade for Human Rights Education (1995–2004), human rights principles and language in a world characterized by migration and globalization may provide a missing link for students in today’s medical ethics and law curriculum. Medical ethics and law education in the United Kingdom is undergoing continuous transformation. We anticipate the event to be the first of many focusing on medical ethics, law and human rights. The ‘Spotlight on: Medical Ethics & Law’ conference, supported by the Institute of Medical Ethics, is currently in preparation and will be held at the Royal Society of Medicine. If you are interested in contributing, or would like further information, please contact Joseph.fitchett@doctors.org.uk."
1a08dd7b1f67fe3e0eba56589fc79dabbfd3d461,
5ee651e40aa220b178757035fb8ca8a4031bc064,
fa12952db73c352c0059bdb5519047cf93f42131,An unusual case of a Harrington rod migrating out of the abdominal cavity.
30c40c7a41e5f60d32631c5d26eb5b2293f3e0c9,
c57a5eb363ced1ef5f1591917e9fc93eb2d1186d,
b93bfd30638410cf55da2d6a0e9bea3e290ada0c,
ee150e16f019514ba09e04c36fa48a6811c4c4ec,
79f6a4090577988a04d2a93ccbd1c2b3e0bfb28e,
2f9ca91bbd4ec71d65fb87739a80ad857572e0c4,"Background Chronic exposure to excess arsenic in drinking water has been strongly associated with increased risks of multiple cancers, diabetes, heart disease, and reproductive and developmental problems in humans. We previously demonstrated that As, a potent endocrine disruptor at low, environmentally relevant levels, alters steroid signaling at the level of receptor-mediated gene regulation for all five steroid receptors. Objectives The goal of this study was to determine whether As can also disrupt gene regulation via the retinoic acid (RA) receptor (RAR) and/or the thyroid hormone (TH) receptor (TR) and whether these effects are similar to previously observed effects on steroid regulation. Methods and results Human embryonic NT2 or rat pituitary GH3 cells were treated with 0.01–5 μM sodium arsenite for 24 hr, with or without RA or TH, respectively, to examine effects of As on receptor-mediated gene transcription. At low, noncytotoxic doses, As significantly altered RAR-dependent gene transcription of a transfected RAR response element–luciferase construct and the native RA-inducible cytochrome P450 CYP26A gene in NT2 cells. Likewise, low-dose As significantly altered expression of a transfected TR response element–luciferase construct and the endogenous TR-regulated type I deiodinase (DIO1) gene in a similar manner in GH3 cells. An amphibian ex vivo tail metamorphosis assay was used to examine whether endocrine disruption by low-dose As could have specific pathophysiologic consequences, because tail metamorphosis is tightly controlled by TH through TR. TH-dependent tail shrinkage was inhibited in a dose-dependent manner by 0.1– 4.0 μM As. Conclusions As had similar effects on RAR- and TR-mediated gene regulation as those previously observed for the steroid receptors, suggesting a common mechanism or action. Arsenic also profoundly affected a TR-dependent developmental process in a model animal system at very low concentrations. Because RAR and TH are critical for both normal human development and adult function and their dysregulation is associated with many disease processes, disruption of these hormone receptor–dependent processes by As is also potentially relevant to human developmental problems and disease risk."
6bd7c657bd10f0bcee81507a0dcf7bbd9db666c4,"Background HIV testing among patients with malignant lymphoma (PWML) is variably implemented. We evaluated HIV testing among PWML, and mapped factors influencing hematologists’ testing behavior. Materials We conducted a mixed-methods study assessing HIV testing among PWML, factors influencing HIV testing and opportunities for improvement in five hospitals in the region of Amsterdam, the Netherlands. The proportion of PWML tested for HIV within 3 months before or after lymphoma diagnosis and percentage positive were assessed from January 2015 through June 2020. Questionnaires on intention, behavior and psychosocial determinants for HIV testing were conducted among hematologists. Through twelve semi-structured interviews among hematologists and authors of hematology guidelines, we further explored influencing factors and opportunities for improvement. Findings Overall, 1,612 PWML were included for analysis, including 976 patients newly diagnosed and 636 patients who were referred or with progressive/relapsed lymphoma. Seventy percent (678/976) of patients newly diagnosed and 54% (343/636) of patients with known lymphoma were tested for HIV. Overall, 7/1,021 (0.7%) PWML tested HIV positive, exceeding the 0.1% cost-effectiveness threshold. Questionnaires were completed by 40/77 invited hematologists, and 85% reported intention to test PWML for HIV. In the interviews, hematologists reported varying HIV testing strategies, including testing all PWML or only when lymphoma treatment is required. Recommendations for improved HIV testing included guideline adaptations, providing electronic reminders and monitoring and increasing awareness. Conclusions Missed opportunities for HIV testing among PWML occurred and HIV test strategies varied among hematologists. Efforts to improve HIV testing among PWML should include a combination of approaches."
bf8b29110538b8bca269f3a70a2b321ea636459b,
c50d214482b33e6ad9e2d57023409e24a560f880,"Introduction In Europe, half of people living with HIV (PLWH) present late to care, with associated higher morbidity and mortality. This study aims to assess short- and long-term costs of HIV-care based on time of presentation and identify other factors contributing to higher costs in the first and fifth year after antiretroviral therapy (ART) initiation. Material and methods We included ATHENA cohort data which prospectively includes 98% of PLWH in the Netherlands. PLWH who initiated ART in 2013 were included and followed over five years. PLWH were divided in three categories based on CD4 cell-count at time of ART initiation: timely presentation (CD4>350cells/μL), late presentation (CD4 200-350cells/μL or >350cells/μL with AIDS-defining illness) and very late presentation (CD4<200cells/μL). The total HIV-care cost was calculated distinguishing ART medication and non-ART medication costs (hospitalization, outpatient clinic visits, co-medications, and HIV-laboratory tests). Results From 1,296 PLWH, 273 (21%) presented late and 179 (14%) very late. Nearly half of those who entered HIV-care in a very late stage were of non-Dutch origin, with 21% originating from sub-Saharan Africa. The mean cost per patient in the first year was €12,902 (SD€11,098), of which about two-thirds due to ART (€8,250 (SD€3,142)). ART costs in the first and fifth year were comparable regardless of time of presentation. During the first year on treatment, non-ART medication costs were substantially higher among those with late presentation (€4,749 (SD€8,009)) and very late presentation (€15,886 (SD€ 21,834)), compared with timely presentation (€2,407(SD€4,511)). Higher non-ART costs were attributable to hospitalization and co-medication. The total non-ART costs incurred across five years on treatment were 56% and 246% higher for late and very late presentation respectively as compared to timely presentation. Conclusion Very late presentation is associated with substantial costs, with non-ART costs nearly seven times higher than for those presenting timely. Hospitalization and co-medication costs are likely to continue to drive higher costs for individuals with late presentation into the future. Programs that identify individuals earlier will therefore likely provide significant short- and long-term health cost savings."
0072715885f56b340e3131ccf277aa8f785d9c56,
039a0b4d4e152e3233d205ada79c945ca43175e5,"Abstract BACKGROUND AND AIMS Kidney transplant recipients (KTRs) are still at risk of fatal COVID-19 disease after SARS-CoV-2 vaccination, even after a third booster vaccination. With the spread of new SARS-CoV-2 variants, great urgency exists for a better understanding of the factors that impact the immune response in these patients. Our aim was to predict nonseroconversion after SARS-CoV-2 vaccination to understand the factors that may disrupt the humoral response in KTRs. METHOD A multivariable logistic regression model was developed and validated that uses routinely available clinical and laboratory information to predict nonseroconversion after two doses of SARS-CoV-2 mRNA vaccination in KTRs. KTRs were prospectively enrolled to the Dutch REnal patients COVID-19 VACcination (RECOVAC) consortium, specifically to the Immune Response (IR) study with four participating university medical centres in the Netherlands. The discovery cohort consisted of three participating centres (Amsterdam UMC, Radboud UMC Nijmegen and Erasmus MC Rotterdam), and the validation cohort of patients treated in UMC Groningen. A large second validation set from the RECOVAC consortium (LESS-CoV-2) was used to test a more simplified version of the model without lymphocyte counts. All participants received two doses of the mRNA-1273 COVID-19 vaccine (Moderna) and had no history of SARS-CoV-2 infection. Participants were classified as responder or non-responder based on seroconversion at day 28 following the second vaccination with a threshold for seropositivity based on receiver operator curve analysis set at S1-specific IgG antibody concentration ≥10 BAU/mL. RESULTS The discovery cohort included 215 KTRs of which 126 responders and 89 non-responders. After backward selection, 6 out of 19 factors remained predictive for nonseroconversion: increased age, lower lymphocyte count, lower estimated glomerular filtration rate (eGFR), shorter time after transplantation, not using steroids and the use of mycophenolate mofetil/mycophenolic acid (MMF/MPA) (Figure 1). The area under the curve (AUC) of the receiver operating characteristics was 0.83 (95% confidence interval 0.78–0.89) in the discovery cohort after adjustment for optimism and 0.84 (0.74–0.94) in external validation of the UMC Groningen cohort (n = 73), and 0.75 (0.72–0.77) in external validation of the LESS-CoV-2 dataset (n = 2484). In addition, MMF/MPA appeared to have a dose-dependent unfavourable association with the S1 IgG antibody titer (Figure 2).FIGURE 1: The effect of MMF/MPA dosing (mg/day) on the log S1 IgG antibody titer (BAU/mL).FIGURE 2: Nomogram for the prediction of nonseroconversion after SARS-CoV-2 vaccination in KTRs. CONCLUSION Six predictors allow for a better understanding of the process of the development of the humoral response in KTRs. These predictors could be applied to individualized patient counseling and treatment strategy during the COVID-19 pandemic and future innovative vaccine trial design for this complex patient group."
0d20dbdf2bf7991de456c3d7906ceb4162d6658d,
0d20dbdf2bf7991de456c3d7906ceb4162d6658d,
1a8160f86fcb5827e2ab69fe6c6b34caa96d15e5,"Abstract Background Patients with haematological malignancies frequently endure neutropenia and gastrointestinal (GI)-mucositis after high-dose chemotherapy. In these patients, ciprofloxacin is used for Gram-negative infection prophylaxis. Objectives We investigate ciprofloxacin pharmacokinetics after oral administration in patients with haematological malignancies and explore the impact of GI-mucositis on oral bioavailability and clearance in order to assure adequate systemic exposure. Methods Adult haematological patients from two Dutch University Medical Centres received 500 mg twice daily oral ciprofloxacin for Gram-negative prophylaxis. The ciprofloxacin plasma concentrations were collected at various timepoints after oral ciprofloxacin administration and at various days after completion of chemotherapy. Data obtained after oral and intravenous ciprofloxacin administration in 28 healthy volunteers without mucositis served as a control group (391 samples). For haematological patients the degree of GI-mucositis was assessed using the Daily Gut Score (DGS), plasma citrulline and albumin. Data were analysed by non-linear mixed-effects modelling. Results In total, 250 blood samples were collected in 47 patients with a wide variety of haematological malignancies between 0–30 days after start of chemotherapy. Mucositis was generally mild [DGS median (IQR) 1 (1–1) and citrulline 16 μmol/L (12–23)]. The time to Cmax was slower in haematological patients compared with healthy volunteers although no association with the degree of mucositis (defined as DGS or citrulline) could be identified. Ciprofloxacin bioavailability and clearance were 60% and 33.2 L/h, respectively. Conclusions This study supports oral dosing of ciprofloxacin as Gram-negative infection prophylaxis in haematological patients with mild-to-moderate mucositis capable of oral intake."
2374d78335812298be0214dd5544cb65a22775a8,"Reporting and learning from preventable adverse events is crucial to improve patient safety. Although physicians should file and analyse adverse events by law in The Netherlands, it is unknown if these reporting systems are sufficiently used in clinical practice. This study is a substudy of the multicenter RICAT trial, a successful quality improvement project to reduce inappropriate use of intravenous and urinary catheters in medical wards in seven hospitals, in which we screened 5696 patients and documented 803 catheter-related complications. We also checked the adverse events reporting systems of these patients and found that only 13 (1.6%) of 803 catheter-related complications were registered. Of the infectious complications only five (10.9%) of 46 catheter-associated bloodstream infections and urinary tract infections were registered. We conclude that the reported complications were a major underestimation of the real complication practice in medical wards in The Netherlands. The RICAT trial is registered at Netherlands Trial Register, trial NL5438."
3bbee31d79e58b8f5c99f274efdc23a9e8306051,
43e4e92b6f07470e75b02bf6ef1a1decd2aedbf9,
627ac692f272087db3944108956b625c14d64e7e,"In frail older adults, antibiotics are often inappropriately prescribed for suspected urinary tract infections (UTIs). We describe three cases in the general practice, nursing home, and emergency department setting to illustrate how to improve diagnosing UTIs in frail older patients. Nonspecific symptoms, e.g., behavioral change or smelly urine, often trigger a UTI suspicion followed by immediate urine testing and antibiotic treatment. However, nonspecific symptoms should trigger a broad differential diagnosis and thorough evaluation. The value of urine tests is limited due to the high prevalence of asymptomatic bacteriuria in this patient group; a UTI is thus a clinical diagnosis not solely based on a positive urine test. Antibiotic treatment is recommended only in case of symptoms referable to the urinary tract or systemic symptoms in patients without a urinary catheter. In patients with a urinary catheter, antibiotic treatment is recommended in case of systemic symptoms without any other focus."
6ab2e8f13024c707dfb43b2d5790a46521791e4f,
728be0d6d492c5ce6ee7b2802a650bd82bdc4616,"Background. Kidney transplant recipients (KTRs) are still at risk of severe COVID-19 disease after SARS‑CoV‑2 vaccination, especially when they have limited antibody formation. Our aim was to understand the factors that may limit their humoral response. Methods. Our data are derived from KTRs who were enrolled in the Dutch Renal Patients COVID-19 Vaccination consortium, using a discovery cohort and 2 external validation cohorts. Included in the discovery (N = 1804) and first validation (N = 288) cohorts were participants who received 2 doses of the mRNA-1273 vaccine. The second validation cohort consisted of KTRs who subsequently received a third dose of any SARS-CoV-2 vaccine (N = 1401). All participants had no history of SARS-CoV-2 infection. A multivariable logistic prediction model was built using stepwise backward regression analysis with nonseroconversion as the outcome. Results. The discovery cohort comprised 836 (46.3%) KTRs, the first validation cohort 124 (43.1%) KTRs, and the second validation cohort 358 (25.6%) KTRs who did not seroconvert. In the final multivariable model‚ 12 factors remained predictive for nonseroconversion: use of mycophenolate mofetil/mycophenolic acid (MMF/MPA); chronic lung disease, heart failure, and diabetes; increased age; shorter time after transplantation; lower body mass index; lower kidney function; no alcohol consumption; ≥2 transplantations; and no use of mammalian target of rapamycin inhibitors or calcineurin inhibitors. The area under the curve was 0.77 (95% confidence interval [CI], 0.74-0.79) in the discovery cohort after adjustment for optimism, 0.81 (95% CI, 0.76-0.86) in the first validation cohort, and 0.67 (95% CI, 0.64-0.71) in the second validation cohort. The strongest predictor was the use of MMF/MPA, with a dose-dependent unfavorable effect, which remained after 3 vaccinations. Conclusions. In a large sample of KTRs, we identify a selection of KTRs at high risk of nonseroconversion after SARS-CoV-2 vaccination. Modulation of MMF/MPA treatment before vaccination may help to optimize vaccine response in these KTRs. This model contributes to future considerations on alternative vaccination strategies."
a45ee1da99fb5cf369ff2c2367f72e003f68a238,"(1) Background: In the emergency department (ED), ordering urine tests in patients without symptoms of a urinary tract infection can lead to inappropriate antimicrobial treatment. We aimed to identify factors contributing to the unnecessary ordering of urinalyses in the ED. (2) Methods: An online survey study among nurses and physicians working in the EDs of five hospitals in the Netherlands was conducted. (3) Results: The overall response rate was 26% (221/850; 85 nurses and 136 physicians). The vast majority of the respondents reported knowing when to order urine tests (197/221; 90%). Almost two-thirds of the respondents (145/221; 66%) agreed that they ordered urinalyses because it is rapid and non-invasive to patients. Most nurses (66/86; 78%) said they informed the doctor if they thought the urine test would not contribute to the patient’s diagnosis, but only one-third of the physicians agreed with this statement (44/136; 32%). Most respondents (160/221; 72%) thought guidelines or protocols about urinalyses in the ED would be functional. (4) Conclusions: These results suggest urinalyses were frequently ordered in the ED to achieve a fast work process. Nurses and physicians could improve their communication about the indications for urine tests. Developing diagnostic guidelines for urine testing may be convenient."
ad610a89be024ef36954a120962e78515a32c616,"PURPOSE
Understanding residents' workplace learning could be optimized by not only considering attending physicians' role but also the role of nurses. While previous studies described nurses' role during discrete activities (e.g., feedback), a more profound understanding of how nurses contribute to residents' learning remains warranted. Therefore, we used the educational concept of guidance and explored the extent to which residents' and nurses' perceptions align regarding nurses' guiding role and which reasons they provide for their perceptions.


METHOD
This mixed-method study was conducted at four Dutch University Medical Centers in 2021. We simultaneously collected quantitative and qualitative data from 103 residents and 401 nurses through a theory-informed questionnaire with a Likert-scale and open-ended questions. We analyzed quantitative data to explore respondents' perceptions of nurses' guiding role by using ANOVA. The thematically analyzed qualitative open-comments explored respondents' reasons for their perceptions.


RESULTS
Nurses indicated to provide significantly more support (p = .01) and guidance on learning from patient care (p < .01) than perceived by residents. Moreover, nurses indicated that attending physicians did not always involve them in guiding residents, whereas residents perceived nurses were being involved (p <.001). Themes suggest that nurses and residents could be divided into two groups: (1) respondents who felt that guiding was inextricably linked to good interprofessional collaboration and patient care, and (2) respondents who saw the guiding role as limited and emphasized the distinct fields of expertise between nurses and physicians.


CONCLUSIONS
Residents and nurses felt that nurses played an important role in guiding residents' workplace learning. However, some residents did not always perceive to be guided. To further capitalize on nurses' guiding role, we suggest that residents can be encouraged to engage in the learning opportunities nurses provide to achieve optimal team-based patient care."
b7dc81533a0318b35102afd12b5ab55ae5372012,"Background: Quality Improvement (QI) is the key for every healthcare organization. QI programs may help healthcare professionals to develop the needed skills for interprofessional collaboration through interprofessional education. Furthermore, the role of diversity in QI teams is not yet fully understood. This evaluation study aimed to obtain in-depth insights into the expectations and experiences of different stakeholders of a hospital-wide interprofessional QI program. Methods: This qualitative study builds upon 20 semi-structured interviews with participants and two focus groups with the coaches and program advisory board members of this QI program. Data were coded and analyzed using thematic analysis. Results: Three themes emerged from the analysis: “interprofessional education”, “networking” and “motivation: presence with pitfalls”. Working within interprofessional project groups was valuable, because participants with different experiences and skills helped to move the QI project forward. It was simultaneously challenging because IPE was new and revealed problems with hierarchy, communication and planning. Networking was also deemed valuable, but a shared space to keep in contact after finalizing the program was missing. The participants were highly motivated to finish their QI project, but they underestimated the challenges. Conclusions: A hospital-wide QI program must explicitly pay attention to interprofessional collaboration and networking. Leaders of the QI program must cherish the motivation of the participants and make sure that the QI projects are realistic."
c38f7c9cca0680ccab12f24ebf4baec6966374f8,"Background: Although unbound ciprofloxacin is responsible for antibacterial effects, assays measuring the unbound drug plasma concentrations are scarce. This study aimed to develop and validate a rapid, reproducible, and sensitive liquid chromatography–tandem mass spectrometry assay for the determination of total and unbound ciprofloxacin plasma concentrations. Methods: The determination of total ciprofloxacin concentrations required a 10 μL sample, while for unbound ciprofloxacin concentrations, it was 100 μL. Unbound ciprofloxacin was separated from protein-bound ciprofloxacin through ultrafiltration. A deuterated internal standard was used, and the sample preparation involved protein precipitation. The method was fully validated over a concentration range of 0.02–5.0 mg/L, according to the US Food and Drug Administration guidelines. In addition, its clinical application was demonstrated. Results: The total run time was 1.5 minutes. For total ciprofloxacin plasma concentrations, the mean accuracy ranged from 94.5% to 105.0% across the validated range, the intraday imprecision was ≤7.6%, and the interday imprecision was ≤9.8%. For unbound ciprofloxacin plasma concentrations, the mean accuracy ranged from 92.8% to 102.1% across the validated range, the intraday imprecision was ≤7.0%, and the interday imprecision was ≤9.6%. Ciprofloxacin in plasma and ultrafiltrate remained stable for at least 96 hours at room temperature, at least 4 years at −80°C, and at least 3 freeze/thaw cycles (−80°C), with a minimum interval of 24 hours. Conclusions: The presented method is precise and accurate. It has been implemented in clinical care and research projects at a university hospital, permitting rapid determination of total and unbound ciprofloxacin."
c6dfec8bf73f7376c60cd690763c532bfcf13a13,
ed1f5223f592c457b5b6bc6c3feacd4eed9d530f,
f0c63bc05d282990af2852335e4fdd054ee8f912,
f2658c85473dac83a009aace482d540c4e787a6a,"Asymptomatic bacteriuria (ASB) is a common finding in certain populations. This study assessed general practitioners’ (GPs’) knowledge about ASB and their current clinical practice regarding urine testing. Methods: An online survey was used for GPs in the Netherlands from October to December 2020. Results: In total, 99 surveys were included in the analyses. All GPs strongly agreed with the statements about their knowledge and self-confidence regarding urine diagnostics and treatment of ASB. The median knowledge score was 4 out of 6 (IQR 2 to 6). Most GPs (64 of 92; 70%) followed the guideline for the choice of urine diagnostics and reported appropriate indications for urine testing. However, 71/94 (75.5%) GPs would treat patients for ASB if they have diabetes mellitus. Further, 34 (37%) of 92 participants would inappropriately repeat a urine test after a patient was treated for a urinary tract infection (UTI). One-third of the GPs responded that ASB was insufficiently addressed within the guidelines for UTI. Conclusion: These results indicate that knowledge about ASB could be improved in primary care in the Netherlands, mainly in diabetic patients that have ASB, as well as for follow-up tests after treatment for UTI."
0a344e8b33eaa6641179e002f1c6e971fd13b10e,
128cacc7fd685c1e24d65ba59ff724d47a4e1f07,
288864fcd9baae8b0bd5c89cb929d86d25084504,"Objectives General practitioners (GPs) and sexual health centres (SHCs) are the main providers of HIV testing and diagnose two-thirds of HIV infections in the Netherlands. We compared regional HIV testing and positivity by GPs versus SHCs to gain insight into strategies to improve HIV testing, to enable timely detection of HIV infections. Methods Laboratory data (2011–2018) on HIV testing by GPs and SHCs in five Dutch regions with varying levels of urbanisation were evaluated. Regional HIV testing rates per 10 000 residents ≥15 years (mean over period and annual) were compared between providers using negative binomial generalised additive models and additionally stratified by sex and age (15–29 years, 30–44 years, 45–59 years, ≥60 years). χ2 tests were used to compare positivity percentage between the two groups of providers. Results In the study period, 505 167 HIV tests (GP 36%, SHC 64%) were performed. The highest HIV testing rates were observed in highly urbanised regions, with large regional variations. The HIV testing rates ranged from 28 to 178 per 10 000 residents by GPs and from 30 to 378 per 10 000 by SHCs. Testing rates by GPs were lower than by SHCs in three regions and comparable in two. In all regions, men were tested less by GPs than by SHCs; for women, this varied by region. Among those aged 15–29 years old, GPs’ testing rates were lower than SHCs’, while this was reversed in older age categories in four out of five regions. The overall mean HIV positivity was 0.4%. In contrast to other regions, positivity in Amsterdam was significantly higher among individuals tested by GPs than by SHCs. Conclusions This retrospective observational study shows that besides SHCs, who perform opt-out testing for key groups, GPs play a prominent role in HIV testing, especially in non-key populations, such as women and older individuals. Large regional variation exists, requiring region-specific interventions to improve GPs’ HIV testing practices."
2fbcadbd35c1da5e08b4f236fc18a50868bc5124,"Abstract Background We aimed to determine the noninferiority of fosfomycin compared to ciprofloxacin as an oral step-down treatment for Escherichia coli febrile urinary tract infections (fUTIs) in women. Methods This was a double-blind, randomized, controlled trial in 15 Dutch hospitals. Adult women who were receiving 2–5 days of empirical intravenous antimicrobials for E. coli fUTI were assigned to step-down treatment with once-daily 3g fosfomycin or twice-daily 0.5g ciprofloxacin for 10 days of total antibiotic treatment. For the primary end point, clinical cure at days 6–10 post-end of treatment (PET), a noninferiority margin of 10% was chosen. The trial was registered on Trialregister.nl (NTR6449). Results After enrollment of 97 patients between 2017 and 2020, the trial ended prematurely because of the coronavirus disease 2019 pandemic. The primary end point was met in 36 of 48 patients (75.0%) assigned to fosfomycin and 30 of 46 patients (65.2%) assigned to ciprofloxacin (risk difference [RD], 9.6%; 95% confidence interval [CI]: –8.8% to 28.0%). In patients assigned to fosfomycin and ciprofloxacin, microbiological cure at days 6–10 PET occurred in 29 of 37 (78.4%) and 33 of 35 (94.3%; RD, –16.2%; 95% CI: –32.7 to –0.0%). Any gastrointestinal adverse event was reported in 25 of 48 (52.1%) and 14 of 46 (30.4%) patients (RD, 20.8%; 95% CI: 1.6% to 40.0%), respectively. Conclusions Fosfomycin is noninferior to ciprofloxacin as oral step-down treatment for fUTI caused by E. coli in women. Fosfomycin use is associated with more gastrointestinal events. Clinical Trial Registration Trial NL6275 (NTR6449)."
2fbcadbd35c1da5e08b4f236fc18a50868bc5124,"Abstract Background We aimed to determine the noninferiority of fosfomycin compared to ciprofloxacin as an oral step-down treatment for Escherichia coli febrile urinary tract infections (fUTIs) in women. Methods This was a double-blind, randomized, controlled trial in 15 Dutch hospitals. Adult women who were receiving 2–5 days of empirical intravenous antimicrobials for E. coli fUTI were assigned to step-down treatment with once-daily 3g fosfomycin or twice-daily 0.5g ciprofloxacin for 10 days of total antibiotic treatment. For the primary end point, clinical cure at days 6–10 post-end of treatment (PET), a noninferiority margin of 10% was chosen. The trial was registered on Trialregister.nl (NTR6449). Results After enrollment of 97 patients between 2017 and 2020, the trial ended prematurely because of the coronavirus disease 2019 pandemic. The primary end point was met in 36 of 48 patients (75.0%) assigned to fosfomycin and 30 of 46 patients (65.2%) assigned to ciprofloxacin (risk difference [RD], 9.6%; 95% confidence interval [CI]: –8.8% to 28.0%). In patients assigned to fosfomycin and ciprofloxacin, microbiological cure at days 6–10 PET occurred in 29 of 37 (78.4%) and 33 of 35 (94.3%; RD, –16.2%; 95% CI: –32.7 to –0.0%). Any gastrointestinal adverse event was reported in 25 of 48 (52.1%) and 14 of 46 (30.4%) patients (RD, 20.8%; 95% CI: 1.6% to 40.0%), respectively. Conclusions Fosfomycin is noninferior to ciprofloxacin as oral step-down treatment for fUTI caused by E. coli in women. Fosfomycin use is associated with more gastrointestinal events. Clinical Trial Registration Trial NL6275 (NTR6449)."
35d26a5456680fb62afb746a12f453f73a0f0d3b,"Abstract Objective Urinary tract infections are among the most common infections during pregnancy. The association between symptomatic lower urinary tract infections during pregnancy and fetal and maternal complications such as preterm birth and low birthweight remains unclear. The aim of this research is to evaluate the association between urinary tract infections during pregnancy and maternal and neonatal outcomes, especially preterm birth. Study Design This study is a secondary analysis of a multicenter prospective cohort study, which included patients between October 2011 and June 2013. The population consists of women with low risk singleton pregnancies. We divided the cohort into women with and without a symptomatic lower urinary tract infection after 20 weeks of gestation. Baseline characteristics and maternal and neonatal outcomes were compared between the two groups. Multivariable logistic regression analysis was used to correct for confounders. The main outcome was spontaneous preterm birth at <37 weeks. Results We identified 4,918 pregnant women eligible for enrollment, of whom 9.4% had a symptomatic lower urinary tract infection during their pregnancy. Women with symptomatic lower urinary tract infections were at increased risk for both preterm birth in general (12 vs. 5.1%, adjusted OR 2.5; 95% CI 1.7–3.5) as well as a spontaneous preterm birth at <37 weeks (8.2 vs. 3.7%, adjusted OR 2.3; 95% CI 1.5–3.5). This association was also present for early preterm birth at <34 weeks. Women with symptomatic lower urinary tract infections during pregnancy are also at increased risk of endometritis (8.9 vs. 1.8%, adjusted OR 5.3; 95% CI 1.4–20) and mastitis (7.8 vs. 1.8%, adjusted OR 4.0; 95% CI 1.6–10) postpartum. Conclusion Low risk women with symptomatic lower urinary tract infections during pregnancy are at increased risk of spontaneous preterm birth. In addition, an increased risk for endometritis and mastitis postpartum was found in women with symptomatic lower urinary tract infection during pregnancy. Key Points UTIs increase the risk of preterm birth. UTIs increase the risk of endometritis postpartum. UTIs increase the risk of mastitis postpartum."
45cb7f0f35010ee9800c5512a270a24b0068e54b,"Background Compared to HIV-negative individuals, people living with HIV have a higher risk of malignant lymphoma. To what extent hematologists miss HIV testing opportunities among lymphoma patients (LP) in the Amsterdam University Medical Centers (UMC) is currently unknown. We quantified the HIV testing rate among LP and assessed factors influencing hematologists’ HIV testing behavior in LP at both Amsterdam UMC locations. Methods In this mixed-methods study, quantitative data from 2015 to 2019 from electronic health records of LP were retrospectively collected to assess both the HIV testing rate within three months around lymphoma diagnosis and the HIV positivity prevalence. An online survey among hematologists and semi-structured interviews among hematologists and authors of hematology guidelines were conducted. Results Data from 656 LP and 21 hematologists who responded to the survey out of 40 were used. Interviews were held with four hematologists and two authors of hematology guidelines. The HIV testing rates were 56.4% and 57.6%, and the HIV positivity prevalences were 0.8% and 0.9% for both locations. Eighty-five percent (18/21) of hematologists indicated they often offered an HIV test to LP in the past year, and 42% (9/21) revealed HIV testing in LP is often discussed in their department. The type of lymphoma, lack of awareness regarding HIV testing recommendations in LP, low perceived risk of HIV among LP because of older age, and lack of guidelines recommending universal HIV testing regardless of lymphoma type, each influenced hematologists’ HIV testing behavior. Conclusion Our results show routine HIV testing among LP and hematologists’ attitudes and knowledge towards testing at the Amsterdam UMC are suboptimal, despite HIV testing being recommended in most lymphoma guidelines. This highlights the need for interventions to improve hematologists’ behavior towards testing, such as education, as well as expansion and enhanced implementation of the hematology guidelines."
58ef3ddba526edea5e7df4eb9bb24447c55eeb71,
65470542b4a3e53ba2bafc629155994073fcde1d,"Received 5 February 2020; editorial decision 13 April 2020; accepted 29 April 2020; published online May 5, 2020. Members of the H-TEAM Initiative are listed in the Appendix. Correspondence: M. Dijkstra, Public Health Service Amsterdam, Nieuwe Achtergracht 100, 1018 WT, Amsterdam, The Netherlands (mdijkstra@ggd.amsterdam.nl). Clinical Infectious Diseases 2021;72(11):1952–60 © The Author(s) 2020. Published by Oxford University Press for the Infectious Diseases Society of America. This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs licence (http://creativecommons.org/licenses/ by-nc-nd/4.0/), which permits non-commercial reproduction and distribution of the work, in any medium, provided the original work is not altered or transformed in any way, and that the work is properly cited. For commercial re-use, please contact journals.permissions@oup.com DOI: 10.1093/cid/ciaa505 Decreased Time to Viral Suppression After Implementation of Targeted Testing and Immediate Initiation of Treatment of Acute Human Immunodeficiency Virus Infection Among Men Who Have Sex With Men in Amsterdam"
6c9e6b1b5db3cda7dfbc288307281a9c8d5bef4f,No abstract
6cae89a21725f788aaa5aeb266df7c4a35a749d9,
7648962c90ae19c239970e6556d56d917af712eb,"Abstract Background Various metrics of hospital antibiotic use might assist in guiding antimicrobial stewardship (AMS). Objectives To compare patient outcomes in association with three methods to measure and feedback information on hospital antibiotic use when used in developing an AMS intervention. Methods Three methods were randomly allocated to 42 clusters from 21 Dutch hospitals: (1) feedback on quantity of antibiotic use [DDD, days-of-therapy (DOT) from hospital pharmacy data], versus feedback on (2) validated, or (3) non-validated quality indicators from point prevalence studies. Using this feedback together with an implementation tool, stewardship teams systematically developed and performed improvement strategies. The hospital length of stay (LOS) was the primary outcome and secondary outcomes included DOT, ICU stay and hospital mortality. Data were collected before (February–May 2015) and after (February–May 2017) the intervention period. Results The geometric mean hospital LOS decreased from 9.5 days (95% CI 8.9–10.1, 4245 patients) at baseline to 9.0 days (95% CI 8.5–9.6, 4195 patients) after intervention (P < 0.001). No differences in effect on LOS or secondary outcomes were found between methods. Feedback on quality of antibiotic use was used more often to identify improvement targets and was preferred over feedback on quantity of use. Consistent use of the implementation tool seemed to increase effectiveness of the AMS intervention. Conclusions The decrease in LOS versus baseline likely reflects improvement in the quality of antibiotic use with the stewardship intervention. While the outcomes with the three methods were otherwise similar, stewardship teams preferred data on the quality over the quantity of antibiotic use."
85cabd40f72339e9313c9820d75052d29756034b,"Background: At border sites, and in internal organs, tissue resident memory T cells (TRM) contribute to the immune barrier against pathogens like viruses, bacteria, fungi, and cancer. However, information on the presence and function of these cells in the human kidney is scant. In order to better understand the T cell-mediated immunological defense in this organ, we aimed to determine phenotypic and functional aspects of CD8 and CD4 T cells present in healthy and allograft kidney tissue. Methods: Using multichannel flow cytometry, we assessed the phenotype and function of T cells in healthy renal tissue samples (n = 5) and kidney allograft tissue (n = 7) and compared these aspects to T cells in peripheral blood from healthy controls (n = 13). Results: Kidney tissue samples contained substantial amounts of CD8 and CD4 T cells. In contrast to the circulating cells, kidney T cells frequently expressed CD69 and CD103, and were more often actively cycling. Furthermore, nearly all kidney T cells expressed CXCR3, and often expressed CXCR6 compared to T cells in the circulation. Markedly, kidney T cells produced greater quantities of IFNγ than circulating cells and were frequently polyfunctional. Conclusion: Functional T cells with the characteristic traits of TRM reside in human kidney tissues. These cells are more often actively cycling and frequently express CXCR3 and CXCR6."
9a7d9bfe20c8f2b78edb68350bbba83215632d06,
ad0143fb80f9650e50eb6fa786499e65d019ce80,"Background Choosing Wisely aims to reduce low-value care to improve quality and lower costs. In the Netherlands, this campaign offers three recommendations for internal medicine applicable in emergency departments (EDs): (1) do not place an indwelling urinary catheter in non-critically ill patients who can void; (2) do not order plain abdominal radiographs in patients with acute abdominal pain; and (3) discuss whether treatment limitations are needed. This quality improvement project aims to increase the implementation of the recommendations by patient information leaflets. Methods In a prospective before–after study, we collected data every other week during baseline and intervention periods (both 7 months) in two university medical centres. The primary outcomes were the adherence rates to the recommendations. Results 805 patients visited the EDs for internal medicine, of whom 391 (48.6%) were hospitalised. Only 153 (19%) patients received the information leaflet. We found no change in implementation rates of the recommendations after the introduction of the patient information leaflet. In the baseline period, 28 patients received a urinary catheter, of whom 5 (17.9%) had no appropriate indication, compared with 4 (25.0%) of 16 patients in the intervention period (p=0.572). Unnecessary abdominal X-ray occurred once in the baseline period and not in the intervention period. Treatment limitations were not reported in 13 (6.5%) of 200 hospitalised patients in the baseline period, and in 17 (8.9%) of 191 patients in the intervention period (p=0.373). Conclusions Patient information leaflets did not increase the implementation of Choosing Wisely recommendations, which can be due to a high baseline rate and a poor dissemination of leaflets. Our ED seems not to be a practicable setting for dissemination of leaflets, since staff engagement was not possible due to high workload and shortage of qualified nursing staff in the Netherlands."
ae0dd2ea2668b7cd1de803196ed69708505ef224,
b2fb548564b2489f23798702e09ccec07dbe35d4,
c2b7978fb342a6c1d978892bb479f98ac01521da,"Background In the Netherlands, general practitioners (GPs) diagnose 79% of STIs and 36% of HIV infections, but opportunities for earlier HIV diagnosis are being missed in primary care. We assessed changes in GPs’ HIV testing behaviour following an educational intervention using competitive feedback, to improve HIV testing in primary care in Amsterdam. Methods The educational intervention, open for all Amsterdam GPs, was implemented from 2015 to 2020. The mean annual number of HIV tests per GP from 2011–2019 was calculated using data from diagnostic laboratories for primary care, and stratified by 4-digit postal code (PC4). Questionnaires and semi-structured interviews were conducted to identify perceived barriers and facilitators to HIV testing. Results In total, 229 GPs (42%) participated in the educational intervention. Participation varied per PC4 area (median 27%, IQR 0%-60%). At baseline, the mean annual number of HIV tests per GP was similar for participants versus non-participants (26.8 versus 24.7, respectively). The number of tests per GP declined from 2011 to 2014 from 29.5 to 20.7, and increased thereafter to 27.1 in 2019. Testing was highest in PC4 areas with highest HIV prevalence. Qualitative analyses revealed various barriers to HIV testing, including taboo and stigma, a shrinking epidemic, and financial barriers. The use of competitive feedback was perceived as a motivator to improve testing behaviour. Of 59 GPs that completed the questionnaire, 68% stated the programme provided eye-openers, and 72% declared it improved their HIV testing behaviour. Conclusion The observed increase in HIV testing coincided with the implementation of our intervention, but there was marked heterogeneity, with testing seemingly associated with local HIV prevalence. Amsterdam is well on its way towards zero new hiv infections, but it will be challenging to keep GPs engaged in proactive testing to prevent late presentations and missed opportunities for HIV diagnosis in primary care."
cd15a1ca495267770b44221e1e2065d80ee64639,"ABSTRACT During the COVID-19 pandemic, resident well-being has been shown to be at risk, which may interfere with residents’ process of professional development during their educational trajectory. Therefore, we developed a well-being program for residents, aimed to help residents maintain their well-being during the COVID-19 pandemic. We explored residents’ perceptions of their well-being as well as their perceived support of the well-being program during the COVID-19 pandemic. We invited all internal medicine residents and residents working in the ICU (N = 203) of one academic medical center to participate. The well-being program included a combination of (1) well-being measurements and (2) organizational support. The repeated well-being measurements involved a well-being survey on six measurement points from April to June 2020, and organizational support combined the provision of institutional interventions and promotion of individual strategies to help residents maintain their well-being during a pandemic. In total, 103 residents (50.1%) participated, showing that residents working in the ICU reported significantly lower levels of mental well-being than residents not working on the ICU. Furthermore, residents did not perceive the institutional interventions to benefit their well-being, while residents’ reported engagement in individual strategies was significantly positively associated with their well-being. As ICU residents reported lower levels of mental well-being, well-being programs need to address ICU-specific stressors while enhancing supervision and peer support. Furthermore, the individual strategies of the well-being program should be tailored to residents’ well-being needs as these were positively associated with resident well-being."
ce33420e6e00cd83735ee612a412e8267fec20d5,
cee9a1c746101563aebf445e6aa484594eeff3b5,"COVID‐19 is a pandemic with high morbidity and mortality. In an autopsy cohort of COVID‐19 patients, we found extensive accumulation of the tryptophan degradation products 3‐hydroxy‐anthranilic acid and quinolinic acid in the lungs, heart, and brain. This was not related to the expression of the tryptophan‐catabolizing indoleamine 2,3‐dioxygenase (IDO)‐1, but rather to that of its isoform IDO‐2, which otherwise is expressed rarely. Bioavailability of tryptophan is an absolute requirement for proper cell functioning and synthesis of hormones, whereas its degradation products can cause cell death. Markers of apoptosis and severe cellular stress were associated with IDO‐2 expression in large areas of lung and heart tissue, whereas affected areas in brain were more restricted. Analyses of tissue, cerebrospinal fluid, and sequential plasma samples indicate early initiation of the kynurenine/aryl‐hydrocarbon receptor/IDO‐2 axis as a positive feedback loop, potentially leading to severe COVID‐19 pathology. © 2021 The Authors. The Journal of Pathology published by John Wiley & Sons, Ltd on behalf of The Pathological Society of Great Britain and Ireland."
d333519f6afb112e6f60d7232f480e5e386eedb3,"Background General practitioners (GPs) and sexual health centres (SHCs) are the main providers of STI and HIV testing in the Netherlands. We compared HIV testing by GPs versus SHCs to gain insight in strategies to improve HIV testing, stimulating timely HIV diagnosis. Methods Laboratory data (2011–2018) on HIV testing by GPs and SHCs in five Dutch regions with varying levels of urbanisation were used. Mean regional HIV testing rates per 10,000 residents ≥15 years were compared between providers using negative binomial generalised additive models, and additionally stratified by sex and age (15–29y, 30–44y, 45–59y, ≥60y). Chi-squared tests were used to compare percentage positive between providers. Results Analysed data included 507,197 HIV tests (GP 36%, SHC 64%). The highest HIV testing rates and positivity were observed in highly urbanised regions, with large variation between regions. The HIV testing rates ranged from 28–178/10,000 residents by GPs and from 30–379/10,000 residents by SHCs. Testing rates by GPs were lower than by SHCs in two regions, while these rates were comparable in the others. In all regions, men were tested less by GPs than by SHCs; for women this varied per region. Among 15–29 year olds, GPs’ testing rates were lower than SHCs’, while this was reversed in older age categories. The overall mean HIV positivity was 0.5% for GPs and 0.4% for SHCs. In Amsterdam, positivity was higher among individuals tested by GPs, but in the other regions no difference was observed. Conclusion This is the first study comparing HIV testing by GPs versus SHCs using laboratory data in the Netherlands. Our results show that besides SHCs, GPs also play a key role, especially in some subgroups, but large regional variation exists. Regional-specific interventions to improve GPs’ HIV testing practices are needed for populations not attending the SHCs to ensure timely HIV diagnosis."
d4adf0e4c48874254151651f2004cbac5f5ffeac,
d7f13adb1963a86b971d93b6ef848089f47c3a6e,No abstract
ec43ab3979cd537d42b0f98c6fdbf053542542da,"Introduction Antimicrobial treatment of asymptomatic bacteriuria (ASB) is one of the most common unnecessary uses of antimicrobials. Earlier studies have shown that the prevalence of this inappropriate treatment ranges from 45% to 83%. Multifaceted interventions based on international guidelines and antimicrobial stewardship can decrease overtreatment of ASB. We have designed a study protocol with the main objective of reducing overtreatment of ASB by 50% through use of a deimplementation strategy. Methods and analysis We will use a stepped-wedge cluster randomised design, comparing outcomes before and after introduction of our intervention in the emergency department (ED) of five hospitals (clusters) in the Netherlands. All patients (≥18 years old) who have a urine test performed in the ED will be screened for eligibility. The deimplementation strategy consists of a combination of interventions, including education, audit and feedback. The primary endpoint is overtreatment of ASB in patients without risk factors (eg, pregnancy, planned invasive urological procedures and neutropenia). Secondary endpoints are the duration of antimicrobial treatment for ASB, the number of urine cultures and urinalysis per 1000 patients, and overtreatment of positive urinalysis in asymptomatic patients. Ethics and dissemination Ethical approval was obtained from the medical ethics research committee of the Academic Medical Centre (Amsterdam, the Netherlands) with a waiver for informed consent. Local feasibility was obtained by the local institutional review boards of all participating hospitals. Our study aims to reduce inappropriate screening and treatment of ASB in EDs, improve healthcare quality, lower the increase in antimicrobial resistance and save costs. If proven (cost)-effective, this study provides a well-suited strategy for a nationwide approach to reduce overtreatment of ASB. Relevant results of our study will be disseminated through publications in peer-reviewed journals and presentations at relevant (scientific) conferences. Trial registration number NL8242; Pre-results."
ed9f7f8644e2706fbcc4a23f56cd39525ee182ac,
efbae6ec3c105c9c2f44735e411fa6bc6faab03c,"Symptomatic urinary tract infections are associated with preterm birth. However, data on risk indicators for urinary tract infections are limited and outdated. The research is a secondary analysis. The study was a prospective multicenter cohort study of low-risk pregnant women. Logistic regression was used to identify risk indicators for urinary tract infections. The incidence of urinary tract infections was 9.4%. Multivariate logistic regression showed that a history of recurrent urinary tract infections and the presence of asymptomatic bacteriuria in the present pregnancy were associated with urinary tract infections (resp. OR 3.14, 95%CI 1.40–7.02 and OR 1.96 95%CI 1.27–3.03). Women with a urinary tract infection were at increased risk of preterm birth compared to women without a urinary tract infection (12 vs. 5.1%; adjusted HR 2.5 95%CI 1.8–3.5). This increased risk was not found in women with the identified risk indicators (resp. 5.3% vs. 5.1%, adjusted HR 0.35 95%CI 0.00–420 and adjusted HR 1.5 95CI% 0.59–3.9). In conclusion, in low-risk pregnant women, risk indicators for urinary tract infections are: a history of recurrent urinary tract infections and the presence of asymptomatic bacteriuria. The risk of preterm birth is increased in women with a urinary tract infection in this pregnancy. However, women with recurrent urinary tract infections and asymptomatic bacteriuria this pregnancy appear not to be at increased risk of preterm birth."
f735655262df1d9282ddf4500c5af5387dd0d4af,
f735655262df1d9282ddf4500c5af5387dd0d4af,
0300d32778b67059571a3595eadd3a59d93fccd1,
0b68eeef3f895d8bbf75c0097fe7210b6c77ef13,"Mucosal‐associated invariant T (MAIT) cells are innate‐like T‐cells that recognize bacterial riboflavin metabolites. They are present in human blood but are abundant at barrier sites, including the liver, lungs, and kidneys, where they possess a CD69+/CD103+/− tissue‐resident phenotype. In renal tissue, MAIT cells likely defend against the ascending uropathogens responsible for urinary tract infections (UTIs), which are common, especially among renal transplant recipients (RTRs). Nevertheless, the functional role for MAIT cells in renal tissue and the influence of renal transplantation on MAIT cells remains unclear. Using multiparameter flow cytometry and the MR1‐tetramer, we characterized MAIT cell phenotype and function in healthy renal tissue (n = 6), renal transplants explanted after allograft failure (n = 14) and in blood from healthy controls (n = 20) and RTRs before and 1‐year after transplantation (n = 21). MAIT cells in renal tissue constitute a distinct CD69+CD103+/− population that displays typical phenotypic features of tissue‐resident T‐cells and is skewed toward IL‐2, GM‐CSF, and IL‐17A production upon stimulation. The circulating MAIT cell population was not decreased in number in RTRs pre‐ or post‐transplantation. Tissue‐resident MAIT cells in the kidney represent a functionally distinct population. This shows how MAIT cells in the kidney may be involved in the protection against microorganisms."
1fa781d3f12dfe9460ba77365e9bbaae1853dc64,
2fad54ead8730d50b1c64311ae200c67a2f9d2af,
4530bf4898a16c37983600f1522165e082acd04a,"BACKGROUND
The Choosing Wisely campaign aims to reduce low-value care to improve quality and lower healthcare costs. Our objective was to determine the current implementation of the Choosing Wisely Netherlands campaign and the 10 recommendations (released in 2014) for internal medicine.


METHODS
We actively surveyed physicians and residents in the departments of internal medicine in 13 hospitals in the Netherlands. The survey was performed during a presentation about Choosing Wisely and we asked whether they thought that the recommendations were implemented.


RESULTS
Between May and November 2018, we surveyed 281 physicians and residents, of which we received 2625 answers (response rate 85%). We found that 178 (68.5%) of 260 physicians were unaware of the Choosing Wisely campaign. For the implementation of recommendations, 1506 (75.2%) of 2003 answers stated that physicians applied the recommendations in clinical practice. We found no differences in implementation of physicians who were aware or unaware of the campaign, respectively 529 (76.1%) of 695 versus 854 (74.2%) of 1151 of the recommendations were implemented; p = 0.357. The recommendation that was implemented least was 'Do not routinely order coagulation tests before invasive procedures', in which 28% stated that they applied this in clinical practice.


CONCLUSION
Four years after the introduction, only one-third of physicians and residents of internal medicine were aware of the Choosing Wisely Netherlands campaign. Nevertheless, most Choosing Wisely recommendations were implemented sufficiently in clinical practice. There is room for improvement, mainly in recommendations that need a multidisciplinary approach."
4b3bf8e5c6e86ada1ce6982cfa070d4675e20d19,
69ff9699be5e93f243c03dc45dbe059b9b1bc234,
6ca46a44b2192cf3d08e200a06d31bea5acc7077,"Urinary tract infection recurrence is common, particularly in women and immunocompromised patients, such as renal transplant recipients (RTRs). Mucosal‐associated invariant T (MAIT) cells play a role in the antibacterial response by recognizing bacterial riboflavin metabolites produced by bacteria such as Escherichia coli. Here, we investigated whether MAIT cells are involved in the pathogenesis of recurrent urinary tract infections (RUTIs)."
6dae02b5ccba4f078993e7e34ca07f02385a21a9,
a8a43c7919de1490046dacde5bc87ad270aadbba,
aa1ae79bdd241371d3b9a4492729cca8a4693ea5,
aa514c1269259f55d987acde8d9c2c252c48a112,"Abstract Objectives In the Netherlands, general practitioners (GPs) perform two-thirds of sexually transmitted infection (STI) consultations and diagnose one-third of HIV infections. GPs are, therefore, a key group to target to improve provider-initiated HIV testing. We describe the design and implementation of an educational intervention to improve HIV testing by Amsterdam GPs and explore trends in GPs’ testing behaviour. Methods Interactive sessions on HIV and STI using graphical audit and feedback started in 2015. Participating GPs developed improvement plans that were evaluated in follow-up sessions. Laboratory data on STI testing by Amsterdam GPs from 2011 to 2017 were collected for graphical audit and feedback and effect evaluation. The primary outcome was the HIV testing rate: number of HIV tests per 10 000 person-years (PY). Secondary endpoints were chlamydia and gonorrhoea testing rates and HIV positivity ratios. Results Since 2015, 41% of GPs participated. HIV testing rate declined from 2011 to 2014 (from 175 to 116 per 10 000 PY), more in women than men (176 to 101 versus 173 to 132), and stabilized from 2015 to 2017. The HIV positivity ratio declined from 0.8% in 2011 to 0.5% in 2017. From 2011 to 2017, chlamydia and gonorrhoea testing rates declined in women (from 618 to 477 per 10 000 PY) but remained stable in men (from 270 to 278). Conclusions The stabilization of the downward trend in HIV testing coincided with this educational intervention. Follow-up data are needed to formally assess the intervention’s impact on GP testing behaviour whilst considering contextual factors and secular trends."
b2f62e224a2b341caffdb315bc907c21306068ad,
deccdec226e4d404031653bc4ad9f9ee4fa0d2a0,"BACKGROUND
Blood cultures are essential diagnostic tools to identify pathogens in systemic infections. However, logistics of blood culture performance is often suboptimal. This study analyses the pre-analytic phase of blood culture processing through different types of risk assessments.


METHODS
We performed direct observations to gain in-depth knowledge of the root causes of suboptimal blood culture performance. These findings were summarised in a Bow-Tie chart. We then utilised a healthcare failure mode and effect analysis to prioritise failures per step in the process and to organise improvement activities. Finally, improvement actions were planned.


RESULTS
Not obtaining a second set of blood cultures in the logistics of blood culture performance had the highest priority for action. Several failure modes, including human and system factors, were identified. Improvement actions included training and clinical lessons for nurses in the emergency department, updating hospital search engines to ease identification of relevant protocols, and an evaluation of the workload at the emergency department. Failure modes caused by human factors appear easy to address, however changing human behaviour is challenging.


CONCLUSIONS
The analysis provided useful insight into the different steps in the logistics of blood culture performance and facilitated the organisation of actions focused on addressing the most urgent root causes."
0acdaa00b1c2757d991cb247ee6d734ae07025ff,
17ff2c043df801cf4898818ed95476ebae704851,
3b50f728a99ce7d9eeb906a166c478f42fc44f28,
4322a9c3a6688cd789ea96601b320e098f7be3f5,
48149a4f62c0f36cc31304546a2e499fd56d1ec9,
4d6dc6b8390cc422a58887d1894d529ec14aa10a,
5454df2f469c8af67b2a4ea6e594f1f19f83d77d,"Objective Patient handovers are often delayed, patients are hardly involved in their discharge process and hospital-wide standardised discharge procedures are lacking. The aim of this study was to implement a structured discharge bundle and to test the effect on timeliness of medical and nursing handovers, length of hospital stay (LOS) and unplanned readmissions. Design Interrupted time series with six preintervention and six postintervention data collection points (September 2015 to June 2017). Setting Internal medicine and surgical wards Participants Patients (≥18 years) admitted for more than 48 hours to surgical or internal medicine wards. Intervention The Transfer Intervention Procedure (TIP), containing four elements: planning the discharge date within 48 hours postadmission; arrangements for postdischarge care; preparing handovers and personalised patient discharge letter; and a discharge conversation 12–24 hours before discharge. Outcome measures The number of medical and nursing handovers sent within 24 hours. Secondary outcomes were median time between discharge and medical handovers, LOS and unplanned readmissions. Results Preintervention 1039 and postintervention 1052 patient records were reviewed. No significant change was observed in the number of medical and nursing handovers sent within 24 hours. The median (IQR) time between discharge and medical handovers decreased from 6.15 (0.96–15.96) to 4.08 (0.33–13.67) days, but no significant difference was found. No intervention effect was observed for LOS and readmission. In subgroup analyses, a reduction of 5.6 days in the median time between discharge and medical handovers was observed in hospitals with high protocol adherence and much attention for implementation. Conclusion Implementation of a structured discharge bundle did not lead to improved timeliness of patient handovers. However, large interhospital variation was observed and an intervention effect on the median time between discharge and medical handovers was seen in hospitals with high protocol adherence. Future interventions should continue to create awareness of the importance of timely handovers. Trial registration number NTR5951; Results."
58ba133ca4d78350f2fc770ae97cd0fe4f5fa2b6,
5dd38915dd1a5e1715a787edbafc9c3a7fe493f7,"Asymptomatic bacteriuria (ASB) is a common finding in many populations, including healthy women and persons with underlying urologic abnormalities. The 2005 guideline from the Infectious Diseases Society of America recommended that ASB should be screened for and treated only in pregnant women or in an individual prior to undergoing invasive urologic procedures. Treatment was not recommended for healthy women; older women or men; or persons with diabetes, indwelling catheters, or spinal cord injury. The guideline did not address children and some adult populations, including patients with neutropenia, solid organ transplants, and nonurologic surgery. In the years since the publication of the guideline, further information relevant to ASB has become available. In addition, antimicrobial treatment of ASB has been recognized as an important contributor to inappropriate antimicrobial use, which promotes emergence of antimicrobial resistance. The current guideline updates the recommendations of the 2005 guideline, includes new recommendations for populations not previously addressed, and, where relevant, addresses the interpretation of nonlocalizing clinical symptoms in populations with a high prevalence of ASB."
6ba269751004b4176881199e7a9a6c9af1f78ab1,
a74d259a3bad9ecaa814992491c4f7e75dfd7157,"Received 29 November 2018; editorial decision 20 December 2018; accepted 27 December 2018; published online March 21, 2019. The guidelines represent the proprietary and copyrighted property of the Infectious Diseases Society of America (IDSA). Copyright 2018 IDSA. All rights reserved. No part of these guidelines may be reproduced, distributed, or transmitted in any form or by any means, including photocopying, recording, or other electronic or mechanical methods, without the prior written permission of IDSA. Permission is granted to physicians and healthcare providers solely to copy and use the guidelines in their professional practices and clinical decision making. No license or permission is granted to any person or entity, and prior written authorization by IDSA is required, to sell, distribute, or modify the guidelines, or to make derivative works of or incorporate the guidelines into any product, including but not limited to clinical decision support software or any other software product. Any person or entity desiring to use the guidelines in any way must contact IDSA for approval in accordance with IDSA’s terms and conditions of third party-use, in particular, any use of the guidelines in any software product."
aee5c5fa42ec9bf9cab3c4947c4cd4c59e4695e8,"Background: Chronic kidney disease (CKD) is associated with a decreased intestinal barrier function, causing bacterial translocation over the intestinal wall and triggering a systemic inflammatory response. Butyrate, a short-chain fatty acid produced by certain bacterial strains, is considered instrumental to keep the intestinal barrier intact. There are indications that a decreased amount of these specific bacterial species is part of the cause of the decreased intestinal barrier function in CKD. The aim of this study is (i) to determine if Dutch patients with end-stage renal disease (ESRD) have a decreased amount of butyrate-producing species and butyrate-producing capacity and (ii) whether this correlates with systemic inflammation. Methods: We used qPCR to evaluate the most abundant butyrate-producing species F. prauznitzii, E. rectale and Roseburia spp. and the BCoAT gene, which reflects the butyrogenic capacity of the intestinal microbiota. Fecal samples were collected from healthy kidney donors (n=15), preemptive renal transplant recipients (n=4) and dialysis patients (n=31). Markers of inflammation (CRP and IL-6) and intestinal permeability (D-lactate) were measured in plasma. Results: Patients with ESRD did not have a significantly decreased amount F. prauznitzii, E. rectale and Roseburia spp. or the BCoAT gene. Neither was there a significant correlation with CRP, IL-6 or D-lactate. On the individual level, there were some patients with decreased BCoAT levels and increased levels of CRP, IL-6 and D-lactate. Conclusions: Patients with ESRD do not have a decreased amount of the most abundant butyrate-producing species nor a decreased butyrate-producing capacity."
e63b9167af11920189bf94ab6c08c4b376ded5dc,
fffdbf30ae2f3368cb46dcfe663bdce10aa5139c,"Asymptomatic bacteriuria (ASB) is a common finding in many populations, including healthy women and persons with underlying urologic abnormalities. The 2005 guideline from the Infectious Diseases Society of America recommended that ASB should be screened for and treated only in pregnant women or in an individual prior to undergoing invasive urologic procedures. Treatment was not recommended for healthy women; older women or men; or persons with diabetes, indwelling catheters, or spinal cord injury. The guideline did not address children and some adult populations, including patients with neutropenia, solid organ transplants, and nonurologic surgery. In the years since the publication of the guideline, further information relevant to ASB has become available. In addition, antimicrobial treatment of ASB has been recognized as an important contributor to inappropriate antimicrobial use, which promotes emergence of antimicrobial resistance. The current guideline updates the recommendations of the 2005 guideline, includes new recommendations for populations not previously addressed, and, where relevant, addresses the interpretation of nonlocalizing clinical symptoms in populations with a high prevalence of ASB."
00eef2781a4154c0199b97e388a1af364f3d3c53,
0144dc3f63b891108919afabc917ceec37e6dffa,Direct‐acting antivirals (DAAs) for treatment of chronic hepatitis C virus (HCV) infection can cause drug–drug interactions (DDIs) with combination antiretroviral therapy (cART) and non‐cART co‐medication. We mapped how physicians manage DDIs between DAAs and co‐medication and analysed treatment outcomes.
25f1f82b9509906d2485c408f244b3494b42ecea,
2d4b21463169a0233203b9eb86c0f22c69fe081a,"Objectives: The health-related quality of life (HRQOL) of people with HIV is lower than in the general population, but it is unknown how it compares with that of persons with other chronic medical conditions. We compared HRQOL in HIV with HRQOL in diabetes mellitus type 1, diabetes mellitus type 2 and rheumatoid arthritis (RA). In addition, we investigated factors associated with HRQOL in HIV. Design: Cross-sectional study. Methods: HRQOL was measured with the Medical Outcomes Study Short Form 36-item Health Survey in a nationwide sample of people with HIV in care in the Netherlands and on combination antiretroviral therapy for at least 6 months. We added data from studies in diabetes mellitus types 1 and 2, and RA. Logistic regression analysis was used to examine: the association between disease group and a poor HRQOL, and patient factors associated with poor HRQOL in HIV. Results: The odds of a poor physical HRQOL in the HIV group were comparable with the odds in diabetes mellitus types 1 and 2, but lower than in RA patients. The odds of a poor mental HRQOL in HIV were higher than in the other groups. In HIV, a history of AIDS, longer duration of combination antiretroviral therapy and severe comorbidity were associated with a poor physical HRQOL. Sub-Saharan African descent and CD4+ cell count of less than 350 cells/&mgr;l were associated with poor mental HRQOL. Conclusion: People with HIV were more likely to have a poor mental HRQOL than patients with other chronic conditions. Addressing mental health should be an integral part of outpatient HIV care."
6c2260d61a80c8fbefc0350412b0118dec103a2b,
6c685e3cafbb44ad60c4f7903674db9323a66670,"Abstract Background Catheter-associated urinary tract infection (UTI) and catheter-associated bloodstream infection (BSI) are common healthcare-associated infections (HAI). Therefore, catheters should only be used if indicated. However, based on the literature up to 65% of the urinary catheters and 56% of the peripheral intravenous catheters have an inappropriate indication. So, an efficient way to reduce HAIs is to avoid unnecessary use of catheters. Our quality improvement project aims to reduce unnecessary use of catheters. Methods In a multicenter, interrupted time series study, several interventions to avoid inappropriate use of catheters were carried out in internal medicine and nonsurgical subspecialty wards in seven hospitals in the Netherlands. The indications for catheter use were based on (inter)national guidelines. The primary endpoint is the percentage of inappropriate indications on the day of data collection. Secondary endpoints are catheter-associated infections, length of hospital stay and mortality. Data were collected once per 2 weeks during baseline (7 months) and post-intervention (7 months). Preliminary analyses compared incidence rates before and after the intervention. Results Data were obtained from 5,691 observed patients. The rate of inappropriate use of urinary catheters decreased from 32.1 to 23.7% (incidence rate ratio 0.74, 95% CI 0.58–0.94, P = 0.013), and inappropriate use of peripheral intravenous catheters decreased from 22.0 to 15.2% (incidence rate ratio 0.69, 95% CI 0.60–0.80, P < 0.001). The overall urinary and intravenous catheter use was stable, resp. 12.2% (n = 324) to 12.5% (n = 380) and 62.8% (n = 1,665) to 62.1% (n = 1,887). Most inappropriate indications were due to prolonged catheter use. The indications which expire frequently are”Accurate measurements of urinary output in critically ill patients’ for urinary and”IV fluids and antibiotic therapy’ for intravenous catheters. Subsequent analyses will take into account the interrupted time series design, and evaluate catheter-associated UTI and BSI rates. Conclusion Our de-implementation strategy reduces unnecessary use of urinary and intravenous catheters in non-ICUs. It is important to increase awareness for inappropriate use of catheters. Disclosures S. E. Geerlings, Nordic Pharma: Consultant and Fosfomycin iv, consulting fee."
82796c2c80ac579f7aefe6bb425801b74f20149f,"QUALITY PROBLEM
Unplanned hospital readmissions frequently occur and have profound implications for patients. This study explores chronically ill patients' experiences and perceptions of being discharged to home and then acutely readmitted to the hospital to identify the potential impact on future care transition interventions.


INITIAL ASSESSMENT AND IMPLEMENTATION
Twenty-three semistructured interviews were conducted with chronically ill patients who had an unplanned 30-day hospital readmission at a university teaching hospital in the Netherlands.


CHOICE OF SOLUTION
A constructive grounded theory approach was used for data analysis.


EVALUATION
The core category identified was 'readiness for hospital discharge,' and the categories related to the core category are 'experiencing acute care settings' and 'outlook on the recovery period after hospital discharge.' Patients' readiness for hospital discharge was influenced by the organization of hospital care, patients' involvement in decision-making and preparation for discharge. The experienced difficulties during care transitions might have influenced patients' ability to cope with challenges of recovery and dependency on others.


LESSONS LEARNED
The results demonstrated the importance of assessing patients' readiness for hospital discharge. Health care professionals are recommended to recognize patients and guide them through transitions of care. In addition, employing specifically designated strategies that encourage patient-centered communication and shared decision-making can be vital in improving care transitions and reduce hospital readmissions. We suggest that health care professionals pay attention to the role and capacity of informal caregivers during care transitions and the recovery period after hospital discharge to prevent possible postdischarge problems."
9b0f39b8327945eb89c8cec718f4cbe67fbc4938,
e7f4d4c178ca9b3979d518a38b27f6df11f8842b,"Background
The Netherlands has provided unrestricted access to direct-acting antivirals (DAAs) since November 2015. We analyzed the nationwide hepatitis C virus (HCV) treatment uptake among patients coinfected with human immunodeficiency virus (HIV) and HCV.


Methods
Data were obtained from the ATHENA HIV observational cohort in which >98% of HIV-infected patients ever registered since 1998 are included. Patients were included if they ever had 1 positive HCV RNA result, did not have spontaneous clearance, and were known to still be in care. Treatment uptake and outcome were assessed. When patients were treated more than once, data were included from only the most recent treatment episode. Data were updated until February 2017. In addition, each treatment center was queried in April 2017 for a data update on DAA treatment and achieved sustained virological response.


Results
Of 23574 HIV-infected patients ever linked to care, 1471 HCV-coinfected patients (69% men who have sex with men, 15% persons who [formerly] injected drugs, and 15% with another HIV transmission route) fulfilled the inclusion criteria. Of these, 87% (1284 of 1471) had ever initiated HCV treatment between 2000 and 2017, 76% (1124 of 1471) had their HCV infection cured; DAA treatment results were pending in 6% (92 of 1471). Among men who have sex with men, 83% (844 of 1022) had their HCV infection cured, and DAA treatment results were pending in 6% (66 of 1022). Overall, 187 patients had never initiated treatment, DAAs had failed in 14, and a pegylated interferon-alfa-based regimen had failed in 54.


Conclusions
Fifteen months after unrestricted DAA availability the majority of HIV/HCV-coinfected patients in the Netherlands have their HCV infection cured (76%) or are awaiting DAA treatment results (6%). This rapid treatment scale-up may contribute to future HCV elimination among these patients."
ef61ce3ce4efcea651f51dc73266776c994505af,"Abstract Background Mucosal associated invariant T (MAIT) cells are innate-like T-cells involved in the antibacterial and fungal response by recognizing riboflavin metabolites produced by these organisms. MAIT cells are present in blood and are highly abundant in the mucosa of the liver, lungs and intestines. In murine models of urinary tract infection (UTI), MAIT cells appear to migrate to the bladder and decrease the bacterial load. It is however unknown whether MAIT cells reside in the human urogenital tract and renal tissue and whether they play a role in the first-line defense against (recurrent) UTI (RUTI). Methods We used a fluorescently labelled MR1-tetramer in conjunction with 14-color flowcytometry to identify and characterize MAIT cells in renal allografts after allograft failure caused by RUTI (n = 6) or rejection (n = 6) and in healthy kidney tissue surgically removed because of renal cell carcinoma (adjacent nontumorous tissue) (n = 5). Results The mean percentage of MAIT cells within the lymphogate was higher in the RUTI kidneys (2.24%) compared with the rejection kidneys (0.14%) and the control kidneys (0.11%) (P < 0.05). Characterization of MAIT cells was impossible in some control samples due to MAIT cells counts <25 (predefined cutoff value), therefore the control group was excluded from further statistical analysis. MAIT cells in RUTI kidneys appear to have a less activated profile compared with the rejection kidneys, with a lower expression of Ki67 (P < 0.01). Though the expression of the tissue resident marker CD69/CD103 was higher in 4/6 RUTI kidneys, this difference was not significant. Conclusion MAIT cells are present in renal tissue that is or has been subjected to an immunologic response. MAIT cells in RUTI kidneys display a more quiescent and in some samples more tissue resident phenotype than MAIT cells in rejection kidneys. These findings may suggest that (I) MAIT cells play a role in the first-line defense in the kidney and (II) that after RUTI, MAIT cells remain in renal tissue in a quiescent state. We postulate that this might be favorable in case of a second hit from an uropathogen.Figure 1. Presence and characterization of MAIT cells in renal tissue. Disclosures F. Bemelman, Astellas: We received an unrestricted grant from Astellas to establish a Biobank for patients with renal diseases. The samples described in this abstract are obtained from this Biobank., Grant recipient."
f09841a19cfc900715cd570c146bd5f646b76030,
f2301c8c6a169c95e7a5f230b1bc249928359c31,"The population of people living with HIV is ageing. As a result, an increasing proportion of people on combination antiretroviral therapy will experience comorbidities and polypharmacy, with the risk of drug-drug interactions. These comorbidities will also be treated by physicians who are not specialised in HIV. Moreover, early diagnosis and treatment improve the prognosis of HIV infection, but 11% of the people living with HIV are currently undiagnosed. Therefore, physicians should be alert to the possibility of HIV also with regard to older adults. Since risk assessment may be challenging, testing for HIV upon diagnosis of an indicator condition could prove a useful strategy to enhance earlier diagnosis for all physicians."
0ec81c2e62bdc0d39ecee108fac09067d8564551,"Background: Asymptomatic bacteriuria (ASB) and urinary tract 
infections (UTI) during pregnancy may contribute to adverse pregnancy outcomes. Diabetes 
mellitus (DM) and gestational diabetes mellitus (GDM) are considered to be 
important additional risk factor for ASB and UTI during pregnancy. Aims: To investigate differences in prevalence of ASB and incidence of UTI in 
pregnant women with and without DM and GDM to inform ASB screening and 
treatment policies. Methods: Data from 214 pregnant women who gave birth 
during 2010 at the Women’s and Children’s Hospital, Adelaide, Australia where 
cases were women with a clinical diagnosis of (G)DM and controls were matched 
on date of birth. ASB was defined as the growth of at least 10e5 colony forming 
units/ml of one organism or any presence of group B streptococcus (GBS) at the 
first urine culture collected during pregnancy without complaints of a UTI. A 
clinical UTI was diagnosed by the treating physician, in combination with a 
positive urine culture it was defined as culture-confirmed UTI. Results: No significant differences in prevalence of ASB (5.6% and 3.7%; relative risk 
(RR) 1.50; 95% confidence intervals (CI) 0.44 - 5.17), 
incidence of clinical UTI (4.7% and 11.2%; RR 0.42; 95% CI 0.15 - 1.14) 
or culture-confirmed UTI (2.8% and 3.7%; RR 0.75; 
95% CI 0.17 - 3.27) between pregnant women with and without (G)DM were present. No 
association was found between ASB and UTI. GBS was the most common causative 
organism of ASB in women with and without DM (66.7% and 50.0%). Conclusion: In contrast with earlier research, no 
significant differences in prevalence of ASB or incidence of UTI was found 
between pregnant women with and without (G)DM."
25fe6b7bbd6916d827e58e9926a8278093a42b81,
29ff310bd7104871aa3936996a1e8b8844b07380,"When patients are transferred from the hospital to other health care settings, responsibility for the patient is transferred from the treating physician, nurse, paramedic or pharmacist at the hospital to the next health care provider. Good patient handovers from hospital to other healthcare settings are essential to ensure continuity of care. However, handovers are often delayed or incomplete and the patient is barely involved in her or his own transfer. Risks related to an incomplete handover may be considerable. More than half of the preventable adverse events that occur after discharge are attributable to ineffective communication between hospital and other healthcare providers. Through the implementation of some adjustments, the discharge process can become a point of focus during a patient's hospital stay. Examples to improve patient handovers are standardizing the discharge process and the content of patient handovers, planning a target discharge date, starting the collection of transfer data on time and involving the patient in her or his transfer."
4402cb3f10b748d72d2ac13673a3b7fabd5d8671,"Objectives
An antibiotic checklist was introduced in nine Dutch hospitals to improve appropriate antibiotic use. We estimated the cost-effectiveness of checklist use.


Methods
We compared 853 patients treated with an antibiotic before checklist introduction (usual care group) with 1207 patients treated after introduction (checklist group). We calculated the change of costs between these groups per unit effect [incremental cost-effectiveness ratio (ICER)]: per extra patient receiving appropriate treatment; and per day reduction in length of hospital stay (LOS). We also calculated the benefit-to-cost ratio per day reduction in LOS. Finally, we estimated the number of checklists after which the expected benefits would compensate for costs in one hospital.


Results
The cost of checklist use per patient was €10.10. Of the usual care patients, 48.8% received appropriate antibiotic treatment compared with 67.5% of the checklist patients (+18.7%). The ICER was €54.01 (1010/18.7) per extra patient with appropriate treatment. In a model calculation the expected effect of appropriate antibiotic use was a reduction in LOS of 1.05 days, which was extrapolated to a reduction of 19.64 hospital days per 100 patients. The ICER was €51.43 (1010/19.64) per day reduction in LOS. The estimated benefit of a 1 day reduction was €611. The benefit-to-cost ratio was 11.9 (611/51.43) per day reduction in LOS, indicating a cost saving of €12 for every euro spent on checklist use. The benefits would compensate for costs after use of 11 checklists.


Conclusions
Efforts for further implementation of the antibiotic checklist can be justified by potential economic benefits."
4435f18fc4e343b7fb281946931379271555ef37,"Objective To evaluate whether ethnicity is independently associated with vaginal microbiota (VMB) composition in women living in Amsterdam, the Netherlands, as has been shown for American women. Methods Women (18–34 years, non-pregnant, N = 610) representing the six largest ethnic groups (Dutch, African Surinamese, South-Asian Surinamese, Turkish, Moroccan, and Ghanaian) were sampled from the population-based HELIUS study. Sampling was performed irrespective of health status or healthcare seeking behavior. DNA was extracted from self-sampled vaginal swabs and sequenced by Illumina MiSeq (16S rRNA gene V3-V4 region). Results The overall prevalence of VMBs not dominated by lactobacilli was 38.5%: 32.2% had a VMB resembling bacterial vaginosis and another 6.2% had a VMB dominated by Bifidobacteriaceae (not including Gardnerella vaginalis), Corynebacterium, or pathobionts (streptococci, staphylococci, Proteus or Enterobacteriaceae). The most prevalent VMB in ethnically Dutch women was a Lactobacillus crispatus-dominated VMB, in African Surinamese and Ghanaian women a polybacterial G. vaginalis-containing VMB, and in the other ethnic groups a L. iners-dominated VMB. After adjustment for sociodemographic, behavioral and clinical factors, African Surinamese ethnicity (adjusted odds ratio (aOR) 5.1, 95% confidence interval (CI) 2.1–12.0) and Ghanaian ethnicity (aOR 4.8, 95% CI 1.8–12.6) were associated with having a polybacterial G. vaginalis-containing VMB, and African Surinamese ethnicity with a L. iners-dominated VMB (aOR 2.8, 95% CI 1.2–6.2). Shorter steady relationship duration, inconsistent condom use with casual partners, and not using hormonal contraception were also associated with having a polybacterial G. vaginalis-containing VMB, but human papillomavirus infection was not. Other sexually transmitted infections were uncommon. Conclusions The overall prevalence of having a VMB not dominated by lactobacilli in this population-based cohort of women aged 18–34 years in Amsterdam was high (38.5%), and women of sub-Saharan African descent were significantly more likely to have a polybacterial G. vaginalis-containing VMB than Dutch women independent of modifiable behaviors."
44a0b98aabcede43c3966da53bbca4d0e93a4144,
4ba3f06f3a6705d511ea4c340da4470bb454bc33,
54bfca3ff0990748054e582dc97cd88c9a5d5eb2,
55438088e39a18c22299e1917fb87f45cb37fc0f,"BACKGROUND
In the Netherlands a substantial proportion of newly diagnosed human immunodeficiency virus (HIV) patients present late for care and an estimated 12-34% of people living with HIV are undiagnosed. Linkage to care of these patients is important to decrease HIV transmission and to improve individual patient outcomes. We investigated if non-targeted HIV testing in emergency departments is a useful and cost-effective way to identify these patients.


METHODS
In a cross-sectional multicentre study, eligible adult patients who underwent phlebotomy were given an active choice to be additionally tested for HIV. In a subset of patients, risk factors for HIV infection were asked for. A cost-effectiveness analysis was conducted.


RESULTS
Of 7577 eligible patients, 3223 patients were tested, and two new HIV infections were diagnosed (0.06%). Both patients had risk factors for HIV infection. Non-targeted HIV testing in the emergency department was not considered cost-effective, with a cost per quality adjusted life years gained of € 77,050, more than triple the Dutch cost-effectiveness threshold of € 20,000.


CONCLUSION
Non-targeted HIV testing in emergency departments in the Netherlands had a low yield of newly diagnosed HIV infections and was not cost-effective. Our data suggest that targeted HIV testing may offer an alternative approach to decrease the number of undiagnosed people living with HIV."
645eb67e2f8124c2db10675e01078089f6a00646,"Background Uncomplicated Urinary Tract Infections (UTIs) are common in primary care resulting in substantial costs. Since antimicrobial resistance against antibiotics for UTIs is rising, accurate diagnosis is needed in settings with low rates of multidrug-resistant bacteria. Objective To compare the cost-effectiveness of different strategies to diagnose UTIs in women who contacted their general practitioner (GP) with painful and/or frequent micturition between 2006 and 2008 in and around Amsterdam, The Netherlands. Methods This is a model-based cost-effectiveness analysis using data from 196 women who underwent four tests: history, urine stick, sediment, dipslide, and the gold standard, a urine culture. Decision trees were constructed reflecting 15 diagnostic strategies comprising different parallel and sequential combinations of the four tests. Using the decision trees, for each strategy the costs and the proportion of women with a correct positive or negative diagnosis were estimated. Probabilistic sensitivity analysis was used to estimate uncertainty surrounding costs and effects. Uncertainty was presented using cost-effectiveness planes and acceptability curves. Results Most sequential testing strategies resulted in higher proportions of correctly classified women and lower costs than parallel testing strategies. For different willingness to pay thresholds, the most cost-effective strategies were: 1) performing a dipstick after a positive history for thresholds below €10 per additional correctly classified patient, 2) performing both a history and dipstick for thresholds between €10 and €17 per additional correctly classified patient, 3) performing a dipstick if history was negative, followed by a sediment if the dipstick was negative for thresholds between €17 and €118 per additional correctly classified patient, 4) performing a dipstick if history was negative, followed by a dipslide if the dipstick was negative for thresholds above €118 per additional correctly classified patient. Conclusion Depending on decision makers’ willingness to pay for one additional correctly classified woman, the strategy consisting of performing a history and dipstick simultaneously (ceiling ratios between €10 and €17) or performing a sediment if history and subsequent dipstick are negative (ceiling ratios between €17 and €118) are the most cost-effective strategies to diagnose a UTI."
7bb948e9c099923795e8131f34b2029ef03488cb,"AIM
Increasing antimicrobial resistance (AMR) requires rapid surveillance tools, such as Lot Quality Assurance Sampling (LQAS).


MATERIALS & METHODS
LQAS classifies AMR as high or low based on set parameters. We compared classifications with the underlying true AMR prevalence using data on 1335 Escherichia coli isolates from surveys of community-acquired urinary tract infection in women, by assessing operating curves, sensitivity and specificity.


RESULTS
Sensitivity and specificity of any set of LQAS parameters was above 99% and between 79 and 90%, respectively. Operating curves showed high concordance of the LQAS classification with true AMR prevalence estimates.


CONCLUSION
LQAS-based AMR surveillance is a feasible approach that provides timely and locally relevant estimates, and the necessary information to formulate and evaluate guidelines for empirical treatment."
7bcdb041e33ad2d35abc50b3af206f74a52c5dbf,
82fd136c9d84f237e895a6d3a7cc2485ffa6b7a7,
84e10cb224225e5ff874f1a8fdd9eafa24033207,"Background In 2008, a bundle of care to prevent Surgical Site Infections (SSIs) was introduced in the Netherlands. The bundle consisted of four elements: antibiotic prophylaxis according to local guidelines, no hair removal, normothermia and ‘hygiene discipline’ in the operating room (i.e. number of door movements). Dutch hospitals were advised to implement the bundle and to measure the outcome. This study’s goal was to assess how effective the bundle was in reducing SSI risk. Methods Hospitals assessed whether their staff complied with each of the bundle elements and voluntary reported compliance data to the national SSI surveillance network (PREZIES). From PREZIES data, we selected data from 2009 to 2014 relating to 13 types of surgical procedures. We excluded surgeries with missing (non)compliance data, and calculated for each remaining surgery with reported (non)compliance data the level of compliance with the bundle (that is, being compliant with 0, 1, 2, 3, or 4 of the elements). Subsequently, we used this level of compliance to assess the effect of bundle compliance on the SSI risk, using multilevel logistic regression techniques. Results 217 489 surgeries were included, of which 62 486 surgeries (29%) had complete bundle reporting. Within this group, the SSI risk was significantly lower for surgeries with complete bundle compliance compared to surgeries with lower compliance levels. Odds ratios ranged from 0.63 to 0.86 (risk reduction of 14% to 37%), while a 13% risk reduction was demonstrated for each point increase in compliance-level. Sensitivity analysis indicated that due to analysing reported bundles only, we probably underestimated the total effect of implementing the bundle. Conclusions This study demonstrated that adhering to a surgical care bundle significantly reduced the risk of SSIs. Reporting of and compliance with the bundle compliance can, however, still be improved. Therefore an even greater effect might be achieved."
8e434b71fbcd6ff9095d67f2ac73befda2ef04ec,"The increasing use of antibiotics is the main driving force behind the rise of antibiotic resistance. Furthermore, there is a large variation in antibiotic use amongst prescribers. We describe the current duration of antibiotic therapy for common infections in the Netherlands and the new studies we can expect in this field in the years to come. We think that more research is needed to determine the duration of antibiotic therapy on the basis of different patient characteristics. It has, for example, recently been shown that the sex of the patient plays an important role in optimal duration of therapy for febrile urinary tract infections. Therefore, it is important to identify patients who fail on shorter courses of antibiotics in order to avoid overtreatment of all patients and to reduce the use of antibiotics in the future"
97d03e456ec994385915635cc6fd58b6771ccbfc,"OBJECTIVE
To map regions of the Netherlands with high HIV prevalence for surveillance and prevention purposes.


METHOD
Information on numbers of HIV patients receiving clinical care on 31 December 2014 per postcode region was requested from the HIV monitoring foundation (SHM). These details were related to data from Statistics Netherlands on the number of residents per municipal area or district with the aid of a geographic information system (GIS).


RESULTS
Distribution mapping showed that ten municipal areas in the Netherlands have an HIV prevalence of 2 or more per 1000 residents aged 15-60 years. We discovered the highest prevalence in Amsterdam (8.1) and suburbs, Rotterdam (3.4), The Hague (2.7) and Arnhem (2.5). Large differences were seen between districts, particularly in Amsterdam where HIV was concentrated within two districts: Central Amsterdam (9-28) and Amsterdam Southeast (5-20). In Rotterdam and The Hague, HIV prevalence rates are lower and differences between districts are smaller.


CONCLUSION
Geographical analyses show differences in HIV prevalence for municipal areas and districts in big cities in the Netherlands. These data can be used for new interventions, to better focus HIV detection."
9b0d492ce62fc0c4bd45fdd2e4d407d89f4c483d,
a38e1afef1347bd06bc3bd2659bfed8748355605,
ab12a5c3d61f7c415e9d27021c7242bab06e2989,"OBJECTIVE Surveillance is an important strategy to reduce the incidence of surgical site infections (SSIs). We investigated whether prior, multiple-, or repetitive surgeries are risk factors for SSI and whether they should be preserved in the protocol of the Dutch national SSI surveillance network. METHODS Dutch national SSI surveillance data 2012–2015 were selected, including 34 commonly performed procedures from 8 major surgical specialties. Definitions of SSIs followed international standardized criteria. We used multivariable multilevel logistic regression techniques to evaluate whether prior, multiple-, or repetitive procedure(s) are risk factors for SSIs. We considered surgeries clustered within partnerships of medical specialists and within hospitals (random effects) and different baseline risks between surgical specialties (fixed effects). Several patient and surgical characteristics were considered possible confounders and were included where necessary. We performed analyses for superficial and deep SSIs combined as well as separately. RESULTS In total, 115,943 surgeries were reported by 85 hospitals; among them, 2,960 (2.6%) resulted in SSIs (49.3% deep SSIs). The odds ratio (OR) for having prior surgery was 0.94 (95% confidence interval [CI], 0.74–1.20); the OR for repetitive surgery was 2.39 (95% CI, 2.06–2.77); and the OR for multiple surgeries was1.27 (95% CI, 1.07–1.51). The latter effect was mainly caused by prolonged duration of surgery. CONCLUSIONS Multiple- and repetitive surgeries significantly increased the risk of an SSI, whereas prior surgery did not. Therefore, prior surgery is not an essential data item to include in the national SSI surveillance network. The increased risk of SSIs for multiple surgeries was mainly caused by prolonged duration of surgery, therefore, it may be sufficient to report only duration of surgery to the surveillance network, instead of both (the variables duration of surgery and multiple surgeries). Infect Control Hosp Epidemiol 2017;38:1298–1305"
b9238c573f1dc193f3ea4e56f83f495f2a7506ca,"Objectives: Urinary tract infections (UTIs) are a common reason for empirical treatment with broad-spectrum antibiotics worldwide. However, population-based antimicrobial resistance (AMR) prevalence data to inform empirical treatment choice are lacking in many regions, because of limited surveillance capacity. We aimed to assess the prevalence of AMR to commonly used antimicrobial drugs in Escherichia coli and Klebsiella pneumoniae isolated from patients with community- or healthcare-associated UTIs on two islands of Indonesia. Methods: We performed a cross-sectional patient-based study in public and private hospitals and clinics between April 2014 and May 2015. We screened patients for symptoms of UTIs and through urine dipstick analysis. Urine culture and susceptibility testing were supported by telemicrobiology and interactive virtual laboratory rounds. Surveillance data were entered in forms on mobile phones. Results: Of 3424 eligible patients, 3380 (98.7%) were included in the final analysis, and yielded 840 positive cultures and antimicrobial susceptibility data for 657 E. coli and K. pneumoniae isolates. Fosfomycin was the single oral treatment option with resistance prevalence <20% in both E. coli and K. pneumoniae in community settings. Tigecycline and fosfomycin were the only options for treatment of catheter-associated UTIs with resistance prevalence <20%, whilst the prevalence of resistance to meropenem was 21.3% in K. pneumoniae. Conclusions: Patient-based surveillance of AMR in E. coli and K. pneumoniae causing UTIs indicates that resistance to the commonly available empirical treatment options is high in Indonesia. Smart AMR surveillance strategies are needed to inform policy makers and to guide interventions."
d42c5fb77ef879342f2f42d550d083906c9d9d0f,"OBJECTIVE
To map regions of the Netherlands with high HIV prevalence for surveillance and prevention purposes.


METHOD
Information on numbers of HIV patients receiving clinical care on 31 December 2014 per postcode region was requested from the HIV monitoring foundation (SHM). These details were related to data from Statistics Netherlands on the number of residents per municipal area or district with the aid of a geographic information system (GIS).


RESULTS
Distribution mapping showed that ten municipal areas in the Netherlands have an HIV prevalence of 2 or more per 1000 residents aged 15-60 years. We discovered the highest prevalence in Amsterdam (8.1) and suburbs, Rotterdam (3.4), The Hague (2.7) and Arnhem (2.5). Large differences were seen between districts, particularly in Amsterdam where HIV was concentrated within two districts: Central Amsterdam (9-28) and Amsterdam Southeast (5-20). In Rotterdam and The Hague, HIV prevalence rates are lower and differences between districts are smaller.


CONCLUSION
Geographical analyses show differences in HIV prevalence for municipal areas and districts in big cities in the Netherlands. These data can be used for new interventions, to better focus HIV detection."
e64ed1c8f4346a046050c7b17f03359167205e46,"ABSTRACT Interprofessional communication and collaboration during hospitalisation is critically important to provide safe and effective care. Clinical rounds are an essential interprofessional process in which the clinical problems of patients are discussed on a daily basis. The objective of this exploratory study was to identify healthcare professionals’ perspectives on the “ideal” interprofessional round for patients in a university teaching hospital. Three focus groups with medical residents, registered nurses, medical specialists, and quality improvement officers were held. We used a descriptive method of content analysis. The findings indicate that it is important for professionals to consider how team members and patients are involved in the decision-making process during the clinical round and how current social and spatial structures can affect communication and collaboration between the healthcare team and the patient. Specific aspects of communication and collaboration are identified for improving effective interprofessional communication and collaboration during rounds."
f69ffaa3c1f5aa50480ed54bc7fd56bc4bada6b4,"The increasing use of antibiotics is the main driving force behind the rise of antibiotic resistance. Furthermore, there is a large variation in antibiotic use amongst prescribers. We describe the current duration of antibiotic therapy for common infections in the Netherlands and the new studies we can expect in this field in the years to come. We think that more research is needed to determine the duration of antibiotic therapy on the basis of different patient characteristics. It has, for example, recently been shown that the sex of the patient plays an important role in optimal duration of therapy for febrile urinary tract infections. Therefore, it is important to identify patients who fail on shorter courses of antibiotics in order to avoid overtreatment of all patients and to reduce the use of antibiotics in the future."
f82ea0c8b25db30edb6da9db7d5ee1fef249f56c,
fc923a4529289f3853d9dbf71345c57cf64112da,
0febab8b0f8bbdca9a09fea667f40271579d5499,"OBJECTIVE
To develop, implement and evaluate a personalized patient discharge letter (PPDL) to improve the quality of handoff communication from hospital to home.


DESIGN
From the end of 2006-09 we conducted a quality improvement project; consisting of a before-after evaluation design, and a process evaluation.


SETTING
Four general internal medicine wards, in a 1024-bed teaching hospital in Amsterdam, the Netherlands.


PARTICIPANTS
All consecutive patients of 18 years and older, admitted for at least 48 h.


INTERVENTIONS
A PPDL, a plain language handoff communication tool provided to the patient at hospital discharge.


MAIN OUTCOME MEASURES
Verbal and written information provision at discharge, feasibility of integrating the PPDL into daily practice, pass rates of PPDLs provided at discharge.


RESULTS
A total of 141 patients participated in the before-after evaluation study. The results from the first phase of quality improvement showed that providing patient with a PPDL increased the number of patients receiving verbal and written information at discharge. Patient satisfaction with the PPDL was 7.3. The level of implementation was low (30%). In the second phase, the level of implementation improved because of incorporating the PPDL into the electronic patient record (EPR) and professional education. An average of 57% of the discharged patients received the PPDL upon discharge. The number of discharge conversations also increased.


CONCLUSION
Patients and professionals rated the PPDL positively. Key success factors for implementation were: education of interns, residents and staff, standardization of the content of the PPDL, integrating the PPDL into the electronic medical record and hospital-wide policy."
131ba120019ffd4c343723a2d22ef7d78f2d881a,"Increasing antimicrobial resistance has stimulated interest in non-antibiotic prophylaxis of recurrent urinary tract infections (UTIs). Well-known steps in the pathogenesis of UTIs are urogenital colonization and adherence of uropathogens to uroepithelial cell receptors. To prevent colonization in postmenopausal women, vaginal, but not oral, estrogens have been shown to restore the vagina lactobacilli flora, reduce vaginal colonization with Enterobacteriaceae, and reduce the number of UTIs compared to placebo. Different lactobacilli strains show different results in the prevention of recurrent UTIs. Intravaginal suppositories with Lactobacillus crispatus in premenopausal women and oral capsules with Lactobacillus rhamnosus GR-1 and Lactobacillus reuteri RC-14 in postmenopausal women are promising. Ascorbic acid (vitamin C) cannot be recommended for the prevention of UTIs. Cranberries are thought to contain proanthocyanidins that can inhibit adherence of P-fimbriated E. coli to the uroepithelial cell receptors. Cranberry products decreased UTI recurrences about 30%–40% in premenopausal women with recurrent UTIs, but are less effective than low-dose antimicrobial prophylaxis. However, the optimal dose of cranberry product has still to be determined. Initially OM-89, a vaccine with 18 heat-killed E. coli extracts, seemed promising, but this was not confirmed in a recently randomized trial."
198e683ca32f85ca2e676ee6bf92e25d4d6bb662,"Objectives Early testing for HIV and entry into care are crucial to optimise treatment outcomes of HIV-infected patients and to prevent spread of HIV. We examined risk factors for presentation with late or advanced disease in HIV-infected patients in the Netherlands. Methods HIV-infected patients registered in care between January 1996 and June 2014 were selected from the ATHENA national observational HIV cohort. Risk factors for late presentation and advanced disease were analysed by multivariable logistic regression. Furthermore, geographical differences and time trends were examined. Results Of 20 965 patients, 53% presented with late-stage HIV infection, and 35% had advanced disease. Late presentation decreased from 62% (1996) to 42% (2013), while advanced disease decreased from 46% to 26%. Late presentation only declined significantly among men having sex with men (MSM; p <0.001), but not among heterosexual males (p=0.08) and females (p=0.73). Factors associated with late presentation were: heterosexual male (adjusted OR (aOR), 1.59; 95% CI 1.44 to 1.75 vs MSM), injecting drug use (2.00; CI 1.69 to 2.38), age ≥50 years (1.46; CI 1.33 to 1.60 vs 30–49 years), region of origin (South-East Asia 2.14; 1.80 to 2.54, sub-Saharan Africa 2.11; 1.88 to 2.36, Surinam 1.59; 1.37 to 1.84, Caribbean 1.31; 1.13 to 1.53, Latin America 1.23; 1.04 to 1.46 vs the Netherlands), and location of HIV diagnosis (hospital 3.27; 2.94 to 3.63, general practitioner 1.66; 1.50 to 1.83, antenatal screening 1.76; 1.38 to 2.34 vs sexually transmitted infection clinic). No association was found for socioeconomic status or level of urbanisation. Compared with Amsterdam, 2 regions had higher adjusted odds and 2 regions had lower odds of late presentation. Results were highly similar for advanced disease. Conclusions Although the overall rate of late presentation is declining in the Netherlands, targeted programmes to reduce late HIV diagnoses remain needed for all risk groups, but should be prioritised for heterosexual males, migrant populations, people aged ≥50 years and certain regions in the Netherlands."
19b57471f38f69fb26055b78887f433b5b34f5e8,
522de4664c9edeb7d3971ec80c5bae745d274963,
5d0a8ab5cd305d0267654465e0db6077cd9ccf3f,"Objectives Prior research has shown that Dutch general practitioners (GPs) do not always offer HIV testing and the number of undiagnosed HIV patients remains high. We aimed to further investigate the frequency and reasons for (not) testing for HIV and the contribution of GPs to the diagnosis of HIV infections in the Netherlands. Design Observational study. Setting (1) Dutch primary care network of 42–45 sentinel practices where report forms during sexually transmitted infection (STI)-related consultations were routinely collected, 2008–2013. (2) Dutch observational cohort with medical data of HIV-positive patients in HIV care, 2008–2013. Outcome measures The proportion of STI-related consultations in patients from high-risk groups tested for HIV, with additional information requested from GPs on HIV testing preconsultation or postconsultation for whom HIV testing was indicated, but not performed. Next, information was collected on the profile of HIV-positive patients entering specialised HIV care following diagnosis by GPs. Results Initially, an HIV test was reported (360/907) in 40% of STI-related consultations in high-risk groups. Additionally, in 26% of consultations an HIV test had been performed in previous or follow-up consultations or at different STI-care facilities. The main reasons for not testing were perceived insignificant risk; ‘too’ recent risk according to GPs or the reluctance of patients. The initiative of the patient was a strong determinant for HIV testing. GPs diagnosed about one third of all newly found cases of HIV. Compared with STI clinics, HIV-positive patients diagnosed in general practice were more likely to be older, female, heterosexual male or sub-Saharan African. Conclusions In one-third of the STI-related consultations of persons from high-risk groups, no HIV test was performed in primary care, which is lower than previously reported. Risk-based testing has intrinsic limitations and implementation of new additional strategies in primary care is warranted."
653cad98bd50e3f5d37dca7496390e1b5a461740,"Objective In the Netherlands the incidence of cervical cancer is higher among ethnic minority populations compared with the general Dutch population. We investigated the prevalence of, and risk factors associated with, vaginal high-risk human papillomavirus (hrHPV) infection in women of six different ethnicities living in Amsterdam. Methods For this cross-sectional study we selected women aged 18–34 years old of six ethnicities from the large-scale multiethnic HEalthy LIfe in an Urban Setting study. Self-collected vaginal swabs were tested for HPV DNA and genotyped using a highly sensitive PCR and reverse line blot assay (short PCR fragment (SPF)10-PCR DNA enzyme immunoassay/LiPA25-system version-1, delft diagnostic laboratory (DDL)). Participants completed a questionnaire regarding demographics and sexual behaviour. Logistic regression using generalised estimating equations was used to assess risk factors of hrHPV, and to investigate whether prevalence of hrHPV differed among ethnicities. Results The study population consisted of 592 women with a median age of 27 (IQR: 23–31) years. Dutch and African Surinamese women reported the highest sexual risk behaviour. HrHPV prevalence was highest in the Dutch (40%) followed by the African Surinamese (32%), Turkish (29%), Ghanaian (26%), Moroccan (26%) and South-Asian Surinamese (18%). When correcting for sexual risk behaviour, the odds to be hrHPV-positive were similar for all non-Dutch groups when compared with that of the Dutch group. Conclusions We found an overall higher hrHPV prevalence and higher sexual risk behaviour in the native Dutch population. Further research is needed to unravel the complex problem concerning cervical cancer disparities, such as differences in participation in the cervical cancer screening programme, or differences in clearance and persistence of hrHPV."
66b43b980874e1b43f86dc9d77f5d55619a68b51,"OBJECTIVE
To compare the effectiveness of two strategies to improve antibiotic use in patients with a complicated urinary tract infection.


DESIGN
Multicentre cluster randomised unblinded trial.


METHOD
The departments of Internal Medicine and Urology from 19 hospitals in the Netherlands took part in this trial. Based on retrospective patient record investigations we performed baseline measurements on the scores of a validated set of quality indicators for antibiotic use in a minimum of 50 patients with a complicated urinary tract infection per department in 2009. A similar post-trial measurement took place in 2012. In 2010 we randomised the hospitals between 2 improvement strategies: a multifaceted strategy that included results of the baseline measurements, education, reminders and assistance with optional improvement interventions, and a competitive feedback strategy, in which the departments only received results of the baseline measurements and non-anonymous results from the other departments in this study arm. The primary outcome measure was improvement of the quality indicator scores. Secondary outcome measures were determinants of improvement of the indicators. (Netherlands Trial Register: NTR1742) RESULTS: The baseline and post-trial measurements were performed on 1,964 patients and 2,027 patients, respectively. Post-trial measurements revealed a significant, but limited, improvement of several indicators compared with baseline measurements. We found no significant difference in improvement between the two strategies for any indicator. The intensity with which the departments implemented improvement strategies was mostly suboptimal, but intensive implementation of a strategy was associated with greater improvement.


CONCLUSION
The effectiveness of both improvement strategies was comparable, but limited. For real improvement in antibiotic use in patients with complicated urinary tract infections, improvement interventions should be developed and applied by local professionals themselves."
66c427a089c27b45922778ab96702bb8f73bd995,"Urinary tract infection (UTI) is one of the most common bacterial infections, and the incidence in women is much higher than in men. The diagnosis of a UTI can be made based on a combination of symptoms and a positive urine analysis or culture. Most UTIs are uncomplicated UTIs, defined as cystitis in a woman who is not pregnant, is not immunocompromised, has no anatomical and functional abnormalities of the urogenital tract, and does not exhibit signs of tissue invasion and systemic infection. All UTIs that are not uncomplicated are considered to be complicated UTIs. Differentiation between uncomplicated and complicated UTIs has implications for therapy because the risks of complications or treatment failure are increased for patients with a complicated UTI. Asymptomatic bacteriuria (ASB) is defined as the presence of a positive urine culture collected from a patient without symptoms of a UTI. Concerning the complicated UTI, it is possible to make a differentiation between UTI with systemic symptoms (febrile UTI) and UTI in a host, which carries an increased risk to develop complications of this UTI. Febrile UTIs are urosepsis, pyelonephritis, and prostatitis. A complicated host is defined as one that has an increased risk for complications, to which the following groups belong: men, pregnant women, immunocompromised patients, or those who have an anatomical or functional abnormality of the urogenital tract (e.g., spinal cord-injury patients, renal stones, urinary catheter). CLINICAL SYNDROMES AND DEFINITIONS Urinary tract infection (UTI) is one of the most common bacterial infections. Bacteria live around the urethra and colonize the bladder, but are washed out during micturition. The shorter distance to the bladder in women (as compared to men) makes it easier for bacterial colonizers to reach the bladder. Furthermore, the urethral opening in women is close to the rectum. Urogenital manipulations associated with daily living or medical interventions facilitate the movement of bacteria to the urethra (1). The diagnosis of a UTI can be made by a combination of symptoms and a positive urine analysis or culture. In most patient groups, the threshold for bacteriuria is considered to be 1,000 colony-forming units (cfu)/ml, based on studies correlating midstream-urine specimens with catheterized collection to demonstrate bladder bacteriuria. However, up to 20% of women with classical urinary symptoms can have negative cultures, depending on the cut-off value used (1). The differentiation between uncomplicated and complicated UTIs has implications for therapy because the risks of complications or treatment failure are increased for patients with a complicated UTI. In general, the following definitions are used: an uncomplicated UTI is an episode of cystitis in a woman who is not pregnant, is not immunocompromised, has no anatomical and functional abnormalities of the urogenital tract, and does not exhibit signs of tissue invasion and systemic infection. Received: 29 June 2012, Accepted: 1 April 2016, Published: 9 September 2016 Editors: Matthew A. Mulvey, University of Utah, Salt Lake City, UT; Ann E. Stapleton, University of Washington, Seattle, WA; and David J. Klumpp, Northwestern University, Chicago, IL Citation: Geerlings SE. 2016. Clinical presentations and epidemiology of urinary tract infections. Microbiol Spectrum 4(5):UTI-0002-2012. doi:10.1128/microbiolspec.UTI-0002-2012. Correspondence: Suzanne E. Geerlings, S.E.Geerlings@amc.uva.nl © 2016 American Society for Microbiology. All rights reserved. ASMscience.org/MicrobiolSpectrum 1 Downloaded from www.asmscience.org by"
6a3264b9050c069def6dd1ae6b3b5843ed7660a7,
8f2f8d2b50c15a23f3ce784b72f993accb429db3,
96098937e7f91cbfd23e1efd418091d0a6dbb57e,"Purpose of review Complicated infections of the urinary tract (UTI) including pyelonephritis and urosepsis are also called febrile UTI. This review describes insights from the literature on this topic since July 2014. Recent findings Recent studies regarding risk factors and consequences of febrile UTI confirmed existing knowledge. It remains difficult to obtain insight into the epidemiology of febrile UTI because urine and blood cultures are frequently missing. The relationship between host and virulence factors of Escherichia coli was further explored showing that less virulent strains can cause infection in immunocompromised patients. In contrast to obstructive uropathy, diabetes, and being older, neutropenia was not a risk factor for lower UTI or urosepsis. A randomized controlled trial revealed that ceftolozane–tazobactam was marginally superior to levofloxacin as treatment for complicated UTI. Case series supported the notion that xanthogranulomatous and emphysematous pyelonephritis are more common in diabetic patients and that drainage or surgery is often required. Summary Neutropenia was not a risk factor for lower UTI or urosepsis. When local resistance percentages to the frequently prescribed fluoroquinolones are high, the combination of ceftolozane–tazobactam may be an alternative as treatment for complicated UTI. Xanthogranulomatous and emphysematous pyelonephritis need to be considered in diabetic patients presenting with UTI symptoms."
9fcf91348794bab8051485e1ccf9163f4437b88a,"ABSTRACT Policy-makers and clinicians are faced with a gap of evidence to guide policy on standards for HIV outpatient care. Ongoing debates include which settings of care improve health outcomes, and how many HIV-infected patients a health-care provider should treat to gain and maintain expertise. In this article, we evaluate the studies that link health-care facility and care provider characteristics (i.e., structural factors) to health outcomes in HIV-infected patients. We searched the electronic databases MEDLINE, PUBMED, and EMBASE from inception until 1 January 2015. We included a total of 28 observational studies that were conducted after the introduction of combination antiretroviral therapy in 1996. Three aspects of the available research linking the structure to quality of HIV outpatient care were evaluated: (1) assessed structural characteristics (i.e., health-care facility and care provider characteristics); (2) measures of quality of HIV outpatient care; and (3) reported associations between structural characteristics and quality of care. Rather than scarcity of data, it is the diversity in methodology in the identified studies and the inconsistency of their results that led us to the conclusion that the scientific evidence is too weak to guide policy in HIV outpatient care. We provide recommendations on how to address this heterogeneity in future studies and offer specific suggestions for further reading that could be of interest for clinicians and researchers."
a18a77e8462ac2152774dd4a410da23d0c9c651d,
a712ebb63ff7130f41fad48a8f7e525525e7704a,"The impact of allograft pyelonephritis (AGPN) on renal allograft function is controversial. In this study, we evaluated the incidence, risk factors, and the impact of AGPN on renal allograft function."
ad03f8cc8861985bf0467db7bcadcadadb8f9bc3,"AIM
To evaluate whether intestinal microbiota predicts the development of new-onset urinary tract infections (UTIs) in postmenopausal women with prior recurrent UTIs (rUTIs).


PATIENTS & METHODS
Fecal samples (n = 40) originated from women with rUTI who received 12 months' prophylaxis of either trimethoprim-sulfamethoxazole (TMP-SMX) or lactobacilli. Microbial composition was assessed by 16S rRNA pyrosequencing.


RESULTS
At baseline, fecal microbiota of women with zero and more than or equal to four UTIs during follow-up showed no significant differences. Only TMP-SMX prophylaxis resulted in reduced microbial diversity. Microbial structure of two samples from the same woman showed limited relatedness.


CONCLUSION
In postmenopausal women with rUTI, the intestinal microbiota was not predictive for new-onset UTIs. Only TMP-SMX, and not lactobacilli, prophylaxis had effects on the microbial composition. Data in ENA:PRJEB13868."
c406702c745a5f35b13d6986a337b0aae88c7762,"AIM
To evaluate methods measuring the intestinal per-meability in chronic kidney disease (CKD) and clarify whether there is an increased intestinal permeability in CKD.


METHODS
We reviewed the literature in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) protocol and performed a systematic literature search through MEDline and EMBASE. All controlled trials and cohort studies using non-invasive methods to assess intestinal permeability in CKD patients were included. Excluded were: Conference abstracts and studies including patients younger than 18 years or animals. From the included studies we summarized the used methods and their advantages and disadvantages. For the comparison of their results we divided the included studies in two categories based on their included patient population, either assessing the intestinal permeability in mild to moderate CKD patients or in end stage renal disease (ESRD) patients. Results were graphically displayed in two plots, one comparing the intestinal permeability in mild to moderate CKD patients to healthy controls and one comparing the intestinal permeability in ESRD patients to healthy controls.


RESULTS
From the 480 identified reports, 15 met our inclusion criteria. Methods that were used to assess the intestinal permeability varied from markers measured in plasma to methods based on calculating the urinary excretion of an orally administered test substance. None of the applied methods has been validated in CKD patients and the influence of decreased renal function on the different methods remains unclear to a certain extent. Methods that seem the least likely to be influenced by decreased renal function are the quantitative PCR (qPCR) for bacterial DNA in blood and D-lactate. Considering the results published by the included studies; the studies including patients with mild to moderate CKD conducted conflicting results. Some studies did report an increase in intestinal permeability whilst other did not find a significant increased permeability. However, despite the variety in used methods among the different studies, all studies measuring the intestinal permeability in ESRD point out a significant increased intestinal permeability. Results should nevertheless be interpreted with caution due to the possible influence of a decreased glomerular filtration rate on test results.


CONCLUSION
The intestinal permeability in CKD: (1) could be measured by qPCR for bacterial DNA in blood and D-lactate; and (2) seems to be increased in ESRD."
c4cc78a9013d66d8f1ef16668555f52643461f5c,"Objective:Successful treatment of people infected with HIV requires that patients are retained in HIV care, use combination antiretroviral therapy (cART) and ultimately reach and sustain viral suppression. Our aim was to identify health facility characteristics associated with these steps in the cascade of HIV care. Design:Retrospective cohort study. Methods:We included data from all adult HIV-1-infected patients who entered care in the Netherlands between 2007 and 2013 (N = 7120). Multivariate logistic regression was used to examine the associations between health facility characteristics and the outcomes ‘currently in care’, ‘initiated cART’, and ‘viral suppression’. Results:The proportion of patients ‘currently in care’ was high in all 26 treatment centres. cART initiation was positively associated with the accreditation of the health facility [OR (odds ratio): 1.62; 95% CI (confidence interval): 1.18–2.23] and the performance of an internal audit in the preceding 3 years (OR: 1.36; 95% CI: 1.02–1.81). The odds of cART initiation were higher in middle-sized (OR: 2.00; 95% CI: 1.25–3.21) and large HIV treatment centres (OR: 1.80; 95% CI: 1.14–2.84) compared with small centres (<300 HIV-infected patients). Viral suppression was negatively associated with the presence of a social worker in the HIV treatment team (OR: 0.62; 95% CI: 0.43–0.91). Conclusions:Our results confirm that appointing expert HIV treatment centres facilitates retention in care and that a minimum volume requirement may be desirable. Our findings suggest that quality assessment through accreditation and the measurement of performance benefits the delivery of HIV care."
ce64b5d0f89efcd7816f091616a4a2538d06badb,"Urinary tract infection (UTI) is one of the most common bacterial infections, and the incidence in women is much higher than in men. The diagnosis of a UTI can be made based on a combination of symptoms and a positive urine analysis or culture. Most UTIs are uncomplicated UTIs, defined as cystitis in a woman who is not pregnant, is not immunocompromised, has no anatomical and functional abnormalities of the urogenital tract, and does not exhibit signs of tissue invasion and systemic infection. All UTIs that are not uncomplicated are considered to be complicated UTIs. Differentiation between uncomplicated and complicated UTIs has implications for therapy because the risks of complications or treatment failure are increased for patients with a complicated UTI. Asymptomatic bacteriuria (ASB) is defined as the presence of a positive urine culture collected from a patient without symptoms of a UTI. Concerning the complicated UTI, it is possible to make a differentiation between UTI with systemic symptoms (febrile UTI) and UTI in a host, which carries an increased risk to develop complications of this UTI. Febrile UTIs are urosepsis, pyelonephritis, and prostatitis. A complicated host is defined as one that has an increased risk for complications, to which the following groups belong: men, pregnant women, immunocompromised patients, or those who have an anatomical or functional abnormality of the urogenital tract (e.g., spinal cord-injury patients, renal stones, urinary catheter)."
d8183bd49398234de2690fdb5e4cbabd55b073ec,
eb7e3301fb7002a9114209e93dbb0dea70fc82d2,"Objectives: To define appropriate antibiotic use in hospitalized adults treated for a bacterial infection, we previously developed and validated a set of six generic quality indicators (QIs) covering all steps in the process of antibiotic use. We assessed the association between appropriate antibiotic use, defined by these QIs, and length of hospital stay (LOS). Methods: An observational multicentre study in 22 hospitals in the Netherlands included 1890 adult, non-ICU patients using antibiotics for a suspected bacterial infection. Performance scores were calculated for all QIs separately (appropriate or not), and a sum score described performance on the total set of QIs. We divided the sum scores into two groups: low (0%–49%) versus high (50%–100%). Multilevel analyses, correcting for confounders, were used to correlate QI performance (single and combined) with (log-transformed) LOS and in-hospital mortality. Results: The only single QI associated with shorter LOS was appropriate intravenous–oral switch (geometric means 6.5 versus 11.2 days; P < 0.001). A high sum score was associated with a shorter LOS in the total group (10.1 versus 11.2 days; P = 0.002) and in the subgroup of community-acquired infections (9.7 versus 10.9 days; P = 0.007), but not in the subgroup of hospital-acquired infections. We found no association between performance on QIs and in-hospital mortality or readmission rate. Conclusions: Appropriate antibiotic use, defined by validated process QIs, in hospitalized adult patients with a suspected bacterial infection appears to be associated with a shorter LOS and therefore positively contributes to patient outcome and healthcare costs."
ecd449ea5c96210a3c5ab2003b6a8a49f26c0209,
fbe333c5954ad980b6da79dee18f5083f31495a2,"DOI:10.1097/QCO.0000000000000232 Urinary tract infections (UTIs) remain an important and interesting topic: important as it is still one of the most common bacterial infections and causes of sepsis (urosepsis), and interesting as it is a great example of how new techniques contribute to our understanding of the pathogenesis of infectious diseases in general. Even though clinical urine specimens have always been considered to be sterile when they do not yield uropathogens using standard operating procedures, this is now being questioned in several studies applying new sensitive methods such as DNA sequencing techniques [1,2]. Bacteria that are not routinely cultivated have been identified by new techniques in voided urine, urine collected by transurethral catheter and by suprapubic aspirate, regardless of whether the patients had urinary symptoms [1]. Thus, a urine specimen that was previously assumed to be sterile appeared to obtain bacterial products. These findings have opened the discussion considering the accuracy of our current culture techniques, the presence of (uncultivated) bacteria in urine, the microbiome of the urogenital tract and the importance of the microorganism– host interaction. These new insights, combined with the already known aspects of UTIs, are discussed in four reviews that are considered in this issue of Current Opinion of Infectious Diseases. As each review is focussing on a different patient population susceptible for UTIs, this issue provides a clear overview on where we stand at this moment concerning the epidemiology, pathophysiology, risk factors, consequences and treatment of UTIs. The epidemiology of UTIs shows a great variety depending on the geographic location and patient population. An important aspect in this is the onset location, as the bacterial spectrum and resistance rates appear to greatly vary between communityassociated UTIs and healthcare-associated UTIs. Tondogdu and Wagenlehner (pp. 73–79) describes the global epidemiology of UTIs, the dependence on geographic location and the influence of changes in our healthcare system. The shorter duration of hospital stay has led to an increase in admissions, which"
fdfa8500a2fca9741f85bc8cc96457457e8aaa95,
0bd80e1c6071a50670a950aaa51c4cd94900fa6c,"BACKGROUND
Recurrent urinary tract infections (RUTI) are common in women who are pregnant and may cause serious adverse pregnancy outcomes for both mother and child including preterm birth and small-for-gestational-age babies. Interventions used to prevent RUTI in women who are pregnant can be pharmacological (antibiotics) or non-pharmacological (cranberry products, acupuncture, probiotics and behavioural modifications). So far little is known about the best way to prevent RUTI in pregnant women.


OBJECTIVES
To assess the effects of interventions for preventing RUTI in pregnant women.The primary maternal outcomes were RUTI before birth (variously defined) and preterm birth (before 37 weeks). The primary infant outcomes were small-for-gestational age and total mortality.


SEARCH METHODS
We searched the Cochrane Pregnancy and Childbirth Group's Trials Register (20 May 2015) and reference lists of retrieved articles.


SELECTION CRITERIA
Published, unpublished and ongoing randomised controlled trials (RCTs), quasi-RCTs, clustered-randomised trials and abstracts of any intervention (pharmacological and non-pharmacological) for preventing RUTI during pregnancy (compared with another intervention, placebo or with usual care).


DATA COLLECTION AND ANALYSIS
Two review authors independently assessed trials for inclusion and risk of bias, extracted data and checked them for accuracy.


MAIN RESULTS
The review included one trial involving 200 women and was at moderate to high risk of bias.The trial compared a daily dose of nitrofurantoin and close surveillance (regular clinic visit, urine cultures and antibiotics when a positive culture was found) with close surveillance only. No significant differences were found for the primary outcomes: recurrent pyelonephritis (risk ratio (RR) 0.89, 95% confidence interval (CI) 0.31 to 2.53; one study, 167 women), RUTI before birth (RR 0.30, 95% CI 0.06 to 1.38; one study, 167 women), and preterm birth (before 37 weeks) (RR 1.18, 95% CI 0.42 to 3.35; one study, 147 women). The overall quality of evidence for these outcomes as assessed using GRADE was very low. There were no significant differences between the two comparison groups for any of the following secondary outcomes, birthweight less than 2500 (g) (RR 2.03, 95% CI 0.53 to 7.80; one study, 147 infants), birthweight (mean difference (MD) -113.00, 95% CI -327.20 to 101.20; one study, 147 infants), five-minute Apgar score less than seven (RR 2.03, 95% CI 0.19 to 21.87; one study, 147 infants) and miscarriages (RR 3.11, 95% CI 0.33 to 29.29; one study, 167 women). The evidence for these secondary outcomes was also of very low quality. The incidence of asymptomatic bacteriuria (ASB) (at least 10(3) colonies per mL) (secondary outcome), only reported in women with a clinic attendance rate of more than 90% (RR 0.55, 95% CI 0.34 to 0.89; one study, 102 women), was significantly reduced in women who received nitrofurantoin and close surveillance. Data on total mortality and small-for-gestational-age babies were not reported.


AUTHORS' CONCLUSIONS
A daily dose of nitrofurantoin and close surveillance has not been shown to prevent RUTI compared with close surveillance alone. A significant reduction of ASB was found in women with a high clinic attendance rate and who received nitrofurantoin and close surveillance. There was limited reporting of both primary and secondary outcomes for both women and infants. No conclusions can be drawn regarding the optimal intervention to prevent RUTI in women who are pregnant. Randomised controlled trials comparing different pharmacological and non-pharmacological interventions are necessary to investigate potentially effective interventions to prevent RUTI in women who are pregnant."
134a5c99d25fe083f76bd287c14c7f4c9bf0b9fe,
2e238ed0e6ecb8567da092e5980a2d2475545fab,"Patient handover is of major importance for continuity of care and contributes to patient safety. According to Joint Commission International (JCI), an American quality institute, 67% of medical errors result from miscommunication. More than half of these errors appear to be attributable to poor medical handover. JCI and the World Health Organisation recommend standardising handover and training doctors in order to improve the quality of medical handover. Little attention is paid to handover as an essential medical competence during training to become a doctor or medical specialist. Many hospitals lack either training or a standardised format for handover. In this paper we discuss 10 tips for improving the quality of intradisciplinary handover."
3ac4c61a487fb33e7dc04feab4af65a2fb1df320,"OBJECTIVES
The population-level appropriateness of empirical antibiotic therapy can be conventionally measured by ascertainment of treatment coverage. This method involves a complex resource-intensive case-by-case assessment of the prescribed antibiotic treatment and the resistance of the causative microorganism. We aimed to develop an alternative approach based, instead, on the use of routinely available surveillance data.


METHODS
We calculated a drug effectiveness index by combining three simple aggregated metrics: relative frequency of aetiological agents, level of resistance and relative frequency of antibiotic use. To evaluate the applicability of our approach, we used this metric to estimate the population-level appropriateness of guideline-compliant and non-guideline-compliant empirical treatment regimens in the context of the Dutch national guidelines for complicated urinary tract infections.


RESULTS
The drug effectiveness index agrees within 5% with results obtained with the conventional approach based on a case-by-case ascertainment of treatment coverage. Additionally, we estimated that the appropriateness of 2008 antibiotic prescribing regimens would have declined by up to 4% by year 2011 in the Netherlands due to the emergence and expansion of antibiotic resistance.


CONCLUSIONS
The index-based framework can be an alternative approach to the estimation of point values and counterfactual trends in population-level empirical treatment appropriateness. In resource-constrained settings, where empirical prescribing is most prevalent and comprehensive studies to directly measure appropriateness may not be a practical proposition, an index-based approach could provide useful information to aid in the development and monitoring of antibiotic prescription guidelines."
4473c7139fec33a84e49b6fd0915ffdb20ffd109,
5d8fa69fc6b492b641a2f20ad2e4f571d24c7980,"Purpose of review Bacteriuria is common among renal allograft recipients. It can be categorized into asymptomatic bacteriuria (ASB) and urinary tract infection (UTI). However, in medical literature, the classifications of bacteriuria are often not clear or ASB is also classified as a UTI. This contributes to difficulties in interpretation of the incidence and risk factors of these two entities. In this review, we describe the epidemiology, risk factors, management and the impact on renal allograft function of these two entities separately according to the recent literature. Recent findings Risk factors for ASB are not completely comparable to the risk factors of UTIs. Persistent ASB has been associated with development of acute rejection and allograft pyelonephritis. The available data suggest that treatment of ASB is not very effective. Prophylaxis with trimethoprim–sulfamethoxazole does not prevent UTIs such as allograft pyelonephritis. Blood stream infections and emphysematous allograft pyelonephritis are associated with renal allograft loss. Summary ASB is the most common manifestation of bacteriuria after renal transplantation. More effective interventions are needed to prevent bacteriuria. Renal allograft recipients with persistent ASB should be closely monitored since they could be at risk for developing not only UTIs, such as allograft pyelonephritis, but also acute rejection."
5df54eda609422a4c6c12e767564f0cee3afa77b,"Several reviews in this issue concern urinary tract infections (UTIs), which are one of the most common bacterial infections. The self-reported annual incidence of UTI in women is 12%, and by the age of 32 years, 50% of all women report having had at least one UTI."
6979a30145e282f4a287f8d3b0a5836ab2fb1a00,"Background Up to 50% of hospital antibiotic use is inappropriate and therefore improvement strategies are urgently needed. We compared the effectiveness of two strategies to improve the quality of antibiotic use in patients with a complicated urinary tract infection (UTI). Methods In a multicentre, cluster-randomized trial 19 Dutch hospitals (departments Internal Medicine and Urology) were allocated to either a multi-faceted strategy including feedback, educational sessions, reminders and additional/optional improvement actions, or a competitive feedback strategy, i.e. providing professionals with non-anonymous comparative feedback on the department’s appropriateness of antibiotic use. Retrospective baseline- and post-intervention measurements were performed in 2009 and 2012 in 50 patients per department, resulting in 1,964 and 2,027 patients respectively. Principal outcome measures were nine validated guideline-based quality indicators (QIs) that define appropriate antibiotic use in patients with a complicated UTI, and a QI sumscore that summarizes for each patient the appropriateness of antibiotic use. Results Performance scores on several individual QIs showed improvement from baseline to post-intervention measurements, but no significant differences were found between both strategies. The mean patient’s QI sum score improved significantly in both strategy groups (multi-faceted: 61.7% to 65.0%, P = 0.04 and competitive feedback: 62.8% to 66.7%, P = 0.01). Compliance with the strategies was suboptimal, but better compliance was associated with more improvement. Conclusion The effectiveness of both strategies was comparable and better compliance with the strategies was associated with more improvement. To increase effectiveness, improvement activities should be rigorously applied, preferably by a locally initiated multidisciplinary team. Trial Registration Nederlands Trial Register 1742"
9cbbc6cc79d2da7848735606f74fce1e295a6c41,
a145e30be20d845c32b32937bbc9f6b4aacd0896,"Objectives Recent guidelines advocate accelerated provider-initiated HIV testing by general practitioners (GPs). We aimed to identify the number of patient consultations in six general practices in the South-East of Amsterdam, and the incidence of HIV indicator conditions reported in their medical files prior to diagnosis. Methods A cross-sectional search in an electronic general practice database. We used a case–control design to identify those conditions most associated with an HIV-positive status. Results We included 102 HIV cases diagnosed from 2002 to 2012, and matched them with 299 controls. In the year prior to HIV diagnosis, 61.8% of cases visited their GP at least once, compared with 38.8% of controls. In the 5 years prior to HIV diagnosis, 58.8% of HIV cases had exhibited an HIV indicator condition, compared with 7.4% of controls. The most common HIV-related conditions were syphilis and gonorrhoea. The most common HIV-related symptoms were weight loss, lymphadenopathy and peripheral neuropathy. During this period, average HIV prevalence among people aged 15–59 years increased from 0.4% to 0.9%. Conclusions This study revealed many opportunities for HIV indicator condition-guided testing in primary care. As yet, however, HIV indicator conditions are not exploited as triggers for early HIV testing."
c939e27029c6cee112ae4500c982202b16e7fce7,"Patient handover is of major importance for continuity of care and contributes to patient safety. According to Joint Commission International (JCI), an American quality institute, 67% of medical errors result from miscommunication. More than half of these errors appear to be attributable to poor medical handover. JCI and the World Health Organisation recommend standardising handover and training doctors in order to improve the quality of medical handover. Little attention is paid to handover as an essential medical competence during training to become a doctor or medical specialist. Many hospitals lack either training or a standardised format for handover. In this paper we discuss 10 tips for improving the quality of intradisciplinary handover."
de6e146ba98273c9be6a5f5c80fb599c7ebc9d53,
e9d75e520eeeb9688a8388e8989fcc9bf8161c52,"Introduction American women of African or Hispanic ancestry have increased risk of vaginal microbiome dysbiosis compared to women of European or Asian ancestry. However, the association between vaginal microbiome composition and ethnicity within Europe is largely unknown. We investigated this association in Amsterdam, The Netherlands. Methods Non-pregnant women (18–34 years, n = 564) representing six ethnic origins (Dutch, South-Asian/Indonesian Surinamese, African Surinamese, Ghanaian, Turkish, and Moroccan) were cross-sectionally selected from the ongoing HELIUS multi-ethnic cohort study in Amsterdam for vaginal microbiome analysis. Extracted DNA from self-sampled vaginal swabs was sequenced targeting the V3V4 region of the 16S rRNA gene and using the Illumina MiSeq platform, and sequence reads were clustered using hierarchical clustering. Results Clustering of 502/564 samples with sufficient read counts resulted in microbiome clusters dominated by Lactobacillus crispatus (n = 120), L. iners (n = 168), L. jensenii (n = 8), L. gasseri (n = 10), Streptococcus agalactiae (n = 8), Bifidobacteriaceae/Bifidobacterium spp. (n = 10), Gardnerella vaginalis (n = 78), and a mixture of anaerobes (n = 100), respectively. Microbiome composition was significantly associated with ethnic origin (P = 0.002). Women of Dutch ethnic origin had the highest prevalence of L.crispatus-dominated microbiome (40% vs 16–26% in the other ethnic groups), the lowest prevalence of L. iners-dominated microbiome (28% vs 31–39% in the other groups), and the lowest prevalence of clusters dominated by G. vaginalis or a mixture of anaerobes (25% vs 30–45% in other groups). Turkish women and South-Asian/Indonesian Surinamese women had the highest prevalence of L. iners-dominated microbiome (38% and 39%, respectively), and women from African descent (African Surinamese and Ghanaian women) the highest prevalence of clusters dominated by G. vaginalis or a mixture of anaerobes (48% and 44%, respectively). Conclusion This large multi-ethnic study shows that dysbiotic vaginal microbiome compositions are significantly increased in women of non-Dutch ethnic origin. Therefore, these women may be at increased risk of STI acquisition and adverse reproductive health outcomes. Disclosure of interest statement The HELIUS study is funded by the Academic Medical Centre Amsterdam, the Public Health Service of Amsterdam, the Dutch Heart Foundation (project number 2010T084), the Netherlands Organisation for Health Research and Development (ZonMw; project number 200500003), and the European Union (FP-7; project number 278901). The vaginal microbiome analyses were funded by the Aids Fonds Netherlands (project number 201102). The authors declare no conflicts of interest."
f3b59f29987db3caa4e3aa4c4b82b1477738efd2,"BACKGROUND
An important requirement for an effective antibiotic stewardship program is the ability to measure appropriateness of antibiotic use. The aim of this study was to develop quality indicators (QIs) that can be used to measure appropriateness of antibiotic use in the treatment of all bacterial infections in hospitalized adult patients.


METHODS
A RAND-modified Delphi procedure was used to develop a set of QIs. Potential QIs were retrieved from the literature. In 2 questionnaire mailings with an in-between face-to-face consensus meeting, an international multidisciplinary expert panel of 17 experts appraised and prioritized these potential QIs.


RESULTS
The literature search resulted in a list of 24 potential QIs. Nine QIs describing recommended care at patient level were selected: (1) take 2 blood cultures, (2) take cultures from suspected sites of infection, (3) prescribe empirical antibiotic therapy according to local guideline, (4) change empirical to pathogen-directed therapy, (5) adapt antibiotic dosage to renal function, (6) switch from intravenous to oral, (7) document antibiotic plan, (8) perform therapeutic drug monitoring, and (9) discontinue antibiotic therapy if infection is not confirmed. Two QIs describing recommended care at the hospital level were also selected: (1) a local antibiotic guideline should be present, and (2) these local guidelines should correspond to the national antibiotic guidelines.


CONCLUSIONS
The selected QIs can be used in antibiotic stewardship programs to determine for which aspects of antibiotic use there is room for improvement. At this moment we are testing the clinimetric properties of these QIs in 1800 hospitalized patients, in 22 Dutch hospitals."
0121373afacbc9abe1d67d5a3344edcc6d3ce4aa,
0218c06f10ac01562ba9369121c4f9ceaf44aa6e,"In an attempt to decolonise the patient from the ESBL producing E. coli, he underwent donor feces infusion in May 2013. Prior to this intervention, the presence of ESBL producing E. coli in the large intestine was again confirmed by a positive rectal culture. In addition his throat and perineum were also screened for the presence of ESBL producing Enterobacteriaceae, but both were negative. Culture of the urine was not possible because of the anuric condition of the patient. The donor feces infusion was performed according to the protocol as used in the FECAL trial [1]. In summary, donor feces were obtained from a young healthy Caucasian adult, who was periodically screened for various infectious and gastro-intestinal diseases. Feces from the donor were collected and processed within 6 h after production. First, the feces were diluted with sterile saline, and then poured through unfolded gauze in a funnel, in order to obtain a solution which was free of debris and solid particles. This solution was immediately infused in the patient through a nasoduodenal tube. Donor feces infusion was preceded by full colon lavage without prior use of antibiotics. Within the first 2 days after donor feces infusion the patient experienced mild diarrhoea and abdominal cramps, but no other adverse events occurred. Follow-up ESBL swab cultures of the rectum, perineum and throat were taken at week one, two, four, and twelve after the donor feces infusion. During these four follow-up time points the ESBL cultures of the"
0508a357f9af843d6d976842896bc1a77b6ea4a6,"We encountered a case of severe murine typhus complicated by acute respiratory distress syndrome. To determine worldwide prevalence of such cases, we reviewed the literature and found that respiratory symptoms occur in ≈30% of murine typhus patients. In disease-endemic areas, murine typhus should be considered for patients with respiratory symptoms and fever."
32d09c235a921598bb21e3f462ceab84a1544768,"Purpose of review Asymptomatic bacteriuria (ASB) and urinary tract infections (UTIs) in women with diabetes mellitus and during pregnancy are common and can have far-reaching consequences for the woman and neonate. This review describes epidemiology, risk factors, complications and treatment of UTI and ASB according to recent developments in these two groups. Recent findings Most articles addressing the epidemiology and risk factors of ASB and UTI in diabetic and pregnant women confirmed existing knowledge. New insights were obtained in the association between sodium–glucose cotransporter-2 (SGLT2) inhibitors, as medication for diabetes mellitus type 2, and a small increased risk for UTI due to glucosuria and the possible negative effects of UTI, including urosepsis,on bladder and kidney function in diabetic women. Predominantly, potential long-term effects of antibiotic treatment of ASB or UTI during pregnancy on the neonate have received attention, including antibiotic resistance and epilepsy. Summary SGLT2 inhibitors were associated with a small increased risk for UTI, UTI in diabetic women may lead to bladder and kidney dysfunction, and antibiotic treatment of ASB and UTI during pregnancy was associated with long-term effects on the neonate. Up-to-date research on the effectiveness and long-term effects of ASB screening and treatment policies, including group B Streptococcus bacteriuria in pregnancy, is warranted to inform clinical practice."
3efe7d02512fa40b83eda5c21bef9a893ff53cb0,"This review gives an outline of the indications for faecal microbiota transplantation (FMT) for diseases other than Clostridium difficile (C. difficile) infection. The remarkable efficacy of FMT against C. difficile infection has already been demonstrated. The use of FMT for other diseases, such as inflammatory bowel disease (IBD), irritable bowel syndrome (IBS), and metabolic syndrome, is now being evaluated. The currently available data suggest that FMT might be beneficial for IBD (including ulcerative colitis and, to some extent, Crohn's disease), IBS, and insulin resistance. Several randomized clinical trials are currently being performed, and data are eagerly awaited. A new field of research for the implementation of FMT is the eradication of pathogenic and multiresistant enteric microorganisms. A few animal studies have been performed within this field, but hardly any research data from human studies are available at present."
43300553f5b0824431bcf7e6a6c7c21c678be108,"We evaluated 800 hospitalized patients with a complicated urinary tract infection, from whom both a blood and a urine culture were obtained on the first day of antibiotic treatment. Urine cultures were positive in 70% of patients, and blood cultures were positive in 29%. In 7% of patients, uropathogens caused bacteraemia with a pathogen that was not isolated from urine. Receiving antibiotic therapy at the moment of hospitalization was the only factor independently associated with discordant culture results (OR, 2.06; 95% CI, 1.18-3.61). For those receiving antibiotics at the moment of hospitalization, blood cultures have additional diagnostic value over urine cultures."
51f4a1c8049914686b36a34517cca479f7ba2247,"BACKGROUND
To define appropriate antibiotic use for patients with a complicated urinary tract infection (UTI), we developed in a previous study a key set of 4 valid, guideline-based quality indicators (QIs). In the current study, we evaluated the association between appropriate antibiotic use for patients with a complicated UTI, as defined by these QIs, and length of hospital stay (LOS).


METHODS
A retrospective, observational multicenter study included 1252 patients with a complicated UTI, hospitalized at internal medicine and urology departments of 19 university and nonuniversity Dutch hospitals. Data from the patients' medical charts were used to calculate QI performance scores. Multilevel mixed-model analyses were performed to relate LOS to QI performance (appropriate use or not). We controlled for the potential confounders sex, age, (urological) comorbidity, febrile UTI, and intensive care unit admission <24 hours.


RESULTS
Prescribing therapy in accordance with local hospital guidelines was associated with a shorter LOS (7.3 days vs 8.7 days; P = .02), as was early intravenous-oral switching (4.8 days vs 9.1 days; P < .001). There was an inverse relationship between the proportion of appropriate use in a patient (QI sum score/number of applicable QIs) and LOS (9.3 days for lower tertile vs 7.2 days for upper tertile; overall P < .05).


CONCLUSIONS
Appropriate antibiotic use in patients with a complicated UTI seems to reduce length of hospital stay and therefore favors patient outcome and healthcare costs. In particular, adherence to the total set of QIs showed a significant dose-response relationship with a shorter LOS."
555ca1120c170e25a16470a8b3a88d99a8e0c832,
5bc33b57241adb99d7c916859b5fa13d24a38b7e,"The costs of combination antiretroviral therapy (cART) consisting of separate, particularly generic, components are generally much lower than of a single tablet regimen (STR) including the same active ingredients. Our aim was to evaluate whether patients in care in the Netherlands would be willing to take separate component regimens (SCR) instead of an STR and to examine whether willingness was associated with particular patient characteristics."
5c750b057c114fb06308b9a5a63ae88821ade60e,"Antimicrobial resistance is an increasing health care problem, both abroad and in the Netherlands, and its main driving force is antibiotic use. In patients with pneumonia or sepsis, appropriate antibiotic use has been associated with improved clinical outcome, shorter length of hospital stay, decreased bacterial resistance and reduced costs. In a recent study in 19 hospitals in the Netherlands, we found that appropriate antibiotic use in patients with another common type of infection, i.e. urinary tract infection, was also associated with a reduction in length of hospital stay, thus improving patient outcome and healthcare costs. A social and behavioural scientific approach with different intervention strategies - restrictive and persuasive - is required if antibiotic prescription in hospitals is to be improved. As a part of this antimicrobial stewardship programme, antibiotic teams (A-teams) must be introduced in all hospitals in 2014."
6d87fa2cc2203be7a2fd349ec3fdf4b34532590d,"Background Complicated urinary tract infections (c-UTIs) are among the most common nosocomial infections and a substantial part of the antimicrobial agents used in hospitals is for the treatment of c-UTIs. Data from surveillance can be used to guide the empirical treatment choices of clinicians when treating c-UTIs. We therefore used nation-wide surveillance data to evaluate antimicrobial coverage of agents for the treatment of c-UTI in the Netherlands. Methods We included the first isolate per patient of urine samples of hospitalised patients collected by the Infectious Disease Surveillance Information System for Antibiotic Resistance (ISIS-AR) in 2012, and determined the probability of inadequate coverage for antimicrobial agents based on species distribution and susceptibility. Analyses were repeated for various patient groups and hospital settings. Results The most prevalent bacteria in 27,922 isolates of 23,357 patients were Escherichia coli (47%), Enterococcus spp. (14%), Proteus mirabilis (8%), and Klebsiella pneumoniae (7%). For all species combined, the probability of inadequate coverage was <5% for amoxicillin or amoxicillin-clavulanic acid combined with gentamicin and the carbapenems. When including gram-negative bacteria only, the probability of inadequate coverage was 4.0%, 2.7%, 2.3% and 1.7%, respectively, for amoxicillin, amoxicillin-clavulanic acid, a second or a third generation cephalosporin in combination with gentamicin, and the carbapenems (0.4%). There were only small variations in results among different patient groups and hospital settings. Conclusions When excluding Enterococcus spp., considered as less virulent, and the carbapenems, considered as last-resort drugs, empirical treatment for c-UTI with the best chance of adequate coverage are one of the studied beta-lactam-gentamicin combinations. This study demonstrates the applicability of routine surveillance data for up-to-date clinical practice guidelines on empirical antimicrobial therapy, essential in patient care given the evolving bacterial susceptibility."
a26a0aa8460f7ff143e6840c954ec59df74abc1d,
c07481774e2900509f7bd77a44313cf7a702fc1a,"BACKGROUND
Hospitalised patients are especially vulnerable in times of transitions in care. Structured discharge planning might improve patient outcomes. We implemented and assessed the effect of a multidisciplinary discharge bundle to reduce 30-day readmission.


METHODS
A pre-post-test design study with a follow-up of one month at four internal medicine wards in a Dutch university teaching hospital. Eligible patients were 18 years and older, acutely admitted and hospitalised for at least 48 hours. The discharge bundle consisted of (1) planning the date of discharge within 48 hours after admission, (2) a discharge checklist, (3) a personalised patient discharge letter, and (4) multidisciplinary patient education. The primary outcome measure was unplanned 30-day readmission.


RESULTS
Participants in the post-test group (n = 204) did not have a lower rate of unplanned hospital readmission than those receiving usual care (n = 224) (12.9 vs. 13.2%, p = 0.93). The medical discharge summaries were sent to the general practitioner faster in the post-test period (median of 14 days pre-test vs. 5 days post-test, p < 0.001) and this group also had a trend towards a longer time to first readmission (14 vs. 10 days, p = 0.06). Patient satisfaction was high in both groups (7.5 and 7.4 points, (p = 0.49)).


CONCLUSIONS
The comprehensive discharge bundle was not effective in reducing the rate of readmission and increasing patient satisfaction, but medical discharge summaries were sent faster to the general practitioner and a trend to a longer time to readmission was present."
c2b461acf1a1e4b7526d9f3069bc8e46a670b3c3,
c5c2b5bfc8bf390cd1a42c4f5f46e7ca080e2294,"Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: http://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible."
d17d6f342a203df577afb3b0e902fac65b460651,"BACKGROUND
Human immunodeficiency virus (HIV)-infected individuals may be at increased risk of age-associated noncommunicable comorbidities (AANCCs).


METHODS
Cross-sectional analyses of AANCC prevalence (including cardiovascular, metabolic, pulmonary, renal, bone, and malignant disease) and risk factors in a prospective cohort study of HIV type 1-infected individuals and HIV-uninfected controls, who were aged ≥45 years and comparable regarding most lifestyle and demographic factors.


RESULTS
HIV-infected participants (n = 540) had a significantly higher mean number of AANCCs than controls (n = 524) (1.3 [SD, 1.14] vs 1.0 [SD, 0.95]; P < .001), with significantly more HIV-infected participants having ≥1 AANCC (69.4% vs 61.8%; P = .009). Hypertension, myocardial infarction, peripheral arterial disease, and impaired renal function were significantly more prevalent among HIV-infected participants. Risk of AANCC by ordinal logistic regression was independently associated with age, smoking, positive family history for cardiovascular/metabolic disease, and higher waist-to-hip ratio, but also with HIV infection (odds ratio, 1.58 [95% confidence interval, 1.23-2.03]; P < .001). In those with HIV, longer exposure to CD4 counts <200 cells/µL, and, to a lesser extent, higher levels of high-sensitivity C-reactive protein and soluble CD14, and longer prior use of high-dose ritonavir (≥400 mg/24 hours) were each also associated with a higher risk of AANCCs.


CONCLUSIONS
All AANCCs were numerically more prevalent, with peripheral arterial, cardiovascular disease, and impaired renal function significantly so, among HIV-infected participants compared with HIV-uninfected controls. Besides recognized cardiovascular risk factors, HIV infection and longer time spent with severe immunodeficiency increased the risk of a higher composite AANCC burden. There was a less pronounced contribution from residual inflammation, immune activation, and prior high-dose ritonavir use."
d3398733180aaee1ae41c35b093ea54d5d36f159,"Transitional care interventions aim to improve care transitions from hospital to home and to reduce hospital readmissions for chronically ill patients. The objective of our study was to examine if these interventions were associated with a reduction of readmission rates in the short (30 days or less), intermediate (31-180 days), and long terms (181-365 days). We systematically reviewed twenty-six randomized controlled trials conducted in a variety of countries whose results were published in the period January 1, 1980-May 29, 2013. Our analysis showed that transitional care was effective in reducing all-cause intermediate-term and long-term readmissions. Only high-intensity interventions seemed to be effective in reducing short-term readmissions. Our findings suggest that to reduce short-term readmissions, transitional care should consist of high-intensity interventions that include care coordination by a nurse, communication between the primary care provider and the hospital, and a home visit within three days after discharge."
f43dae8c9f45081bdb7e2dd135a8e3338f97dfcc,
f4bd919fcd0a3d776b30d408112fa76736c484f5,
fd6923ebc384d6b85335dcbb25714fcc49f35ff2,"HIV/AIDS continues to place a devastating toll on individuals, families and communities globally, and western industrialized countries are by no means exempt. Today, there are more than 1 million Americans and 100,000 Britons living with HIV, with a disproportionate burden of new and prevalent HIV infections borne by gay, bisexual and other men who have sex with men (MSM), racial/ethnic minorities, migrants and persons who use drugs. Epidemic concentration in urban areas, especially among: population sub‐groups with high prevalence of risk behaviours; the socio‐economically marginalized; or those with poor access to services, has been well documented. Recent increases in HIV incidence in the rural south US, and in MSM in both countries, reflect the dynamic and evolving nature of these epidemics. New national HIV prevention strategies in both countries have refocused attention on these domestic epidemics, prioritizing HIV testing scaling up, linkage to quality care and tackling long‐standing health inequalities. There are also significant differences between the two countries – in part a reflection of the different health and social care systems; historical approaches to the funding and coordination of HIV prevention; and underlying patterns of health inequalities and their social and structural determinants. In addition, the social–political acceptability of using the sexual health frame to guide more holistic and integrated approaches to HIV prevention efforts remains a key difference. This presentation will compare and contrast HIV prevention responses in the US and UK over the past decade, identifying opportunities for enhancing the prevention response in these and other western industrialized countries in the 21st century."
fe17e7be099c6512542d4bad80fc58100087f72a,"Background Urinary tract infections (UTIs) are common and result in an enormous economic burden. The increasing prevalence of antibiotic-resistant microorganisms has stimulated interest in non-antibiotic agents to prevent UTIs. Objective To evaluate the cost-effectiveness of cranberry prophylaxis compared to antibiotic prophylaxis with trimethoprim-sulfamethoxazole (TMP-SMX) over a 12 month period in premenopausal women with recurrent UTIs. Materials and Methods An economic evaluation was performed alongside a randomized trial. Primary outcome was the number of UTIs during 12 months. Secondary outcomes included satisfaction and quality of life. Healthcare utilization was measured using questionnaires. Missing data were imputed using multiple imputation. Bootstrapping was used to evaluate the cost-effectiveness of the treatments. Results Cranberry prophylaxis was less effective than TMP-SMX prophylaxis, but the differences in clinical outcomes were not statistically significant. Costs after 12 months in the cranberry group were statistically significantly higher than in the TMP-SMX group (mean difference €249, 95% confidence interval 70 to 516). Cost-effectiveness planes and cost-effectiveness acceptability curves showed that cranberry prophylaxis to prevent UTIs is less effective and more expensive than (dominated by) TMP-SMX prophylaxis. Conclusion In premenopausal women with recurrent UTIs, cranberry prophylaxis is not cost-effective compared to TMP-SMX prophylaxis. However, it was not possible to take into account costs attributed to increased antibiotic resistance within the framework of this randomized trial; modeling studies are recommended to investigate these costs. Moreover, although we based the dosage of cranberry extract on available evidence, this may not be the optimal dosage. Results may change when this optimal dosage is identified. Trial Registration ISRCTN.org ISRCTN50717094"
0343a8251d705151bbb2dafa54bec6c8b01c42df,"Background The most effective way to reduce catheter-associated urinary tract infections (CA-UTIs) is to avoid unnecessary urinary catheterisation and to minimise the duration of catheterisation. Aim To implement and assess the effect of an intervention to reduce the duration of urinary tract catheterisation. Methods This quality improvement project was set up as a before–after comparison consisting of a 2-month pre-intervention period, a period in which the intervention was implemented and a 2-month post-intervention period. The intervention included educational sessions to increase physicians' awareness and the daily reassessment of catheter use. The primary endpoint was the duration of catheterisation. Secondary endpoints were the catheter utilisation ratio, the length of hospital stay, the number of hospital-acquired symptomatic CA-UTIs and the number of appropriate indications for catheterisation. Results During the total study period, 149 patients (18.3%) were catheterised at some time during their hospital stay. There was a statistically significant decrease in the duration of catheterisation (median 7 vs 5 days; p<0.01), length of hospital stay (median 13 vs 9 days; p<0.01), and number of hospital-acquired CA-UTIs (4 vs 0, p=0.04) in the pre-intervention versus post-intervention period. Conclusions An intervention to raise more awareness of the risks of inappropriate catheterisation can reduce the duration of catheterisation along with the length of hospital stay and the number of hospital-acquired symptomatic CA- UTIs, even in a short period of time."
1cde6f885edd2087544279104a0eb9fdb46204ef,
247a06317b9e3c5f5ace99ef8d2b4844e9b5e2f4,
332722ce12b6c61b4f88add16f07ad971d31c560,Objective:To estimate the cost-effectiveness of anorectal chlamydia screening among men who have sex with men (MSM) in care at HIV treatment centers. Design:Transmission model combined with economic analysis over a 20-year period. Setting and participants:MSM in care at HIV treatment centers. Intervention:Once-yearly or twice-yearly screening for anorectal chlamydia among MSM in care at HIV treatment centers. Main outcome measures:Averted HIV and chlamydia infections; discounted quality-adjusted life-years and costs; incremental cost-effectiveness ratio (ICER). Results:Costs will be saved by routine chlamydia screening of MSM in care at HIV treatment centers if these patients seek little or no screening elsewhere. Nonroutine screening is considerably more expensive than routine screening offered within a scheduled visit. Adding once-yearly chlamydia screening for MSM in care at HIV treatment centers is cost saving when 30% or fewer of those men seek once-yearly screening elsewhere (&OV0556;1.5 to &OV0556;8.1 million saved). Twice-yearly routine screening at HIV treatment centers is cost-effective only when routine screening takes place without additional nonroutine screening (&OV0556;1.9 million saved). Conclusions:Adding annual chlamydia screening to the HIV consultation will be cost saving as long as only a limited proportion of men are nonroutinely screened. The ICER was most sensitive to the percentage of MSM that continue to be screened elsewhere.
380e878647b0937d5bab13dc8d1625d3ecbfba24,"OBJECTIVE: To estimate and compare contamination rates of three different urine-sampling methods in pregnant women to assess bacteriuria. METHODS: In this cross-sectional study, 113 pregnant women collected three different midstream urine samples consecutively: morning (first void); midstream (void without further instructions); and clean-catch sample (void after cleaning). The following end points were considered contaminants: epithelial cells, Gram-positive rods or mixed bacteria in the Gram stain, and mixed growth or skin flora in the urine culture. Intraindividual variability in contaminants was quantified with Fleiss-Cohen’s weighted &kgr; statistic. Differences between samples were assessed using generalized estimating equations. RESULTS: Mainly low numbers of Gram-positive rods were more likely to be present in Gram stains of midstream samples compared with clean-catch samples (77.7% compared with 66.7%, P=.022). Morning samples showed more mixed growth compared with midstream samples (6.2% compared with 0.9%, P=.050). No consistency in quantity of contaminants was found in midstream samples compared with morning and clean-catch samples. No differences were found between the other end points in all three urine samples (P>.05). The study could detect an odds ratios of 2.0 for differences in urine-sampling methods with 80% power and 5% significance for most end points. CONCLUSION: In pregnant women, the contamination rate of midstream samples is comparable with the contamination rates of morning and clean-catch samples. The quantity of contaminants varied among the three samples collected by one woman. These results show that more complex, unpractical, and time-consuming morning and clean-catch samples are not superior. Therefore, we recommend a midstream sample to assess bacteriuria in pregnant women. LEVEL OF EVIDENCE: II"
5d81c43c5e4b2bfb811239bf6bf678305a837ac0,"The global incidence of human immunodeficiency virus (HIV) infection has decreased by 15% over the past years, but is still too high. Despite current programs to reduce the incidence of HIV infection, further approaches are needed to limit this epidemic. Oral antiretroviral pre-exposure prophylaxis (PrEP) is currently one of the most discussed possible prevention methods. This literature study demonstrates whether orally antiretroviral chemoprophylaxis in HIV-uninfected individuals with high-risk behaviour reduces the transmission of HIV. We used the PICO method and conducted a search to identify relevant studies. Subjects of the study were HIV-uninfected individuals with high-risk behaviour. Intervention was oral PrEP with tenofovir disoproxil fumarate (TDF) alone or plus emtricitabine (FTC) versus placebo. The primary outcome was the HIV incidence among this high-risk group. Secondary outcomes were adherence to PrEP, frequency and type of adverse effects. We identified ten studies from which five randomised control trials (RCTs) were included after screening. The results from three out of five trials showed a reduction, but two trials showed no protection in acquiring HIV infection. There were no significant differences in adverse events. The adherence was different among different groups and affected the outcome of the studies. In conclusion, this prophylaxis might offer protection when used in combination with intense monitoring and guidance in uninfected individuals with a high risk of HIV acquisition. However, there are still many unresolved questions. Drug adherence seems to be a crucial factor in the effectiveness of PrEP. Therefore, individual risk behaviour remains an important determinant for success in the prevention of HIV transmission."
5e7bd0d536a01d0c71d2715e61efcde401bb3be8,
747da35e826221da15f954606315e9b0423b82fe,"Objectives Document progress in HIV-treatment in the Netherlands since 1996 by reviewing changing patterns of cART use and relating those to trends in patients' short-term clinical outcomes between 1996 and 2010. Design and Methods 1996–2010 data from 10,278 patients in the Dutch ATHENA national observational cohort were analysed. The annual number of patients starting a type of regimen was quantified. Trends in the following outcomes were described: i) recovery of 150 CD4 cells/mm3 within 12 months of starting cART; ii) achieving viral load (VL) suppression ≤1,000 copies/ml within 12 months of starting cART; iii) switching from first-line to second-line regimen within three years of starting treatment; and iv) all-cause mortality rate per 100 person-years within three years of starting treatment. Results Between 1996 and 2010, first-line regimens changed from lamivudine/zidovudine-based or lamivudine/stavudine-based regimens with unboosted-PIs to tenofovir with either emtricitabine or lamivudine with NNRTIs. Mortality rates did not change significantly over time. VL suppression and CD4 recovery improved over time, and the incidence of switching due to virological failure and toxicity more than halved between 1996 and 2010. These effects appear to be related to the use of new regimens rather than improvements in clinical care. Conclusion The use of first-line cART in the Netherlands closely follows changes in guidelines, to the benefit of patients. While there was no significant improvement in mortality, newer drugs with better tolerability and simpler dosing resulted in improved immunological and virological recovery and reduced incidences of switching due to toxicity and virological failure."
802e938edf78cacbf58b111b9c4fbee8d8e0daa0,
89f88485f9c5c301c3d8ecd01628b919913a1238,
90b84e494d394f87444eb8703764c188b8dae1b4,"Dr. S.E. Geerlings (coordinator, SWAB), Internal Medicine/Infectious Diseases specialist, Department of Internal Medicine, Division of Infectious Diseases, Academic Medical Center, Amsterdam Dr. C. van Nieuwkoop (VIZ, NIV), Internal Medicine, Emergency Medicine and Infectious Diseases specialist, Department of Internal Medicine, Hagaziekenhuis, the Hague E. van Haarst (NVU), Urologist, Department of Urology, St. Lucas Andreas Hospital, Amsterdam Dr. M. van Buren (NFN), Internal Medicine and Nephrology specialist, Department of Internal Medicine, Hagaziekenhuis, the Hague Dr. B.J. Knottnerus (NHG), General Practitioner, Department General Practice, Academic Medical Center, Amsterdam Dr. E. E. Stobberingh (NVMM), Medical microbiologist, Lab Medical Microbiology, Maastricht Univerisity Medical Center, Maastricht Prof. dr. C.J. de Groot (NVOG), Gynaecologist, Department of Obstetrics and Gynaecology, Vrije Universiteit Medical Center, Amsterdam Prof. dr. J.M. Prins (SWAB), Internal Medicine/Infectious Diseases specialist, Department of Internal Medicine, Division of Infectious Diseases, Academic Medical Center, Amsterdam"
9166b9ad8270c3817316232d4d3c5d93a506fbd4,
a2562c3832126823c20f7b94fc2ceea4063df255,"ABSTRACT Asymptomatic Chlamydia trachomatis infections are common in HIV-infected men who have sex with men (MSM). Although C. trachomatis combined with HIV would be likely to enhance inflammation, the asymptomatic course suggests otherwise. We assessed local inflammation, mucosal damage, and cytokine concentrations in rectal mucosal fluid samples from patients with HIV (with or without the use of combination antiretroviral therapy [cART]) and with or without the presence of rectal C. trachomatis. Rectal swabs from 79 MSM (with and without C. trachomatis, HIV, and cART use) who reported a history of receptive anal sex were analyzed for neutrophil activation (measured by myeloperoxidase [MPO]), mucosal leakage (measured by albumin and alpha-2-macroglobulin), and proinflammatory and anti-inflammatory cytokines. C. trachomatis infection, HIV infection, and cART use in MSM had no differential effects on rectal neutrophilic inflammation and mucosal damage. Interleukin 8 (IL-8) was found to correlate with MPO, and MPO correlated with markers of mucosal damage. In HIV-negative participants, men with C. trachomatis infection had lower concentrations of monocyte chemotactic protein 1 (MCP-1), IL-1α, and IL-1 receptor antagonist (IL-1RA) than men without rectal C. trachomatis infection (P = 0.005, 0.007, and 0.07, respectively). We found no difference in anal cytokine concentrations in HIV-infected participants in relation to the presence of C. trachomatis infection or cART use. In participants with rectal C. trachomatis infection, those who were HIV negative had lower median concentrations of IL-8 and IL-1α than those with HIV (P = 0.05 and 0.06, respectively). The slope of the regression line between MPO and IL-8 was reduced in participants with rectal C. trachomatis infection. C. trachomatis dampens cytokine concentrations but not in HIV-infected patients. The extent of mucosal damage was comparable in all patient groups. The apparent reduced neutrophil response to IL-8 in HIV-infected patients with C. trachomatis infection is in accordance with its asymptomatic course."
d0ce02b5cce5138dc8279f7fcf3f9af41244fdf5,
d81c21399f7f038007d80a646aa3b96e2400979a,"The Dutch College of General Practitioners (NHG) practice guideline 'Urinary tract infections' intended for primary health care and the Dutch Working Party on Antibiotic Policy (SWAB) practice guideline 'Antimicrobial therapy in complicated urinary tract infections' intended for specialists in secondary care, were reviewed together. - In the NHG guideline the differentiation between 'complicated' and 'uncomplicated' urinary tract infections has been replaced by categorisation into age, sex, risk group and the presence of fever, or invasion of tissues.- If urinary tract infection has been diagnosed, a dip slide test can be used to determine resistance.- The guidelines recommend the most narrow-spectrum antibiotic to reduce further increase in antimicrobial resistance.- A chapter about women with recurrent urinary tract infections has been added to the SWAB guideline. Amongst other things, the chapter provides information on the prescription of prophylactic lactobacillus in secondary care."
dc7d3afaabda44b4a12c5b27f57e3826d23dcfee,
dcbe82654e75c6c4abcedfd92efe60d3a5ea274f,"PURPOSE Whereas a diagnosis of acute uncomplicated urinary tract infection (UTI) in clinical practice comprises a battery of several diagnostic tests, these tests are often studied separately (in isolation from other test results). We wanted to determine the value of history and urine tests for diagnosis of uncomplicated UTIs, taking into account their mutual dependencies and information from preceding tests. METHODS Women with painful and/or frequent micturition answered questions about their signs and symptoms (history) of UTIs and underwent urine tests. A culture was the reference standard (103 colony-forming units per milliliter). A diagnostic index was derived using logistic regression with bootstrapped backward selection and parameter-wise shrinkage. Risk thresholds for UTI of 30% and 70% were used to analyze discriminative properties. Six models were compared: (1) history only, (2) history+ urine dipstick, (3) history+ urine dipstick + urinary sediment, (4) history+ urine dipstick+ dipslide, and (5) history+ urine dipstick+ urinary sediment+ dipslide; we then added (6) a test only for patients with an intermediate risk (between 30% and 70%) after the preceding test. RESULTS One hundred ninety-six women were included (UTI prevalence 61%). Seven variables were selected from history (3), dipstick (2), sediment (1), and dipslide (1). History correctly classified 56% of patients as having a UTI risk of either <30% or >70%. History and urine dipstick raised this to 73%. The 3 models with the addition of urinary sediment and dipslide, separately and in combination, performed hardly better. The sixth model, in which those at intermediate risk after history and received an additional test, correctly classified 83%. The patient’s suspicion of a UTI and a positive nitrite test were the strongest indicators of a UTI. CONCLUSIONS Most women with painful and/or frequent micturition can be correctly classified as having either a low or a high risk of UTI by asking 3 questions: Does the patient think she has a UTI? Is there at least considerable pain on micturition? Is there vaginal irritation? Other women require additional urine dipstick investigation. Sediment and dipslide have little added value. External validation of these recommendations is required before they are implemented in practice."
061ea453bdef0029cc99ef0f977a1a90abd72ec0,
0999b0c37b018dbf655794e28dd9298bed1a3448,"Chronic kidney disease is an increasing public health problem. In the United States, the prevalence is estimated to be approximately 11% of the adult population. Chronic kidney disease may progress to end-stage renal failure, a condition associated with high morbidity and mortality. Diabetes mellitus (DM) is one of the main causes of kidney disease and endstage renal failure. In the United States, DM is the primary diagnosis in 44% of all new cases of renal replacement therapy. Vascular complications are the most common cause of diabetic nephropathy, but it is possible that urinary tract infections (UTIs) also contribute to renal insufficiency in patients with DM."
139436680e8d6acc38d8a99472508703272a3461,"Two-hundred and forty-five heterosexual HIV-infected patients (58% women; median age 41 years) were screened for asymptomatic sexually transmitted infections (STIs) during a routine visit at a large HIV outpatient clinic in the Netherlands. High-risk sexual behaviour was rare and STI prevalence was low: three Chlamydia trachomatis infections and one case of syphilis were diagnosed. These results suggest that, in the Netherlands, screening for STI during routine visits is currently not needed for asymptomatic heterosexual HIV-infected patients."
16835ce9910f3065babc7bd90808a7725512a6db,"Objectives: Postmenopausal women with diabetes mellitus (DM) have an increased incidence of urinary tract infections (UTI) compared to women without DM. The aim of this study is to compare recurrence rates of UTI in postmenopausal women with DM after treatment with nitrofurantoin, the agent of first choice following the Dutch guidelines, with two other common prescribed antibiotics trimethoprim and norfloxacin. Methods: We used a PHARMO database with pharmacy dispensing data. A total of 8534 postmenopausal (>55 years) women with DM who received a first course of nitrofurantoin, trimethoprim or norfloxacin were included. The UTI recurrence rates after treatment with these three different antimicrobial agents were compared. Recurrence was defined as a second prescription for nitrofurantoin, trimethoprim or norfloxacin or a first with fosfomycin, amoxicillin, fluoroquinolones, or trimethoprim/sulfamethoxazole between 6 and 30 days after inclusion. Results: Postmenopausal women with DM had significantly more UTI recurrences when they were treated with nitrofurantoin (22.7%) compared to trimethoprim (17.7%) or norfloxacin (14.2%) irrespective of the treatment duration. There was a trend that longer treatment duration was associated with higher recurrence rates. Conclusions: Postmenopausal women with DM had more UTI recurrences when they are treated with nitrofurantoin, agent of first choice, compared to trimethoprim or norfloxacin."
1773af318b8d627872516ae5c990359e38f47e9a,"Background: In the Netherlands, no guidelines exist for routine sexually transmitted infection (STI) screening of human immunodeficiency virus (HIV)-infected men having sex with men (MSM). We assessed prevalence and factors associated with asymptomatic STI. Methods: MSM visiting HIV outpatient clinics of academic hospitals were tested for Chlamydia trachomatis (CT), Neisseria gonorrhoeae (NG), syphilis, and hepatitis B and C infection. Prevalence and risk factors were studied using logistic regression. Results: In total, 659 MSM were included between 2007 and 2008. STI were found in 16.0% of patients, mostly anal CT and syphilis. One new hepatitis B and 3 new hepatitis C infections were identified. In multivariate analyses, any STI (syphilis, CT, or NG) was associated with patient's age below 40 years (odds ratio [OR]: 2.5, 95% confidence interval [CI]: 1.3–5.0), having had sex with 2 or more sexual partners (OR 2.1, 95% CI: 1.2–3.5), the use of the same sexual toys with a sexual partner (OR 2.2, 95% CI: 1.0–4.9), and enema use before sex (OR: 2.3, 95% 1.2–4.2). Syphilis was independently associated with fisting with gloves versus no fisting (OR: 4.9, 95% CI: 1.7–13.7) and with rimming (OR: 5.0, 95% CI: 1.7–15.0). CT or NG were associated with age below 45 years (age 40–44 years: OR: 2.4, 95% CI: 1.1–5.3; age <40 years: OR: 2.4, 95% CI: 1.1–5.4), enema use before sex (OR: 2.4, 95% CI: 1.3–4.4) and drug use during sex (OR: 2.4, 95% CI: 1.4–4.0). Conclusions: High-risk sexual behavior was very common, and 16% of HIV-infected MSM in HIV care had an asymptomatic STI, mostly anal CT and syphilis. Development of STI screening guidelines is recommended."
236443e1b7234cfdf6b41b9a08be875a60abd332,"A significant proportion of women develop a recurrence following an initial urinary tract infection (UTI). In women with recurrent UTI, the predictive value of asymptomatic bacteriuria (ASB) for the development of a subsequent UTI has not yet been established and it is not known whether information from an asymptomatic sample is useful in guiding antimicrobial therapy. To address these questions, we used data that originated from the 'Non-antibiotic prophylaxis for recurrent urinary tract infections' (NAPRUTI) study: two randomized controlled trials on the prevention of recurrent UTI in non-hospitalized premenopausal and postmenopausal women (n=445). During 15months of follow-up, no difference was observed in the time to a subsequent UTI between women with and without ASB at baseline (hazard ratio: 1.07, 95% CI 0.80-1.42). The antimicrobial susceptibility and pulsed-field gel-electrophoresis (PFGE) pattern of 50 Escherichia coli strains causing a UTI were compared with those of the ASB strain isolated 1month previously. The predictive values of the susceptibility pattern of the ASB strain, based on resistance prevalence at baseline, were ≥76%, except in the case of nitrofurantoin- and amoxicillin-clavulanic acid-resistance. Asymptomatic and symptomatic isolates had similar PFGE patterns in 70% (35/50) of the patients. In the present study among women with recurrent UTI receiving prophylaxis, ASB was not predictive for the development of a UTI. However, the susceptibility pattern of E. coli strains isolated in the month before a symptomatic E. coli UTI can be used to make informed choices for empirical antibiotic treatment in this patient population."
251406abcf95a2e967aa84a82fa4b8c825c5d1d5,
2e4811114d4d7cdef8928b7d11fc64cc89da5878,"BACKGROUND
Recurrent urinary tract infections (RUTI) are common in women who are pregnant and may cause serious adverse pregnancy outcomes for both mother and child including preterm birth and small-for-gestational-age babies. Interventions used to prevent RUTI in women who are pregnant can be pharmacological (antibiotics) or non-pharmacological (cranberry products, acupuncture, probiotics and behavioural modifications). So far little is known about the best way to prevent RUTI in pregnant women.


OBJECTIVES
To assess the effects of interventions for preventing recurrent urinary tract infections in pregnant women.The primary maternal outcomes were RUTI before birth (variously defined) and preterm birth (before 37 weeks). The primary infant outcomes were small-for-gestational age and total mortality.


SEARCH METHODS
We searched the Cochrane Pregnancy and Childbirth Group's Trials Register (8 June 2012) and reference lists of retrieved articles.


SELECTION CRITERIA
Published, unpublished and ongoing randomised controlled trials (RCTs), quasi-RCTs, clustered-randomised trials and abstracts of any intervention (pharmacological and non-pharmacological) for preventing RUTI during pregnancy (compared with another intervention, placebo or with usual care).


DATA COLLECTION AND ANALYSIS
Two review authors independently evaluated the one identified trial for inclusion and assessed trial quality. Two review authors extracted data. Data were checked for accuracy.


MAIN RESULTS
The review included one trial involving 200 women. The trial compared a daily dose of nitrofurantoin and close surveillance (regular clinic visit, urine cultures and antibiotics when a positive culture was found) with close surveillance only. No significant differences were found for the primary outcomes: recurrent pyelonephritis (risk ratio (RR) 0.89, 95% confidence interval (CI) 0.31 to 2.53, one study, 167 women), recurrent urinary tract infection before birth (RR 0.30, 95% CI 0.06 to 1.38; one study 167 women) and preterm birth (before 37 weeks) (RR 1.18, 95% CI 0.42 to 3.35; one study 147 women). The incidence of asymptomatic bacteriuria (ASB) (at least 10(3) colonies per mL) (secondary outcome), only reported in women with a clinic attendance rate of more than 90% (RR 0.55, 95% CI 0.34 to 0.89; one study, 102 women), was significantly reduced in women who received nitrofurantoin and close surveillance.


AUTHORS' CONCLUSIONS
A daily dose of nitrofurantoin and close surveillance has not been shown to prevent RUTI compared with close surveillance alone. A significant reduction of ASB was found in women with a high clinic attendance rate and who received nitrofurantoin and close surveillance. There was limited reporting of both primary and secondary outcomes for both women and infants. No conclusions can be drawn regarding the optimal intervention to prevent RUTI in women who are pregnant. Randomised controlled trials comparing different pharmacological and non-pharmacological interventions are necessary to investigate potentially effective interventions to prevent RUTI in women who are pregnant."
30a6dcbb11abb15dfb7e9390423542005ba878e4,"Many hospitalized patients receive a urinary catheter during their stay. In 21-54% of patients, however, there is no appropriate indication for this. The most significant complication caused by the use of urinary catheters is the development of a urinary tract infection (UTI), one of the most common nosocomial infections. In 71-80% of hospital acquired UTIs a urinary catheter is present. The duration of the presence of a catheter is the major risk factor for catheter-associated UTI. Reducing the number of inappropriate catheterisations is an effective way of preventing catheter-related UTIs. Inappropriate use of indwelling urinary catheters can be reduced by maintaining strict guidelines on justifiable indications for inserting a urinary catheter, verifying daily whether the indication still applies, and by timely removal of the catheter when it is not or no longer needed."
70f1584b4bf4432422df2af999238144a9d07e77,
87dba9f592e18cb557af41ec3893d77e0696a18c,"For women with recurrent urinary tract infections (rUTI), the contribution of antibiotic use versus patient-related factors in determining the presence of antimicrobial resistance in faecal and urinary Escherichia coli, obtained from the same patient population, has not been assessed yet. Within the context of the ‘Non-antibiotic prophylaxis for recurrent urinary tract infections’ (NAPRUTI) study, the present study assessed determinants of antimicrobial resistance in E. coli isolated from urinary and faecal samples of women with rUTIs collected at baseline. Potential determinants of resistance were retrieved from self-administered questionnaires. From 434 asymptomatic women, 433 urinary and 424 faecal samples were obtained. E. coli was isolated from 146 (34%) urinary samples and from 336 (79%) faecal samples, and subsequently tested for antimicrobial susceptibility. Multivariable analysis showed trimethoprim/sulfamethoxazole (SXT) use three months prior to inclusion to be associated with urine E. coli resistance to amoxicillin (OR 3.6, 95% confidence interval: 1.3–9.9), amoxicillin-clavulanic acid (OR 4.4, 1.5–13.3), trimethoprim (OR 3.9, 1.4–10.5) and SXT (OR 3.2, 1.2–8.5), and with faecal E. coli resistance to trimethoprim (OR 2.0, 1.0–3.7). The number of UTIs in the preceding year was correlated with urine E. coli resistance to amoxicillin-clavulanic acid (OR 1.11, 1.01–1.22), trimethoprim (OR 1.13, 1.03–1.23) and SXT (OR 1.10, 1.01–1.19). Age was predictive for faecal E. coli resistance to amoxicillin (OR 1.02, 1.00–1.03), norfloxacin and ciprofloxacin (both OR 1.03, 1.01–1.06). In conclusion, in women with rUTI different determinants were found for urinary and faecal E. coli resistance. Previous antibiotic use and UTI history were associated with urine E. coli resistance and age was a predictor of faecal E. coli resistance. These associations could best be explained by cumulative antibiotic use."
8e332c434ab3c937b235c464bafeccd10f79ae87,
8eefed587a309f536c5a1a4f5d5a0c908cc3e46c,What's known on the subject? and What does the study add?
967dcb07a76f1634632f1f8939c3f108b066e1ca,"BACKGROUND
The efficacies and adverse effects of different antibiotics for uncomplicated urinary tract infections (UTIs) have been studied by standard meta-analytic methods using pairwise direct comparisons of antimicrobial treatments: the effects of one treatment are compared to those of either another treatment or placebo. However, for clinical decisions, we need to know the effectiveness of each possible treatment in comparison with all relevant alternatives, not with just one.


OBJECTIVES
To compare the efficacies and adverse effects of all relevant antibiotics for UTI treatment simultaneously by performing a network meta-analysis using direct and indirect treatment comparisons.


METHODS
Using logistic regression analysis, we performed a network meta-analysis of randomized controlled trials (RCTs) published after 1999 that compared different oral antibiotic or placebo regimens for UTI treatment in general practice or outpatient settings. We looked at five binary outcomes: early clinical, early bacteriological, late clinical and late bacteriological outcomes, as well as adverse effects. Consequently, a ranking of the antibiotic regimens could be composed.


RESULTS
Using a network structure, we could compare and rank nine treatments from 10 studies. Overall, ciprofloxacin and gatifloxacin appeared the most effective treatments, and amoxicillin-clavulanate appeared the least effective treatment. In terms of adverse effects, there were no significant differences.


DISCUSSION
Network meta-analysis shows some clear efficacy differences between different antibiotic treatments for UTI in women. It provides a useful tool for clinical decision making in everyday practice. Moreover, the method can be used for meta-analyses of RCTs across primary care and beyond."
a6978553bf8b92a5e60527f70fbb5a7ece6bc0c0,Toll‐like receptors (TLRs) are transmembrane receptors that activate cells of the innate immune systems upon recognition of pathogen‐associated molecular patterns. The TLR4 is an essential component of the innate immune response to various microorganisms. We investigated the impact of TLR4 polymorphism on development of opportunistic diseases in HIV‐infected patients.
ad4e7533a922bb355ea6d6429462c244021e2302,"Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible."
aed4daffd6b01196ca6a60ddb63d5b068cd63cad,
b97b12eb2b5b9356d6ba448097270b439748d501,"Chronic kidney disease is an increasing public health problem. In the United States, the prevalence is estimated to be approximately 11% of the adult population. Chronic kidney disease may progress to end-stage renal failure, a condition associated with high morbidity and mortality. Diabetes mellitus (DM) is one of the main causes of kidney disease and endstage renal failure. In the United States, DM is the primary diagnosis in 44% of all new cases of renal replacement therapy. Vascular complications are the most common cause of diabetic nephropathy, but it is possible that urinary tract infections (UTIs) also contribute to renal insufficiency in patients with DM."
c237b42493b4fd1a1703bb3455084bc7499e16c8,"BACKGROUND
Growing antibiotic resistance warrants studying nonantibiotic prophylaxis for recurrent urinary tract infections (UTIs). Use of lactobacilli appears to be promising.


METHODS
Between January 2005 and August 2007, we randomized 252 postmenopausal women with recurrent UTIs taking part in a double-blind noninferiority trial to receive 12 months of prophylaxis with trimethoprim-sulfamethoxazole, 480 mg, once daily or oral capsules containing 109 colony-forming units of Lactobacillus rhamnosus GR-1 and Lactobacillus reuteri RC-14 twice daily. Primary end points were the mean number of symptomatic UTIs, proportion of participants with at least 1 UTI during 12 months, time to first UTI, and development of antibiotic resistance by Escherichia coli.


RESULTS
The mean number of symptomatic UTIs in the year preceding randomization was 7.0 in the trimethoprim-sulfamethoxazole group and 6.8 in the lactobacilli group. In the intention-to-treat analysis, after 12 months of prophylaxis, these numbers were 2.9 and 3.3, respectively. The between-treatment difference of 0.4 UTIs per year (95% CI, -0.4 to 1.5) was outside our noninferiority margin. At least 1 symptomatic UTI occurred in 69.3% and 79.1% of the trimethoprim-sulfamethoxazole and lactobacilli participants, respectively; median times to the first UTI were 6 and 3 months, respectively. After 1 month of trimethoprim-sulfamethoxazole prophylaxis, resistance to trimethoprim-sulfamethoxazole, trimethoprim, and amoxicillin had increased from approximately 20% to 40% to approximately 80% to 95% in E coli from the feces and urine of asymptomatic women and among E coli causing a UTI. During the 3 months after trimethoprim-sulfamethoxazole discontinuation, resistance levels gradually decreased. Resistance did not increase during lactobacilli prophylaxis.


CONCLUSIONS
In postmenopausal women with recurrent UTIs, L rhamnosus GR-1 and L reuteri RC-14 do not meet the noninferiority criteria in the prevention of UTIs when compared with trimethoprim-sulfamethoxazole. However, unlike trimethoprim-sulfamethoxazole, lactobacilli do not increase antibiotic resistance. TRIAL REGISTRATION isrctn.org Identifier: ISRCTN50717094."
0a82fae2fbaa2d82173918a32b42700eba3edb25,"Background In the Netherlands no guidelines exist for routine STI screening of HIV-infected patients. In a study in two academic hospitals in the Netherlands, 16% of HIV-infected MSM in HIV care had an asymptomatic STI, making regular STI screening in this group appropriate. It is unclear whether regular STI screening should also be considered for HIV-infected heterosexual men and women. Therefore, we studied the prevalence of, and factors associated with asymptomatic STI in a representative group of HIV positive heterosexual men and women. Methods HIV-1 infected heterosexual patients visiting the HIV outpatient clinic of the Academic Medical Center in Amsterdam, the Netherlands, were screened for STI during a routine visit. Patients spontaneously reporting symptoms compatible with STI were excluded. Chlamydia trachomatis (CT) and Neisseria gonorrhoeae (NG) were tested by PCR on throat swabs, vaginal or anal self swabs and urine samples, depending on gender and sexual behaviour. Hepatitis B virus (HBV) and hepatitis C virus (HCV) serology were performed and patients were screened for syphilis by TPHA and RPR. Participants were interviewed by a trained interviewer about sexual risk behaviour in the previous 6 months. Results Between October 2007 and June 2008, 248 men and women (median age 41 years, range 18–82) were included. 56% was female and 42% had Dutch ethnicity, the main other ethnicities were Ghanese, Surinamese/Antillean and Sub-Saharan African. The Abstract P1-S5.27 table 1 shows sexual behaviour of participants in the preceding 6 months. Reported history of STI in the last 6 months was 2%, and lifetime history of STI (non-HIV) was 49%. 29% of participants had had STI testing in the preceding 6 months, and 64% had an STI test more than 6 months ago. Only four patients (1.6%) had an asymptomatic STI. Two women were diagnosed with vaginal CT, 1 man with urethral CT and a female intravenous drug user with a history of syphilis had a new syphilis infection. We also identified one hitherto undiagnosed HCV infection; this was in a male intravenous drug user without a history of sex with men; we did not classify this as an STI. Abstract P1-S5.27 Table 1 Sexual behaviour of HIV-infected heterosexual males and females visiting an HIV outpatient clinic in the Netherlands, 2007–2008 Sexual behaviour +/total % Did not have sex in last 6 months 65/197 33.0 Had 1 sexual partner last 6 months 110/193 60.1 Had 2 or more sexual partners last 6 months 19/193 9.8 Had casual sexual partner(s) in last 6 months 24/193 12.4 Had unprotected vaginal sex in last 6 months 46/193 20.7 Had unprotected anal sex in last 6 months 4/193 2.0 Conclusions In this population of HIV-1 infected heterosexual patients in care for HIV infection high risk sexual behaviour is rare and asymptomatic STI are uncommon. Our study results suggest that routine screening in asymptomatic heterosexual patients is currently not needed."
519ded64e4a10e3f168be802ff767a72a79cef0a,"As in other countries, the growing resistance to antimicrobial drugs is also taking place in the Netherlands; the primary cause being the total consumption of antibiotics. Given the steady decline in the discovery of new antimicrobials, better use of agents currently available is warranted. Guidelines describing appropriate antimicrobial therapy play an important role; however, such guidelines are not optimally used in daily practice. Quality indicators can be used to assess the quality of antibiotic treatment and evaluate the impact of interventions aimed at improving care. Quality indicators used for evaluating treatment of infections of the respiratory and urinary tracts are developed previously. A comprehensive set of indicators that could be used to assess the quality of hospital antibiotic use for all bacterial infections has not yet been developed. A new project has recently been started in the Netherlands called 'The development of Reliable generic quality Indicators for the optimalisation of ANTibiotic use in the hospital' (RIANT study) for developing such a set of comprehensive indicators."
616fb9d70920acc4ad20e4bbf9bee1b313b86a22,"BACKGROUND
The increasing prevalence of uropathogens resistant to antimicrobial agents has stimulated interest in cranberries to prevent recurrent urinary tract infections (UTIs).


METHODS
In a double-blind, double-dummy noninferiority trial, 221 premenopausal women with recurrent UTIs were randomized to 12-month prophylaxis use of trimethoprim-sulfamethoxazole (TMP-SMX), 480 mg once daily, or cranberry capsules, 500 mg twice daily. Primary end points were the mean number of symptomatic UTIs over 12 months, the proportion of patients with at least 1 symptomatic UTI, the median time to first UTI, and development of antibiotic resistance in indigenous Escherichia coli.


RESULTS
After 12 months, the mean number of patients with at least 1 symptomatic UTI was higher in the cranberry than in the TMP-SMX group (4.0 vs 1.8; P = .02), and the proportion of patients with at least 1 symptomatic UTI was higher in the cranberry than in the TMP-SMX group (78.2% vs 71.1%). Median time to the first symptomatic UTI was 4 months for the cranberry and 8 months for the TMP-SMX group. After 1 month, in the cranberry group, 23.7% of fecal and 28.1% of asymptomatic bacteriuria E coli isolates were TMP-SMX resistant, whereas in the TMP-SMX group, 86.3% of fecal and 90.5% of asymptomatic bacteriuria E coli isolates were TMP-SMX resistant. Similarly, we found increased resistance rates for trimethoprim, amoxicillin, and ciprofloxacin in these E coli isolates after 1 month in the TMP-SMX group. After discontinuation of TMP-SMX, resistance reached baseline levels after 3 months. Antibiotic resistance did not increase in the cranberry group. Cranberries and TMP-SMX were equally well tolerated.


CONCLUSION
In premenopausal women, TMP-SMX, 480 mg once daily, is more effective than cranberry capsules, 500 mg twice daily, to prevent recurrent UTIs, at the expense of emerging antibiotic resistance.


TRIAL REGISTRATION
isrctn.org Identifier: ISRCTN50717094."
ccb73644cbe613c8980c716297fe56bbbbd3eae3,"BACKGROUND
Pneumocystis jiroveci pneumonia (PCP) is an important cause of morbidity and mortality in renal transplant recipients (RTRs). Chemoprophylaxis with trimethoprim/sulphamethoxazole is recommended during the early post-transplantation period, but the optimal duration has not been determined and a main drawback of chemoprophylaxis is the development of resistance of the commensal faecal flora. A cluster outbreak of PCP occurred in our outpatient Renal Transplant Unit. We aimed to investigate risk factors for PCP in RTRs to determine who should receive long-term chemoprophylaxis.


METHODS
In a case-control study, we investigated common demographic variables and immunological parameters. Nine PCP cases diagnosed between August 2006 and April 2007 were matched with 18 control patients, who did not develop PCP, received their transplant in the same time-period and had a similar follow-up period with a comparable immunosuppressive drug regimen.


RESULTS
The median time from transplantation to PCP was 19 months. We observed no significant differences in gender, age, donor type or number of rejections. In PCP cases, the median lymphocyte count just before PCP diagnosis was 0.49 (0.26-0.68), which was significantly reduced compared to the control patients after a similar follow-up period (median 1.36, 0.59-3.04, P = 0.002). This lymphocytopaenia was chronic and existed in most patients already for many months. CD4(+) T-cell counts were also significantly reduced in the PCP cases. We found no difference in the Th1, Th2 and Th17 subsets between PCP cases and control patients.


CONCLUSION
Long-term prophylactic therapy for PCP may be indicated for RTR with persistent severe lymphocytopaenia."
d98c7131bbabb11d13c8e39786b8d7989b8af59b,
079706128a537a83b8d8aa53e513a981efc6ce94,
4b1c16d6cce8cb1735f91295df1c8b3b5b4fb0e1,"Leydon and colleagues found that women attributed urinary tract infection (UTI) to lifestyle habits and behaviours.1 Evidence is accumulating that patient education and counselling are essential for effective prevention in chronic diseases. This is also true for UTI,2 but information about the knowledge of this patient group is limited.

We …"
8265d9640576586604d0d99b21916916804d33f4,
add680021b392cc7373e0cfd20c4174d474077bc,"Background We investigated differences in immune restoration and onset of new AIDS-defining events on combination antiretroviral therapy (cART) among HIV type-1 (HIV-1)-infected patients of different regional origin now living in the Netherlands. Methods Treatment-naive adults reaching plasma viral load (pVL)<400 copies/ml within 9 months of starting cART were selected from the Netherlands ATHENA cohort. CD4+ T-cell response on cART was determined over 7 years using mixed models. CD4+ T-cell counts were excluded from the analyses at the first of two consecutive measurements of pVL≥400 copies/ml following prior suppression to <400 copies/ml. Multivariate analyses included gender, age, CD4+ T-cell count and pVL prior to cART, hepatitis coinfection, HIV-1 transmission and region of origin (Western Europe/North America [WN], sub- Saharan Africa [SSA], Southeast Asia [SEA], Latin America/Caribbean [LAC] or other). Results For 6,057 selected patients (WN 3,947, SSA 989, SEA 237, LAC 695 and other 189), median follow-up was 3.2 years (WN 3.3, SSA 2.9, SEA 3.2, LAC 2.7 and other 2.7). CD4+ T-cell increase in the first 6 months of cART was lower in males than females (-26 cells/mm3; P<0.0001) and in patients from SSA compared with WN (-36 cells/mm3; P<0.0001). Because men from SSA started with lower CD4+ T-cell counts than men from WN, they continued to lag behind and had lower absolute CD4+ T-cell counts after 7 years of cART. Furthermore, cumulative tuberculosis incidence after 7 years of cART was higher in SSA compared with WN (4.5% versus 0.5%, hazard ratio 5.08, 95% confidence interval 2.22–11.60). Conclusions HIV-1-infected immigrants from SSA have blunted immune restoration on fully suppressive cART and should be identified at an earlier disease stage. Our results call for more intensive screening for both latent and active tuberculosis in these patients."
af731aa41a6e8cc76a1d49e076478eb7af0bb47f,
c5e3b444b2bcb1eb2c1a8f15836e3cbd27deaef6,"Guidelines for the diagnosis, prevention, and management of persons with catheter-associated urinary tract infection (CA-UTI), both symptomatic and asymptomatic, were prepared by an Expert Panel of the Infectious Diseases Society of America. The evidence-based guidelines encompass diagnostic criteria, strategies to reduce the risk of CA-UTIs, strategies that have not been found to reduce the incidence of urinary infections, and management strategies for patients with catheter-associated asymptomatic bacteriuria or symptomatic urinary tract infection. These guidelines are intended for use by physicians in all medical specialties who perform direct patient care, with an emphasis on the care of patients in hospitals and long-term care facilities."
cf78a6205b96f42a749ffb361df5b4ecd368df68,
d451ea7d9cde7e81c82cd55b92a82946c2037ddd,"In-hospital adult cardiopulmonary resuscitation is successful in only approximately 20% of cases and may result in permanent neurological damage. Two reasons justify not commencing resuscitation: either the patient does not want to be resuscitated, or resuscitation is considered medically futile by the doctor. This subject should be discussed timely with all chronically ill patients who are likely to be admitted to hospital, preferably in the outpatient clinic setting, and results must be communicated with all doctors involved (e.g. general practitioners). Here we describe 3 cases that demonstrate the need to discuss possible restrictions on cardiopulmonary resuscitation with all chronically ill patients, regardless of their age. The first was a 45-year-old HIV-positive male with chronic clinical depression who refused ICU care, the second a 75-year-old patient whose initial 'do no resuscitate' order was reversed based on the wishes of her daughter and the third a 45-year-old female with sickle cell disease who expressed a sustained wish not to be treated in the ICU or to be resuscitated."
ef510611747617d097b0b56e82e48aa942eeada4,
f295e8f042e6f2b38e94aeea08fbab86e29c85bc,"BACKGROUND
Door-to-needle time (DNT), defined as the time between arrival at the emergency department (ED) and intravenous (iv) antibiotic administration is of crucial importance in the treatment of patients suffering from serious infections. The aim of this project was to reduce the DNT for patients with a serious infection as primary outcome parameter.


METHODS
All adult patients arriving at the ED with a suspected infection for whom admission and iv antibiotics were indicated were included.


RESULTS
Firstly, baseline DNT was measured and potential delaying factors were identified. Subsequently, five tailored interventions were implemented at regular intervals and their effects on the DNT were analysed. The interventions were: 1) additional resident attendance during peak hours, 2) immediate examination by residents prior to laboratory results, 3) chest X-ray at the ED instead of the external radiology department, 4) iv antibiotic administration at the ED instead of the ward and finally, 5) primary dipstick urine analysis at the ED. A total of 295 patients were included (53.9% men), median age was 59 years (IQR 46 to 73). Median baseline DNT was 183 min (IQR 122 to 296). Implementation of the first three interventions did not reduce the DNT ; however, after implementation of the fourth (administer all antibiotics at the ED) and finally all five interventions the DNT was reduced by 15.3% (p=0.040) to a final median DNT of 155 min (IQR 95 to 221).


CONCLUSION
Identification of delaying factors and implementation of tailored interventions reduces the DNT ."
0ced86d500a357bccf0dc2fbc6c9dfc059c0be76,"Background Lipoatrophy is known to be associated with stavudine as part of the treatment for HIV infection, but it is less clear if this serious side effect is also related to other nucleoside reverse transcriptase inhibitors like zidovudine. We aimed to determine whether zidovudine-sparing first-line antiretroviral therapy would lead to less lipoatrophy and other metabolic changes than zidovudine-containing therapy. Methodology/Principal Findings Fifty antiretroviral therapy-naïve HIV-1 infected men with an indication to start antiretroviral therapy were included in a randomized single blinded clinical trial. Randomisation was between zidovudine-containing therapy (zidovudine/lamivudine+lopinavir/ritonavir) and zidovudine-sparing therapy (nevirapine+lopinavir/ritonavir). Main outcome measures were body composition assessed by computed tomography and dual-energy X-ray absorptiometry scan and lipid profile before and after 3, 12, 24 months of antiretroviral therapy. In the zidovudine/lamivudine+lopinavir/ritonavir group, from 3 months onward limb fat decreased progressively by 684±293 grams (estimated mean±standard error of the mean)(p = 0.02) up to 24 months whereas abdominal fat increased, but exclusively in the visceral compartment (+21.9±8.1 cm2, p = 0.008)). In contrast, in the nevirapine+lopinavir/ritonavir group, a generalized increase in fat mass was observed. After 24 months no significant differences in high density lipoprotein and total/high density lipoprotein cholesterol ratio were found between both treatment groups, but total and low density lipoprotein cholesterol levels were higher in the nevirapine+lopinavir/ritonavir group (6.1±0.2 versus 5.3±0.2 and 3.6±0.1 versus 2.8±0.1 mmol/l respectively, p<0.05). Virologic response and safety were comparable in both groups. Conclusions/Significance Zidovudine/lamivudine+lopinavir/ritonavir, but not nevirapine+lopinavir/ritonavir in antiretroviral therapy-naïve patients, is associated with lipoatrophy and greater relative intraabdominal lipohypertrophy, suggesting that zidovudine/lamivudine contributes to both these features of lipodystrophy. These findings support to no longer consider zidovudine/lamivudine as one of the preferred possible components of first-line antiretroviral therapy where alternative treatments are available. Trial Registration ClinicalTrials.gov NCT 00122226"
34f4a0a03412607f73c83db4bd0be2441a6d324e,"Three patients with central nervous system aspergillosis due to azole-resistant Aspergillus fumigatus (associated with a leucine substitution for histidine at codon 98 [L98H] and a 34-base pair repeat in tandem in the promoter region) are described. The patients were treated with combination therapy or with polyenes, but all patients died. Azole resistance significantly complicates the management of aspergillosis by delaying the initiation of adequate therapy and because effective alternative antifungal drugs are lacking."
71f8fceac38dc7b78128f10dd34dbc8a95bc6c9b,"Objective:We studied changes in bone mineral density (BMD) and bone turnover after initiation of combination antiretroviral therapy (cART) and the contribution of zidovudine/lamivudine (ZDV/3TC) in particular. Design:Randomized clinical trial comparing lopinavir/ritonavir(LPV/r) + ZDV/3TC with LPV/r + nevirapine (NVP) in 50 cART-naive men. Methods:Dual energy X-ray absorptiometry (DXA) and quantitative computed tomography scans (QCT) were performed at baseline and 3, 12, and 24 months after cART initiation. Serum 25-hydroxy-vitamin D3, parathyroid hormone (PTH), osteocalcin, and urine deoxypyridinoline (DPD)/creatinine ratio were measured. Results:BMD decreased rapidly in both femoral neck and lumbar spine after cART initiation. BMD loss during 24 months measured by DXA, but not by QCT, was greater in the ZDV/3TC/LPV/r group compared to the NVP/LPV/r group [femoral neck: −6.3% ± 1.0% (P < 0.0001) compared to −2.3% ± 0.9% (P = 0.01), between-group P = 0.0006); lumbar spine: −5.1% ± 0.8% (P < 0.0001) compared to −2.6% ± 0.7% (P = 0.0006), between-group P = 0.07]. Osteocalcin [+1.60 ± 0.32 (P < 0.0001) and +1.81 ± 0.29 (P < 0.0001) nmol/l] and the urine DPD/creatinine ratio [+1.35 ± 0.44 (P = 0.0029) and +1.19 ± 0.38 nmol/mmol (P = 0.0024)] increased in both groups over 24 months, with no significant difference between groups. PTH increased to a greater degree in the NVP/LPV/r group [+2.0 ± 0.31 pmol/l (P < 0.0001)] compared to [+0.81 ± 0.33 pmol/l (P = 0.021) in the ZDV/3TC/LPV/r group]. Conclusion:BMD in both femoral neck and lumbar spine decreased rapidly after initiation of cART, in parallel to an increase in bone turnover. The greater bone loss in the ZDV/3TC/LPV/r group compared to the NVP/LPV/r group suggests that ZDV/3TC contributes to this process. The PTH increase does not explain this greater bone loss."
8f9c5c5e7a6a1dfde860b68bf9182ae14978f1aa,"Summary.  The aim of this study was to study the development of HCV‐specific T cell immunity during acute HCV infection in the presence of an existing HIV‐1 infection in four HIV‐1 infected men having sex with men. A comprehensive analysis of HCV‐specific T cell responses was performed at two time points during acute HCV infection using a T cell expansion assay with overlapping peptide pools spanning the entire HCV genome Three patients with (near) normal CD4+ T cell counts (range 400–970 × 106/L) either resolved (n = 1) or temporary suppressed HCV RNA. In contrast, one patient with low CD4+ T cell counts (330 × 106/L), had sustained high HCV RNA levels. All four patients had low HCV‐specific CD8+ T cell responses, and similar magnitudes of CD4+ T cell responses. Interestingly, individuals with resolved infection or temporary suppression of HCV‐RNA had HCV‐specific CD4+ T cell responses predominantly against nonstructural (NS) proteins. While the individual with high HCV RNA plasma concentrations had CD4+ T cell responses predominantly directed against Core. Our data show that an acute HCV infection in an HIV‐1 infected person can be suppressed in the presence of HCV‐specific CD4+ T cell response targeting non‐structural proteins. However further research is needed in a larger group of patients to evaluate the role of HIV‐1 on HCV‐specific T cell responses in relation to outcome of acute HCV infection."
b5b030bb3f1ceae167c78c4c331fa66ae9c99b31,"A fatal case is reported concerning a severely immunocompromised 50-year-old female renal transplant recipient who developed fever and confusion. Cerebral imaging with contrast-enhanced computed tomography (CT) scans showed no abnormalities while subsequently performed magnetic resonance imaging (MRI ) showed clear abnormalities in the basal ganglia. By that time serology and polymerase chain reaction had confirmed the diagnosis of cerebral toxoplasmosis. Because of the suboptimal sensitivity of these tests negative results should be handled with care. Once cerebral toxoplasmosis is suspected in at-risk patients, treatment should be started empirically pending the confirmation of the diagnosis. A normal cerebral CT scan does not preclude cerebral toxoplasmosis.In these situations MRI can give important additional information."
e04fa45544c1d73505f5746868192479c37cc372,"Immune reconstitution inflammatory syndrome (IRIS) occurs in a subpopulation of HIV-infected patients after the introduction of antiretroviral therapy (ART). The purpose of this review is to describe the immunopathogenesis, risk factors, diagnostic problems, treatment and prevention of IRIS. A literature search was performed and finally 15 recent articles were selected. The immunopathogenesis of IRIS is characterised by a dysbalanced restoration of the immune system resulting in pathological inflammation. Risk factors are low baseline CD4-cell count, an excellent virological response, an increased antigenic burden of an opportunistic infection and early initiation of ART after an opportunistic infection. The differential diagnosis of IRIS is elaborate. Treatment options include discontinuation of ART , corticosteroids or pathogen-specific therapy. Diagnosis can be difficult, because IRIS may manifest with a diverse range of clinical presentations. Adopting one case definition and performing more research regarding diagnosis and treatment of IRIS are important recommendations for future studies."
028ebb694c117f352d77ff3f8492cba8610bc4fd,
30e793e0a6679e19bd196ead2b02c55b8d0c428b,
3d9ff8b234103ed905d992c0e0dc428626df5bb8,
67ffc66ba00f82029a2002276550ff82a26aaf9f,"OBJECTIVE—Women with diabetes have a high incidence and complication rate of urinary tract infections (UTIs). Our aims were to compare current treatment strategies with respect to recurrence rates in women with diabetes with those without diabetes. RESEARCH DESIGN AND METHODS—We used a Dutch registration database containing pharmacy dispensing data. A total of 10,366 women with diabetes (17.5% premenopausal) (aged ≤55 years) and 200,258 women without diabetes (68% premenopausal) who received a first course of trimethoprim, nitrofurantoin, fosfomycin, or norfloxacin between January 1999 and January 2006 were included. We compared short (≤5 days) with long (>5 days) prescriptions and norfloxacin with trimethoprim, nitrofurantoin, and fosfomycin. A recurrence was defined as a second prescription for one of the above-mentioned agents or a first with amoxicillin (clavulanic acid), fluoroquinolones, or trimethoprim/sulfamethoxazole between 6 and 30 days after inclusion. RESULTS—Premenopausal women with diabetes more often received a long (26.5 vs. 19.2%; P < 0.001) treatment with norfloxacin (10.7 vs. 6.2%; P < 0.001) but still had a higher recurrence rate (16.1 vs. 12.2%; P = 0.003) compared with those without diabetes. Similarly, postmenopausal women with diabetes more often received a longer (32.8 vs. 28.8%; P < 0.001) treatment with norfloxacin (15.2 vs. 12.7%; P < 0.001) but had a higher recurrence rate (19.1 vs. 16.4%; P < 0.001) compared with those without diabetes. CONCLUSIONS—Despite the fact that patients with diabetes more often received longer and more potent initial treatment than patients without diabetes, pre- and postmenopausal women with diabetes more often had recurrences of their UTIs."
6e05de3e19299cd90c6ec022e65a74c3d2e26ad2,
91270b38295e0788eb064b0dcdc95361f44ef0ed,"BACKGROUND
Uncomplicated urinary tract infections (UTIs) are common among female patients. According to the national guidelines of the Dutch College of General Practitioners (GPs), the drugs of first and second choice as therapy for UTIs are nitrofurantoin and trimethoprim with resistance percentages of 2% and 23%, respectively. The third choice is fosfomycin tromethamine for which no current resistance data from The Netherlands are available. The aim of this study was to determine these resistance percentages.


METHODS
During 2003-04, urine samples were collected from a representative sample of 21 general practices spread over The Netherlands, the Sentinel Stations of The Netherlands Institute for Health Services Research (NIVEL). Escherichia coli isolated from female patients visiting their GP with symptoms of an acute, uncomplicated UTI were used. Fosfomycin tromethamine susceptibility was determined by Etests. An MIC of fosfomycin tromethamine of 64 mg/L or lower was considered to indicate susceptibility, and MIC values of 96 mg/L or higher were considered to indicate resistance. E. coli ATCC 25922 was used as a reference strain.


RESULTS
In total, 1705 E. coli strains were tested, of which 11 (0.65%) were resistant to fosfomycin tromethamine. The MIC(50) and MIC(90) values for this population were 1 and 4 mg/L, respectively. Within the inhibition zone of 162 susceptible E. coli, resistant mutant colonies were observed, of which after repetition of the susceptibility testing 68 were resistant. In total, 79 (5%) strains were resistant to fosfomycin tromethamine. There was no cross-resistance observed between fosfomycin tromethamine and other antimicrobial agents tested previously.


CONCLUSIONS
The high in vitro susceptibility to fosfomycin tromethamine in this population and the lack of cross-resistance between fosfomycin tromethamine and other agents together with the extensive global clinical experience support the choice of the national guidelines of the Dutch College of GPs to include fosfomycin tromethamine as a therapeutic option in general practice for uncomplicated UTIs."
bee7be220f57c4aa5659e826e67ec13d1c12b306,"BACKGROUND
Appropriateness of antibiotic treatment of urinary tract infection (UTI) is important. The aim of this study was to develop a set of valid, reliable, and applicable indicators to assess the quality of antibiotic use in the treatment of hospitalized patients with complicated UTI.


METHODS
A multidisciplinary panel of 13 experts reviewed and prioritized recommendations extracted from a recently developed evidence-based national guideline for the treatment of complicated UTI. The content validity was assessed in 2 consecutive rounds with an in-between discussion meeting. Next, we tested the feasibility, interobserver reliability, opportunity for improvement, and case-mix stability of the potential indicators for a data set of 341 inpatients and outpatients with complicated UTIs who were treated at the urology or internal medicine departments at 4 hospitals.


RESULTS
The panel selected and prioritized 13 indicators. Four and 9 indicators were performed satisfactorily in the urology and internal medicine departments, as follows: performance of urine culture, prescription of treatment in accordance with guidelines, tailoring of treatment on the basis of culture results, and a switch to oral treatment when possible in the urology and internal medicine departments; and selective use of fluoroquinolones, administration of treatment for at least 10 days, prescription of treatment for men in accordance with guidelines, replacement of catheters in patients with UTI, and adaptation of the dosage on the basis of renal function in the internal medicine department.


CONCLUSION
A systemic evidence- and consensus-based approach was used to develop a set of valid quality indicators. Tests of the applicability of these indicators in practice in different settings is essential before they are used in quality-improvement strategies."
d0c72bc73511d045ed074964c12427ee0a0172e7,
32c4ba7a0a0fbd05925b6422afd4dc42627c2e4e,
8f5b87fd593e1fad836c0c9a79659a58a851033c,"Background:The extent to which the prognosis for AIDS and death of patients initiating highly active antiretroviral therapy (HAART) continues to be affected by their characteristics at the time of initiation (baseline) is unclear. Methods:We analyzed data on 20,379 treatment-naive HIV-1-infected adults who started HAART in 1 of 12 cohort studies in Europe and North America (61,798 person-years of follow-up, 1844 AIDS events, and 1005 deaths). Results:Although baseline CD4 cell count became less prognostic with time, individuals with a baseline CD4 count <25 cells/μL had persistently higher progression rates than individuals with a baseline CD4 count >350 cells/μL (hazard ratio for AIDS = 2.3, 95% confidence interval [CI]: 1.0 to 2.3; mortality hazard ratio = 2.5, 95% CI: 1.2 to 5.5, 4 to 6 years after starting HAART). Rates of AIDS were persistently higher in individuals who had experienced an AIDS event before starting HAART. Individuals with presumed transmission by means of injection drug use experienced substantially higher rates of AIDS and death than other individuals throughout follow-up (AIDS hazard ratio = 1.6, 95% CI: 0.8 to 3.0; mortality hazard ratio = 3.5, 95% CI: 2.2 to 5.5, 4 to 6 years after starting HAART). Conclusions:Compared with other patient groups, injection drug users and patients with advanced immunodeficiency at baseline experience substantially increased rates of AIDS and death up to 6 years after starting HAART."
ea83b742eb2a3c185c091786956379292c499001,"BACKGROUND
We sought to investigate whether Escherichia coli bacteriuria is associated with a decline in renal function or with the development of end-stage renal failure after long-term follow-up.


METHODS
We performed a full cohort analysis for women who participated in 2 population-based studies. The baseline cohort consisted of women who collected morning midstream urine samples that were stored. In the cohort study, the presence of E coli bacteriuria was subsequently determined by real-time polymerase chain reaction. After a mean +/- SD follow-up of 11.5 +/- 1.7 years, blood samples were drawn from 490 women. In the nested case-control study, cases comprised all women who underwent kidney therapy (hemodialysis or renal transplantation) between participation in the baseline cohort study and a mean +/- SD of 13.8 +/- 7.4 years later.


RESULTS
The mean +/- SD age at baseline was 45.0 +/- 3.2 years, and 48 women (10%) had E coli bacteriuria. After 11.5 years, the mean +/- SD creatinine clearance (Cockroft-Gault formula) was similar between the 2 groups (87 +/- 21 mL/min [1.5 +/- 0.4 mL/s] and 85 +/- 18 mL/min [1.4 +/- 0.3 mL/s] for women who had and those who did not have bacteriuria, respectively). In the nested case-control study, the prevalence of E coli bacteriuria was 14% among cases and control subjects. The odds ratio corrected for age for the development of end-stage renal failure in the presence of E coli bacteriuria at baseline was 1.1 (95% confidence interval, 0.4-2.8; P = .86).


CONCLUSION
Escherichia coli bacteriuria is not associated with a decline in renal function or with the development of end-stage renal failure in a population of generally healthy women during 12 to 14 years of follow-up."
4c74c524799d8f383ac7dc92a0a6e3625bec9eb3,
75d025c1aaa68e92da86b421ad91112073fa5968,"Urinary-tract infections (UTIs) occur frequently and hence have significant financial implications. Antibiotic prophylaxis can be considered in women with recurrent UTIs. However, frequent exposure to antibiotics can lead to antimicrobial resistance and side effects. The most important steps in the pathogenesis of UTIs are the colonisation and adherence of uropathogens. Lactobacilli impede intravaginal colonisation by competing with uropathogens. Cranberries interfere with the adherence of uropathogens to uroepithelial cells. Therefore, cranberries and lactobacilli are potential alternatives in the prophylaxis of UTIs. Randomised clinical trials comparing these compounds with long-term, low-dose antibiotics for the prevention of recurrent UTIs in women have not yet been conducted. Such a trial has recently been started in The Netherlands: the 'Non-antibiotic versus antibiotic prophylaxis for recurrent urinary-tract infections' (NAPRUTI) study."
90836f20e2a516da8003baed985138233562932d,"BACKGROUND
The long-term consequences of asymptomatic bacteriuria (ASB) on renal function in women with diabetes mellitus (DM) are unknown.


METHODS
A prospective study was performed among women with type 1 or type 2 DM. Women with ASB (diagnosis based on findings from 1 urine culture specimen) were compared with women without ASB for differences in renal function development and incidence of hypertension.


RESULTS
A total of 644 women were included in the study (296 with type 1 DM and 348 with type 2 DM; mean [SD] age, 51 [15] years) and followed up for a mean (SD) duration of 6.1 (1.9) years. The prevalence of ASB was 17%. In women with DM and ASB, the creatinine clearance decreased from 87 mL/min (1.45 mL/s) at baseline to 76 mL/min (1.27 mL/s) at study end point; in women with DM without ASB the creatinine clearance decreased from 97 to 88 mL/min (from 1.62 to 1.47 mL/s). In the multivariate analyses, adjusted for age, length of follow-up, duration of DM, and microalbuminuria at baseline, no association was found between ASB and the relative or the absolute decrease in creatinine clearance; the same results were shown also when women with DM type 1 and women with DM type 2 were analyzed separately. Women with ASB developed hypertension more often than women without ASB (54% vs 37%; P = .045), but there was no significant association in the multivariate analysis (odds ratio, 1.5; 95% confidence interval, 0.7-3.6).


CONCLUSION
Women with DM (type 1 or type 2) with ASB do not have an increased risk for a faster decline in renal function or the development of hypertension after 6 years of follow-up."
f92c63a822ff5a816267ed6c22a36dcd3381df38,"Patient enrolment in the 'Non-antibiotic versus antibiotic prophylaxis for recurrent urinary-tract infections' (NAPRUTI) study was started in September 2005. In this study of women with recurrent urinary-tract infections we aim to investigate the effect of 12 months of non-antibiotic prophylaxis in comparison with antibiotic prophylaxis on the rate of recurrence of urinary-tract infections and the development of antibiotic resistance. The study consists of two interlinked, randomised, clinical non-inferiority trials. In one trial, 280 premenopausal women will receive either cranberry capsules (twice daily 500 mg) or standardised antibiotic therapy (once daily 480 mg trimethoprim-sulfamethoxazole). In the other trial, 280 postmenopausal women will receive either oral lactobacilli (twice daily a capsule with > 10(9) colony-forming units of Lactobacillus rhamnosus GR-1 and Lactobacillus reuteri RC-14) or standardised antibiotic therapy. Non-inferiority of non-antibiotic prophylaxis would be attractive given its potential to reduce the prevalence of microbial resistance to antibiotics significantly."
ffe282cf91be6f7207e29e5bb9ffe6e4c390b18c,"The 'Stichting Werkgroep Antibioticabeleid' (SWAB; Dutch Working Party on Antibiotic Policy) has developed an evidence-based guideline for the empirical antimicrobial treatment of complicated urinary tract infections (UTIs) in hospitalised adult patients. The choice of treatment is based on recent Dutch data on the resistance ofuropathogens to the most frequently used antibiotics. The first choice for empirical antibiotic treatment in a patient with a complicated UTI is a 2nd or 3rd generation cephalosporin or the combination of amoxicillin and gentamicin. Amoxicillin-clavulanic-acid intravenously is the second empirical choice. The treatment duration must be at least 10 days. The treatment must be adjusted after the results of the urine culture become known and made more specific if possible. Oral treatment can be given if the patient's clinical situation allows it. There are separate recommendations for the treatment ofUTIs in the following patient categories: men, pregnant women, patients with a urinary catheter, patients with diabetes mellitus and patients with renal diseases, congenital polycystic kidney disease or pyocystis."
0cddb177b2cfef62822324ba44b5e38c0b1dc53f,
9be4f5abfa098dc1d2025d03a1b227b4a4f99c17,"A woman aged 36 injured herself on a needle that had been used to take an iliac-crest biopsy from an HIV-positive patient and a man aged 34 and a woman aged 35 had sexual contact with their HIV-positive partners during which the condom tore. They were given post-exposure prophylaxis (PEP) which was formulated using medication and virus resistance data from the HIV-positive individual. At 3 and 6 months the patients were all still HIV-negative. After occupational or non-occupational exposure to HIV, PEP is initiated if there is a reasonable risk of transmission of HIV. In The Netherlands a combination of 3 antiretroviral drugs is advised based on demonstrated antiviral effectiveness in the regular treatment of HIV-infections. Frequently a standard PEP-regimen is prescribed. If the source patient has a history of antiretroviral therapy, the virus might be resistant to standard PEP-regimens. In these cases the choice of drugs in the PEP-regimen can be individualised based on the antiretroviral medication history of the source patient and known resistance patterns of the source virus."
131a70cac29bb8fb9653af5eb56c484aa2f19e4e,
3ad404863616bf270415adf73044161c864a79dd,"PURPOSE
With the increasing problem of resistance in pathogenic microorganisms the development of nonantimicrobial therapies is important. Diabetes mellitus (DM) is associated with an increased incidence of urinary tract infections. The majority of Escherichia coli strains, which is the most prevalent uropathogen, have type 1 fimbriae that bind to uroplakin in the bladder, as mediated by the adhesin FimH. A vaccine is being developed based on FimH adhesion.


MATERIALS AND METHODS
The sequence of FimH adhesion of 29 E. coli strains isolated from women with DM was determined. For adherence experiments we used E. coli isolated from women with DM and a T24 bladder cell line as well as the 2 well-defined type 1 fimbriated E. coli strains Ctrl 39 and NU14, and uroepithelial cells from women with DM.


RESULTS
The fimH sequence of E. coli strains isolated from women with DM was highly homologous to the known fimH sequence of E. coli from patients without DM. Adherence assays in a T24 bladder cell line showed that adherence of these E. coli strains from women with DM could be inhibited by pre-incubation with antiserum raised against the chaperone-adhesin complex FimC-FimH. AntiFimCH antiserum also inhibited the adherence of the 2 well-defined E. coli strains expressing type 1 fimbriae, NU14 and Ctrl 39, but not of the FimH mutant strain NU14 H-, to uroepithelial cells from women with DM.


CONCLUSIONS
These findings suggest that a vaccine based on FimH adhesin of type 1 fimbriated E. coli is a potential method of preventing urinary tract infection in women with DM."
caa363f6a67454831ca17789060f40e65b9e67eb,
3fcfcdbe0ba38264c5cd6845b9c781902a041bb7,
f43422259d50a60cad6c3d7c46c5375fd28554d7,
7fb072de17d88034d04c9aa27cfef4140620d2e1,
995efbafcc2d6c4ff9d4fff5d09597661e91c2fd,"OBJECTIVE
Women with diabetes have bacteriuria more often than women without diabetes. Because Escherichia coli adhere better to vaginal cells of nondiabetic patients with recurrent urinary tract infections (UTIs) than to those obtained from healthy control subjects, it was hypothesized that E. coli adhere more to the uroepithelial cells of diabetic women, either because of substances excreted in the urine (e.g., albumin, glucose, and Tamm Horsfall protein) or because of a difference in the uroepithelial cells.


RESEARCH DESIGN AND METHODS
A T24 bladder cell line and uroepithelial cells of 25 diabetic women and 19 control subjects were incubated with three different E. coli strains.


RESULTS
The mean numbers of type 1-fimbriated E. coli that adhered to diabetic and control cells were 12.9 and 6.1 (P = 0.001), respectively, whereas those of P-fimbriated E. coli were 8.8 and 8.1 (P = 0.8), and those of nonfimbriated E. coli were 2.7 and 3.4 (P = 0.4). The addition of various substances did not influence the adherence of E. coli to a T24 bladder cell line.


CONCLUSIONS
Type 1-fimbriated E. coli adhere more to diabetic than to control uroepithelial cells."
e3ef027b02c9be52fee462de01dfdc4090262500,
0071fa5b120aef8a11f34abdd29d5ef0029359d1,
39bc6f38f50b6bbab6faf588b7ae2230248309f1,"Women with diabetes mellitus (DM) have asymptomatic bacteriuria (ASB) and urinary tract infections (UTIs) more frequently than women without DM. For type 1 diabetes mellitus, risk factors for asymptomatic bacteriuria include a longer duration of diabetes, peripheral neuropathy and macroalbuminuria. For type 2 diabetes, the risk factors are higher age, macroalbuminuria and a recent symptomatic UTI. Poorly-controlled diabetes and residual urine after urination are no risk factors. The most important risk factor for a UTI in type 1 diabetes patients is sexual intercourse. In type 2 diabetes patients the major risk factor is the presence of asymptomatic bacteriuria. This higher prevalence does not appear to be based on a difference in virulence of the causative microorganism. Differences in host response may explain this higher prevalence: E. coli with type 1 fimbriae adhere better to uroepithelial cells in women with DM than to those in women without DM; women with DM and ASB have lower urinary cytokine concentrations and leukocyte counts compared to women without DM and ASB; in vitro studies show that E. coli grow better when glucose is present in urine. There is no consensus on whether ASB should be treated in these patients. There are indications that UTIs in diabetes patients should be treated as complicated UTIs."
828c8fe589faeefef976bdb476c735cf20f0f42e,
89b97a5a0f076e8cd5f536452c3eee51c3880a17,"We read with interest the letter of Cantagallo and Castelli (1), which describes the prevalence of asymptomatic bacteriuria (ASB) in women with diabetes before and after hygiene modifications. They showed that even after the modification of hygiene habits, diabetic women still had a 20.2% prevalence of ASB, which is comparable with the prevalence of ASB …"
976d2e20dc7d52977984f6433441cae430de270d,"BACKGROUND
Women with diabetes mellitus (DM) have asymptomatic bacteriuria (ASB) more often than women without DM. It is unknown, however, what the consequences of ASB are in these women.


OBJECTIVE
To compare women with DM with and without ASB for the development of symptomatic urinary tract infections (UTIs), renal function, and secondary complications of DM during an 18-month follow-up period.


METHODS
In this multicenter study we monitored women with DM with and without ASB for the development of symptomatic UTIs, renal function, and secondary complications (ie, retinopathy, neuropathy, microvascular, or macrovascular diseases). Data on the first 18-month follow-up period are presented.


RESULTS
At least 1 uncontaminated urine culture was available from 636 women (258 with type 1 DM and 378 with type 2 DM). The prevalence of ASB at baseline was 26% (21% for those with type 1 DM and 29% for those with type 2 DM). Follow-up results were available for 589 (93%) of the 636 women. Of these 589 women, 115 (20%) (14% with type 1 DM and 23% with type 2 DM) developed a symptomatic UTI. Women with type 2 DM and ASB at baseline had an increased risk of developing a UTI during the 18-month follow-up (19% without ASB vs 34% with ASB, P =.006). In contrast, there was no difference in the incidence of symptomatic UTI between women with type 1 DM and ASB and those without ASB (12% with ASB vs 15% without ASB). However, women with type 1 DM and ASB had a tendency to have a faster decline in renal function than those without ASB (relative increase in serum creatinine level 4.6% vs 1.5%, P = 0.2).


CONCLUSION
Women with type 2 DM and ASB have an increased risk of developing a symptomatic UTI than those without ASB."
109643db356f7d15d96d58b40cf03f455d5caaaa,
5456bdcc511607de4c4313bbe93a30a4b5403091,
9587f7b66bc42fa758214a047a5fcb4f82585da0,"As women with diabetes mellitus (DM) have an increased prevalence of asymptomatic bacteriuria (ASB) and it is known that a correlation exists between the increased prevalence of genitourinary tract infection and impaired cytokine production in women infected with Human Immunodeficiency Virus (HIV), we studied urinary cytokine excretion in diabetic women and compared it with that of nondiabetic controls."
a698b39e7793c1154e6dfa8efb30e8a140f58128,"By use of pulse-field gel electrophoresis, we evaluated the molecular identity of 32 Escherichia coli isolates obtained in 2 consecutive urine cultures from 16 patients as part of a large study of asymptomatic bacteriuria in diabetic women and found different E. coli isolates in 7 of 16 patients, meaning that nearly half (44%) of the patients who had been previously classified as having asymptomatic bacteriuria were reinfected with a different strain."
289851b396ec59dc2d4c217907b90b7ac1825cca,"Patients with diabetes mellitus (DM) have infections more often than those without DM. The course of the infections is also more complicated in this patient group. One of the possible causes of this increased prevalence of infections is defects in immunity. Besides some decreased cellular responses in vitro, no disturbances in adaptive immunity in diabetic patients have been described. Different disturbances (low complement factor 4, decreased cytokine response after stimulation) in humoral innate immunity have been described in diabetic patients. However, the clinical relevance of these findings is not clear. Concerning cellular innate immunity most studies show decreased functions (chemotaxis, phagocytosis, killing) of diabetic polymorphonuclear cells and diabetic monocytes/macrophages compared to cells of controls. In general, a better regulation of the DM leads to an improvement of these cellular functions. Furthermore, some microorganisms become more virulent in a high glucose environment. Another mechanism which can lead to the increased prevalence of infections in diabetic patients is an increased adherence of microorganisms to diabetic compared to nondiabetic cells. This has been described for Candida albicans. Possibly the carbohydrate composition of the receptor plays a role in this phenomenon."
8bf8f7f241af64d6c827630fba6f41e8ca797430,"It is generally assumed that one of the reasons why diabetics are more susceptible to urinary tract infections than non-diabetics is their 'sweet urine'. However, very little information is available on this subject. Therefore, the growth rates of different Escherichia coli strains were studied in human urine with and without added glucose and with and without a constant pH, and compared with their growth rates in Mueller-Hinton broth (MHB). Eight isolates were used (three from blood cultures from urosepsis patients, two urinary isolates, two faecal isolates and one laboratory strain K12). All isolates grew better in MHB than in urine, but with the exception of the laboratory strain, they had the same growth rate in urine. No significant difference was found between the growth rate in urine from diabetics without glucosuria and that in urine from non-diabetics. The addition of glucose (up to a concentration of 1000 mg/dl) to urine and MHB enhanced the growth rate of all isolates. However, very high concentrations of glucose (up to 10000 mg/dl) in urine and MHB caused a decrease in bacterial growth rate when the urinary pH was not kept constant. The stationary phase was reached later and the final bacterial yield was greater when the urine was made less acidic. As the uropathogenic strains did not grow better in urine than the other isolates, it may be concluded that better growth in urine is not one of the causes of the greater virulence of these strains."
5085bcf7dc01afc4f297598a011283745fec5d2d,
ab0c8abe570d38bab8e90adc82acbee3e5fcd3cb,
c6a59d303f68bc4f430a6af37f7fafe324f96147,
99be1a1e923f8e2d20af8d512e51b05fdb56ffaf,"At oesophagogastroscopy a web was seen in the upper oesophagus in a female of 73 years with dysphagia. Because she also had a smooth tongue, a low serum iron level and anaemia, the syndrome of Plummer-Vinson was diagnosed. After treatment with ferrous fumarate the dysphagia, the web and the anaemia disappeared and the serum iron rose. The symptomatology of this syndrome is discussed. Remarkably, the pathogenesis is not completely known. There are indications that this uncommon syndrome is a premalignant disorder."
dc4cb55e73609e0c796fbd9552c73175b6a61acb,
e94955763d54bccd1cf0c135e52fb9aaa9ebe583,
58161ed77598a303f40690a28a2217592438c9e0,
5cdc35a999461c25a9272f6e69ba55998ff285bf,
23bfda7a0a64e5cc06db641a4ffc489f00ef828b,
201b88f9f66c0c763642a375e69cd914b45d8924,
689ef166ea0fdbe87d53fc665850dcb47841a2f5,
8622d383bbcdb230b67180c6bf119f99358377cb,
bcdeb2f00c093b52f94a8ed71aa1fe7dbcdf1e01,
c8f10a4c9e74cad1fb633c074d46a9fd5de5c7cd,
89581e3d7c0b9ad3bf07b004a8459ee91f509806,"Checklists and action plans are a proven mechanism for project-based collaboration. Synthesizing project-specific plans is challenging, as project managers must consider multiple sources of information, from structured surveys to semi-structured conversations with stakeholders. In a needfinding study with project managers, we identified challenges in creating action plans for teams. We built MixTAPE , a mixed-initiative system that addressed these challenges with three components: a semi-structured note-taking interface for capturing stakeholder conversations, a plan generator for automatically combining multi-source information into action plans, and classification models for assigning and prioritizing action items. We evaluated MixTAPE in an observational study of 32 website design projects. Compared to a previously unstructured process, MixTAPE generated 1 . 45 X as many tasks that are more consistent, while reducing the plan creation time by 33 . 70%. Through interviews and surveys, we found that participants rate MixTAPE highly across several measures. Based on our findings, we discuss the implications and opportunities for mixed-initiative action plan creation."
0ab0fb6ad72622f4c35276bc2eb38e855c9d0c91,"Spreadsheets are one of the most popular tools for ad-hoc exploration and analysis of data. Despite that, exploring and analyzing spreadsheet datasets that span more than a few screens via operations such as scrolling or issuing formulae, is often overwhelming for end-users. Users easily lose context as they explore the data via scrolling and suffer from cognitive and mechanical burdens while issuing formulae on data spanning multiple screens. We propose integrating a navigation plug-in with spreadsheets to support the seamless exploration of large datasets that are increasingly the norm. Our interface, NOAH, developed using lessons from classical overview+detail interfaces, embeds a multi-granularity zoomable overview alongside the spreadsheet. Users can employ the overview to explore the data at various granularities. Furthermore, they can issue formulae over subsets of data without performing cumbersome scrolling or range selection operations, enabling users to gain a high or low-level perspective of the spreadsheet data. NOAH preserves spreadsheet semantics and look and feel, while introducing such enhancements. Our user study demonstrates that NOAH makes it more intuitive, easier, and faster to navigate spreadsheet data compared to traditional spreadsheets like Microsoft Excel, for a variety of navigational tasks; participants made 2.5× fewer mistakes in NOAH than in Excel while being twice as fast in completing the tasks."
83fa51b14566a4348b63c90e4b4bcd74584524c9,
cbabe7c144f6d00a11d50be639c2327ba4d4baa4,"
 Data analysts often build visualizations as the first step in their analytical workflow. However, when working with high-dimensional datasets, identifying visualizations that show relevant or desired trends in data can be laborious. We propose S
 ee
 DB, a visualization recommendation engine to facilitate fast visual analysis: given a subset of data to be studied, S
 ee
 DB intelligently explores the space of visualizations, evaluates promising visualizations for trends, and recommends those it deems most ""useful"" or ""interesting"". The two major obstacles in recommending interesting visualizations are (a)
 scale
 : evaluating a large number of candidate visualizations while responding within interactive time scales, and (b)
 utility
 : identifying an appropriate metric for assessing interestingness of visualizations. For the former, S
 ee
 DB introduces
 pruning optimizations
 to quickly identify high-utility visualizations and
 sharing optimizations
 to maximize sharing of computation across visualizations. For the latter, as a first step, we adopt a deviation-based metric for visualization utility, while indicating how we may be able to generalize it to other factors influencing utility. We implement S
 ee
 DB as a middleware layer that can run on top of any DBMS. Our experiments show that our framework can identify interesting visualizations with high accuracy. Our optimizations lead to
 multiple orders of magnitude speedup
 on relational row and column stores and provide recommendations at interactive time scales. Finally, we demonstrate via a user study the effectiveness of our deviation-based utility metric and the value of recommendations in supporting visual analytics.
"
0876057b1d8479b02e0e15736d561007328c4370,"
 Spreadsheet systems are by far the most popular platform for data exploration on the planet, supporting millions of rows of data. However, exploring spreadsheets that are this large via operations such as scrolling or issuing formulae can be overwhelming and error-prone. Users easily lose context and suffer from cognitive and mechanical burdens while issuing formulae on data spanning multiple screens. To address these challenges, we introduce
 dynamic hierarchical overviews
 that are embedded alongside spreadsheets. Users can employ this overview to explore the data at various granularities, zooming in and out of the spreadsheet. They can issue formulae over data subsets without cumbersome scrolling or range selection, enabling users to gain a high or low-level perspective of the spreadsheet. An implementation of our dynamic hierarchical overview, NOAH, integrated within DataSpread, preserves spreadsheet semantics and look and feel, while introducing such enhancements. Our user studies demonstrate that NOAH makes it more intuitive, easier, and faster to navigate spreadsheet data compared to traditional spreadsheets like Microsoft Excel and spreadsheet plug-ins like Pivot Table, for a variety of exploration tasks; participants made fewer mistakes in NOAH while being faster in completing the tasks.
"
506fdf4e82626ec118ff85057625cf5094e03efe,"Background: We assessed the clinical utility of a first-degree breast cancer family history and polygenic risk score (PRS) to inform screening decisions among women aged 30-50 years. Methods: Two established breast cancer models evaluated digital mammography screening strategies in the 1985 US birth cohort by risk groups defined by family history and PRS based on 313 single nucleotide polymorphisms. Strategies varied in initiation age (30, 35, 40, 45, and 50 years) and interval (annual, hybrid, biennial, triennial). The benefits (breast cancer deaths averted, life-years gained) and harms (false-positive mammograms, overdiagnoses) were compared with those seen with 3 established screening guidelines. Results: Women with a breast cancer family history who initiated biennial screening at age 40 years (vs 50 years) had a 36% (model range1⁄4 29%-40%) increase in life-years gained and 20% (model range1⁄4 16%-24%) more breast cancer deaths averted, but 21% (model range 1⁄4 17%-23%) more overdiagnoses and 63% (model range1⁄4 62%-64%) more false positives. Screening tailored to PRS vs biennial screening from 50 to 74 years had smaller positive effects on life-years gained (20%) and breast cancer deaths averted (11%) but also smaller increases in overdiagnoses (10%) and false positives (26%). Combined use of family history and PRS vs biennial screening from 50 to 74 years had the greatest increase in life-years gained (29%) and breast cancer deaths averted (18%). Conclusions: Our results suggest that breast cancer family history and PRS could guide screening decisions before age 50 years among women at increased risk for breast cancer but expected increases in overdiagnoses and false positives should be expected. Routine mammography screening starting at age 45 or 50 years has been shown to reduce population breast cancer mortality for women at average risk (1). It remains uncertain whether the current screening guidelines (2-4) are optimal for individual women considering the variability in breast cancer risk at any given age. The American Cancer Society (ACS) and the United States Preventive Services Task Force (USPSTF) recommend that women discuss their individual risk and screening options with their healthcare providers before the age of 45 or 50 years, yet there are limited data to inform such discussions. Risk-based screening has been proposed as a way to inform decisions about the starting age and frequency of screening for women at different levels of risk. Recent discoveries in the field of breast cancer genetics may hold the potential to guide risk-based screening strategies. The risk of developing breast cancer approximately doubles for women with a first-degree family member with A R T IC LE Received: December 12, 2019; Revised: August 10, 2020; Accepted: August 18, 2020 © The Author(s) 2020. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/ licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com 434 JNCI J Natl Cancer Inst (2021) 113(4): djaa127 doi: 10.1093/jnci/djaa127 First published online August 27, 2020"
4845ba0f3b16243837b6e7ea854bd7bb4ce8649a,
82592fc508f556561fc26c1f652c18b40f9f37af,
8520711d8e53d623b073f96c470155888fbb64b4,"Abstract Background We assessed the clinical utility of a first-degree breast cancer family history and polygenic risk score (PRS) to inform screening decisions among women aged 30-50 years. Methods Two established breast cancer models evaluated digital mammography screening strategies in the 1985 US birth cohort by risk groups defined by family history and PRS based on 313 single nucleotide polymorphisms. Strategies varied in initiation age (30, 35, 40, 45, and 50 years) and interval (annual, hybrid, biennial, triennial). The benefits (breast cancer deaths averted, life-years gained) and harms (false-positive mammograms, overdiagnoses) were compared with those seen with 3 established screening guidelines. Results Women with a breast cancer family history who initiated biennial screening at age 40 years (vs 50 years) had a 36% (model range = 29%-40%) increase in life-years gained and 20% (model range = 16%-24%) more breast cancer deaths averted, but 21% (model range = 17%-23%) more overdiagnoses and 63% (model range = 62%-64%) more false positives. Screening tailored to PRS vs biennial screening from 50 to 74 years had smaller positive effects on life-years gained (20%) and breast cancer deaths averted (11%) but also smaller increases in overdiagnoses (10%) and false positives (26%). Combined use of family history and PRS vs biennial screening from 50 to 74 years had the greatest increase in life-years gained (29%) and breast cancer deaths averted (18%). Conclusions Our results suggest that breast cancer family history and PRS could guide screening decisions before age 50 years among women at increased risk for breast cancer but expected increases in overdiagnoses and false positives should be expected."
be923977fce06fc2aa3094b445cfc2fb88cf660b,
cbe18d9843bc404d50464f1485def9d1c84c6a61,"The area under the receiver operating characteristic (ROC) curve (AUC) is commonly used for assessing the discriminative ability of prediction models even though the measure is criticized for being clinically irrelevant and lacking an intuitive interpretation. Every tutorial explains how the coordinates of the ROC curve are obtained from the risk distributions of diseased and non-diseased individuals, but it has not become common sense that therewith the ROC plot is just another way of presenting these risk distributions. We show how the ROC curve is an alternative way to present risk distributions of diseased and non-diseased individuals and how the shape of the ROC curve informs about the overlap of the risk distributions. For example, ROC curves are rounded when the prediction model included variables with similar effect on disease risk and have an angle when, for example, one binary risk factor has a stronger effect; and ROC curves are stepped rather than smooth when the sample size or incidence is low, when the prediction model is based on a relatively small set of categorical predictors. This alternative perspective on the ROC plot invalidates most purported limitations of the AUC and attributes others to the underlying risk distributions. AUC is a measure of the discriminative ability of prediction models. The assessment of prediction models should be supplemented with other metrics to assess their clinical utility."
d7d099d251422953f7d0bae548e79b961ce5c1ad,
1a79365828a835234402366e1f626f24142da940,
1c37cd142f8332967309b181ebf698803303b4d0,
46e1d4fb07227b19cc27794ba127259a5344579e,"Copyright © 2019 Wolters Kluwer Health, Inc. Unauthorized reproduction of this article is prohibited. 342 | www.epidem.com Epidemiology • Volume 30, Number 3, May 2019 Editor’s Note: A related article appears on p. 334. From the Department of Epidemiology, Rollins School of Public Health, Emory University, Atlanta, GA. The author reports no conflicts of interest. Correspondence: A. Cecile J.W. Janssens, Department of Epidemiology, Rollins School of Public Health, Emory University, 1518 Clifton Road NE, Atlanta, GA 30322. E-mail: cecile.janssens@emory.edu."
660bbb04599c2704a21fb5f944691844817b97dd,"Direct-to-consumer genetic testing companies aim to predict the risks of complex diseases using proprietary algorithms. Companies keep algorithms as trade secrets for competitive advantage, but a market that thrives on the premise that customers can make their own decisions about genetic testing should respect customer autonomy and informed decision making and maximize opportunities for transparency. The algorithm itself is only one piece of the information that is deemed essential for understanding how prediction algorithms are developed and evaluated. Companies should be encouraged to disclose everything else, including the expected risk distribution of the algorithm when applied in the population, using a benchmark DNA dataset. A standardized presentation of information and risk distributions allows customers to compare test offers and scientists to verify whether the undisclosed algorithms could be valid. A new model of oversight in which stakeholders collaboratively keep a check on the commercial market is needed."
e7f427b278da862f9e362860b008bef9f8b432c6,"Abstract Background Although uniform colonoscopy screening reduces colorectal cancer (CRC) mortality, risk-based screening may be more efficient. We investigated whether CRC screening based on polygenic risk is a cost-effective alternative to current uniform screening, and if not, under what conditions it would be. Methods The MISCAN-Colon model was used to simulate a hypothetical cohort of US 40-year-olds. Uniform screening was modeled as colonoscopy screening at ages 50, 60, and 70 years. For risk-stratified screening, individuals underwent polygenic testing with current and potential future discriminatory performance (area under the receiver-operating curve [AUC] of 0.60 and 0.65–0.80, respectively). Polygenic testing results were used to create risk groups, for which colonoscopy screening was optimized by varying the start age (40–60 years), end age (70–85 years), and interval (1–20 years). Results With current discriminatory performance, optimal screening ranged from once-only colonoscopy at age 60 years for the lowest-risk group to six colonoscopies at ages 40–80 years for the highest-risk group. While maintaining the same health benefits, risk-stratified screening increased costs by $59 per person. Risk-stratified screening could become cost-effective if the AUC value would increase beyond 0.65, the price per polygenic test would drop to less than $141, or risk-stratified screening would lead to a 5% increase in screening participation. Conclusions Currently, CRC screening based on polygenic risk is unlikely to be cost-effective compared with uniform screening. This is expected to change with a greater than 0.05 increase in AUC value, a greater than 30% reduction in polygenic testing costs, or a greater than 5% increase in adherence with screening."
ee11c6900c7563c32301e84006af62624420b5e5,"Abstract Polygenic risk scores (PRSs) have become the standard for quantifying genetic liability in the prediction of disease risks. PRSs are generally constructed as weighted sum scores of risk alleles using effect sizes from genome-wide association studies as their weights. The construction of PRSs is being improved with more appropriate selection of independent single-nucleotide polymorphisms (SNPs) and optimized estimation of their weights but is rarely reflected upon from a theoretical perspective, focusing on the validity of the risk score. Borrowing from psychometrics, this paper discusses the validity of PRSs and introduces the three main types of validity that are considered in the evaluation of tests and measurements: construct, content, and criterion validity. This introduction is followed by a discussion of three topics that challenge the validity of PRS, namely, their claimed independence of clinical risk factors, the consequences of relaxing SNP inclusion thresholds and the selection of SNP weights. This discussion of the validity of PRS reminds us that we need to keep questioning if weighted sums of risk alleles are measuring what we think they are in the various scenarios in which PRSs are used and that we need to keep exploring alternative modeling strategies that might better reflect the underlying biological pathways."
18d730716a96b87a9332bcae957708bd0424d3c8,"In Reply This systematic review and meta-analysis1 addressed 1 of 6 research questions that comprised a larger project funded by the Agency for Healthcare Research and Quality in partnership with the National Heart, Lung, and Blood Institute. The protocol was registered with PROSPERO (CRD42016047985) and the final report is publicly available.2 Prior to publication, the draft report was posted for public comment and underwent peer review. This project was designed and conducted by a team without related conflicts of interest and no biases toward a particular result. As such, to our knowledge, no unpublished data were intentionally removed from any of the analyses. Regarding the analysis comparing SMART with a higher dose of inhaled corticosteroids and LABA controller therapy with an end point of exacerbation risk, the 2 studies in question were excluded from the pooled estimate for valid reasons. First, the study by Ställberg et al3 included patients with both the same and higher inhaled corticosteroid doses in the comparator group; therefore, it was not pooled with studies that examined solely higher dosing. Second, the study by Pavord et al4 did not specifically provide the number of study participants in each group who experienced a severe exacerbation; rather, they reported the time to first exacerbation or mean rate of severe exacerbations per patient-year. Therefore, the results shown in Figure 3 represented the best available evidence at the time for the outcome of exacerbation risk."
861209d4a46cbb3719247cbb79fd3f3ad4fe49d3,
90243b4b78261555fded1919c2777de2145b7d08,"BACKGROUND & AIMS
Relative risk of colorectal cancer (CRC) decreases with age among individuals with a family history of CRC. However, no screening recommendations specify less frequent screening with increasing age. We aimed to determine whether such a refinement would be cost effective.


METHODS
We determined the relative risk for CRC for individuals based on age and number of affected first-degree relatives (FDRs) using data from publications. For each number of affected FDRs, we used the Microsimulation Screening Analysis model to estimate costs and effects of colonoscopy screening strategies with different age ranges and intervals. Screening was then optimized sequentially, starting with the youngest age group, and allowing the interval of screening to change at certain ages. Strategies with an incremental cost effectiveness ratio below $100,000 per quality-adjusted life year were considered cost effective.


RESULTS
For people with 1 affected FDR (92% of those with a family history), screening every 3 years beginning at an age of 40 years is most cost effective. If no adenomas are found, the screening interval can gradually be extended to 5 and 7 years, at ages 45 and 55 years, respectively. From a cost-effectiveness perspective, individuals with more affected FDRs should start screening earlier and at shorter intervals. However, frequency can be reduced if no abnormalities are found.


CONCLUSIONS
Using a microsimulation model, we found that for individuals with a family history of CRC, it is cost effective to gradually increase the screening interval if several subsequent screening colonoscopies have negative results and no new cases of CRC are found in family members."
d236ceb62b7dfe8fa0ef2ad4ed5e336d51f5ae18,"Irreplicability is framed as crisis, blamed on sloppy science motivated by perverse stimuli in research. Structural changes to the organization of science, targeting sloppy science (e.g., open data, pre‐registration), are proposed to prevent irreplicability. While there is an unquestionable link between sloppy science and failures to replicate/reproduce scientific studies, they are currently conflated. This position can be understood as a result of the erosion of the role of theory in science. The history, sociology, and philosophy of science reveal alternative explanations for irreplicability to show it is part of proper, informative and valuable science. Irreplicability need not equate research waste. Sloppy science is the problem, also when results do replicate. Hence, the solution should focus on opposing sloppy research."
f0aca63522bdd67d2a6bdba4a797cc6a6443b318,
432d593140c865c9be8f246a7baf927a782378e7,
4692f0cc2fb766cd8655fd6faab75a826c3bcd2a,"The US National Institutes of Health (NIH) recently announced that they would limit the number of grants per scientist and redistribute their funds across a larger group of researchers. The policy was withdrawn a month later after criticism from the scientific community. Even so, the basis of this defunct policy was flawed and it merits further examination. The amount of grant support would have been quantified using a new metric, the Grant Support Index (GSI), and limited to a maximum of 21 points, the equivalent of three R01 grants. This threshold was decided based upon analysis of a new metric of scientific output, the annual weighted Relative Citation Ratio, which showed a pattern of diminishing returns at higher values of the GSI. In this commentary, we discuss several concerns about the validity of the two metrics and the quality of the data that the NIH had used to set the grant threshold. These concerns would have warranted a re-analysis of new data to confirm the legitimacy of the GSI threshold. Data-driven policies that affect the careers of scientists should be justified by nothing less than a rigorous analysis of high-quality data. In May 2017, the US National Institutes of Health (NIH) announced that they would place a cap on the number of grants per scientist to redistribute funds across a larger group of researchers in an attempt to optimize output and maximize impact. 2 The proposal received immediate criticism from the scientific community. Researchers disputed the point values of the Grant Support Index (GSI), worried that the point system would discourage the application of training and collaborative grants, and urged the NIH to find other ways to support more researchers. Within a month, the NIH withdrew the proposal and replaced it with the Next Generation Researchers Initiative, which aims to bolster the funding of early-stage and mid-career researchers without capping the number of grants for others. PeerJ Preprints | https://doi.org/10.7287/peerj.preprints.3106v1 | CC BY 4.0 Open Access | rec: 25 Jul 2017, publ: 25 Jul 2017 2 The initial policy to limit the number of grants per researcher was put in place because the NIH had suggested that the output of investigators with multiple grants, on average, did not increase proportionately with their amount of funding compared to those with fewer grants. The NIH had analyzed data on grant output from 71,936 principal investigators and observed diminishing returns in the annual weighted Relative Citation Ratio (RCR) beyond a GSI of 21, which is the equivalent of 3 R01 grants held by a single principal investigator (Figure 1). While the attempt to optimize equity and productivity is laudable, the fairness of this data-driven cap depended squarely on the validity of the metrics and the quality of the data. We had several concerns. The GSI is a metric that assigns points to grant types depending on their complexity and size. The metric was designed by assigning seven points to the most common grant, the R01, and giving more points to larger grants and fewer points to the smaller ones. 6 This assignment of points lacked calibration: it is unclear how the GSI correlates with other metrics of grant support and whether more or lesser points need to be assigned to certain grant mechanisms. The GSI should be considered an arbitrary point system until further analyses have demonstrated its validity. The annual weighted RCR quantifies the output of a researcher’s portfolio. This metric is based on the RCR, which indicates the influence of a single article, calculated as the ratio of the article citation rate and the expected citation rate for articles in its field. The RCR is normalized against a large set of NIH publications so that a value of 1 indicates that the citation rate of an article is on par with the mean of articles in its field. The weighted RCR is the sum of RCRs for all articles published by a researcher in a certain period, and the annual weighted RCR is this sum divided by the number of publication years. The annual weighted RCR has shortcomings that may have introduced errors in setting the GSI threshold. The calculation of the annual weighted RCR of researchers’ portfolios assumes that the RCR of a scientific article is constant over time, but this is not the case: the RCR decreases when an article is past its heyday. The citation rate of an article (the numerator in the RCR equation) decreases over time when its number of citations ‘stabilizes’ and the number of years keeps increasing. The expected citation rate (the denominator in the RCR equation) is a normalized 2year citation rate of the journals in which the ‘field’ articles were published. Using a 2-year citation rate assumes that the citation rate of an article is constant over time and always as high as in the first two years, which is unrealistic for older articles. A decreasing numerator and overestimated denominator result in reduced RCRs for most older articles. When the RCRs of scientific articles decrease over time, the annual weighted RCR is not a suitable indicator for the current or recent scientific influence of midand late-career researchers, as their older work pulls down their portfolio RCRs. The reduction of their annual weighted RCR might, at least in part, explain the diminishing returns that were observed in the NIH data as researchers with higher GSI scores had substantially longer histories of grant funding (median of 19 years among GSI>21 as compared to 3 years among GSI≤7). A metric that better reflects PeerJ Preprints | https://doi.org/10.7287/peerj.preprints.3106v1 | CC BY 4.0 Open Access | rec: 25 Jul 2017, publ: 25 Jul 2017 3 current or recent scientific influence of researchers’ portfolios might have shown less diminishing returns, would have led to a higher cap and allowed more grants per researcher. The legitimacy of the threshold is not only determined by the validity of the metrics, but also by the quality and validity of the data that were used to obtain the GSIs and annual weighted RCRs. The NIH clarified in detail how they calculated the GSI for each year, but not how they calculated nor which data they used to obtain the annual weighted RCRs. Figure 1 shows that many scientists had annual weighted RCRs below 1, which means that their total annual output was not even the equivalent of one ‘average’ article with an RCR of 1. This low number raises questions about the data that were used to quantify the output in researchers’ portfolios. The NIH explained that they only considered publications in which grant funding was acknowledged, and they assigned publications only to the PI(s), not to co-authors, but it is unknown how they determined the number of publication years to calculate annual weighted RCRs. They may have used the number of years between the first and last publication, the number of years with grant funding, or even the number of years for which publication data were available (1996-2014) because the exact number of publication years for each researcher could not be determined. The latter would overestimate the number of years for many researchers and could explain the low annual weighted RCRs. An alternative explanation is that the low annual weighted RCRs were observed because the NIH used funding and publication data from the same period. Using data from the same period artificially increases the number of grants with no or little output because recently-awarded grants had not had the time to generate a competitive number of publications that were cited frequently enough to yield above average RCR ratings. The median lag time between the start of a grant and the first publication is three years, and many grants continue to deliver publications after a grant is completed. When recent grants are removed from the NIH analysis, the size of the lowest GSI category, which set the expected output for the rest, will be strongly reduced as these researchers had a median of 3 years of funding. Removing recent grants may reduce the expected returns from research funding for researchers with more grants and change the observation of diminishing returns. Restricting the amount of support would have meant that scientifically outstanding grants, based on evaluation by study sections, would not be funded when they caused researchers to exceed their maximum allowable grant support. Overruling the award of competitive grants to successful researchers is a decision that should not be taken lightly. A data-driven threshold that affects the careers of scientists should be justified by nothing less than a rigorous analysis using valid metrics in high-quality data. As a science-based organization, the NIH values rigor, transparency, and high-quality data. These values must be upheld when developing metrics and policies that impact the scientific enterprise and the careers of scientists. The analytic justification of the GSI threshold did not meet that standard. PeerJ Preprints | https://doi.org/10.7287/peerj.preprints.3106v1 | CC BY 4.0 Open Access | rec: 25 Jul 2017, publ: 25 Jul 2017 4 References 1. Collins FS. New NIH Approach to Grant Funding Aimed at Optimizing Stewardship of Taxpayer Dollars 2017 [Available from: https://www.nih.gov/about-nih/who-we-are/nihdirector/statements/new-nih-approach-grant-funding-aimed-optimizing-stewardship-taxpayerdollars accessed July 12 2017]. 2. Lauer M. Implementing Limits on Grant Support to Strengthen the Biomedical Research Workforce 2017 [Available from: https://nexus.od.nih.gov/all/2017/05/02/nih-grant-supportindex/ accessed July 12 2017]. 3. Kaiser J. NIH scales back plan to curb support for big labs after hearing concerns. Science 2017(6338) doi: 10.1126/science.aan6901. 4. Lauer M. NIH's Next Generation Researchers Initiative 2017 [Available from: https://nexus.od.nih.gov/all/2017/06/16/nih-next-generation-researchers-initiative/ accessed July 12 2017]. 5. Lauer M, Roychowdhury D, Patel K, et al. Marginal r"
476204a14fb80664957182a0a798e01dfa28a634,
5568780660babd5e4182369e15ef1c9a9b696864,
c78a8ca01d049a70651f5b55c09d2466025cf7ea,
edd077cc4378c7ffdaa8d5961dfa0edd150145a4,"The influence of scientific publications is increasingly assessed using quantitative approaches, but most available metrics have limitations that hamper their utility [1]. Hutchins and colleagues recently proposed the Relative Citation Ratio (RCR) [2], which compares the citation rate of an article against the citation rate that is expected for its field. The metric is an attractive and intuitive solution to indicate whether an article is cited more or less frequently than its “peer” publications. As a ratio of rates, RCR is an article-level metric that is field and time independent and strongly correlated with peer review evaluations [2]; the metric was central in the proposed (and withdrawn) grant management policy of the National Institutes of Health (NIH) [3] even though the RCR has been criticized for lacking a theoretical model, having insufficient transparency, and having poor correlation with other peer evaluations [4,5]. We analyzed the algorithm behind the RCR and report several concerns about the calculation of the metric that may limit its utility."
f6cc8b2ec40a36f91d1feb6845e58039d20eaa3a,
0a870d4b7cda1a8b760c30f598186e4a3d2f5cd6,
54cc8a3b85f0ae2ef7f5fce7c4885a745b293823,
6155d7739cd9f14f5b20118aabe66e65b7ef5f13,
69926101faaecbc362ec6285599e9dd4d4c271e2,"Background Modeling studies using hypothetical polygenic risk data can be an efficient tool for investigating the effectiveness of downstream applications such as targeting interventions to risk groups to justify whether empirical investigation is warranted. We investigated the assumptions underlying a method that simulates risk data for specific values of the area under the receiver operating characteristic curve (AUC). Methods The simulation method constructs risk data for a hypothetical population based on the population disease risk, and the odds ratios and frequencies of genetic variants. By systematically varying the parameters, we investigated under what conditions AUC values represent unique ROC curves with unique risk distributions for patients and nonpatients, and to what extend risk data can be simulated for precise values of the AUC. Results Using larger number of genetic variants each with a modest effect, we observed that the distributions of estimated risks of patients and nonpatients were similar for various combinations of the odds ratios and frequencies of the risk alleles. Simulated ROC curves overlapped empirical curves with the same AUC. Conclusions Polygenic risk data can be effectively and efficiently created using a simulation method. This allows to further investigate the potential applications of stratifying interventions on the basis of polygenic risk."
a801e734a88363896e79cd243ab3494f68dc898e,
becfc0b7f396951dbf9d4100e7499806efb82444,Amin Al Olama and colleagues investigated the predictive ability of a polygenic risk score and observed that the risk of men in the top 1% of the distribution was 30.6-fold compared with men in the bottom 1% and 4.2-fold compared with the median risk ([1][1]). The authors conclude that “genetic
c207d2e37558eb359ec20698e169f3b03cc7187c,"Importance
Electrocardiography (ECG) may detect subclinical cardiovascular disease (CVD) in asymptomatic individuals, but its role in assessing adverse events beyond traditional risk factors is not clear. Interval and vector data that are commonly available on modern ECGs may offer independent prognostic information that improves risk classification.


Objectives
To derive and validate a CVD risk equation based on ECG metrics and to determine its incremental benefit in addition to the Framingham risk score (FRS).


Design, Setting, and Participants
This study included 3640 randomly selected community-based adults aged 40 to 74 years without known CVD from the First National Health and Nutrition Examination Survey (NHANES I) cohort (1971-1975) and 6329 from the NHANES III cohort (1988-1994). Participants were sampled from across the United States. A risk score to assess incident nonfatal and fatal CVD events was derived based on computer-generated ECG data, including frontal P, R, and T axes; heart rate; and PR, QRS, and QT intervals from NHANES I. The most prognostic variables, along with age and sex, were incorporated into the NHANES ECG risk equation. The equation was evaluated in the NHANES III cohort for an independent validation. Follow-up in the NHANES III cohort was completed on December 31, 2006. Data for this study were analyzed from August 11, 2015, to May 20, 2016.


Main Outcomes and Measures
The primary end point was CVD death. Secondary outcomes included 10-year ischemic heart disease and all-cause death.


Results
The final study sample included 9969 participants (4714 men [47.3%]; 5255 women [52.7%]; mean [SD] age, 55.3 [10.1] years) from both cohorts. Frontal T axis, heart rate, and heart rate-corrected QT interval were the most significant ECG factors in the NHANES I cohort. In the validation cohort (NHANES III), the equation provided for prognostic information for fatal CVD with a hazard ratio (HR) of 3.23 (95% CI, 2.82-3.72); the C statistic was 0.79 (95% CI, 0.76-0.81). When added to the FRS in Cox proportional hazards regression models, the categorical (1%, 5%, and 10% cutoffs) net reclassification improvement was 24%. When the FRS and ECG scores were combined in a single model, the C statistic improved by 0.04 (95% CI, 0.02-0.06) to 0.80 (95% CI, 0.77-0.82). Similar improvements were noted when the ECG score was added to the pooled cohort equation. When the equation for prognostic information about ischemic heart disease and all-cause death was evaluated, the results were similar.


Conclusions and Relevance
An ECG risk score based on age, sex, heart rate, frontal T axis, and QT interval assesses the risk for CVD and compares favorably with the FRS alone in an independent cohort of asymptomatic individuals. Although the ECG risk equation is low cost, further research is needed to ascertain whether this additional step in risk stratification may improve prevention efforts and reduce CVD events."
ed4b2f81f5ceb62cf855f7615f14547f2c6a0ac7,
fcc1fc51ceee344a5ad1b19bd5597767a05fefac,
2d79c227f4f071d40148737b8f63434ca449676f,
6ca4a545d993a9c1046edf0909d33ce230266f2e,
6cbbe5919306ee155c4e2eaab934185360ab0227,
7adaff0efb1b3a747693b521debdecbf0cb54265,
a64bf12f76016b97572698aabd478e539642929f,
b6866acacae7740f684323a1580f2097988e2b92,
cb3eb33897d927e05661b2141b3a73438185a48a,"AIMS AND OBJECTIVES
The aim of this discussion paper is to enable nurses to understand how deoxyribonucleic acid analysis can be predictive for some diseases and not predictive for others. This will facilitate nurses to interpret genomic test results and explain them to patients.


BACKGROUND
Advances in technology mean that genetic testing is now commonly performed by sequencing the majority of an individual's genome or exome. This results in a huge amount of data, some of which can be used to predict or diagnose disease.


DESIGN
This is a discussion paper.


METHODS
This paper emerged from multiple discussions between the three authors over many months, culminating in a writing workshop to prepare this text.


RESULTS
The results of DNA analysis can be used to diagnose or predict rare diseases that are caused by a mutation in a single gene. However, while there are a number of genetic factors that contribute to common diseases, the ability to predict whether an individual will develop that condition is limited by the overall heritability of the condition. Environmental factors (such as lifestyle) are likely to be more useful in predicting common disease than genomic testing. Genomic tests may be of use to inform management of diseases in specific situations.


CONCLUSIONS
Genomic testing will be of use in diagnosing disorders due to single gene mutations, but the use of genomic testing to predict the chance of a patient being affected in the future by a common disease is unlikely to be a realistic option within a health service setting.


RELEVANCE TO CLINICAL PRACTICE
Nurses will increasingly be involved in the use of genomic tests in mainstream patient care. However, they need to understand and be able to explain to patients the practical applications of and limitations of such tests."
ec11d5efd3aab833e6a46a1f51dab20d8fb2e83a,
0c1a1e6a268773ffcde0e00d8858cfa16c72a731,"AIMS
B-type natriuretic peptide (BNP) and C-reactive protein (CRP) predict atrial fibrillation (AF) risk. However, their risk stratification abilities in the broad community remain uncertain. We sought to improve risk stratification for AF using biomarker information.


METHODS AND RESULTS
We ascertained AF incidence in 18 556 Whites and African Americans from the Atherosclerosis Risk in Communities Study (ARIC, n=10 675), Cardiovascular Health Study (CHS, n = 5043), and Framingham Heart Study (FHS, n = 2838), followed for 5 years (prediction horizon). We added BNP (ARIC/CHS: N-terminal pro-B-type natriuretic peptide; FHS: BNP), CRP, or both to a previously reported AF risk score, and assessed model calibration and predictive ability [C-statistic, integrated discrimination improvement (IDI), and net reclassification improvement (NRI)]. We replicated models in two independent European cohorts: Age, Gene/Environment Susceptibility Reykjavik Study (AGES), n = 4467; Rotterdam Study (RS), n = 3203. B-type natriuretic peptide and CRP were significantly associated with AF incidence (n = 1186): hazard ratio per 1-SD ln-transformed biomarker 1.66 [95% confidence interval (CI), 1.56-1.76], P < 0.0001 and 1.18 (95% CI, 1.11-1.25), P < 0.0001, respectively. Model calibration was sufficient (BNP, χ(2) = 17.0; CRP, χ(2) = 10.5; BNP and CRP, χ(2) = 13.1). B-type natriuretic peptide improved the C-statistic from 0.765 to 0.790, yielded an IDI of 0.027 (95% CI, 0.022-0.032), a relative IDI of 41.5%, and a continuous NRI of 0.389 (95% CI, 0.322-0.455). The predictive ability of CRP was limited (C-statistic increment 0.003). B-type natriuretic peptide consistently improved prediction in AGES and RS.


CONCLUSION
B-type natriuretic peptide, not CRP, substantially improved AF risk prediction beyond clinical factors in an independently replicated, heterogeneous population. B-type natriuretic peptide may serve as a benchmark to evaluate novel putative AF risk biomarkers."
2dfa2905d44503f1399ad62db55828f65cd09fc2,
2e17a4d0a430734133afd50f80fa7191e66459cb,"In “Raw personal data: providing access” (Policy Forum, 24 January, p. [373][1]), J. E. Lunshof and colleagues argue that donors should have access to raw data derived from their contribution to research or clinical repositories. Fairness, reciprocity, and respect for autonomy are compelling ethical reasons for access, if not for one major problem: the intrinsic inaccuracy of most research data.

Even the best-documented population studies cannot guarantee accurate data for individual participants. Limited research budgets force researchers to decide between assessing a few variables at high quality or many variables at lower quality, and they typically choose the latter. More data means more research opportunities, and suboptimal data quality is perfect enough when conclusions are drawn for populations at large. Yet, the data cannot be used to inform about individual participants.

![Figure][2] 

CREDIT: T. POPOVA/ISTOCKPHOTO

To illustrate the moral obligation for granting access, the authors draw an excellent analogy with money banks, but the example actually undercuts their point. Money banks would never provide customers access to their bank accounts if they had even the slightest doubt about the accuracy of the balances. Inaccurate account data not only harm individual customers, who then remain uncertain about their financial position, but also destroy public trust in money banks. This is a risk that banks would not even think of taking, and scientists should not either.

High-quality online genome data interpretation tools, health professionals, and other independent experts cannot make sense of data when they cannot rely on the quality. A disclaimer concerning data accuracy, as the authors propose, does not solve that problem. If researchers respect their participants, take them seriously, and want to do more good than harm ([ 1 ][3]), they do not give them all they have, but give something valuable in a responsible way ([ 2 ][4]). And that is not merely access to data.

1. [↵][5] The National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, Ethical Principles and Guidelines for the Protection of Human Subjects of Research (1979); [www.hhs.gov/ohrp/humansubjects/guidance/belmont.html][6].
 

2. [↵][7] 1. J. P. Evans, 2. B. B. Rothschild
 , Genet. Med. 14, 358 (2012).
 [OpenUrl][8][CrossRef][9][PubMed][10]

 [1]: /lookup/doi/10.1126/science.1249382
 [2]: pending:yes
 [3]: #ref-1
 [4]: #ref-2
 [5]: #xref-ref-1-1 ""View reference 1 in text""
 [6]: http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html
 [7]: #xref-ref-2-1 ""View reference 2 in text""
 [8]: {openurl}?query=rft.jtitle%253DGenetics%2Bin%2Bmedicine%2B%253A%2B%2Bofficial%2Bjournal%2Bof%2Bthe%2BAmerican%2BCollege%2Bof%2BMedical%2BGenetics%26rft.stitle%253DGenet%2BMed%26rft.aulast%253DEvans%26rft.auinit1%253DJ.%2BP.%26rft.volume%253D14%26rft.issue%253D4%26rft.spage%253D358%26rft.epage%253D360%26rft.atitle%253DReturn%2Bof%2Bresults%253A%2Bnot%2Bthat%2Bcomplicated%253F%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fgim.2012.8%26rft_id%253Dinfo%253Apmid%252F22481183%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx
 [9]: /lookup/external-ref?access_num=10.1038/gim.2012.8&link_type=DOI
 [10]: /lookup/external-ref?access_num=22481183&link_type=MED&atom=%2Fsci%2F343%2F6174%2F968.1.atom"
30b57446afa48a000700353369af77221e5601e5,
5009f9dbc2500464ea11c3ade0ba0a9970f6a934,
61e9c7129f49eb28fb159d05acba958092768b5c,"Biobanks should return clinically significant and actionable research findings to donors who have given physical material to the biobank. Because the clinical significance of a research finding is hard to determine for the individual donor, a procedure to decide on clinical significance should be incorporated into a structure for the actual feedback of research results. Most published studies show that donors expect return of individual research results, but there is almost no experience with it. Explorative questionnaire-based research among Dutch biobanks from the BioBanking Medical Research Infrastructure (BBMRI.NL) shows that a substantial group of biobanks can return individual research findings from data analysis. On the basis of these experiences a return of results policy may be drafted that answers to the interests of donors and the possibilities of biobanks."
6ff3d5e39ffb84dc9d6f56cf57b8beb480c91f23,
bca61669e3fd5cc8ba9de8b7db84a3c096f1aa7c,"Background: There is increasing interest in investigating genetic risk models in empirical studies, but such studies are premature when the expected predictive ability of the risk model is low. We assessed how accurately the predictive ability of genetic risk models can be estimated in simulated data that are created based on the odds ratios (ORs) and frequencies of single-nucleotide polymorphisms (SNPs) obtained from genome-wide association studies (GWASs). Methods: We aimed to replicate published prediction studies that reported the area under the receiver operating characteristic curve (AUC) as a measure of predictive ability. We searched GWAS articles for all SNPs included in these models and extracted ORs and risk allele frequencies to construct genotypes and disease status for a hypothetical population. Using these hypothetical data, we reconstructed the published genetic risk models and compared their AUC values to those reported in the original articles. Results: The accuracy of the AUC values varied with the method used for the construction of the risk models. When logistic regression analysis was used to construct the genetic risk model, AUC values estimated by the simulation method were similar to the published values with a median absolute difference of 0.02 [range: 0.00, 0.04]. This difference was 0.03 [range: 0.01, 0.06] and 0.05 [range: 0.01, 0.08] for unweighted and weighted risk scores. Conclusions: The predictive ability of genetic risk models can be estimated using simulated data based on results from GWASs. Simulation methods can be useful to estimate the predictive ability in the absence of empirical data and to decide whether empirical investigation of genetic risk models is warranted."
be2469c690f46ea23c4ce7e2c1f1128eb58c17c0,
fa3e7608931cc06ae19b39068f8823294db34d31,"In ethical and regulatory discussions on new applications of genomic testing technologies, the notion of ‘personal utility’ has been mentioned repeatedly. It has been used to justify direct access to commercially offered genomic testing or feedback of individual research results to research or biobank participants. Sometimes research participants or consumers claim a right to genomic information with an appeal to personal utility. As of yet, no systematic account of the umbrella notion of personal utility has been given. This paper offers a definition of personal utility that places it in the middle of the spectrum between clinical utility and personal perceptions of utility, and that acknowledges its normative charge. The paper discusses two perspectives on personal utility, the healthcare perspective and the consumer perspective, and argues that these are too narrow and too wide, respectively. Instead, it proposes a normative definition of personal utility that postulates information and potential use as necessary conditions of utility. This definition entails that perceived utility does not equal personal utility, and that expert judgment may be necessary to help determine whether a genomic test can have personal utility for someone. Two examples of genomic tests are presented to illustrate the discrepancies between perceived utility and our proposed definition of personal utility. The paper concludes that while there is room for the notion of personal utility in the ethical evaluation and regulation of genomic tests, the justificatory role of personal utility is not unlimited. For in the absence of clinical validity and reasonable potential use of information, there is no personal utility."
0e8d7863facc3dbfe7ab23725fb2c6d270b298ec,
5ff5ace7e7610550dc06c3adadb4f1f648ddf21c,
6c2b488f4ea6f37c5650a573ae31937f5ab89021,
7642c5b1e84c6519f98aa5192e96b515b9a73a58,"Objective To develop and validate a prognostic model for incident knee osteoarthritis (KOA) in a general population and determine the value of different risk factor groups to prediction. Methods The prognostic model was developed in 2628 individuals from the Rotterdam Study-I (RS-I). Univariate and multivariate analyses were performed for questionnaire/easily obtainable variables, imaging variables, genetic and biochemical markers. The extended multivariate model was tested on discrimination (receiver operating characteristic curve and area under the curve (AUC)) in two other population-based cohorts: Rotterdam Study-II and Chingford Study. Results In RS-I, there was moderate predictive value for incident KOA based on the genetic score alone in subjects aged <65 years (AUC 0.65), while it was only 0.55 for subjects aged ≥65 years. The AUC for gender, age and body mass index (BMI) in prediction for KOA was 0.66. Addition of the questionnaire variables, genetic score or biochemical marker urinary C-terminal cross-linked telopeptide of type II collagen to the model did not change the AUC. However, when adding the knee baseline KL score to the model the AUC increased to 0.79. Applying external validation, similar results were observed in the Rotterdam Study-II and the Chingford Study. Conclusions Easy obtainable ‘Questionnaire’ variables, genetic markers, OA at other joint sites and biochemical markers add only modestly to the prediction of KOA incidence using age, gender and BMI in an elderly population. Doubtful minor radiographic degenerative features in the knee, however, are a very strong predictor of future KOA. This is an important finding, as many radiologists do not report minor degenerative changes in the knee."
919d119b25eb1534206c3b687101eac2a62f3849,"Background Tools for the prediction of atrial fibrillation (AF) may identify high‐risk individuals more likely to benefit from preventive interventions and serve as a benchmark to test novel putative risk factors. Methods and Results Individual‐level data from 3 large cohorts in the United States (Atherosclerosis Risk in Communities [ARIC] study, the Cardiovascular Health Study [CHS], and the Framingham Heart Study [FHS]), including 18 556 men and women aged 46 to 94 years (19% African Americans, 81% whites) were pooled to derive predictive models for AF using clinical variables. Validation of the derived models was performed in 7672 participants from the Age, Gene and Environment—Reykjavik study (AGES) and the Rotterdam Study (RS). The analysis included 1186 incident AF cases in the derivation cohorts and 585 in the validation cohorts. A simple 5‐year predictive model including the variables age, race, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and history of myocardial infarction and heart failure had good discrimination (C‐statistic, 0.765; 95% CI, 0.748 to 0.781). Addition of variables from the electrocardiogram did not improve the overall model discrimination (C‐statistic, 0.767; 95% CI, 0.750 to 0.783; categorical net reclassification improvement, −0.0032; 95% CI, −0.0178 to 0.0113). In the validation cohorts, discrimination was acceptable (AGES C‐statistic, 0.664; 95% CI, 0.632 to 0.697 and RS C‐statistic, 0.705; 95% CI, 0.664 to 0.747) and calibration was adequate. Conclusion A risk model including variables readily available in primary care settings adequately predicted AF in diverse populations from the United States and Europe."
9d3254f1b28891e36f43d84f8de44df1d0982138,
aa172cb5650649ff4e991a394fc4656da434d050,
b0efca0a35cc1d6911534acbfae11dc055bbf0af,
fa8514687d2dd0804f65615c1008a32aa66204a8,"For several years personal genome tests have been offered directly to consumers via the internet. Based on single genome scans, these direct-toconsumer (DTC) tests predict susceptibility to common multifactorial diseases, such as Type 2 diabetes, coronary heart disease and nonfamilial cancer, inform about predisposition to drug response, report carrier status for monogenic diseases, or provide all of the above. The market is served by a few major players, such as 23andMe and deCODEme, and numerous lesser-known companies such as YouScript, GenePlanet and Theranostics [101–105]. The market for DTC personal genome testing is steadily increasing, even though the evidence on the predictive ability and clinical utility of these tests is limited. The few available studies have shown that predicted risks of multifactorial diseases differed between companies and were sometimes even contradictory for the same individual [1,106], but large-scale studies on the predictive ability are lacking. From prediction studies that investigated genetic risk models based on different but comparable selections of SNPs, we know that the predictive ability of genetic testing for multifactorial diseases is low to moderate, except when one or more SNPs had substantial impact on disease risk [2]. From this indirect evidence it is concluded that the DTC offer of genetic testing for multifactorial diseases is premature. To date, most of the discussion about DTC personal genome tests has focused on the prediction of these multifactorial diseases and less attention has been given to the predictive ability and clinical utility of pharmacogenetic testing. Yet, many companies offer pharmacogenetic testing to inform about the genetic susceptibility to drug response and side effects of treatment [3], such as the efficacy of clopidogrel and the risk"
00c470f01b2d7640afde318c70912aff0072f912,
4a221a3bc379e6e30839ce5e6cf55399db049a98,
650f74c3dbbecd477d2ced04073a6e2d26dae881,"Phospho- and sphingolipids are crucial cellular and intracellular compounds. These lipids are required for active transport, a number of enzymatic processes, membrane formation, and cell signalling. Disruption of their metabolism leads to several diseases, with diverse neurological, psychiatric, and metabolic consequences. A large number of phospholipid and sphingolipid species can be detected and measured in human plasma. We conducted a meta-analysis of five European family-based genome-wide association studies (N = 4034) on plasma levels of 24 sphingomyelins (SPM), 9 ceramides (CER), 57 phosphatidylcholines (PC), 20 lysophosphatidylcholines (LPC), 27 phosphatidylethanolamines (PE), and 16 PE-based plasmalogens (PLPE), as well as their proportions in each major class. This effort yielded 25 genome-wide significant loci for phospholipids (smallest P-value = 9.88×10−204) and 10 loci for sphingolipids (smallest P-value = 3.10×10−57). After a correction for multiple comparisons (P-value<2.2×10−9), we observed four novel loci significantly associated with phospholipids (PAQR9, AGPAT1, PKD2L1, PDXDC1) and two with sphingolipids (PLD2 and APOE) explaining up to 3.1% of the variance. Further analysis of the top findings with respect to within class molar proportions uncovered three additional loci for phospholipids (PNLIPRP2, PCDH20, and ABDH3) suggesting their involvement in either fatty acid elongation/saturation processes or fatty acid specific turnover mechanisms. Among those, 14 loci (KCNH7, AGPAT1, PNLIPRP2, SYT9, FADS1-2-3, DLG2, APOA1, ELOVL2, CDK17, LIPC, PDXDC1, PLD2, LASS4, and APOE) mapped into the glycerophospholipid and 12 loci (ILKAP, ITGA9, AGPAT1, FADS1-2-3, APOA1, PCDH20, LIPC, PDXDC1, SGPP1, APOE, LASS4, and PLD2) to the sphingolipid pathways. In large meta-analyses, associations between FADS1-2-3 and carotid intima media thickness, AGPAT1 and type 2 diabetes, and APOA1 and coronary artery disease were observed. In conclusion, our study identified nine novel phospho- and sphingolipid loci, substantially increasing our knowledge of the genetic basis for these traits."
696d5be5f8d0ff976606c657d305ec08e52213fb,
73009bb00e2cdfef4b1976c103a6ddbf605da405,
7643db243a48daf529488d5c4987a6e3a929f477,"The discrimination of a risk prediction model measures that model's ability to distinguish between subjects with and without events. The area under the receiver operating characteristic curve (AUC) is a popular measure of discrimination. However, the AUC has recently been criticized for its insensitivity in model comparisons in which the baseline model has performed well. Thus, 2 other measures have been proposed to capture improvement in discrimination for nested models: the integrated discrimination improvement and the continuous net reclassification improvement. In the present study, the authors use mathematical relations and numerical simulations to quantify the improvement in discrimination offered by candidate markers of different strengths as measured by their effect sizes. They demonstrate that the increase in the AUC depends on the strength of the baseline model, which is true to a lesser degree for the integrated discrimination improvement. On the other hand, the continuous net reclassification improvement depends only on the effect size of the candidate variable and its correlation with other predictors. These measures are illustrated using the Framingham model for incident atrial fibrillation. The authors conclude that the increase in the AUC, integrated discrimination improvement, and net reclassification improvement offer complementary information and thus recommend reporting all 3 alongside measures characterizing the performance of the final model."
7d8dec24a3fdbc4a545b15f00da9940adaa97f1f,
9dd2d5af91d0b738feea4adbfbc2cceb7f1afee7,"Citation for published version (APA): Amin, N., Byrne, E., Johnson, J., Chenevix-Trench, G., Walter, S., Nolte, I. M., Vink, J. M., Rawal, R., Mangino, M., Teumer, A., Keers, J. C., Verwoert, G., Baumeister, S., Biffar, R., Petersmann, A., Dahmen, N., Doering, A., Isaacs, A., Broer, L., ... kConFab Investigators (2012). Genome-wide association analysis of coffee drinking suggests association with CYP1A1/CYP1A2 and NRCAM. Molecular Psychiatry, 17(11), 1116-1129. https://doi.org/10.1038/mp.2011.101"
a4426bb3eecd0235e0cf22c0a3cb2086088d3dbe,"To newly identify loci for age at natural menopause, we carried out a meta-analysis of 22 genome-wide association studies (GWAS) in 38,968 women of European descent, with replication in up to 14,435 women. In addition to four known loci, we identified 13 loci newly associated with age at natural menopause (at P < 5 x 10(-8)). Candidate genes located at these newly associated loci include genes implicated in DNA repair (EXO1, HELQ, UIMC1, FAM175A, FANCI, TLK1, POLG and PRIM1) and immune function (IL11, NLRP11 and PRRC2A (also known as BAT2)). Gene-set enrichment pathway analyses using the full GWAS data set identified exoDNase, NF-kappaB signaling and mitochondrial dysfunction as biological processes related to timing of menopause."
a51744673e569bbf72679cb7a66156bbd51d8de3,
b26f16868211080fc45f3d09b25dccd8256fa6bf,
bad851ed93b35a6417750c9a47e483781b517a6f,"We thank Drs. Cook, Kerr, Bansal, and Pepe for their careful review of our work and insightful critiques. Several of the points they raise require further highlighting, discussion, or rebuttal. 
 
In her commentary, Dr. Cook presents 2 interesting examples that shed additional light on some of the properties of the 3 measures of interest (1). On the basis of the developments presented by us (2), as well as those presented by Kerr et al. (3), the increments between the measures of interest must go in the same direction. Cook's example with diabetes shows that this need not be the case for binary variables. She explains the reason for this anomaly: “The problem is that counterintuitively, there are more cases among people without diabetes than among those with because nondiabetic participants comprise the majority of the cohort” (1, p. 488) and concludes that “[t]he new model may look worse because more are moving in the wrong direction, but the correct changes are larger and the incorrect changes are much smaller” (1, p. 488). This accentuates an important feature of the continuous net reclassification improvement (NRI(>0)); it focuses on the net numbers with altered risks without regard for the magnitude of the change. If we want to weight the movements by their magnitudes, we need to obtain the integrated discrimination improvement (IDI). Although we agree with Cook that helping many people very little may not be better than helping a few people a lot, obtaining such information can actually be valuable, and that information is easy to understand. One additional consideration that might improve the clinical interpretability of the NRI(>0) could be to require that only changes in risks greater than some minimal clinically significant amount be considered. This was the rationale behind our original notation, NRI(>x); x has usually been taken to be 0, but other numbers, for example 0.01 or 0.05, might sometimes be more appropriate. 
 
This example shows the complementarity of the 3 measures in the general setting, an issue raised by Kerr et al (3). A number of other examples can be constructed for further illustration. We give only one here, based on Table 2 of our original article (2). A weak marker is added to a baseline model that has poor performance because of a limited range of some important predictors (for example, age). The observed increase in the area under the receiver operating characteristic curve (AUC) is 0.025, which would usually be considered somewhat promising. However, based on the NRI(>0) of 0.16, we would be able to conclude that it is a weak marker, the impact of which on the AUC will diminish if we extend the range of baseline predictors. Again, the complementarity of these 3 measures lies in their slightly different focuses: The AUC is primarily concerned with the risk model at hand and the NRI(>0) is primarily concerned with the novel marker at hand, whereas the IDI falls in between. 
 
Cook's second example is perhaps even more important. It shows that it is possible (and in our experience quite likely) to have a predictor with no meaningful improvement in the AUC or the IDI but with a highly positive NRI(>0) that is statistically significant. The note of caution that Cook attaches to this observation needs to be highlighted and re-emphasized: Statistical significance of the NRI(>0) indicates practically nothing, and any inference based on this measure needs to be based on its magnitude. Hence, the benchmarks we provide are so important. 
 
Here again, the above example can also serve as an answer to the criticisms of benchmarks by Kerr et al. (3). They note that “the motivation of providing benchmarks actually reinforces previous observations that the problem with these measures is they do not have useful clinical interpretations. If they did, researchers could use the measures directly and benchmarks would not be needed” (3, p. 482). We disagree. First, benchmarks can be helpful whenever binary or ordinal assessment must be imposed on continuous phenomena. Furthermore, it is not clear what the authors mean by clinical interpretation. In our opinion, the distance between average risks for events and nonevents is a very natural and interpretable summary unless one specifically requires incorporation of clinical decision-making, which will necessitate the use of categories and thresholds. This approach has been explicitly endorsed by Dr. Cook in her commentary. In general, we agree that in situations in which established categories exist, their incorporation into the assessment of improvement in model performance affords an additional level of clinical interpretability. However, we do not think that this has to happen to the exclusion of the global measures. 
 
Threshold-based inference may be premature unless improvement in the global performance measures has been established. First, reliance on global measures greatly improves the chances for successful replication of the results. Second, it might theoretically be possible to construct markers that do well based on threshold-based clinical rules but that do poorly when assessed with global measures. However, the chances of discovering such predictors in clinical practice are slim. 
 
Kerr et al. devote a large portion of their commentary to the issue of correlation. They correctly point out that “the notions of incremental value and marginal strength are distinct concepts” (3, p. 482). However, they also use this distinction to criticize our choice of new markers with zero conditional correlation as the reference standard and ascribe to us the desire “to reinforce a common misconception that it is ideal for a new predictor to be uncorrelated with existing predictors” (3, p. 482). In our opinion, their reasoning confuses the issues of a theoretical relation between incremental value and marginal strength and the selection of reference standard and places an overtly large emphasis on the theoretical possibilities. We are aware that in some cases, large correlation can improve discrimination, and the increase in marginal effect size might actually be detrimental to the increase in incremental value. This was shown previously by Cochran (4) and Mardia et al. (5) and is illustrated in Figure 2 of the commentary by Kerr et al. (3). However, the fact that this “paradox” is possible does not mean that it represents the most likely scenario (unconditional correlations commonly observed in practice do not reach the levels required for them to improve discrimination), and it definitely does not represent one that is the easiest to conceptualize. The last 2 features are the key characteristics of a reference standard. Without the loss of generality, any correlated marker can be transformed into an uncorrelated one by a linear transformation that preserves the change in AUC, the IDI, and the NRI(>0). Once such a transformation is performed, predictors can be compared using their conditional effect sizes alone. Thus, even variables with more complex correlation structures can be brought to the plane of our reference standard. Furthermore, working with uncorrelated predictors makes the conceptualization of the increase in model performance easy: The Mahalanobis distance between the event and nonevent populations increases by the square of the effect size of each added variable. Of note, our choice of zero conditional correlation corresponds to a small (<0.1 for most situations) unconditional correlation that is likely to be encountered in practical applications. 
 
Finally, we note that the ideas presented in our article (2) should be considered against the backdrop of the current state of the biomarker research, in which the focus is based solely on statistical significance. Weak predictors are hailed as big winners just because they can have a small enough P value. Our proposal is meant as a first step in the major reorientation of the field to have it rely on the magnitude of the observed effects rather than on their statistical significance. It is hard to imagine that such transformation could take place without the improved understanding of the magnitude of observed effects for which simulations and benchmarks serve as tools."
bc0ab0050a8ea23ab096c36aeeef42c37587504d,
ca35d89bbddb90ec69836a063374321b749dd049,
e7da2a1e81e582291d6c15d5753b62efd6f25a27,
f6a9e5a2ed3190a944ed9ea40342a1ee50368f3c,
05eaaaba5014403d89f0208c48df04af812dd67d,
095e45a906de9d48866a86c77f114712ca639a48,
18a9877d302f405b7aafc324364a211a629efbc8,
1998898c9deb7262a562525a593d2d96b2e53b5f,"Background: Medical risk prediction models estimate the likelihood of future health-related events. Many make use of information derived from analysis of the genome. Models predict health outcomes such as cardiovascular disease, stroke and cancer, and for some conditions several models exist. Although risk models can help decision-making in clinical medicine and public health, they can also be harmful, for example, by misdirecting clinical effort away from those who are most likely to benefit towards people with less need, thus exacerbating health inequalities. Discussion: Risk prediction models need careful assessment before implementation, but the current approach to their development, evaluation and implementation is inappropriate. As a result, some models are pressed into use before it is clear whether they are suitable, while in other cases there is confusion about which model to use. This paper proposes an approach to the appraisal of risk-scoring models, based on a conference of UK experts. Summary: By specifying what needs to be known before a model can be judged suitable for translation from research into practice, we can ensure that useful models are taken up promptly, that less well-proven ones undergo further evaluation and that resources are not wasted on ineffective ones."
29e2d6fdbfdc9b8590737bc948337a1a74e781db,
2fb0a96c1394fc18b3baa557dfc2daa96afb0493,"Background. The separate value of endoscopic ultrasonography (EUS), multidetector computed tomography (CT), and F-fluorodeoxyglucose positron emission tomography (FDG-PET) in the optimal sequence in staging esophageal cancer has not been investigated adequately. Methods. The staging records of 216 consecutive operable patients with esophageal cancer were reviewed blindly. Different staging strategies were analyzed, and the likelihood ratio (LR) of each module was calculated conditionally on individual patient characteristics. A logistic regression approach was used to determine the most favorable staging strategy. Results. Initial EUS results were not significantly related to the LRs of initial CT and FDG-PET results. The positive LR (LR?) of EUS-fine-needle aspiration (FNA) was 4, irrespective of CT and FDG-PET outcomes. The LR? of FDG-PET varied from 13 (negative CT) to 6 (positive CT). The LR? of CT ranged from 3–4 (negative FDG-PET) to 2–3 (positive FDG-PET). Age, histology, and tumor length had no significant impact on the LRs of the three diagnostic tests. Conclusions. This study argues in favor of PET/CT rather than EUS as a predictor of curative resectability in esophageal cancer. EUS does not correspond with either CT or FDG-PET. LRs of FDG-PET were substantially different between subgroups of negative and positive CT results and vice versa. Accurate preoperative staging in esophageal cancer is important in the choice of treatment, preventing unnecessary toxic preoperative chemoradiation and/or surgical explorations. Moreover, it is essential to determine optimal treatment and to monitor treatment response after neoadjuvant therapy. Radical surgery with curative intent is only possible if distant metastases (M1) and infiltration of the primary tumor into adjacent vital structures (T4b) are absent. If present, primary (chemo)radiation, brachytherapy or stent placement are more adequate and less invasive alternatives as palliative treatment. Currently, preoperative staging of esophageal cancer includes endoscopic ultrasonography (EUS) with or without fine-needle aspiration (FNA) of suspicious lymph nodes, 16–64 multidetector/slice computed tomography (CT), external ultrasound (US) of the cervical region, and bronchoscopic examination, if indicated, in mid/upper DOI 10.1245/s10434-011-1738-8 Ann Surg Oncol (2016) 23:S1021–S1028 The Author(s) 2011. This article is published with open access at Springerlink.com First Received: 8 October 2010; J. Th. M. Plukker, MD, PhD e-mail: j.th.plukker@chir.umcg.nl Published Online: 6 May 2011 thoracic tumors. To detect distant nodal and systemic metastases, whole-body positron emission tomography with F-fluordeoxyglucose (FDG-PET) or PET/CT is widely used. These staging methods are used in different sequences, according to the guidelines employed. Despite these dedicated staging techniques, surgical resection is still abandoned in 10–50% of all cases due to excessive locoregional tumor extent or presence of distant metastases. Assessment of resectability is based on both local and distant criteria. Imaging techniques are more or less complementary, but outcome may also depend on the sequence of the preoperative workup. Furthermore, a recent study showed significant but small differences in perceived patient burden between PET and CT compared with EUS. Therefore, it is important to know the adequate sequences of these different diagnostic methods and when to use PET/CT or only CT (upfront), followed by EUS, and vice versa. Several studies found that FDG-PET combined with EUS-FNA improved preoperative staging of esophageal cancer. Fusion of FDG-PET and CT images also provided an increase in preoperative management from 6 to 25%. The optimal staging strategy, however, remains unclear, and the additional value of combined PET/CT has not been determined adequately yet. Therefore, we used a logistic regression approach to determine the extent to which the individual value of each diagnostic staging technique depends on the order in which the procedure is applied and to determine if this staging method adds useful information to what is already known, either because of individual characteristics or on the basis of preliminary staging results. Three routine diagnostic staging techniques (EUS, CT, and FDG-PET) were tested in terms of curatively intended resectability of esophageal cancer. For this purpose, we compared the likelihood ratios (LRs) in different staging strategies, calculated at the level of the individual patient. PATIENTS AND METHODS"
3460de93ee846af806ca42d755b62749e5ebb1c0,"markdownabstract* The rapid and continuing progress in gene discovery for complex diseases is fueling interest in the potential application of genetic risk models for clinical and public health practice. 
* The number of studies assessing the predictive ability is steadily increasing, but the quality and completeness of reporting varies. 
* A multidisciplinary workshop sponsored by the Human Genome Epidemiology Network developed a checklist of 25 items recommended for strengthening the reporting of Genetic RIsk Prediction Studies (GRIPS), building on the principles established by prior reporting guidelines. 
* These recommendations aim to enhance the transparency of study reporting, and thereby to improve the synthesis and application of information from multiple studies that might differ in design, conduct, or analysis. 
* A detailed Explanation and Elaboration document is published as Supplemental Digital Content 1, http://links.lww.com/GIM/A165."
3ac9090840764b8a3e97094444afbfd85da2b645,
3b8ec8657aaeaaae27650f96260c4b68cb3da95f,"Guidelines for reporting various research designs have been published, but none is fully suited to genetic risk prediction studies, an emerging field of investigation with specific methodological c..."
3ba4f96c7534a2a034ac6203e0163958f2bd2054,
3da9652e939c5bd0152d0d994a83b8e35a2ba0da,"Background: Genome-wide association studies identified novel breast cancer susceptibility variants that could be used to predict breast cancer in asymptomatic women. This review and modeling study aimed to investigate the current and potential predictive performance of genetic risk models. Methods: Genotypes and disease status were simulated for a population of 10,000 women. Genetic risk models were constructed from polymorphisms from meta-analysis including, in separate scenarios, all polymorphisms or statistically significant polymorphisms only. We additionally investigated the magnitude of the odds ratios (OR) for 1 to 100 hypothetical polymorphisms that would be needed to achieve similar discriminative accuracy as available prediction models [modeled range of area under the receiver operating characteristic curve (AUC) 0.70–0.80]. Results: Of the 96 polymorphisms that had been investigated in meta-analyses, 41 showed significant associations. AUC was 0.68 for the genetic risk model based on all 96 polymorphisms and 0.67 for the 41 significant polymorphisms. Addition of 50 additional variants, each with risk allele frequencies of 0.30, requires per-allele ORs of 1.2 to increase this AUC to 0.70, 1.3 to increase AUC to 0.75, and 1.5 to increase AUC to 0.80. To achieve AUC of 0.80, even 100 additional variants would need per-allele ORs of 1.3 to 1.7, depending on risk allele frequencies. Conclusion: The predictive ability of genetic risk models in breast cancer has the potential to become comparable to that of current breast cancer risk models. Impact: Risk prediction based on low susceptibility variants becomes a realistic tool in prevention of nonfamilial breast cancer. Cancer Epidemiol Biomarkers Prev; 20(1); 9–22. ©2011 AACR."
3ec8f8b7ab333af21a2cccc81c4f26fee618a33a,
428ba9babf9b46acd0ebda6c1fa16926e796b4b8,
4bdb802c9b82a3e8206163bc78d50108c4221332,The number of known genetic markers of risk is increasing but the interpretation of their clinical effect is hampered by poor reporting of prediction studies. These guidelines from the GRIPS group aim to ensure transparent reporting of prediction studies
643570a8f037ede982112ce701768105cc7b2408,
6eb13b9e580b96f05dd9f6056a59624bf44fc7b7,
7aa8878b0b2e216cb052812827637b56e7fc1ba3,Type 2 diabetes (T2D) is a common disease caused by a complex interplay between many genetic and environmental factors. Candidate gene studies and recent collaborative genome-wide association efforts revealed at least 38 common single nucleotide polymorphisms (SNPs) associated with increased risk of T2D. Genetic testing of multiple SNPs is considered a potentially useful tool for early detection of individuals at high diabetes risk leading to improved targeting of preventive interventions.
7c33a580f3b31fa8368808f85e440bad836c5ccf,
7eb9c2a5e5de26bf0aec8a88eefbb38a240dad6d,
9f609a185e6aed7f67e4c2c2b8aeb83e44bc4841,
a0a2d61aa08192a6140ce11e134d285a60fb7c93,
a56fed897f45b00bad3a7267cc563b66b97a8774,
afb41e58e92e91a51fd4d90ad23fbb69ad052495,"Cecile Janssens and colleagues present the GRIPS Statement, a checklist to help strengthen the reporting of genetic risk prediction studies."
b644ec94c6acb88d104ce7fd97eb725103dbbf05,"Objective A recent collaborative genome-wide association study replicated a large number of susceptibility loci and identified novel loci. This increase in known multiple sclerosis (MS) risk genes raises questions about clinical applicability of genotyping. In an empirical set we assessed the predictive power of typing multiple genes. Next, in a modelling study we explored current and potential predictive performance of genetic MS risk models. Materials and Methods Genotype data on 6 MS risk genes in 591 MS patients and 600 controls were used to investigate the predictive value of combining risk alleles. Next, the replicated and novel MS risk loci from the recent and largest international genome-wide association study were used to construct genetic risk models simulating a population of 100,000 individuals. Finally, we assessed the required numbers, frequencies, and ORs of risk SNPs for higher discriminative accuracy in the future. Results Individuals with 10 to 12 risk alleles had a significantly increased risk compared to individuals with the average population risk for developing MS (OR 2.76 (95% CI 2.02–3.77)). In the simulation study we showed that the area under the receiver operating characteristic curve (AUC) for a risk score based on the 6 SNPs was 0.64. The AUC increases to 0.66 using the well replicated 24 SNPs and to 0.69 when including all replicated and novel SNPs (n = 53) in the risk model. An additional 20 SNPs with allele frequency 0.30 and ORs 1.1 would be needed to increase the AUC to a slightly higher level of 0.70, and at least 50 novel variants with allele frequency 0.30 and ORs 1.4 would be needed to obtain an AUC of 0.85. Conclusion Although new MS risk SNPs emerge rapidly, the discriminatory ability in a clinical setting will be limited."
c26bf4b0b90f31b447f031d3aa563f23b1acdd8e,
c373b47ce2a5e339774afbecd0403cd420ba4651,
c3bd5abe731b0df333e1b96b5945dd910c8e6a28,
c85397fdbb75d3895bd79de3678d849aca9a8e26,
c8d9b0b37efa5a9364d866eccef157bd6584753a,
ccd7067d7eb622a24d8ae8a4f81d9371149d032c,
d4533214464d4398d28e112db982543b18f87c93,
da379fc4ea6feb594e59bd552fa65d7620ff5ee7,
dad061f35a7fe0878597a6fd49e870e811bcf728,
e54e66b9a6041c705f3f2ce119d5cad41bfdc892,
e6c92d827bd776c6b4d4e3c5e3352fc8c5d823c2,
f243bd0d5e192c31fbd5b82f291e0e79914ed763,"The recent successes of genome-wide association studies and the promises of whole genome sequencing fuel interest in the translation of this new wave of basic genetic knowledge to health care practice. Knowledge about genetic risk factors may be used to target diagnostic, preventive, and therapeutic interventions for complex disorders based on a person's genetic risk, or to complement existing risk models based on classical nongenetic factors, such as the Framingham risk score for cardiovascular disease. Implementation of genetic risk prediction in health care requires a series of studies that encompass all phases of translational research,1,2 starting with a comprehensive evaluation of genetic risk prediction.

With increasing numbers of discovered genetic markers that can be used in future genetic risk prediction studies, it is crucial to enhance the quality of the reporting of these studies, since valid interpretation could be compromised by the lack of reporting of key information. Information that is often missing includes details in the description of how the study was designed and conducted (eg, how genetic variants were selected and coded, how risk models or genetic risk scores were constructed, and how risk categories were chosen), or how the results should be interpreted. An appropriate assessment of the study's strengths and weaknesses is not possible without this information. There is ample evidence that prediction research often suffers from poor design and bias, and these may also have an impact on the results of the studies and on models of disease outcomes based on these studies.3–5 Although most prognostic studies published to date claim significant results,6,7 very few translate to clinically useful applications. Just as for observational epidemiological studies,8 poor reporting complicates the use of the specific study for research, clinical, or public health purposes and hampers the …"
00d8033e5abf8df04798f49a5ef3283a9ae2db09,"_To the Editor:_ 
According to the review by Dr Tzoulaki and colleagues, most studies that claimed improved predictive performance beyond the Framingham risk score (FRS) had flaws in their design, analyses, and reporting. To interpret these observations, 2 questions are pertinent: what did the original studies aim to investigate, and what constitutes a claim? [...]"
00ed2b89723b638941d8a87eb1c642eb98817add,"CONTEXT
Genome-wide association studies (GWAS) have recently identified CLU, PICALM, and CR1 as novel genes for late-onset Alzheimer disease (AD).


OBJECTIVES
To identify and strengthen additional loci associated with AD and confirm these in an independent sample and to examine the contribution of recently identified genes to AD risk prediction in a 3-stage analysis of new and previously published GWAS on more than 35,000 persons (8371 AD cases).


DESIGN, SETTING, AND PARTICIPANTS
In stage 1, we identified strong genetic associations (P < 10(-3)) in a sample of 3006 AD cases and 14,642 controls by combining new data from the population-based Cohorts for Heart and Aging Research in Genomic Epidemiology consortium (1367 AD cases [973 incident]) with previously reported results from the Translational Genomics Research Institute and the Mayo AD GWAS. We identified 2708 single-nucleotide polymorphisms (SNPs) with P < 10(-3). In stage 2, we pooled results for these SNPs with the European AD Initiative (2032 cases and 5328 controls) to identify 38 SNPs (10 loci) with P < 10(-5). In stage 3, we combined data for these 10 loci with data from the Genetic and Environmental Risk in AD consortium (3333 cases and 6995 controls) to identify 4 SNPs with P < 1.7x10(-8). These 4 SNPs were replicated in an independent Spanish sample (1140 AD cases and 1209 controls). Genome-wide association analyses were completed in 2007-2008 and the meta-analyses and replication in 2009.


MAIN OUTCOME MEASURE
Presence of Alzheimer disease.


RESULTS
Two loci were identified to have genome-wide significance for the first time: rs744373 near BIN1 (odds ratio [OR],1.13; 95% confidence interval [CI],1.06-1.21 per copy of the minor allele; P = 1.59x10(-11)) and rs597668 near EXOC3L2/BLOC1S3/MARK4 (OR, 1.18; 95% CI, 1.07-1.29; P = 6.45x10(-9)). Associations of these 2 loci plus the previously identified loci CLU and PICALM with AD were confirmed in the Spanish sample (P < .05). However, although CLU and PICALM were confirmed to be associated with AD in this independent sample, they did not improve the ability of a model that included age, sex, and APOE to predict incident AD (improvement in area under the receiver operating characteristic curve from 0.847 to 0.849 in the Rotterdam Study and 0.702 to 0.705 in the Cardiovascular Health Study).


CONCLUSIONS
Two genetic loci for AD were found for the first time to reach genome-wide statistical significance. These findings were replicated in an independent population. Two recently reported associations were also confirmed. These loci did not improve AD risk prediction. While not clinically useful, they may implicate biological pathways useful for future research."
01835d4ac324e0347d39877f2754599edbc3e912,"The moderate predictive performance of cardiovascular disease risk models necessitates more studies that investigate the incremental value of novel biomarkers. In recent years, many new biomarkers have been evaluated for their ability to improve prediction of cardiovascular disease beyond traditional risk factors, including C-reactive protein, coronary artery calcium, and single-nucleotide polymorphisms in the 9p21 region.1 The interest in novel biomarkers is propelled, in part, by emerging discoveries from genomewide association studies of genetic variants associated with risk for many common diseases.2 Nonetheless, family history, an “old” tool in clinical practice—crucial for the diagnosis and management of genetic disorders—has not been adequately explored for its value in risk assessment and prevention of common diseases.3 Positive family history is a strong risk factor for cardiovascular diseases,4,5 reflecting the consequences of genetic and nongenetic risk factors that are shared among relatives. From a practical perspective, family history is a strong determinant of disease that is relatively easy to assess. For this reason, family history is advocated as a useful tool for identifying individuals at increased risk of disease and for tailoring preventive interventions.4,6,7 However, the challenge is to show that this knowledge has clinical utility for improving health.8

Article see p 97 

In this issue of Circulation: Cardiovascular Genetics , Scheuner et al9 report the incremental value of adding family history information to the General Cardiovascular Risk Profile to improve identification of individuals with advanced coronary artery calcification. The incremental value of family history in disease prediction is assessed in several questions10: 

1. Does family history improve prediction beyond traditional risk factors?

2. Does improved prediction change treatment decisions or health recommendations?

3. Do these changes improve health outcomes or have other benefits?

4. Do the incremental benefits outweigh the extra costs?

This evaluation is a stepwise …"
03f6550434c5eb6ef30a997e57ad5aecb5721550,"The moderate predictive performance of cardiovascular disease risk models necessitates more studies that investigate the incremental value of novel biomarkers. In recent years, many new biomarkers have been evaluated for their ability to improve prediction of cardiovascular disease beyond traditional risk factors, including C-reactive protein, coronary artery calcium, and single-nucleotide polymorphisms in the 9p21 region.1 The interest in novel biomarkers is propelled, in part, by emerging discoveries from genomewide association studies of genetic variants associated with risk for many common diseases.2 Nonetheless, family history, an “old” tool in clinical practice—crucial for the diagnosis and management of genetic disorders—has not been adequately explored for its value in risk assessment and prevention of common diseases.3 Positive family history is a strong risk factor for cardiovascular diseases,4,5 reflecting the consequences of genetic and nongenetic risk factors that are shared among relatives. From a practical perspective, family history is a strong determinant of disease that is relatively easy to assess. For this reason, family history is advocated as a useful tool for identifying individuals at increased risk of disease and for tailoring preventive interventions.4,6,7 However, the challenge is to show that this knowledge has clinical utility for improving health.8"
0d03df3bef557c807265fdb7b2a0bc33ece30762,
189c519e844e053ee7de1445a9d7c82de92a30e1,
28e7d6d1c83b1e1f8af38ab960cced0738966dce,"Alzheimer's disease (AD) is the most prevalent form of dementia and the number of cases is expected to increase exponentially worldwide. Three highly penetrant genes (AbetaPP, PSEN1, and PSEN2) explain only a small number of AD cases with a Mendelian transmission pattern. Many genes have been analyzed for association with non-Mendelian AD, but the only consistently replicated finding is APOE. At present, possibilities for prevention, early detection, and treatment of the disease are limited. Predictive and diagnostic genetic testing is available only in Mendelian forms of AD. Currently, APOE genotyping is not considered clinically useful for screening, presymptomatic testing, or clinical diagnosis of non-Mendelian AD. However, clinical management of the disease is expected to benefit from the rapid pace of discoveries in the genomics of AD. Following a recently developed framework for the continuum of translation research that is needed to move genetic discoveries to health applications, this paper reviews recent genetic discoveries as well as translational research on genomic applications in the prevention, early detection, and treatment of AD. The four phases of translation research include: 1) translation of basic genomics research into a potential health care application; 2) evaluation of the application for the development of evidence-based guidelines; 3) evaluation of the implementation and use of the application in health care practice; and 4) evaluation of the achieved population health impact. Most research on genome-based applications in AD is still in the first phase of the translational research framework, which means that further research is still needed before their implementation can be considered."
2db8ff538a048a09719fc44c35bf6b2977b11dba,
3f146e6817bd269aa3debaa7abea2de20fb0369a,"Purpose: To address the key question of whether using available “cardiogenomic profiles” leads to improved health outcomes (e.g., reduction in rates of myocardial infarction and stroke) and whether these profiles help in making medical or personal decisions.Methods: A targeted evidence-based review based on published Evaluation of Genomic Applications in Practice and Prevention methodologies.Results: No study addressed the overarching question directly. Evidence for the analytic validity of genomic profiles was inadequate for most genes (scale: convincing, adequate, and inadequate), but based on gray data, the analytic sensitivity and specificity might be adequate. For the 29 candidate genes (58 separate associations reviewed), the credibility of evidence for clinical validity was weak (34 associations) to moderate (23 associations), based on limited evidence, potential biases, and/or variability between included studies. The association of 9p21 variants with heart disease had strong credibility with odds ratios of 0.80 (95% confidence interval: 0.77–0.82) and 1.25 (95% confidence interval: 1.21–1.30), respectively, for individuals with no, or two, at-risk alleles versus those with one at-risk allele. Using a multiplicative model, we combined information from 24 markers predicting heart disease and from 13 markers for stroke. The areas under the curves (64.7% and 55.2%, respectively), and overall screening performance (detection rates of 24% and 14% at a 10% false-positive rate, respectively) do not warrant use as stand-alone tests.Conclusion: Even if genomic markers were independent of traditional risk factors, reports indicate that cardiovascular disease risk reclassification would be small. Improvement in health could occur with earlier initiation or higher adherence to medical or behavioral interventions, but no prospective studies documented such improvements (clinical utility)."
3f7ab6b53f5b924ca6f0e6ebae554050b64e44a8,"In a prospective longitudinal cohort study of dementia and mortality in persons with Down syndrome aged 45 years and older, 85 postmenopausal women were followed for a mean follow-up time of 4.3 years (range 0.0 to 7.4 years). The effect of age at menopause on age at diagnosis of dementia and survival was estimated using correlation analysis and Cox Proportional Hazard Model. We found a significant correlation between age at menopause and age at diagnosis of dementia (rho=0.52; p< 0.001), and between age at menopause and age at death (rho=0.49; p=0.01). Early age at menopause is associated with a 1.8 fold increased risk of dementia: Hazard Ratio (HR): 1.82 (95%Confidence Interval (CI): 1.31-2.52) and with risk of death: HR: 2.05 (95%CI: 1.33-3.16). Our study suggests that age at menopause in women with Down syndrome is a determinant of age at onset of dementia and mortality."
5a911c8081929e5a0d5beeff13d74dc945c591fb,
694a21eb217ce927bc40505fa677fb4e2488b74f,"In the near future it will probably be possible to unravel the DNA code of the human genome for less than US $ 1,000 by means of 'whole genome sequencing' (WGS). However, its usefulness in clinical practice is questionable. Although WGS of an individual may become relatively inexpensive and easily available, knowledge of the complete DNA sequence in itself does not provide clinically useful information. DNA data need to be analyzed and interpreted, but there are still many gaps and uncertainties in our knowledge of DNA variations and their clinical consequences. WGS may be a useful supplementary testing technique for establishing the diagnosis of monogenic disorders and syndromes, but potentially undesirable or unclear findings may cause ethical and practical problems. Therefore, WGS should only be applied very cautiously and after thorough deliberation of its possible consequences."
78a2c0e779d523f871f271b6003819dfb458ec2b,"Objective: To investigate the co-occurrence of migraine and depression and assess whether shared genetic factors may underlie both diseases. Methods: Subjects were 2,652 participants of the Erasmus Rucphen Family genetic isolate study. Migraine was diagnosed using a validated 3-stage screening method that included a telephone interview. Symptoms of depression were assessed using the Center for Epidemiologic Studies Depression scale and the depression subscale of the Hospital Anxiety and Depression Scale (HADS-D). The contribution of shared genetic factors in migraine and depression was investigated by comparing heritability estimates for migraine with and without adjustment for symptoms of depression, and by comparing the heritability scores of depression between migraineurs and controls. Results: We identified 360 migraine cases: 209 had migraine without aura (MO) and 151 had migraine with aura (MA). Odds ratios for depression in patients with migraine were 1.29 (95% confidence interval [CI] 0.98–1.70) for MO and 1.70 (95% CI 1.28–2.24) for MA. Heritability estimates were significant for all migraine (0.56), MO (0.77), and MA (0.96), and decreased after adjustment for symptoms of depression or use of antidepressant medication, in particular for MA. Comparison of the heritability scores for depression between patients with migraine and controls showed a genetic correlation between HADS-D score and MA. Conclusions: There is a bidirectional association between depression and migraine, in particular migraine with aura, which can be explained, at least partly, by shared genetic factors."
7d22af23f92e9ae47b048cdc0abe4d4872bb8c38,
7de9678ca880dec5a811c5bc46c297c317e240e1,
957149f78a58e9d1a0a0146e586b6c1a30cb2467,
9be453c302449a1ffe239d51e558ffc1cd63f76b,"Background Musculoskeletal complaints are associated with a large medical and societal burden. Although acupuncture is a frequently used therapy for musculoskeletal complaints, little is known about the effect on health-related quality of life (HRQoL). Objectives The aim of this study was to (i) compare the HRQoL of patients undergoing routine acupuncture treatment for musculoskeletal complaints with a Dutch population sample; (ii) investigate changes in HRQoL during the course of acupuncture treatment. Methods An observational study of 26 patients between 18 and 65 years of age in a single acupuncture practice was performed. HRQoL was measured on eight functional domains using a RAND-36 health survey at baseline and after six and 12 treatment sessions. Baseline RAND-36 scores were compared to data from a Dutch population sample (n=1063) using t test, and longitudinal data were analysed using repeated measurement analyses. Results At baseline, patients had significantly lower RAND-36 scores compared to the Dutch population sample for three domains: role-physical limitations (51.9 vs 79.4; p<0.001), bodily pain (49.3 vs 79.5; p<0.001) and social functioning (75.5 vs 86.9; p=0.005). During the course of treatment, RAND-36 scores increased significantly for five domains: physical functioning (79.3 vs 97.4; p<0.001), role-physical functioning (51.4 vs 94.1; p<0.001), bodily pain (47.3 vs 95.7, p<0.001), social functioning (74.5 vs 92.0, p<0.001) and vitality (69.1 vs 85.7; p<0.001). Conclusion The observed improvements in HRQoL suggest a subjective, clinically relevant, benefit of routine acupuncture therapy in treating musculoskeletal complaints."
9dcfff51960fc9636155379c595bfc88a720032f,
a208d084ab290acfc79e125e505b7b65bdc787b2,
a5656eedb6f3fe872c7e08290321371f105ee6fa,
c58e7c86a854efd8f1527621ab844f7673072261,
c87e172f97c490cd4c8a911d824917a6069d41a8,"Background Although the vertical cup-disc ratio (VCDR) and intraocular pressure (IOP) are important determinants of open angle glaucoma (OAG), it is unclear to what extent the genetic origin of these traits overlap with those of OAG. We evaluated whether the same genes that determine VCDR and IOP also predict OAG. Methods Genetic risk scores were constructed from single nucleotide polymorphisms (SNPs) using genome wide association data of 9326 participants from the Rotterdam Study cohorts (mean±SD age: 64.6±9.1 years). These risk scores were used to calculate the explained variance of VCDR and IOP in an independent cohort (Erasmus Rucphen Family study) consisting of 1646 participants (mean±SD age: 46.8±14.1 years) and the OAG risk in a subset of the Rotterdam Study cohorts. To evaluate false positive findings, we generated two new variables containing randomly sampled values to serve as a negative control. Results The explained variance of VCDR increased when increasing the number of SNPs included in the risk score, suggesting a polygenic model. We found no clear evidence for a similar model for IOP, suggesting that a small number of SNPs determine the susceptibility to IOP. The SNPs related to IOP in terms of p values contributed little to VCDR. The risk scores associated with VCDR were also associated significantly with OAG. This suggests a common polygenic background for VCDR and OAG Conclusions We found evidence for a polygenic model underlying one of the major traits of OAG, VCDR, and OAG itself. The IOP did not show any evidence for such a model."
e0a98bb919fe73274330c161936a9de0a3782fa0,"Supplementary material http://jama.ama-assn.org/cgi/content/full/303/18/1832/DC1 eSupplement Correction Contact me if this article is corrected. Citations Contact me when this article is cited. This article has been cited 3 times. Topic collections Contact me when new articles are published in these topic areas. Genetics, Other Neurology; Alzheimer Disease; Neurogenetics; Genetics; Genetic Disorders;"
e5172225e229c05198e36d1cd214b732610456bf,"Reclassification is observed even when there is no or minimal improvement in the area under the receiver operating characteristic curve (AUC), and it is unclear whether it indicates improved clinical utility. The authors investigated total reclassification, net reclassification improvement, and integrated discrimination improvement for different DeltaAUC using empirical and simulated data. Empirical analyses compared prediction of type 2 diabetes risk based on age, sex, and body mass index with prediction updated with 18 established genetic risk factors. Simulated data were used to investigate measures of reclassification against DeltaAUCs of 0.005, 0.05, and 0.10. Total reclassification and net reclassification improvement were calculated for all possible cutoff values. The AUC of type 2 diabetes risk prediction improved from 0.63 to 0.66 when 18 polymorphisms were added, whereas total reclassification ranged from 0% to 22.5% depending on the cutoff value chosen. In the simulation study, total reclassification, net reclassification improvement, and integrated discrimination improvement increased with higher DeltaAUC. When DeltaAUC was low (0.005), net reclassification improvement values were close to zero, integrated discrimination improvement was 0.08% (P > 0.05), but total reclassification ranged from 0 to 6.7%. Reclassification increases with increasing AUC but predominantly varies with the cutoff values chosen. Reclassification observed in the absence of AUC increase is unlikely to improve clinical utility."
0f33ed07a886b7902de25632733ba0ca14b33aea,"modifiable genetic risks in a context of important, modifiable environmental risk factors. In this issue of Public Health Genomics, Wade et al. [7] describe a thorough and well thought-out approach to select health conditions and gene variants for translational genomics research. Their selection process involves several rounds of review in which gene variants are evaluated against stringent criteria. Health conditions are selected if they have public health importance, are preventable, adult-onset diseases, and relevant to a broad population. Gene variants are selected if their association has been robustly demonstrated with an odds ratio for association of at least 1.10 and a variant frequency of at least 5%. Using this approach the authors obtained a list of 24 genetic markers, of which 15 were included in the gene panel of their study, the Multiplex Initiative [8] . These 15 variants test for 8 common health conditions: 4 variants for type 2 diabetes, 3 for osteoporosis and for coronary heart disease, and 1 for hypercholesterolemia, hypertension, lung cancer, colorectal cancer, and skin cancer each (http://multiplex.nih.gov). The selection of real gene variants on the basis of robust scientific evidence is a strong quality of their process, because it is widely acknowledged that hypothetical tests produce results that are inconsistent with what is observed in practice [9] . Nevertheless, also ‘realistic’ tests that are based on robustly established variants remain hypothetical, particularly when they are designed to inGenome-wide association studies are rapidly unraveling genetic susceptibility variants that are implicated in the etiology of common multifactorial diseases such as coronary heart disease, diabetes, and cancer [1] . Expectations about the impact of these discoveries on preventive and clinical health care practice are high [2, 3] , but it is widely acknowledged that currently only a fraction of the genetic factors have been identified. Nevertheless, many companies already offer personalized lifestyle health recommendations and nutritional supplements based on clients’ genetic profiles [4] , and increasingly discussions are raised on whether genetic test results should be returned to research participants [5, 6] . To date, not only the health benefits but also the psychological and behavioral impacts of communicating genetic test results are largely unknown. These psychological and behavioral implications can not be inferred from the vast literature on genetic testing for monogenic diseases, because the genetic origin of multifactorial diseases is entirely different. Multifactorial diseases are caused by complex interactions between multiple genetic and environmental factors, with each single genetic susceptibility variant conveying only a minor increase in disease risk. Genetic testing for multifactorial diseases therefore implies the simultaneous testing at multiple genetic loci. New translational research is needed to learn how individuals deal with this complex genomic risk information and how they interpret nonReceived: May 19, 2009 Accepted: May 19, 2009 Published online: September 3, 2009"
1860e99a5ee2a9edf40c013d964ec4fd5c828bec,
2880c6e404d399865a1a0ecadd5cc4d5cde3cd68,
3a1fed29672e1040fd8e1e947cfb67dd4421a1bb,"Purpose: Commercial internet-based companies offer genome-wide scans to predict the risk of common diseases and personalize nutrition and lifestyle recommendations. These risk estimates are updated with every new gene discovery.Methods: To assess the benefits of updating risk information in commercial genome-wide scans, we compared type 2 diabetes risk predictions based on TCF7L2 alone, 18 polymorphisms alone, and 18 polymorphisms plus age, sex, and body mass index. Analyses were performed using data from the Rotterdam study, a prospective, population-based study among individuals aged 55 years and older. Data were available from 5297 participants.Results: The actual prevalence of type 2 diabetes in the study population was 20%. Predicted risks were below average for carriers of the TCF7L2 CC genotype (predicted risk 17.6%) and above average for the CT and TT genotypes (20.8% and 28.0%). Adding the other 17 polymorphisms caused 34% of participants to be reclassified (i.e., switched between below and above average): 24% of the CC carriers changed to increased risk, 52% and 6% of the CT and TT carriers changed to decreased risk. Including information on age, sex, and body mass index caused 29% to change categories (27%, 31%, and 19% for CC, CT, and TT carriers, respectively). In total, 39% of participants changed categories once when risk factors were updated, and 11% changed twice, i.e., back to their initial risk category.Conclusion: Updating risk factors may produce contradictory information about an individual's risk status over time, which is undesirable if lifestyle and nutritional recommendations vary accordingly."
5924944a87e77c4f9e6cb7e28bc548eacd332b83,
59c06395e11b539695f4946a483afa35c75b6f50,
61c9b62941e0a96252b79fe38cf0fc41208273c2,
706280e71c44ebc4bee6a469958d027f1581bf77,
7172a4106e9cc7d376728c0fdf8cea4eb706ff0a,
7dd9703781032a91db0f3918e087c4f45a81d950,"Purpose: Consortia of investigators currently compile sufficiently large sample sizes to investigate the effects of low-risk susceptibility genetic variants. It is not clear how the results obtained by consortia comparewith those derived from meta-analyses of published studies.Methods: We performed meta-analyses of published data for 16 genetic polymorphisms investigated by the Breast Cancer Association Consortium, and comparedsample sizes, heterogeneity, and effect sizes. PubMed, Web of Science, and Human Genome Epidemiology Network databases were searched for breast cancer case-control association studies.Results: We found that meta-analyses of published data and consortium analyses were based on substantially different data. Published data by nonconsortium teams amounted on average to 26.9% of all available data (range 3.0 –50.0%). Both approaches showed statistically significant decreased breast cancer risks for CASP8 D302H. The meta-analyses of published data demonstrated statistically significant results for five other genes and the consortium analyses for two other genes, but the strength of this evidence, evaluated on the basis of the Venice criteria, was not strong.Conclusions: Because both approaches identified the same gene out of 16 candidates, the methods can be complimentary. The expense and complexity of consortium-based studies should be considered vis-à-vis the potential methodological limitations of synthesis of published studies."
a84ff2d451ec63771c70bb30a6d96d4457d48310,"Background Both obesity and lean mass have been correlated with symptoms of depression. Objective To investigate the contribution of genetic and environmental factors in the cooccurrence of obesity and lean mass with symptoms of depression. Methods Individuals were 2383 participants of the Erasmus Rucphen Family study. Symptoms of depression were assessed using the Center for Epidemiologic Studies Depression Scale and the Hospital Anxiety and Depression Scale. Anthropometric and dual-energy X-ray absorptiometry total body scans were obtained for the assessment of body composition. The role of shared genetic and shared environmental factors was quantified by estimating genetic and environmental correlations between symptoms of depression and measures of body composition. Results Phenotypic correlations between body composition and symptoms of depression ranged from −0.08 to 0.08. Heritability estimates for body composition ranged from 0.40 to 0.46 (P<0.001) in women and from 0.35 to 0.51 (P<0.001) in men, and heritability estimates for depression scores were higher in women (0.34 and 0.37) than in men (0.13 and 0.21). No consistent genetic correlations between measures of body composition and symptoms of depression were found. We did find a significant consistent environmental correlation between depression scores and lean mass index (environmental correlation=−0.23 for Center for Epidemiologic Studies Depression Scale and −0.31 for Hospital Anxiety and Depression Scale). Conclusion In our study, there is no evidence that the cooccurrence of symptoms of depression and body composition result from a common genetic pathway."
ab51aee2ffdde8899fe665ca7bc5e635bdfd437b,"OBJECTIVE To assess the potential effectiveness of communicating familial risk of diabetes on illness perceptions and self-reported behavioral outcomes. RESEARCH DESIGN AND METHODS Individuals with a family history of diabetes were randomized to receive risk information based on familial and general risk factors (n = 59) or general risk factors alone (n = 59). Outcomes were assessed using questionnaires at baseline, 1 week, and 3 months. RESULTS Compared with individuals receiving general risk information, those receiving familial risk information perceived heredity to be a more important cause of diabetes (P < 0.01) at 1-week follow-up, perceived greater control over preventing diabetes (P < 0.05), and reported having eaten more healthily (P = 0.01) after 3 months. Behavioral intentions did not differ between the groups. CONCLUSIONS Communicating familial risk increased personal control and, thus, did not result in fatalism. Although the intervention did not influence intentions to change behavior, there was some evidence to suggest it increases healthy behavior."
b3e3aa8b0d9268a94d42e2a3bbfb30ae4ad18470,"Objective To investigate the extent to which shared genetic factors can explain the clustering of depression among individuals with lower socioeconomic status, and to examine if neuroticism or intelligence are involved in these pathways. Methods In total 2,383 participants (1,028 men and 1,355 women) of the Erasmus Rucphen Family Study were assessed with the Center for Epidemiologic Studies Depression Scale (CES-D) and the Hospital Anxiety and Depression Scale (HADS-D). Socioeconomic status was assessed as the highest level of education obtained. The role of shared genetic factors was quantified by estimating genetic correlations (ρG) between symptoms of depression and education level, with and without adjustment for premorbid intelligence and neuroticism scores. Results Higher level of education was associated with lower depression scores (partial correlation coefficient −0.09 for CES-D and −0.17 for HADS-D). Significant genetic correlations were found between education and both CES-D (ρG = −0.65) and HADS-D (ρG = −0.50). The genetic correlations remained statistically significant after adjusting for premorbid intelligence and neuroticism scores. Conclusions Our study suggests that shared genetic factors play a role in the co-occurrence of lower socioeconomic status and symptoms of depression, which suggest that genetic factors play a role in health inequalities. Further research is needed to investigate the validity, causality and generalizability of our results."
b79a7ad8c3cd9ca4616c72fb801a693f96973f76,"The increasing availability of personal genomic tests has led to discussions about the validity and utility of such tests and the balance of benefits and harms. A multidisciplinary workshop was convened by the National Institutes of Health and the Centers for Disease Control and Prevention to review the scientific foundation for using personal genomics in risk assessment and disease prevention and to develop recommendations for targeted research. The clinical validity and utility of personal genomics is a moving target with rapidly developing discoveries but little translation research to close the gap between discoveries and health impact. Workshop participants made recommendations in five domains: (1) developing and applying scientific standards for assessing personal genomic tests; (2) developing and applying a multidisciplinary research agenda, including observational studies and clinical trials to fill knowledge gaps in clinical validity and utility; (3) enhancing credible knowledge synthesis and information dissemination to clinicians and consumers; (4) linking scientific findings to evidence-based recommendations for use of personal genomics; and (5) assessing how the concept of personal utility can affect health benefits, costs, and risks by developing appropriate metrics for evaluation. To fulfill the promise of personal genomics, a rigorous multidisciplinary research agenda is needed."
c4758da0ca0610ee5ca1c320c9c43943be99c8b6,"Several studies have investigated the role of the neuronal sortilin-related receptor (SORL1) gene in Alzheimer's disease (AD), but findings have been inconsistent. We conducted a study of 7 single nucleotide polymorphisms (SNPs), rs668387, rs689021, rs641120, rs1699102, rs3824968, rs2282649, and rs1010159, in the SORL1 gene that were associated to AD in previous studies. We tested for association with AD and cognitive function in 6741 participants of the Rotterdam Study and in 2883 individuals from the Erasmus Rucphen Family study. We performed meta-analyses on AD using our data together with those of previous studies published prior to September 2008 in Caucasians. Further, we studied up to 76 SNPs in a 400 kb region within and flanking the gene to evaluate the evidence that other genetic variants are associated with AD or cognitive function. There was no significant evidence for association between SORL1 SNPs and incident AD patients in the Rotterdam Study. In a meta-analysis of our data with those of others, six out of seven SNPs attained borderline significance. However, removal of the first study reporting association from the meta-analysis resulted in non-significant odds ratios for all SNPs. SNPs rs668387, rs689021, and rs641120 were associated with cognitive function in non-demented individuals at borderline statistical significance in two independent Dutch cohorts, but in the opposite direction. Testing for association using dense SNPs in the SORL1 gene did not reveal significant association with AD, or with cognitive function when adjusting for multiple testing. In conclusion, our data do not support the hypothesis that genetic variants in SORL1 are related to the risk of AD."
d0fe2ab5d16309f16e0de08d5265643891a9c12b,
d48f626c8b83ec545eae07a710ed10a5345501bb,"Background: Current MRI criteria can help predict a second attack after a clinically isolated syndrome (CIS). Given the known association between corpus callosum lesions (CC) and multiple sclerosis (MS), such lesions on MRI could provide additional predictive information. This study assessed whether the presence of CC lesion on MRI could, next to the modified Barkhof criteria, further enhance prediction of conversion from CIS to MS. Methods: Follow-up study of 158 patients with CIS who underwent MRI after CIS was performed. MRI were scored for the Barkhof criteria and CC lesion. Patients were classified as having MS according to Poser criteria. Cox regression models were used for the time to conversion from CIS to MS. Results: The Barkhof criteria and CC lesion were strongly associated with conversion to MS with hazard ratios (HR), respectively, of 2.6 (95% confidence interval [CI] 1.5–4.3) and 2.7 (95% CI 1.6–4.5). The HRs of CC lesion adjusted for the Barkhof criteria and the Barkhof criteria adjusted for CC lesion were similar (HRs 1.8, not significant). The combined prediction of the Barkhof criteria and CC lesion was 3.3 (95% CI 1.9–5.7). Patients not fulfilling the Barkhof criteria had a fourfold increased risk of MS (HR 3.8, 95% CI 1.5–9.3) when they had a lesion in the CC. Conclusions: Corpus callosum (CC) lesion and the Barkhof criteria both predicted conversion to multiple sclerosis (MS). When both variables were combined, the association was stronger. The assessment of CC lesion may be a useful additional tool for predicting conversion to MS in patients with clinically isolated syndrome."
d4f8164698ae46ae1fc53219aa7678e4fb23ffb5,
e5eb0bfc7dc49e82b669eb4de12f0a58d0beb3d2,"Genome-wide association studies (GWAS) have led to a rapid increase in available data on common genetic variants and phenotypes and numerous discoveries of new loci associated with susceptibility to common complex diseases. Integrating the evidence from GWAS and candidate gene studies depends on concerted efforts in data production, online publication, database development, and continuously updated data synthesis. Here the authors summarize current experience and challenges on these fronts, which were discussed at a 2008 multidisciplinary workshop sponsored by the Human Genome Epidemiology Network. Comprehensive field synopses that integrate many reported gene-disease associations have been systematically developed for several fields, including Alzheimer's disease, schizophrenia, bladder cancer, coronary heart disease, preterm birth, and DNA repair genes in various cancers. The authors summarize insights from these field synopses and discuss remaining unresolved issues—especially in the light of evidence from GWAS, for which they summarize empirical P-value and effect-size data on 223 discovered associations for binary outcomes (142 with P < 10−7). They also present a vision of collaboration that builds reliable cumulative evidence for genetic associations with common complex diseases and a transparent, distributed, authoritative knowledge base on genetic variation and human health. As a next step in the evolution of Human Genome Epidemiology reviews, the authors invite investigators to submit field synopses for possible publication in the American Journal of Epidemiology."
e7801a191c6c0b487ab09f0bb8c0ea35aea0ba11,
194fac6ae51dfc1af7855ced889a2c03e00876b5,
2a53397b00917e83d1d69a8e0b385651bc8c01ed,
2acc372e9a347cef69ec05a52454bf0f104845be,"OBJECTIVE—Prediction of type 2 diabetes based on genetic testing might improve identification of high-risk subjects. Genome-wide association (GWA) studies identified multiple new genetic variants that associate with type 2 diabetes. The predictive value of genetic testing for prediction of type 2 diabetes in the general population is unclear. RESEARCH DESIGN AND METHODS—We investigated 18 polymorphisms from recent GWA studies on type 2 diabetes in the Rotterdam Study, a prospective, population-based study among homogeneous Caucasian individuals of 55 years and older (genotyped subjects, n = 6,544; prevalent cases, n = 686; incident cases during follow-up, n = 601; mean follow-up 10.6 years). The predictive value of these polymorphisms was examined alone and in addition to clinical characteristics using logistic and Cox regression analyses. The discriminative accuracy of the prediction models was assessed by the area under the receiver operating characteristic curves (AUCs). RESULTS—Of the 18 polymorphisms, the ADAMTS9, CDKAL1, CDKN2A/B-rs1412829, FTO, IGF2BP2, JAZF1, SLC30A8, TCF7L2, and WFS1 variants were associated with type 2 diabetes risk in our population. The AUC was 0.60 (95% CI 0.57–0.63) for prediction based on the genetic polymorphisms; 0.66 (0.63–0.68) for age, sex, and BMI; and 0.68 (0.66–0.71) for the genetic polymorphisms and clinical characteristics combined. CONCLUSIONS—We showed that 9 of 18 well-established genetic risk variants were associated with type 2 diabetes in a population-based study. Combining genetic variants has low predictive value for future type 2 diabetes at a population-based level. The genetic polymorphisms only marginally improved the prediction of type 2 diabetes beyond clinical characteristics."
60476d1db0c4ab718aaa6aabe65a2f1331c91a4a,
67f94c8122bcd8c8cbb2e65e1c4d0b8d761ad18b,
7413d270c3f291248e9431bf56e47a945e92b8b8,
8e60a8dc03db3832e4837e7d7be69c6cf85d7478,
8ffb260d7e9fd975c94b201e7c3d51525b88a651,
9fbf1c97dc49abafa347b5870cfb3935cf14a30b,"AIMS
Prenatal exposure to severe famine has been associated with an increased risk of schizophrenia and affective disorders. We studied the relationship between prenatal exposure to famine during the Dutch hunger winter of 1944-45 and addiction later in life.


DESIGN
A case-control study.


SETTING
The Rotterdam city area during the Dutch hunger winter lasting from mid-October 1944 to mid-May 1945. From February 1945 to mid-May 1945 the hunger winter was characterized by a famine peak.


PARTICIPANTS
Patients are native Dutch addicted patients from the Rotterdam Addiction Treatment Program and controls are native Dutch inhabitants of Rotterdam, born between 1944 and 1947.


MEASUREMENT
Exposure to the whole hunger winter (< 1400 kcal/day) and the peak of the hunger winter (< 1000 kcal/day) was determined for each trimester of gestation. For each trimester the exposed/unexposed ratios were compared between patients and controls and quantified as odds ratios (OR).


FINDINGS
The odds of first-trimester gestational exposure to famine during the total hunger winter was significantly higher among patients receiving treatment for an addictive disorder [OR = 1.34, 95% confidence interval (CI) 1.10-1.64]. Stratification by sex shows that the odds of exposure during the first trimester was significantly higher only among men (OR = 1.34, 95% CI 1.05-1.72), but not among women (OR = 1.26, 95% CI 0.88-1.81). The odds of exposure to the peak of the hunger winter during the first trimester of gestation were also significantly higher among addiction treatment patients (OR = 1.61, 95% CI 1.22-2.12). We did not find any significant differences for the second and third trimesters of gestation.


CONCLUSION
First-trimester prenatal exposure to famine appears to be associated with addiction later in life. The study confirms the adverse influence of severe malnutrition on brain development and maturation, confirms the influence of perinatal insults on mental health in later life and gives rise to great concern about the possible future consequences for the hunger regions in our world."
b16ab29b461cc87e76d13fdefb5bf652b64cc44c,"Common diseases such as type 2 diabetes and coronary heart disease result from a complex interplay of genetic and environmental factors. Recent developments in genomics research have boosted progress in the discovery of susceptibility genes and fueled expectations about opportunities of genetic profiling for personalizing medicine. Personalized medicine requires a test that fairly accurately predicts disease risk, particularly when interventions are invasive, expensive or have major side effects. Recent studies on the prediction of common diseases based on multiple genetic variants alone or in addition to traditional disease risk factors showed limited predictive value so far, but all have investigated only a limited number of susceptibility variants. New gene discoveries from genome-wide association studies will certainly further improve the prediction of common diseases, but the question is whether this improvement is sufficient to enable personalized medicine. In this paper, we argue that new gene discoveries may not evidently improve the prediction of common diseases to a degree that it will change the management of individuals at increased risk. Substantial improvements may only be expected if we manage to understand the complete causal mechanisms of common diseases to a similar extent as we understand those of monogenic disorders. Genomics research will contribute to this understanding, but it is likely that the complexity of complex diseases may ultimately limit the opportunities for accurate prediction of disease in asymptomatic individuals as unraveling their complete causal pathways may be impossible."
cd0077dc9e219a052b2aac75b96d62272046e22b,
f2e26b28713ed3a81397b213d9ab8862cc8cc754,"The longer life expectancy now experienced by persons with Down syndrome (DS) makes it necessary to know the factors influencing survival in older persons with this syndrome. In a prospective longitudinal cohort study of dementia and mortality, 506 persons with DS aged 45 and older were followed for a mean of 4.5 years (range 0.0–7.6 years). Cognitive and social functioning were tested at baseline and annual follow‐up. The diagnosis of dementia was determined according to a standardized protocol. Cox proportional hazards modeling was used for survival analysis."
00f80784eafb30704bec4aca0a05ab0cf411ae59,"Purpose: Single genetic variants in multifactorial disorders typically have small effects, so major increases in disease risk are expected only from the simultaneous exposure to multiple risk genotypes. We investigated the impact of genotype frequencies on the clinical discriminative accuracy for the simultaneous testing of 40 independent susceptibility genetic variants.Methods: In separate simulation scenarios, we varied the genotype frequency from 1% to 50% and the odds ratio for each genetic variant from 1.1 to 2.0. Population size was 1 million and the population disease risk was 10%. Discriminative accuracy was quantified as the area under the receiver-operating characteristic curve. Using an example of genomic profiling for type 2 diabetes, we evaluated the area under the receiver-operating characteristic curve when the odds ratios and genotype frequencies varied between five postulated genetic variants.Results: When the genotype frequency was 1%, none of the subjects carried more than six of 40 risk genotypes, and when risk genotypes were frequent (≥30%), all carried at least six. The area under the receiver-operating characteristic curve did not increase above 0.70 when the odds ratios were modest (1.1 or 1.25), but higher genotype frequency increased the area under the receiver-operating characteristic curve from 0.57 to 0.82 and from 0.63 to 0.93 when odds ratios were 1.5 or 2.0. The example of type 2 diabetes showed that the area under the receiver-operating characteristic curve did not change when differences in the odds ratios were ignored.Conclusions: Given that the effects of susceptibility genes in complex diseases are small, the feasibility of future genomic profiling for predicting common diseases will depend substantially on the frequencies of the risk genotypes."
3e198e7c080a93db8f742638b3345a6ae77f3ca7,
49d89f6fff5eeab54aaeb5a9da67fe8e53edd872,
74bcb1ee9ea1b5c05133df53c40311e50e441dc5,
82f7d5ba36d0dbd342580a38edb581c68c355ace,
aced119628cb6d39eca882e11af75d2c58ac0078,
dd3dc01eb766b7de2316602acb1a263f27a16a83,"Objective: To investigate whether mitochondrial haplogroups are associated with age-related maculopathy (ARM). Methods: We assessed the association between mitochondrial haplogroups and ARM in a population-based sample of 3509 persons aged 49 years or older residing west of Sydney. Retinal photographs of both eyes were taken (1999-2001) and subsequently graded for ARM following the Wisconsin grading system. Genetic analysis for mitochondrial DNA haplogroups was performed. Associations between these genetic markers and risk factors for ARM were assessed. Results: After adjusting for age, sex, and smoking, haplogroup H was associated with a reduced prevalence of any (early and late) ARM (odds ratio [OR], 0.75; 95% confidence interval [CI], 0.58-0.97), early ARM (OR, 0.75; 95% CI, 0.57-0.98), and large distinct and indistinct soft drusen (OR, 0.70; 95% CI, 0.56-0.89). HaplogroupJ was associated with a higher prevalence of large, soft distinct drusen (OR, 1.80; 95% CI, 1.18-2.73). Haplogroup U was associated with an increased prevalence of retinal pigment abnormalities (OR, 1.45; 95% CI, 1.11-1.91). Conclusions: Our findings of associations between different haplogroup types and prevalent ARM or ARM lesions suggest that these haplogroups maybe genetic markers indicative of an individual's susceptibility to ARM."
fdf461794d43511352fc4f7763e485a900e1774a,"To the Editor: Homozygotes for the 405V variant of the cholesteryl ester transfer protein (CETP) have lower CETP levels and higher high-density lipoprotein (HDL) levels than carriers of the 405I variant. Accordingly, it has been shown that the CETP I405V is associated with lower coronary heart disease (CHD) mortality in patients diagnosed with at least 30% stenosis of a major coronary artery. Given the role of CETP I405V in HDL and CHD mortality, the association between this polymorphism and longevity has been studied, but the results have been inconsistent. Investigating an Ashkenazi Jewish population, one study reported that the frequency of the CETP 405 VV genotype was substantially higher in centenarians than in elderly controls who were younger than 100 (24.8% vs 8.6%), but this finding was not replicated in an Italian study (8.6% of centenarians and 10% of controls). We investigated the association between the CETP I405V polymorphism and all-cause mortality and CHD mortality in a prospective population-based cohort of elderly Caucasians. We used data from 7,983 individuals aged 55 and older from the Rotterdam Study, which was approved by the Medical Ethics Committee of the Erasmus MC University Medical Center. Of 6,421 individuals who were successfully genotyped for the CETP I405V polymorphism, 5,781 (90%) had complete information on cardiovascular risk factors at baseline. Mean age standard deviation of the participants was 68.8 8.5, and mean follow-up time was 11.3 3.9 years. Information on the vital status of the participants was obtained at regular intervals from the municipal population registry. Two independent research physicians coded all events according to the International Classification of Diseases, 10th Revision. CHD mortality was defined as death caused by diseases coded I20 to I25, I46, I50, and R96. Genotyping of the CETP I405V polymorphism was performed using a TaqMan allelic discrimination Assay-By-Design (Applied Biosystems, Foster City, CA). Incidence rates for all-cause and CHD mortality were calculated as incidence per 1,000 person-years (pys). Risks of mortality were quantified as hazard ratios from Cox proportional hazards models. In our study population, genotype and allele frequencies were in Hardy-Weinberg equilibrium (II 46%, IV 44%, and VV 10%; P 5.41). Genotype frequencies did not differ between 5-year age groups at baseline. Of the 1,990 participants who died during follow-up, 523 (30%) died from CHD. Incidence rates of all-cause and CHD mortality were significantly higher in men than women (36.3 deaths/ 1,000 py vs 27.1 deaths/1,000 py for all-cause mortality, Po.001 and 10.0 deaths/1,000 py in men vs 6.8 deaths/ 1,000 py in women for CHD mortality, Po.001). We found no statistically significant differences in the risk of all-cause mortality between carriers of the II, IV, and VV genotypes in the overall analysis or in the analysis stratified according to sex. Table 1 shows the incidence rates and hazard ratios for mortality caused by CHD. No differences in the risk of CHD mortality by CETP I405V genotypes were observed. The VV genotype tended to be associated with lower risk of CHD mortality in women, but this difference was not statistically significant. The mechanism through which polymorphisms in the CETP gene lead to higher HDL levels and lower CHD are a matter for further study. Although several polymorphisms in the CETP gene have been associated with levels of HDL, lower cardiovascular risk does not always accompany this association. The fact that the effect of a single gene on a complex trait such CHD is small, as well as changes in the functionality of the CETP and the role of the protein in other cellular functions in the adipocyte favoring insulin resistance, may explain this discrepancy. Previous work group showed that that the VV genotype had a protective"
0a37f1f79db019116c36eca7a5bc7c17eae50694,"Correspondence and requests for reprints: Professor Dr C.M. van Duijn, Department of Epidemiology & Biostatistics, Erasmus University Medical Center, Postbus 1738, 3000 DR Rotterdam, The Netherlands Tel: + 31 10 4087394; fax: + 31 10 4089406; e-mail: c.vanduijn@erasmusmc.nl Sponsorship: S. López León is supported by the Centre for Medical Systems Biology. The Rotterdam Study is supported by the Erasmus University Medical Center and Erasmus University Rotterdam, the Netherlands Organization for Science Research (NWO), the Netherlands Organization for Health Research and Development (ZonMw), the Research Institute for Diseases in the Elderly (RIDE), the Ministry of Education, Culture and Science, the Ministry of Health, Welfare and Sports, the European Commission (DG XII), the Municipality of Rotterdam, and the Interuniversity Attraction Poles (IUAP) program P5/19 of the Belgian Federal Science Policy Office, Belgium."
414cee44f47deb3c2eadb91717bef2711dfb5188,"Objectives: To analyze the diagnostic sensitivity and specificity of various brain-derived proteins (14-3-3, Tau, neuron specific enolase [NSE], and S100b) in the CSF of patients with Creutzfeldt-Jakob disease (CJD) and to analyze biologic factors that modify these parameters. Methods: CSF was tested for 14-3-3, Tau, NSE, and S100b in 1,859 patients with sporadic, genetic, iatrogenic, and variant CJD, and in 1,117 controls. Results: The highest sensitivity was achieved for 14-3-3 and Tau in sporadic CJD (85% and 86%), and a combined determination of 14-3-3 and Tau, S100b, or NSE increased the sensitivity to over 93%. A multivariate analysis showed that the sensitivity of all tests was highest in patients with the shortest disease duration, age at onset >40 years, and homozygosity at codon 129 of the prion protein gene. In a group of patients with repeated lumbar punctures, a second test also increased the diagnostic sensitivity. Conclusions: The detection of elevated levels of brain-derived proteins in the CSF in patients with suspected Creutzfeldt-Jakob disease is a valuable diagnostic test. A second lumbar puncture may be of value in patients with atypical clinical course in whom the first test was negative."
7dbf162d8095f14caf7403f32f20393f13023fe0,"Objective: To investigate the course of anxiety, depression and disease-related distress of patients with multiple sclerosis (MS) and their partners in the first years after diagnosis. Methods: The Hospital Anxiety and Depression Scale (HADS) and Impact of Event Scale (IES) were completed at baseline, six-month, one-and two-year follow-up in 101 recently diagnosed patients and 78 partners. The Expanded Disability Status Scale (EDSS) was assessed annually. Results: Mean time since diagnosis at baseline was 7.8 (SD 6.5) months. Mean anxiety scores of patients and partners did not change during the two years of follow-up and remained higher than that observed in the general population at all assessments (P <0.05). The high levels of disease-related distress at baseline were lower at follow-up. Of the patients and partners with high anxiety scores at baseline (HADS anxiety ≥8), 69% also had high scores at any time during follow-up, compared to 26% in those with low baseline anxiety scores. For severe distress at follow-up, these percentages were 41 and 14%. The sensitivity and specificity of baseline anxiety screening for the prediction of high anxiety or distress scores at follow-up were 55 and 85%. Conclusion: MS patients and their partners continued to have high levels of anxiety and distress in the first years after diagnosis. Screening for anxiety after diagnosis can be used to predict levels of anxiety and distress during two-year follow-up."
8dc5a03ac9f504c3ec0716a605d05110ae3f0f1b,"OBJECTIVE
To investigate the influence of individual patient risk profiles on the value of the HLA-DRB1 shared epitope (SE) as a predictor of severe erosive damage in rheumatoid arthritis (RA).


METHODS
Patient characteristics, clinical signs and symptoms, rheumatoid factor (RF) status, and HLA-DRB1 genotypes were available for 154 Caucasian women with RA. Risk profiles were defined by non-genetic factors that predict severe erosive disease. The additional value of the SE was defined by the likelihood ratios (LR) of SE presence and absence, which were calculated at the individual patient level.


RESULTS
In the total population, the LR of SE presence was 1.42 and the LR of SE absence was 0.37, corresponding to an odds ratio of 3.9, indicating a substantially higher risk of severe erosive disease in those with the SE compared to those without. The LR of SE presence and absence varied depending on the risk profile of the women, from 1.01 to 2.25 for SE presence and 0.22 to 0.49 for SE absence. Considering all the patient characteristics, SE status was most significantly related to RF status. Consequently, the LR of SE presence and absence were higher for RF-negative women compared to RF-positive women (SE presence 1.77 vs 1.40, p < 0.001 and SE absence 0.38 vs 0.30, p < 0.001).


CONCLUSION
The additional value of SE testing for predicting severe erosive disease varies according to patient risk profiles. Given the likely availability of genetic and other novel tests in the future, information about the additional value of test results is needed to ensure the optimal use of such testing in the management of RA."
993132049d225d0c2412275470eb57843e5f3689,
9d2c72aee9b476345d6dabd1c2d3869ac098f9cc,
bd14e5d3b9d21f7016ce5e0eaabede12225106c0,"Purpose: There is ongoing debate about whether testing low-risk genes at multiple loci will be useful in clinical care and public health. We investigated the usefulness of multiple genetic testing using simulated data.Methods: Usefulness was evaluated by the area under the receiver-operating characteristic curve (AUC), which indicates the accuracy of genetic profiling in discriminating between future patients and nonpatients. The AUC was investigated in relation to the number of genes assumed to be involved, the risk allele frequency, the odds ratio of the risk genotypes, and to the proportion of variance explained by genetic factors as an approximation of the heritability of the disease.Results: We demonstrated that a high (AUC > 0.80) to excellent discriminative accuracy (AUC > 0.95) can be obtained by simultaneously testing multiple susceptibility genes. A higher discriminative accuracy is obtained when genetic factors play a larger role in the disease, as indicated by the proportion of explained variance. The maximum discriminative accuracy of future genetic profiling can be estimated at present from the heritability and prevalence of disease.Conclusions: Genetic profiling may have the potential to identify individuals at higher risk of disease depending on the prevalence and heritability of the disease."
e6ac10bd45f952f7e3bf65e54a0fef85ecc913f3,
113e637d188f4c302fafc96a732475a35ec44988,"The value of a dichotomous diagnostic test is often described in terms of sensitivity, specificity, and likelihood ratios (LRs). Although it is known that these test characteristics vary between subgroups of patients, they are generally interpreted, on average, without considering information on patient characteristics, such as clinical signs and symptoms, or on previous test results. This article presents a reformulation of the logistic regression model that allows to calculate the LRs of diagnostic test results conditional on these covariates. The proposed method starts with estimating logistic regression models for the prior and posterior odds of disease. The regression model for the prior odds is based on patient characteristics, whereas the regression model for the posterior odds also includes the diagnostic test of interest. Following the Bayes theorem, the authors demontsrate that the regression model for the LR can be derived from taking the differences between the regression coefficients of the 2 models. In a clinical example, they demonstrate that the LRs of positive and negative test results and the sensitivity and specificity of the diagnostic test varied considerably between patients with different risk profiles, even when a constant odds ratio was assumed. The proposed logistic regression approach proves an efficient method to determine the performance of tests at the level of the individual patient risk profile and to examine the effect of patient characteristics on diagnostic test characteristics."
34038307a8c43c0bba7cd9d619b38e32421e8502,
0a8048aba62c6ba40e1f79214137d16c7ab595b2,
35c62a7e96875fde0525cb36d40e241038a1b7c9,
44565d3e269e3b8c26c9420b450cf619ed797856,"Objective: To investigate quality of life in an international population of patients with late-onset Pompe disease. Methods: Data on quality of life (SF-36), age, sex, disease duration, wheelchair use, and use of artificial ventilation were collected for 210 adults with Pompe disease from Australia, Germany, the Netherlands, the United Kingdom, and the United States. SF-36 scores were compared between countries and related to patient characteristics. In addition, for the Dutch subgroup (n = 51), comparisons with the general population and 1-year follow-up assessments were performed. Results: No significant differences between countries were found for the four physical health scales. Mean scores on the vitality, role functioning-emotional, and mental health scale differed between countries, but these differences were not consistent. Wheelchair use was associated with lower physical and social functioning scores (B = −23.6 and −15.1, p < 0.001), and the use of artificial ventilation with lower physical functioning scores (B = −8.4, p = 0.004). Patients reported significantly poorer quality of life than the general population on the physical functioning, role functioning-physical, general health, vitality, and social functioning scales. No significant differences in SF-36 scores were found between the baseline and 1-year follow-up measurement. Conclusions: Patients with late-onset Pompe disease are, on average, markedly affected on the physical health domains of quality of life, but score only slightly lower than the general population on the mental health domains."
4cf6f8c7c30bb1f9316cc3760f4f24dd51de681c,"textabstractThe usefulness of genetic testing to identify high-risk 
patients for common multifactorial diseases is subject to 
debate. Optimism about the public health opportunities 
is counterbalanced with skepticism, since genetic factors 
appear to play a role in only a minority of patients with 
complex diseases, the number of genes involved is large, 
and their penetrance is incomplete (Holtzman and Marteau 
2000; Vineis et al. 2001)."
56e20be184e4177c9e190e0d624a91a5f55b824d,
5d078e536fe820358146316efc1ac9e061e03e6b,
6ab7f5e218302c39ff3841afc84b82f643ca52ec,"The new diagnostic criteria for multiple sclerosis (MS) allow for a definite diagnosis in earlier stages of the disease. Yet, clinicians may hesitate to pursue a diagnosis of multiple sclerosis at the presentation of first symptoms because they consider an early diagnosis of limited benefit to patients. It is unknown whether patients themselves prefer to be informed in an early phase. We studied satisfaction with the timing of diagnosis in patients recently diagnosed with MS and found that 75% was satisfied, 24% favoured an earlier, and only 6% a later disclosure of the diagnosis. Patients who preferred an earlier diagnosis had a significantly longer interval between their first visit to the neurologist and the disclosure of diagnosis (P < 0.001). The probability that the patient was satisfied with the timing of diagnosis did not substantially decrease in the months following the first visit to the neurologist, leaving ample opportunity for a thorough evaluation of the early clinical course. We conclude that patients with MS prefer an early diagnosis."
e1de7f3cde271e25ce110444a0a346d34e59de86,"EDITOR—We agree with Galea et al that recall bias could be an alternative explanation for our finding that stressful events are associated with the risk of exacerbation in multiple sclerosis.

To minimise the possibility of recall bias we chose a high (weekly) sampling frequency. Nevertheless, when patients experienced an event and still had …"
4c1e6bcea7b4b3b30abe997885d6e20f3065604d,"The aim of the present paper was to quantify expectations of wheelchair‐dependency in patients recently diagnosed with MS (n = 101) and their partners (n = 78). Expectations focused on the risk and seriousness of becoming wheelchair‐dependent in 2 years, 10 years or lifetime. Expectations were compared with natural history data, compared between patients and their partners, and related to clinical characteristics. Our results show that patients overestimated their 2‐year and 10‐year risks of wheelchair‐dependency, but underestimated their lifetime risks. A large number of patients were uncertain about their 2‐year risk, even those with no or only minimal disability [Expanded Disability Status Scale (EDSS) <3.0]. One‐third of the patients perceived the 10‐year and lifetime risk to be 50%, which, as they explained in the interviews, reflected their uncertainty: they did not know what to expect – it might happen or not. Patients with more functional limitations had higher perceptions of risk, but lower perceptions of seriousness. Concordance in perceived risk and seriousness between patients and partners was moderate. The overestimation of the short‐term risks and the substantial differences in expectations within couples warrant further research on the impact of expectations on their treatment decisions and psychological well‐being."
7cd9baaa7c94fd4a4696a216c9907ea71562344f,"Abstract Objective To study the relation between self reported stressful life events not related to multiple sclerosis and the occurrence of exacerbations in relapsing-remitting multiple sclerosis. Design Longitudinal, prospective cohort study. Setting Outpatient clinic of department of neurology in the Netherlands. Participants Patients aged 18-55 with relapsing-remitting multiple sclerosis, who could walk with a cane or better (score of 0-6.0 on the expanded disability status scale), and had had at least two exacerbations in 24 months before inclusion in the study. Patients with other serious conditions were excluded. Main outcome measure The risk of increased disease activity as measured by the occurrence of exacerbations after weeks with stressful events. Results Seventy out of 73 included patients (96%) reported at least one stressful event. In total, 457 stressful life events were reported that were not related to multiple sclerosis. Average follow up time was 1.4 years. Throughout the study, 134 exacerbations occurred in 56 patients and 136 infections occurred in 57 patients. Cox regression analysis with time dependent variables showed that stress was associated with a doubling of the exacerbation rate (relative risk 2.2, 95% confidence interval 1.2 to 4.0, P = 0.014) during the subsequent four weeks. Infections were associated with a threefold increase in the risk of exacerbation, but this effect was found to be independent of experienced stress. Conclusion Stressful events were associated with increased exacerbations in relapsing-remitting multiple sclerosis. This association was independent of the triggering effect of infections on exacerbations of multiple sclerosis."
b1afee09616e79b2fb975b4d63c5c291d6f11b63,"Disability status, depression and anxiety are important determinants of quality of life (Q oL) in patients with multiple sclerosis (MS). We investigated whether anxiety and depression influence the relation between disability status and Q oL in our cohort of recently diagnosed patients. Disability status [Expanded Disability Status Scale (EDSS)], anxiety and depression [Hospital A nxiety and Depression Scale (HADS)], and Q oL (SF-36) were prospectively obtained in 101 MS patients. The relation between EDSS and SF-36 scales was examined using regression analyses, without and with adjustment for anxiety and depression. Interaction effects were investigated by comparing the relation between EDSS and Q oL in patients with high and low anxiety and depression. In the unadjusted analyses, EDSS was significantly related to all SF-36 physical and mental health scales. A fter adjustment for anxiety and depression, EDSS was significantly related only to the SF-36 physical functioning, role-physical functioning and bodily pain scales. The relation between EDSS and these SF-36 scales was consistently higher in patients with more symptoms of anxiety or depression, suggesting that anxiety and depression strengthened the association of EDSS in these SF-36 physical health scales. A fter adjustment for anxiety and depression, EDSS was not significantly related to the SF-36 mental health scales and the general health scale. This finding is compatible with the hypothesis that anxiety and depression are intermediate factors in the association of EDSS with these SF-36 scales. Screening for symptoms of anxiety and depression is recommended in studies that use Q oL as an outcome measure of treatment or intervention efficacy."
ebffab74f6737087ee2c7f47973eb0482f4f53d2,Objectives – Studies demonstrating reduced quality of life and psychological well‐being in multiple sclerosis (MS) have typically investigated patients within more advanced stages of disease. The aim of the present paper was to evaluate the emotional burden and quality of life of recently diagnosed MS patients and their partners.
834192cf3f0c0dfe28b7c6ba33893eaeb760c6dc,"Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the “Related Work” section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step toward a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation."
8dc1e4bac2d0403ba4bec7bcb8abb7534c53ab1f,"ive multi-document summarization. arXiv preprint arXiv:2005.10043 (2020). [63] Xiangci Li, Gully Burns, and Nanyun Peng. 2021. Scientific Discourse Tagging for Evidence Extraction. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2550– 2562. [64] Maria Liakata, Shyamasree Saha, Simon Dobnik, Colin Batchelor, andDietrich Rebholz-Schuhmann. 2012. Automatic recognition of conceptualization zones in scientific articles and two life science applications. Bioinformatics 28, 7"
e71884a9def84a42cb85c47cec89dc93c57f3f2d,"High valine plus glycine content is a feature of the proteins in SARS-CoV-2 and SARS viruses, and it causes the generation of aggregates between proteins and insoluble calcium salts via secondary chemical bonding. Starch-based diet or sugar water with adequate vitamins can go for many days without the generation of essential amino acids such as valine, creating bottlenecks in virion production in human body. Despite its potential carcinogenicity, modest lysine supplement can be favorable as lysine rich proteins gather chloride and solubilize stressful, insoluble and stiff calcium oxalate and calcium phosphate."
fb2aea16f51f1978e84cda634de0a268156fbdd2,"Text segmentation is a natural language processing task with popular applications, such as topic segmentation, element discourse extraction, and sentence tokenization. Much work has been done to develop accurate segmentation similarity metrics, but even the most advanced metrics used today, B, and WindowDiff, exhibit incorrect behavior due to their evaluation of boundaries in isolation. In this paper, we present a new segment-alignment based approach to segmentation similarity scoring and a new similarity metric A. We show that A does not exhibit the erratic behavior of $ and WindowDiff, quantify the likelihood of B and WindowDiff misbehaving through simulation, and discuss the versatility of alignment-based approaches for segmentation similarity scoring. We make our implementation of A publicly available and encourage the community to explore more sophisticated approaches to text segmentation similarity scoring."
203bdaca3986b51f8d011422c04ff1489e425ce5,"Social media has changed the way we engage in social activities. On Twitter, users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism. However, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. We present a Tweet-level hashtag hijacking detection framework focusing on hashtag activism. Our weakly-supervised framework uses bootstrapping to update itself as new Tweets are posted. Our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time."
af714e341bf78c3415c1e2c7c3284a26ddd15a22,"Objective To understand medical students' motivations for choosing neurology and how applicants conceptualize the field, as this information can be used to enhance interest in neurology and develop educational programs to help identify, support, and recruit future neurologists. Background Applicants to neurology residencies submit personal statements describing themselves and their motivations. Textual analysis of personal statements has been performed in internal medicine and general surgery, but never before in neurology. We hypothesized that specific words and themes would be mentioned in residency personal statements with high frequencies indicating students' motivations. Methods We used computational linguistics software to assess key words and thereby study motivations, expectations, and themes present among neurology applicants. A total of 2,405 personal statements submitted over 5 years to our institution were de-identified and compiled into a database for evaluation through 3 computational linguistics software programs. We performed calculations of term frequencies (TF) and TF–inverse document frequencies and performed K-means clustering to identify unique words and common themes. Results Specific disease states were discussed. For example, stroke (TF 2,178), epilepsy (TF 970), and dementia (TF 944) were referenced more often than amyotrophic lateral sclerosis (TF 220) and carpal tunnel (TF 10). The most common proper names cited were Oliver Sacks (TF 94) and Sherlock Holmes (TF 41). Common themes included fascination with the brain, interest in research, desire to help patients, early interests in neurology, continued pursuit of learning, appreciation for time with patients, family history with neurologic illness, and intellectual curiosity. Conclusions This first computational linguistic analysis of neurology personal statements provides understanding into medical students' motivations and interests. Ongoing subgroup and thematic analyses may inform educational strategies and enhance recruitment to our field."
c8884f661309a7cb73cc4fc81c3502e22c9e0c4f,"Both vertical and horizontal components of the strain are common for earthquakes, manifested in the forms of horizontal and upward movements of the earth crust. Horizontal component generates gaps in Point A, and gaps are subsequently closed by horizontal component in Point B. Reiteration of the cycles gives rise to the opening/closing phenomena seen on the ground hit by earthquakes."
ecd85a2c3b95ae04ff7dd28b7a0ee4ec864319da,"Human language encompasses more than just text; it also conveys emotions through tone and gestures. We present a case study of three simple and efficient Transformer-based architectures for predicting sentiment and emotion in multimodal data. The Late Fusion model merges unimodal features to create a multimodal feature sequence, the Round Robin model iteratively combines bimodal features using cross-modal attention, and the Hybrid Fusion model combines trimodal and unimodal features together to form a final feature sequence for predicting sentiment. Our experiments show that our small models are effective and outperform the publicly released versions of much larger, state-of-the-art multimodal sentiment analysis systems."
18f0610160044377a49b0854de732180ab2cf4b7,"We present a robust neural abstractive summarization system for cross-lingual summarization. We construct summarization corpora for documents automatically translated from three low-resource languages, Somali, Swahili, and Tagalog, using machine translation and the New York Times summarization corpus. We train three language-specific abstractive summarizers and evaluate on documents originally written in the source languages, as well as on a fourth, unseen language: Arabic. Our systems achieve significantly higher fluency than a standard copy-attention summarizer on automatically translated input documents, as well as comparable content selection."
3372c2bdc3aa8cf59bad64e7edaba243e26d4d88,"We present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. Our system is capable of aligning semantically similar spans of arbitrary length. We achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the MSR RTE corpus."
42ea1cb9439ef724ae9a0e71477fd3851aea0a8b,"We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web."
d4bf8b2de7d9b8b7f001ac2da5295ecf2c7b674e,"We present novel experiments in modeling the rise and fall of story characteristics within narrative, leading up to the Most Reportable Event (MRE), the compelling event that is the nucleus of the story. We construct a corpus of personal narratives from the bulletin board website Reddit, using the organization of Reddit content into topic-specific communities to automatically identify narratives. Leveraging the structure of Reddit comment threads, we automatically label a large dataset of narratives. We present a change-based model of narrative that tracks changes in formality, affect, and other characteristics over the course of a story, and we use this model in distant supervision and selftraining experiments that achieve significant improvements over the baselines at the task of identifying MREs."
b53df0c015425d602c84264d166f2595d22836ed,"We present novel computational experiments using William Labov’s theory of narrative analysis. We describe his six elements of narrative structure and construct a new corpus based on his most recent work on narrative. Using this corpus, we explore the correspondence between Labovs elements of narrative structure and the implicit discourse relations of the Penn Discourse Treebank, and we construct a mapping between the elements of narrative structure and the discourse relation classes of the PDTB. We present first experiments on detecting Complicating Actions, the most common of the elements of narrative structure, achieving an f-score of 71.55. We compare the contributions of features derived from narrative analysis, such as the length of clauses and the tenses of main verbs, with those of features drawn from work on detecting implicit discourse relations. Finally, we suggest directions for future research on narrative structure, such as applications in assessing text quality and in narrative generation."
20304809b7fa669ea0e8683fd515ee2351d1b7c3,
2671e4220e6e28363d06b2789fd87c4ddf857d51,"Executive functions encompass effortful top‐down cognitive processes crucial for daily functioning. They are particularly vulnerable to aging in older adults and are often affected early in the course of Alzheimer’s disease. However, the neural mechanisms underlying aging‐related decline in executive functions are not well understood. Modal controllability is a structural metric quantifying the ease at which brain regions can move the brain into difficult‐to‐reach states with substantial effort to support diverse cognitive processes. The emphasis on effortful state transitions bears strong similarity to the definition of executive functions as effortful cognitive processes, rendering modal controllability as a putative neural substrate of executive function. Controllability has been associated with cognitive performance in younger populations, but no studies have examined controllability in older adults or its association with executive function over time."
4769fbea62d9c7f8c3136d846cb5e91001abdd0f,"BACKGROUND
Overactivation of the salience network (SN) causes hyperarousal in insomnia patients and is associated with sleep-onset insomnia (SOI). Resting-state microstate 3 (RS-MS3) duration is closely related to SN overactivation. However, whether RS-MS3 duration is a biomarker for SOI has not yet been reported in the literature. In addition, SN activity is also associated with efficiency. However, it is not clear whether there are individual differences in the neural mechanisms of SOI in different efficiency groups.


METHODS
Considering that RS-MS3 duration characterizes the stability and persistent activation of the SN in the resting state, the current study investigated the link between SOI measured by sleep latency of Pittsburg Sleep Quality Index (PSQI), efficiency measured by Kirton Adaption-Innovation Inventory (KAI), and RS-MS3 in a Chinese healthy (subclinical) student population, using electroencephalography (EEG) microstate analysis.


RESULTS
We found that RS-MS3 duration was positively correlated with sleep latency and efficiency. The interaction between sleep latency and efficiency was significant. Simple slope analysis showed that high sleep latency was positively correlated with longer RS-MS3 duration in participants with higher efficiency scores. This correlation did not exist in participants with low efficiency scores.


CONCLUSIONS
RS-MS3 duration may serve as a biomarker for SOI. There is heterogeneity in the relationship between SOI and RS-MS3 duration between individuals with high and low efficiency."
7490a2ecd4b8e1e90aebfa80ebc5c352ef9f40c3,
8aafe8573e2230418e60acdddde09f3738c0da6a,
912587c41bf9de88ba7a201a64cd3a09dd7475d2,"Cognitive control serves a crucial role in human higher mental functions. The Dual Mechanisms of Control (DMC) account provides a unifying theoretical framework that decomposes cognitive control into two qualitatively distinct mechanisms - proactive control and reactive control. While prior behavioral and neuroimaging work has demonstrated the validity of individual tasks in isolating these two mechanisms of control, there has not been a comprehensive, theoretically-guided task battery specifically designed to tap into proactive and reactive control across different domains of cognition. To address this critical limitation and provide useful methodological tools for future investigations, the Dual Mechanisms of Cognitive Control (DMCC) task battery was developed to probe these two control modes, as well as their intra-individual and inter-individual differences, across four prototypical domains of cognition: selective attention, context processing, multi-tasking, and working memory. We present this task battery, along with detailed descriptions of the experimental manipulations used to encourage shifts to proactive or reactive control in each of the four task domains. We rigorously evaluate the group effects of these manipulations in primary indices of proactive and reactive control, establishing the validity of the DMCC task battery in providing dissociable yet convergent measures of the two cognitive control modes."
b2ca403ad3c743b89d2af18209902dc11a5b78a8,
0d933aa8741f903030ec816fd312c70ebbf11bbd,
ac1b20b3f1898c2ff0a909b37e502f436accc6df,
ba04849763d89004048941eef7c70e558a6393c2,
ccfc085de43723d3211ed1bdacef3fedc96990b5,"Abstract We describe an ambitious ongoing study that has been strongly influenced and inspired by Don Stuss's career-long efforts to identify key cognitive processes that characterize executive control, investigate potential unifying dimensions that define prefrontal function, and carefully attend to individual differences. The Dual Mechanisms of Cognitive Control project tests a theoretical framework positing two key control dimensions: proactive and reactive. The framework's central tenets are that proactive and reactive control modes reflect domain-general dimensions of individual variation, with distinctive neural signatures, involving the lateral prefrontal cortex as a central node within associated brain networks (e.g., fronto-parietal, cingulo-opercular). In the Dual Mechanisms of Cognitive Control project, each participant undergoes three separate imaging sessions, while performing theoretically targeted variants of multiple well-established cognitive control tasks (Stroop, Cued task-switching, AX-Continuous Performance Test, Sternberg working memory) in conditions that encourage utilization of different control modes, and also completes an extensive out-of-scanner individual differences battery. Additional key features of the project include a high spatio-temporal resolution (multiband) acquisition protocol and a sample that includes both a substantial subset of monozygotic twin pairs and participants recruited from the Human Connectome Project. Although data collection is still continuing (target n = 200), we provide an overview of the study design and protocol, along with initial results (n = 80) revealing evidence of a domain-general neural signature of cognitive control and its modulation under reactive conditions. Aligned with Don Stuss's legacy of scientific community building, a partial data set has been publicly released, with the full data set released at project completion, so it can serve as a valuable resource."
f8640d6619d39eb79e0fa370c96ccf34ff90dce9,
25603475f4c6fef0b35903ddcdf8bba0dcdd4fa9,
28786c3d0d7ce2058100b31d69f1916c78091e22,"The Dual Mechanisms of Cognitive Control (DMCC) project provides an ambitious and rigorous empirical test of a theoretical framework that posits two key cognitive control modes: proactive and reactive. The framework’s central tenets are that proactive and reactive control reflect domain-general dimensions of individual variation, with distinctive neural signatures, involving lateral prefrontal cortex (PFC) in interactions with other brain networks and circuits (e.g., frontoparietal, cingulo-opercular). In the DMCC project, each participant is scanned while performing theoretically-targeted variants of multiple well-established cognitive control tasks (Stroop, Cued Task-Switching, AX-CPT, Sternberg Working Memory) in three separate imaging sessions, that each encourage utilization of different control modes, plus also completes an extensive out-of-scanner individual differences battery. Additional key features of the project include a high spatio-temporal resolution (multiband) acquisition protocol, and a sample that includes a substantial subset of monozygotic twin pairs and participants recruited from the Human Connectome Project. Although data collection is still continuing (target N=200), we provide an overview of the study design and protocol, planned analytic approaches and methodological development, along with initial results (N=80) revealing novel evidence of a domain-general neural signature of reactive control. In the interests of scientific community building, the dataset will be made public at project completion, so it can serve as a valuable resource."
2f9e1644ade6a1ab3057c9d8ae937f7a372a960b,
45de9725d95856792f5328de8040f7636f532f0d,"The growing popularity of mindfulness-based interventions (MBIs) has prompted exciting scientific research investigating their beneficial effects on well-being and health. Most mindfulness programs are provided as multi-faceted packages encompassing a set of different mindfulness techniques, each with distinct focus and mechanisms. However, this approach overlooks potential individual differences, which may arise in response to practicing various mindfulness techniques. The present study investigated preferences for four prototypical mindfulness techniques [focused attention (FA), open monitoring (OM), loving-kindness (LK), and body scan (BS)] and identified factors that may contribute to individual differences in these preferences. Participants without prior mindfulness experiences were exposed to each technique through audio-guided instructions and were asked to rank their preferences at the end of all practices. Results indicated that preferences for loving-kindness were predicted by empathy, and that females tended to prefer loving-kindness more than males. Conversely, preferences for open monitoring were predicted by nonreactivity and nonjudgment of present moment experiences. Additionally, higher state mindfulness was detected for individuals’ preferred technique relative to other alternatives. These findings suggest that individuals tend to prefer techniques compatible with their personalities, as the predictor variables encompass trait capacities specifically relevant to practicing these techniques. Together, our results suggest the possibility that assessing individual difference and then tailoring MBIs to individual needs could be a useful way to improve intervention effectiveness and subsequent outcomes."
567f49b4ffeedb7ce3073520657ecb7e43d1a162,"Previous studies suggest that the practice of long-term (months to years) mindfulness meditation induces structural plasticity in gray matter. However, it remains unknown whether short-term (<30 days) mindfulness meditation in novices could induce similar structural changes. Our previous randomized controlled trials (RCTs) identified white matter changes surrounding the anterior cingulate cortex (ACC) and the posterior cingulate cortex (PCC) within 2 to 4 weeks, following 5-10 h of mindfulness training. Furthermore, these changes were correlated with emotional states in healthy adults. The PCC is a key hub in the functional anatomy implicated in meditation and other perspectival processes. In this longitudinal study using a randomized design, we therefore examined the effect of a 10 h of mindfulness training, the Integrative Body-Mind Training (IBMT) on gray matter volume of the PCC compared to an active control—relaxation training (RT). We found that brief IBMT increased ventral PCC volume and that baseline temperamental trait—an index of individual differences was associated with a reduction in training-induced gray matter increases. Our findings indicate that brief mindfulness meditation induces gray matter plasticity, suggesting that structural changes in ventral PCC—a key hub associated with self-awareness, emotion, cognition, and aging—may have important implications for protecting against mood-related disorders and aging-related cognitive declines."
6ebbdbe4d3e162d17d95272388eae48f885ac256,
6f12e9423227b1c1808be95a93aceb7786fce512,
7c5043a6a4c27b2e782b2f956b3e81c6823e4ef7,
82dcff037344aee35eacd84ae496f7b977aa554c,"Mindfulness training (MT) has shown promise in improving psychological health among college students yet has rarely been evaluated as an addition to the college academic curriculum. Here, we demonstrate the feasibility and effectiveness of a first-year MT seminar offered to residential students at a selective private university, evaluating its impact on psychological and cognitive functioning in relationship to a comparable positive psychology seminar. The results suggest the potential for first-year programs that promote student well-being."
8ab71e88ca2e1b5c590e21e38aaad1d21d641507,
8b946d4e55f6f463621ed249b3a5bbb3d6646852,"Sleep quality can affect the physical and mental health, as well as the personal development of college students. Mindfulness practices are known to ameliorate sleep disorder and improve sleep quality. Trait mindfulness, an innate capacity often enhanced by mindfulness training, has been shown to relate to better sleep quality and different aspects of psychological well-being. However, how individual difference factors such as trait mindfulness relate to sleep quality remains largely unclear, which limits the optimization and further application of mindfulness-based intervention schemes targeting the improvement of sleep quality. In this study, we aimed to investigate how negative emotions and neuroticism may influence the relationship between trait mindfulness and sleep quality. A conditional process model was built to examine these relationships in 1,423 Chinese young adults. Specifically, the conditional process model was constructed with trait mindfulness as the independent variable, sleep quality as the dependent variable, negative emotions as the mediating variable, and neuroticism as the moderating variable. Our results showed that negative emotions mediated the link between mindfulness and sleep quality and that neuroticism had a moderating effect on the relationship between mindfulness and sleep quality. Together, these findings suggested a potential mechanism of how trait mindfulness influences sleep quality, provided a therapeutic target for which mindfulness-based interventions may act upon to improve sleep quality, and offered a basis for prediction of different intervention effects among individuals."
92d16bbb6c78878fc15006fc3e180f05e7c8605a,"Previous studies have shown that physical exercise and mindfulness meditation can both lead to improvement in physical and mental health. However, it is unclear whether these two forms of training share the same underlying mechanisms. We compared two groups of older adults with 10 years of mindfulness meditation (integrative body-mind training, IBMT) or physical exercise (PE) experience to demonstrate their effects on brain, physiology and behavior. Healthy older adults were randomly selected from a large community health project and the groups were compared on measures of quality of life, autonomic activity (heart rate, heart rate variability, skin conductance response, respiratory amplitude/rate), immune function (secretory Immunoglobulin A, sIgA), stress hormone (cortisol) and brain imaging (resting state functional connectivity, structural differences). In comparison with PE, we found significantly higher ratings for the IBMT group on dimensions of life quality. Parasympathetic activity indexed by skin conductance response and high-frequency heart rate variability also showed more favorable outcomes in the IBMT group. However, the PE group showed lower basal heart rate and greater chest respiratory amplitude. Basal sIgA level was significantly higher and cortisol concentration was lower in the IBMT group. Lastly, the IBMT group had stronger brain connectivity between the dorsal anterior cingulate cortex (dACC) and the striatum at resting state, as well as greater volume of gray matter in the striatum. Our results indicate that mindfulness meditation and physical exercise function in part by different mechanisms, with PE increasing physical fitness and IBMT inducing plasticity in the central nervous systems. These findings suggest combining physical and mental training may achieve better health and quality of life results for an aging population."
a0dff0f147d54179655f300751b1c85c64c720a9,
a102b69b498368dbb33194c9ae37c1cc62e63f40,"[16] Amen DG1, Newberg A, Thatcher R, Jin Y, Wu J, Keator D, Willeumier K.Impact of playing American professional football on long-term brain function. [17] Yi-Yuan Tang,,* Qilin Lu,Hongbo Feng, Rongxiang Tang,5 and Michael I. Posner . Short-term meditation increases blood flow in anterior cingulate cortex and insula. [18] Wang DJ1, Rao H, Korczykowski M, Wintering N, Pluta J, Khalsa DS, Newberg AB. Cerebral blood flow changes associated with different meditation practices and perceived depth of meditation"
c424fb95adbb73f5d03a59f42f0fd2ed27d4a68f,
d90ac3b2f6318feb43083b26355c2f1f10752b14,
e31fd55734d131166ed9b7b755bfa04de29a6772,"A growing body of research indicates that mindfulness training can have beneficial effects on critical aspects of psychological well-being, cognitive function, and brain health. Although these benefits have been generalized to the population level, individual variability in observed effects of mindfulness training has not been systematically investigated. Research on other similar forms of psychological intervention demonstrates that individual differences are prominent in terms of intervention responsiveness and outcomes. Furthermore, individual characteristics such as personality traits have been shown to play a crucial role in influencing the effects of intervention. In light of these lines of evidence, we review representative work on individual differences in mindfulness training and advocate for an individual difference perspective in mindfulness training research. We discuss relevant empirical evidence of individual differences potentially influencing behavioral outcomes of mindfulness training, focusing on both cognitive function and psychological well-being. Finally, theoretical considerations and potentially fruitful research strategies and directions for studying individual differences in mindfulness training are discussed, including those involving cognitive neuroscience methods."
e6c814cd0451504811d03f998439d92150cf7a3b,
ed3b7a65dfcb57c4b2183dfa05437355b5e7b496,
56681f698eb7bc0bd6de5b12cc356f852d7acf10,"Mindfulness training has shown promise in improving psychological health and cognitive function. Mindfulness skills may be particularly beneficial in helping first-year students’ transition to college, as this can be a time period of considerable lifestyle changes and increased stress. Previous research has demonstrated positive effects of mindfulness training in college populations, but primarily by providing standardized mindfulness programs that are distinct from the college curriculum. Such programs may pose greater challenges for student participation, as they require a strong extracurricular time commitment. The present study examined the effects of mindfulness training incorporated into a semester-long college seminar dedicated to both practical learning of mindfulness skills and scientific understanding of mindfulness theory, based on the evidence-based Learning to BREATHE (L2B) curriculum. In a quasi-experimental design, first-year undergraduate students in the mindfulness seminar were compared with a control group enrolled in a positive psychology and study skills seminar. Students in the mindfulness seminar exhibited more improvement in satisfaction with life and trait mindfulness, as well as less anxiety; in contrast, no differences were observed in cognitive function. These results demonstrate the feasibility and potential psychological benefits of integrating mindfulness training into standard college curriculum."
b4b0fc66b53ebbfa9de7d649d36e8a239aeaaa01,
e3670a51ccda1f316200555ba7e421b61e7c52f1,"Psychological well-being is a core feature of mental health, and may be defined as including hedonic (enjoyment, pleasure) and eudaimonic (meaning, fulfillment) happiness, as well as resilience (coping, emotion regulation, healthy problem solving). To promote psychological well-being, it is helpful to understand the underlying mechanisms associated with this construct and then develop targeted and effective training programs. In this perspective article, we discuss key components and potential brain-body mechanisms related to psychological well-being and propose mindfulness training as a promising way to improve it. Based on a series of randomized controlled trial (RCT) studies of one form of mindfulness training in adolescents and adults, the integrative body-mind training (IBMT), we use IBMT as an exemplar to provide research evidence of the positive effects of mindfulness training on psychological well-being. We focus on one of the mechanisms by which IBMT enhances psychological well-being—the interaction between mind (mindfulness) and body (bodifulness)—which involves both the central nervous system (CNS) and the autonomic nervous system (ANS). We also highlight the role of brain self-control networks, including the anterior cingulate cortex/prefrontal cortex (ACC/PFC), in improving psychological well-being. We suggest that mindfulness training may be a promising program that promotes the synergistic engagement of mind and body to achieve the goals of enhancing psychological well-being."
19d9e5ffd53e86126ee8367911b79520414da6c6,
4511488e62b195d04a3d50aa9688fbdbbc5b508d,
90c0c8ea403b5eaa83278a8b2f1a04052cb5a20c,
94dfcec2185503696e659ea47b6e177b09278f75,
a0656f4ae7304c0368e11aaa456b73976bbc04dc,"In this Opinion piece, we take mindfulness as an example of mind-body practice to explore the question of how mind-body practice works based on recent neuroimaging evidence. Mind-body practice encompasses a family of complex practices such as mindfulness meditation, Tai Chi, Yoga, andQi Gong. Of these practices, mindfulness meditation has received themost attention in the field of psychology and neuroscience over the past two decades. In our recent review, we summarize that mindfulness meditation includes three components that interact closely to constitute a process of enhanced self-regulation: enhanced attention control, improved emotion regulation and altered self-awareness. We also pointed out many people use the term “mindfulness,” but often refer to completely different things or ideas (Tang et al., 2015). In the following sections, we provide examples to briefly clarify the fundamental understanding of the mindfulness concept and how mind-body practice works."
ba24557e3a5768435f9adc94c7eee598a6172051,"Emerging evidences have shown that one form of mental training—mindfulness meditation, can improve attention, emotion regulation and cognitive performance through changing brain activity and structural connectivity. However, whether and how the short-term mindfulness meditation alters large-scale brain networks are not well understood. Here, we applied a novel data-driven technique, the multivariate pattern analysis (MVPA) to resting-state fMRI (rsfMRI) data to identify changes in brain activity patterns and assess the neural mechanisms induced by a brief mindfulness training—integrative body–mind training (IBMT), which was previously reported in our series of randomized studies. Whole brain rsfMRI was performed on an undergraduate group who received 2 weeks of IBMT with 30 min per session (5 h training in total). Classifiers were trained on measures of functional connectivity in this fMRI data, and they were able to reliably differentiate (with 72% accuracy) patterns of connectivity from before vs. after the IBMT training. After training, an increase in positive functional connections (60 connections) were detected, primarily involving bilateral superior/middle occipital gyrus, bilateral frontale operculum, bilateral superior temporal gyrus, right superior temporal pole, bilateral insula, caudate and cerebellum. These results suggest that brief mental training alters the functional connectivity of large-scale brain networks at rest that may involve a portion of the neural circuitry supporting attention, cognitive and affective processing, awareness and sensory integration and reward processing."
e6baf509db313401d398e83492f324f239964166,
0467f13f9f3e4a0531285ed8c657c77bbc3f272c,"Prefrontal and parietal cortex, including the default mode network (DMN; medial prefrontal cortex (mPFC), and posterior cingulate cortex, PCC), have been implicated in addiction. Nonetheless, it remains unclear which brain regions play a crucial role in smoking addiction and the relationship among these regions. Since functional connectivity only measures correlations, addiction-related changes in effective connectivity (directed information flow) among these distributed brain regions remain largely unknown. Here we applied spectral dynamic causal modeling (spDCM) to resting state fMRI to characterize changes in effective connectivity among core regions in smoking addiction. Compared to nonsmokers, smokers had reduced effective connectivity from PCC to mPFC and from RIPL to mPFC, a higher self-inhibition within PCC and a reduction in the amplitude of endogenous neuronal fluctuations driving the mPFC. These results indicate that spDCM can differentiate the functional architectures between the two groups, and may provide insight into the brain mechanisms underlying smoking addiction. Our results also suggest that future brain-based prevention and intervention in addiction should consider the amelioration of mPFC-PCC-IPL circuits."
23a30ca55aef3e307c9e761b22a34080ebf6985f,
2622fe7b888d7a47c74fa53ef48b9086b2737970,
30d87386509fa8c1c40115c0900cca48b71e75c0,
334be6d345c1d41fc953877426fbace749f1bf22,
50920b36cca05c524a658a453fd5484e1498b898,
0f2ffe8c51642366e099332d17a32073fc5dfe55,
2495aaa6954805cfd8fb61c5ae1f46eda62ad909,
2a5eca792e190f307e8d983b9aaf1210a859bc1c,
2d8d1fbcb13d1c37d7ccb81e30b84773800d7232,"Asymmetry in frontal electrical activity has been reported to be associated with positive mood. One form of mindfulness meditation, integrative body-mind training (IBMT) improves positive mood and neuroplasticity. The purpose of this study is to determine whether short-term IBMT improves mood and induces frontal asymmetry. This study showed that 5-days (30-min per day) IBMT significantly enhanced cerebral blood flow (CBF) in subgenual/adjacent ventral anterior cingulate cortex (ACC), medial prefrontal cortex and insula. The results showed that both IBMT and relaxation training increased left laterality of CBF, but only IBMT improved CBF in left ACC and insula, critical brain areas in self-regulation."
5e861d4ed63f26662f2393c1a71249053ead2a6e,"This chapter discusses future directions of the field of mindfulness, such as the from state to trait mindfulness, large-scale brain network dynamics in mindfulness, different stages of mindfulness and different states of practice, individual differences, the effects of combining mindfulness with other training regimens, and how to translate mindfulness into clinical practice."
6148da8ee34cc05acddf102195531b189e0794fc,
793b68668c2c5247a78e1b30f0d144e7e89791f6,
8272328e34f62bfc293a10dfa8d64ae0fee15394,
b0f6c834b6cbfc25c48f5b2e24f6757409db9e57,"Resolving conflict is a pivotal self-control ability for human adaptation and survival. Although some studies reported meditation may affect conflict resolution, the neural mechanisms are poorly understood. We conducted a fully randomized 5 h trial of one form of mindfulness meditation—integrative body-mind training (IBMT) in comparison to a relaxation training control. During the Stroop word-color task, IBMT group produced faster resolution of conflict, a smaller N2 and an earlier and larger P3 component of the event-related brain potentials. These results indicate that brief meditation training induces a brain state that improves the resolution of conflict."
c1903f2cd00a82b594f0c7461a6abf2508909fd1,
e2d8bc305de08d3d02d1903678a4fd7b532101bd,"Medication and behavioral treatments have been used for ADHD treatments; however, both have limitations. Mindfulness meditation has been shown to improve attention and self-control, (or self-regulation), which could help the core ADHD symptoms of inattention, impulsivity, and hyperactivity. This chapter aims to review the latest literature on the effectiveness of mindfulness meditation on ADHD, to explore the brain mechanism underlying ADHD intervention, and to propose a mindfulness-based preventive intervention for ADHD symptoms and treatments."
f739c0ca4cb01a795be13f2874481674941e86f4,
1c923f81d891651ccd12686fd6158a7c9ba22b53,
2d2e19cd884dfb878e5e0a358082e1bd667f7788,
36fdd8d8b1170edf278b89e598ea3233b63c6d73,
3ffc1996d0703fa71ec11b8a5e39e20018b98f3a,
6688d875636340a661ca2729e70979821d590082,
88d83f2b1d18d272288bf5e7c03aa4dd0bd0f97d,
a76dc45f16c36406e352e5d61cfce17384ac61fe,"Background: Research has found that improved higher effortful control, a measure of self- regulation, improves performance of middle school students. Integrative body-mind training (IBMT) has been shown to improve attentional networks related to self-regulation. We hypothesize that an IBMT intervention will improve academic performance of adolescents. Methods: Students age 13-18 were recruited from middle and high school in Beijing, China and randomly assigned to either IBMT or a relaxation training control (RT). Students were given 6 weeks of IBMT intervention with 30 min per day at school. The improved performance in attention and aspects of academic performance were measured. Results: Compared to RT, IBMT intervention showed significantly greater improvement in attention and in academic performance (scores of literacy, math, and second language). Conclusions: Brief mindfulness meditation is an effective technique for improving cognitive function, including academic performance."
cd3914f1d478424aadd9df0f8c984ed1ecc09b05,
f11d9c5a78bdedc204ee8690050b8c4d5d0d27d1,
feeff472e37b5d293fa35a6c634582622c11c119,
bde92a17b7f6e614c292ddf6818548df8795dfa6,"Self-transcendence (ST) is one of specific human experiences often related to harmony with nature or feeling oneness with others or the self as an integral part of the whole universe. The Temperament and Character Inventory (TCI) is a widely used personality measure, and ST is one of personality dimensions (Cloninger, 1994; Cloninger et al., 1994). Previous studies showed that ST has significant positive correlation with the sgACC encompassing a ventromedial portion of the prefrontal cortex (vmPFC) using TCI and PET scan (Hakamata et al., 2013). Meanwhile, sgACC/vmPFC activity has been shown to be significantly decreased in patients with anxiety, major depression and mood disorders (Drevets et al., 2008; Shin and Liberzon, 2010; Kuhn and Gallinat, 2013). Altogether, these findings suggest that sgACC/vmPFC play an important role in emotion regulation and ST (Hakamata et al., 2013). 
 
ACC as a part of the brain's limbic system, appears active in many neuroimaging studies (Bush et al., 2000; Posner et al., 2007). In general ACC is involved in cognitive (dorsal division) and emotional (ventral/rostral part) processing (Bush et al., 2000). The sensitivity of the ACC to both reward and pain, and evidence for ACC coupling to cognitive and emotional areas during resting state and task performance, support the role of ACC in self-regulation or self-control including emotional, cognitive and autonomic control. Particularly, v/sgACC and adjacent mPFC area involves in emotional control and autonomic regulation (Luu and Posner, 2003; Posner et al., 2007), consistent with many meditation findings (Holzel et al., 2011; Tang et al., 2012b; Tang and Posner, 2013). 
 
Meditation often exemplifies positive emotion, pleasant feeling and ST experience in practitioners (Cahn and Polich, 2006; Tang et al., 2007). Studies showed ST is positively related to meditation practice (Levenson et al., 2005). One meditation-category—automatic self-transcending includes techniques designed to transcend their own activity and improve ST (Travis and Shear, 2010). Substantial evidences indicate that ACC plays a key role in meditation training (Holzel et al., 2011). For example, compared to non-meditators, long-term Vipassana meditators showed stronger activations in the rostral ACC and adjacent medial PFC bilaterally for the meditation condition (contrasted to arithmetic task). Greater ACC and mPFC activations in meditators may reflect processing of distracting events and emotional processing (Holzel et al., 2007, 2011). Compared with a memory training control, compassion training elicited activity in a neural network including pregenual ACC, medial orbitofrontal cortex and striatum—brain regions previously associated with positive affect and affiliation (Klimecki et al., 2013a,b). In the same vein, 5 days of one form of meditation—integrative body–mind training (IBMT) improves vACC activity compared to same amount of relaxation training (Tang et al., 2009). Meanwhile, 5 days of IBMT also reduces stress, improves positive emotion and self-report of feeling oneness with nature (Tang et al., 2007). Further, 10 days of IBMT increases white matter connectivity surrounding ACC and this brain structural change correlates with emotional regulation (Tang et al., 2012a,b). These results indicate that meditation accompanies positive emotion, ST experience, and ACC functional and structural changes. 
 
ST related meditation not only induces brain and behavioral changes, it often involves brain (mind) and body cooperation indexed by central (CNS) and autonomic (ANS) nervous system interaction (Cahn and Polich, 2006; Holzel et al., 2011). Studies have begun to explore interaction and dynamics between CNS and ANS (Critchley et al., 2003; Tang, 2009; Tang and Posner, 2009; Tang et al., 2009; Holzel et al., 2011; Critchley and Harrison, 2013). For instance, using heart rate variability (HRV), and high- and low-frequency power in the cardiac rhythm, ACC activity related to sympathetic modulation of heart rate was observed (Critchley et al., 2003). We measured the physiological and brain changes at rest before, during, and after 5 days of IBMT and relaxation training. During and after training, the IBMT group showed significantly better physiological reactions in heart rate, respiratory amplitude and rate, and skin conductance response (SCR) than the relaxation control. Differences in HRV and EEG power suggested greater involvement of the ANS in the IBMT group during and after training. Imaging data demonstrated stronger v/sgACC activity in the IBMT group. Frontal midline ACC theta was also correlated with high-frequency HRV, suggesting control by the ACC over parasympathetic activity (Tang et al., 2009). These results indicate that brief IBMT induces better regulation of the ANS by a midline v/sg ACC brain system. This changed state probably reflects training in the coordination of body and mind given in the IBMT but not in the relaxation group. These results indicate body-brain works together to maintain certain consciousness states such as ST that may be related to different performance (Tang et al., 2007; Xue et al., 2011; Tang et al., 2012a,b). Our findings suggest meditation training could induce altered states of consciousness which may allow us to explore the neuroscience of consciousness based on how alterations in normal consciousness result in functional or/and structural brain changes and plasticity. These alterations in consciousness can affect long-term cognitive, affective and social activities, and may help understand the disease states or disorders of consciousness such as coma, vegetative state, etc. (Tang et al., 2013). 
 
In summary, growing empirical evidences indicate meditation has potential to develop ST—a positive relationship between self and other that transcends self-focused needs and increases prosocial characteristics (Holzel et al., 2011; Tang et al., 2012a,b; Vago and Silbersweig, 2012). Future studies could examine the relationship between ST and short-term or long-term meditation, and how meditation shapes the perspectives on the self, self-others, self-nature and its underlying mechanisms using multimodal neuroimaging, physiological, psychosocial and genetic methods."
f7fabfb34c54c62dacc23d637dd814a2f00eeb1a,"More than 5 million deaths a year are attributable to tobacco smoking, but attempts to help people either quit or reduce their smoking often fail, perhaps in part because the intention to quit activates brain networks related to craving. We recruited participants interested in general stress reduction and randomly assigned them to meditation training or a relaxation training control. Among smokers, 2 wk of meditation training (5 h in total) produced a significant reduction in smoking of 60%; no reduction was found in the relaxation control. Resting-state brain scans showed increased activity for the meditation group in the anterior cingulate and prefrontal cortex, brain areas related to self-control. These results suggest that brief meditation training improves self-control capacity and reduces smoking."
507b3686d589fe8bb132aaa9eb0f95a3ad34ced9,
6291bf1800ae1c5955eb7ea75eddc2763c15f6e1,
604888d1acd42de6d0890610652c5b4cc70651be,"Objective: On September 13, 2021, teleworking ended for New York City municipal employees, and Department of Education (DOE) employees returned to reopened schools. On October 29, COVID-19 vaccination was mandated. We assessed these mandates' short-term effects on disease transmission. Methods: Using difference-in-difference analyses, we calculated COVID-19 incidence rate ratios (IRR) among residents 18-64 years-old by employment status pre- and post-policy implementation. Results: IRRs post- (September 23-October 28) vs. pre- (July 5-September 12) return-to-office were similar between office-based City employees and non-City employees. Among DOE employees, the IRR after schools reopened was elevated 28.4% (95% CI: 17.3%-40.3%). Among City employees, the IRR post- (October 29-November 30) vs. pre- (September 23-October 28) vaccination mandate was lowered 20.1% (95% CI: 13.7%-26.0%). Conclusions: Workforce mandates influenced disease transmission, among other societal effects."
15e68c86c920e1a2b405bd35234cabaac7d720a6,
2e5ac25a4f450dc31ade5aa57452ccea09777b47,"Significance Infectious disease control critically depends on surveillance and predictive modeling of outbreaks. We argue that routine mobile-phone use can provide a source of infectious disease information via the measurements of behavioral changes in call-detail records (CDRs) collected for billing. In anonymous CDR metadata linked with individual health information from the A(H1N1)pdm09 outbreak in Iceland, we observe that people moved significantly less and placed fewer, but longer, calls in the few days around diagnosis than normal. These results suggest that disease-transmission models should explicitly consider behavior changes during outbreaks and advance mobile-phone traces as a potential universal data source for such efforts. Epidemic preparedness depends on our ability to predict the trajectory of an epidemic and the human behavior that drives spread in the event of an outbreak. Changes to behavior during an outbreak limit the reliability of syndromic surveillance using large-scale data sources, such as online social media or search behavior, which could otherwise supplement healthcare-based outbreak-prediction methods. Here, we measure behavior change reflected in mobile-phone call-detail records (CDRs), a source of passively collected real-time behavioral information, using an anonymously linked dataset of cell-phone users and their date of influenza-like illness diagnosis during the 2009 H1N1v pandemic. We demonstrate that mobile-phone use during illness differs measurably from routine behavior: Diagnosed individuals exhibit less movement than normal (1.1 to 1.4 fewer unique tower locations; P<3.2×10−3), on average, in the 2 to 4 d around diagnosis and place fewer calls (2.3 to 3.3 fewer calls; P<5.6×10−4) while spending longer on the phone (41- to 66-s average increase; P<4.6×10−10) than usual on the day following diagnosis. The results suggest that anonymously linked CDRs and health data may be sufficiently granular to augment epidemic surveillance efforts and that infectious disease-modeling efforts lacking explicit behavior-change mechanisms need to be revisited."
89f19190efdcfed00383e18b2ae07bae42400ad0,
c35d876fd0ec8edd7180fc2024f07ed05e7e57d9,
c3b210f07c87ef96da75feffd7544748e38a0e48,
d62f05d2e1b856373a83d03cf44decf095be9241,
d9c7a241550858b40fbd51517aa42cade0ec670b,"Global efforts to prevent the spread of the SARS-COV-2 pandemic in early 2020 focused on non-pharmaceutical interventions like social distancing; policies that aim to reduce transmission by changing mixing patterns between people. As countries have implemented these interventions, aggregated location data from mobile phones have become an important source of real-time information about human mobility and behavioral changes on a population level. Human activity measured using mobile phones reflects the aggregate behavior of a subset of people, and although metrics of mobility are related to contact patterns between people that spread the coronavirus, they do not provide a direct measure. In this study, we use results from a nowcasting approach from 1,396 counties across the US between January 22nd, 2020 and July 9th, 2020 to determine the effective reproductive number (R(t)) along an urban/rural gradient. For each county, we compare the time series of R(t) values with mobility proxies from mobile phone data from Camber Systems, an aggregator of mobility data from various providers in the United States. We show that the reproduction number is most strongly associated with mobility proxies for change in the travel into counties compared to baseline, but that the relationship weakens considerably after the initial 15 weeks of the epidemic, consistent with the emergence of a more complex ecosystem of local policies and behaviors including masking. Importantly, we highlight potential issues in the data generation process, representativeness and equity of access which must be addressed to allow for general use of these data in public health."
01b3201fb2287362493eebda0bc739775b738c06,
044ff3db3d9c145d9b0bb99465fe4188c313bc48,
1a8757d6432640b24e9533557f2b8c522ee6ef23,
5214b27de82268d6ae4309db32d162d9a0470c47,
5d162a684cdc229364656a9be09b471d11113fd0,"Significance Understanding the population composition and distribution of a region affected by a major natural disaster is vital for the allocation of resources to communities in need and critical to inform mortality estimates. Currently, the US Census Bureau is the only institution that publishes reliable population estimates for the United States and its territories. Since these are published once per year, it is impossible to use census-based population estimates to assess short-term postdisaster out-of-jurisdiction migration and within-jurisdiction migration. The utilization of social media traces, coupled with mobile phone data, could provide live estimates of postdisaster population changes in disaster-affected areas."
72f755b23380ba16fb3a0cc63f982812fbb06340,
8630e797063b1eb3a5901156a69a1dd492dd1f7f,
9274b188a75638717774889cadfdf82c0a976f3f,
970516143323a9f595ba5176c40397a6ffc1c373,"Staying up-to-date with current medical research can be a challenge for doctors and other medical decision-makers. Systematic reviews are one of the key tools that doctors use to stay informed. These are meta-analyses of all the relevant research with the intention of answering one specific question within the healthcare domain. Cochrane produces systematic reviews of medical research that are globally considered as a gold standard for highquality healthcare information. However, because of the high volume of papers published and the fact that Cochrane’s review authors are volunteers, it can take up to three years to write and publish one of these reviews. Our research focuses on speeding up this process. We propose a hybrid human-AI system to establish the topical area of a newly published paper faster, easing the process of searching for papers to include in a review."
b1c1255f5f5c83de95fd8a0af4a8a51b2419cb1c,"During the COVID-19 crisis, millions of migrants around the world face food insecurity. This could force migrants to travel during the pandemic, exposing them to health risks and accelerating the spread of the virus. Anecdotal evidence demonstrates the importance of enforcing food security policies to tide the crisis. However, the effects of these policies on containing mobility during the crisis remain unknown. Using mobility data from Facebook, we demonstrate that a policy to guarantee food security that has attracted attention from the Supreme Court of India — portable ration cards — is related to lower mobility during India’s COVID-19 lockdown. Intra-state portable ration cards, which give migrants access to food when they move within their state, are associated with 12% lower intra-state mobility. This effect is particularly strong for states that have fully implemented the policy compared to states that have partially implemented it. However, inter-state portability of ration cards is not related to reduced inter-state mobility, suggesting frictions in implementing the policy across state borders. We also find that food distribution activities by ration shops and civil society actors at the local level are associated with reduced mobility both within states and across state borders. Our study provides generalizable lessons for policymakers around the world: food security policies are essential for helping migrants restrict their travel during the pandemic. Policy implementation requires lead time; therefore, central governments need to coordinate with local actors to increase food distribution to migrants in the immediate term."
b20c39c03aed7ee754c7f122f163af2129c7fb19,"There has been a recent surge of interest in using mobility data from mobile phones to monitor social distancing and model the spread of SARS-COV-2, the virus that causes COVID-19. Despite several years of research in this area, standard frameworks for aggregating and making use of different data streams from mobile phones are lacking and difficult to generalize across data providers. Here, we provide a comprehensive set of guidelines for aggregation principles and procedures for different mobile phone data streams, and describe how aggregated data are used in research and policy. We argue that the principles of privacy and data protection are critical in assessing more technical aspects of aggregation, and should be an important central feature guiding partnerships with governments who make use of research products."
c4c787e10e1c5ddbc8cd24cd2ffdd6378a4016a8,
dff64454e7119c7029ed3fa383b782c9b4e065a6,"Population displacement may occur after natural disasters, permanently altering the demographic composition of the affected regions. Measuring this displacement is vital for both optimal post-disaster resource allocation and calculation of measures of public health interest such as mortality estimates. Here, we analyzed data generated by mobile phones and social media to estimate the weekly island-wide population at risk and within-island geographic heterogeneity of migration in Puerto Rico after Hurricane Maria. We compared these two data sources to population estimates derived from air travel records and census data. We observed a loss of population across all data sources throughout the study period, however, the magnitude and dynamics differ by data source. Census data predict a population loss of just over 129,000 from July 17 to July 2018, a 4% decrease; air travel data predicts a population loss of 168,295 for the same period of time, a 5% decrease; mobile phone based estimates predicts a loss of 235,375 form July 2017 to May 2018, an 8% decrease; and social media based estimates predict a loss of 476,779 from August 2017 to August 2018; a 17% decrease. On average, municipalities with smaller population size lost a bigger proportion of their population. Moreover, we infer that these municipalities experienced greater infrastructure damage as measured by the proportion of unknown locations stemming from these regions. Finally, our analysis measures a general shift of population from rural to urban centers within the island."
f27dee103ccade0fa2bf62f8abda3ff34253b89d,"In response to the SARS-CoV-2 pandemic, unprecedented policies of travel restrictions and stay-at-home orders were enacted around the world. Ultimately, the public's response to announcements of lockdowns - defined here as restrictions on both local movement or long distance travel - will determine how effective these kinds of interventions are. Here, we measure the impact of the announcement and implementation of lockdowns on human mobility patterns by analyzing aggregated mobility data from mobile phones. We find that following the announcement of lockdowns, both local and long distance movement increased. To examine how these behavioral responses to lockdown policies may contribute to epidemic spread, we developed a simple agent-based spatial model. We find that travel surges following announcements of lockdowns can increase seeding of the epidemic in rural areas, undermining the goal of the lockdown of preventing disease spread. Appropriate messaging surrounding the announcement of lockdowns and measures to decrease unnecessary travel are important for preventing these unintended consequences of lockdowns."
a866104c63d278510735584e8d67585fb6ee6f8f,"Data collected by mobile devices can augment surveillance of epidemics in real time. However, methods and evidence for the integration of these data into modern surveillance systems are sparse. We linked call detail records (CDR) with an influenza‐like illness (ILI) registry and evaluated the role that Icelandic international travellers played in the introduction and propagation of influenza A/H1N1pdm09 virus in Iceland through the course of the 2009 pandemic."
5a2ce7dd18cc032d8f3dcce3289252fc6b27b663,"n engl j med nejm.org 1 From the Departments of Epidemiology (N.K., A.M., C.O.B.), Social and Behavioral Sciences (M.V.K.), and Biostatistics (R.A.I.) and the Center for Communicable Disease Dynamics (N.K., A.M., C.O.B.) and the François-Xavier Bagnoud Center for Health and Human Rights (A.F., J. Leaning, S.B.), Harvard T.H. Chan School of Public Health, Harvard University, the Department of Emergency Medicine, Beth Israel Deaconess Medical Center and Harvard Medical School (F.R., S.B.), and the Department of Biostatistics and Computational Biology, Dana–Farber Cancer Institute (R.A.I.) — all in Boston; the Department of Psychology, Carlos Albizu University (D.M., I.R.), and the Puerto Rico Science, Technology, and Research Trust (L.M.) — both in San Juan; Keck School of Medicine, University of Southern California, Los Angeles (P.E.); and the Section of Wilderness and Environmental Medicine at the Department of Emergency Medicine, University of Colorado School of Medicine, Aurora (C.S., J. Lemery). Address reprint requests to Dr. Buckee at cbuckee@ hsph . harvard . edu."
e670d5868ddefd3f02dd96270b850a90616a8df7,"BACKGROUND Quantifying the effect of natural disasters on society is critical for recovery of public health services and infrastructure. The death toll can be difficult to assess in the aftermath of a major disaster. In September 2017, Hurricane Maria caused massive infrastructural damage to Puerto Rico, but its effect on mortality remains contentious. The official death count is 64. METHODS Using a representative, stratified sample, we surveyed 3299 randomly chosen households across Puerto Rico to produce an independent estimate of all‐cause mortality after the hurricane. Respondents were asked about displacement, infrastructure loss, and causes of death. We calculated excess deaths by comparing our estimated post‐hurricane mortality rate with official rates for the same period in 2016. RESULTS From the survey data, we estimated a mortality rate of 14.3 deaths (95% confidence interval [CI], 9.8 to 18.9) per 1000 persons from September 20 through December 31, 2017. This rate yielded a total of 4645 excess deaths during this period (95% CI, 793 to 8498), equivalent to a 62% increase in the mortality rate as compared with the same period in 2016. However, this number is likely to be an underestimate because of survivor bias. The mortality rate remained high through the end of December 2017, and one third of the deaths were attributed to delayed or interrupted health care. Hurricane‐related migration was substantial. CONCLUSIONS This household‐based survey suggests that the number of excess deaths related to Hurricane Maria in Puerto Rico is more than 70 times the official estimate. (Funded by the Harvard T.H. Chan School of Public Health and others.)"
baa4ed3d3299309bba26d462371d51ba72d8b150,
ef735d2df717de2bbc2c2be6abdfff72e856c385,
94eaab75e27abecabbfd4389a1b681f8990ffc3c,Stigma and gender-based violence fuel the HIV/AIDS pandemic by limiting access to and use of HIV/AIDS-related services for prevention treatment care and support. Thus HIV/AIDS programs that fail to consider stigma and gender-based violence can be only partially effective at best. This manual is a guide for community-based organizations to facilitate a community-led and -owned process that addresses stigma and gender-based violence in HIV/AIDS prevention efforts. It is based on findings from the Stigma and Violence Reduction Intervention (SVRI) project conducted in Andhra Pradesh India from 2003 to 2005. The project objective was to effect behavioral and attitudinal changes that would reduce the spread of HIV and AIDS among mobile and mobility-affected populations. More complete project findings are summarized separately. The SVRI project explored and described the origins and manifestations of stigma and intimate partner violence including sexual violence experienced by mobile and mobility-affected sex workers truckers helpers and truckers spouses. The project identified these groups as key populations because they often are victims of stigma and violence and they can play an important role in changing norms that condone these behaviors. An intervention was designed and implemented in the project area based on the participatory research conducted with these populations and included cultural shows advocacy meetings networking of service providers and a series of workshops for reflection and change. (excerpt)
007157e7a3540d671f31b63c08cb02c8ccd7c179,
0c92c514f84d8800ea2d245545b671d71305251c,"The reverse dictionary task is a sequence-to-vector task in which a gloss is provided as input, and the output must be a semantically matching word vector. The reverse dictionary is useful in practical applications such as solving the tip-of-the-tongue problem, helping new language learners, etc. In this paper, we evaluate the effect of a Transformer-based model with cross-lingual zero-shot learning to improve the reverse dictionary performance. Our experiments are conducted in five languages in the CODWOE dataset, including English, French, Italian, Spanish, and Russian. Even if we did not achieve a good ranking in the CODWOE competition, we show that our work partially improves the current baseline from the organizers with a hypothesis on the impact of LSTM in monolingual, multilingual, and zero-shot learning. All the codes are available at https://github.com/honghanhh/codwoe2021."
690fb25117c15b36ca6afd119f061aae4cca57b9,"We describe initial work into analysing the language used around environmental, social and governance (ESG) issues in UK company annual reports. We collect a dataset of annual reports from UK FTSE350 companies over the years 2012-2019; separately, we define a categorized list of core ESG terms (single words and multi-word expressions) by combining existing lists with manual annotation. We then show that this list can be used to analyse the changes in ESG language in the dataset over time, via a combination of language modelling and distributional modelling via contextual word embeddings. Initial findings show that while ESG discussion in annual reports is becoming significantly more likely over time, the increase varies with category and with individual terms, and that some terms show noticeable changes in usage."
a0449cb688e65967a8ec21d4d37fed3ea08d93e3,"In light of unprecedented increases in the pop-ularity of the internet and social media, comment moderation has never been a more rel-evant task. Semi-automated comment moderation systems greatly aid human moderators by either automatically classifying the examples or allowing the moderators to prioritize which comments to consider ﬁrst. However, the concept of inappropriate content is often subjective, and such content can be conveyed in many subtle and indirect ways. In this work, we propose CoRAL 1 – a language and cul-turally aware Croatian Abusive dataset cover-ing phenomena of implicitness and reliance on local and global context. We show exper-imentally that current models degrade when comments are not explicit and further degrade when language skill and context knowledge are required to interpret the comment."
a396e3ccd296009c4f65ecce7da71a334f6a5334,"The Internet Engineering Task Force (IETF) has developed many of the technical standards that underpin the Internet. The standards development process followed by the IETF is open and consensus-driven, but is inherently both a social and political activity, and latent influential structures might exist within the community. Exploring and understanding these structures is essential to ensuring the IETF’s resilience and openness. We use network analysis to explore the social graph of IETF participants, based on public email discussions and co-author relationships, and the influence of key contributors. We show that a small core of participants dominates: the top 10% contribute almost half (43.75%) of the emails and come from a relatively small group of organisations. On the other hand, we also find that influence has become relatively more decentralised with time. IETF participants also propose and work on drafts that are either adopted by a working group for further refinement or get rejected at an early stage. Using the social graph features combined with email text features, we perform regression analysis to understand the effect of user influence on the success of new work being adopted by the IETF. Our findings shed useful insights into the behavior of participants across time, correlation between influence and success in draft adoption, and the significance of affiliated organisations in the authorship of drafts."
a40693eefd351659cdeb3885917b1506ea01c38a,"In text-to-SQL tasks — as in much of NLP — 001 compositional generalization is a major chal- 002 lenge: neural networks struggle with compo- 003 sitional generalization where training and test 004 distributions differ. However, most recent at- 005 tempts to improve this are based on word-level 006 synthetic data or speciﬁc dataset splits to gen- 007 erate compositional biases. In this work, we 008 propose a clause-level compositional example 009 generation method. We ﬁrst split the sentences 010 in the Spider text-to-SQL dataset into sub- 011 sentences, annotating each sub-sentence with 012 its corresponding SQL clause, resulting in a 013 new dataset Spider-SS. We then construct a fur- 014 ther dataset, Spider-CG, by composing Spider- 015 SS sub-sentences in different combinations, to 016 test the ability of models to generalize com- 017 positionally. Experiments show that existing 018 models suffer signiﬁcant performance degra- 019 dation when evaluated on Spider-CG, even 020 though every sub-sentence is seen during train- 021 ing. To deal with this problem, we modify a 022 number of state-of-the-art models to train on 023 the segmented data of Spider-SS, and we show 024 that this method improves the generalization 025 performance. 1 026"
af9382455aaa943082ce85d4c9d6b8b4d15562ff,"User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal."
fa437221a3d655fe105b0a8389a3f0855c32e0a1,"There is a global trend for responsible investing and the need for developing automated methods for analyzing and Environmental, Social and Governance (ESG) related elements in financial texts is raising. In this work we propose a solution to the FinSim4-ESG task, consisting of binary classification of sentences into sustainable or unsustainable. We propose a novel knowledge-based latent heterogeneous representation that is based on knowledge from taxonomies and knowledge graphs and multiple contemporary document representations. We hypothesize that an approach based on a combination of knowledge and document representations can introduce significant improvement over conventional document representation approaches. We consider ensembles on classifier as well on representation level late-fusion and early fusion. The proposed approaches achieve competitive accuracy of 89 and are 5.85 behind the best achieved score."
065715bf35d1dba5bf06f59f7e1e8390b93e6adf,"The current dominance of deep neural networks in natural language processing is based on contextual embeddings such as ELMo, BERT, and BERT derivatives. Most existing work focuses on English; in contrast, we present here the first multilingual empirical comparison of two ELMo and several monolingual and multilingual BERT models using 14 tasks in nine languages. In monolingual settings, our analysis shows that monolingual BERT models generally dominate, with a few exceptions such as the dependency parsing task, where they are not competitive with ELMo models trained on large corpora. In cross-lingual settings, BERT models trained on only a few languages mostly do best, closely followed by massively multilingual BERT models."
0b3863c21a7fb5ac61a447611cba0ec9ce1ab4a4,"Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples."
190e722064bad897f6ec21284715db9cc6a2381c,"Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed signiﬁcant differences in how the speech varied from session to session in the control and dementia groups."
322be2c047c4464942a0ef9c7e0bc8bc0290c541,"Alzheimer’s Disease (AD) is associated with many characteristic changes, not only in an individual’s language but also in the interactive patterns observed in dialogue. The most indicative changes of this latter kind tend to be associated with relatively rare dialogue acts (DAs), such as those involved in clarification exchanges and responses to particular kinds of questions. However, most existing work in DA tagging focuses on improving average performance, effectively prioritizing more frequent classes; it thus gives a poor performance on these rarer classes and is not suited for application to AD analysis. In this paper, we investigate tagging specifically for rare class DAs, using a hierarchical BiLSTM model with various ways of incorporating information from previous utterances and DA tags in context. We show that this can give good performance for rare DA classes on both the general Switchboard corpus (SwDA) and an AD-specific conversational dataset, the Carolinas Conversation Collection (CCC); and that the tagger outputs then contribute useful information for distinguishing patients with and without AD"
355d3e4bf5430ba16f95496513b69d5f4753bcef,We present a system for zero-shot cross-lingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of languages without any additional training. Experiments show an impressive ability of both models to generalize from English to other languages. There is however an expected gap in performance between the tested cross-lingual models and the monolingual models. The best performing model (offensive content classifier) is available online as a REST API.
379e9d5bba8a617b3114bd5b562b14aa6abc5282,"Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts for in the text descriptions; (2) removing the need for nested subqueries and set operators; and (3) making schema linking easier by reducing the required number of schema items. On Spider, a challenging textto-SQL benchmark that contains complex and nested SQL queries, we demonstrate that NatSQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing models that do not support executable SQL generation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy 1."
4a0771380439287db22cb5c87ad4762d803ae927,"Platforms that feature user-generated content (social media, online forums, newspaper comment sections etc.) have to detect and filter offensive speech within large, fast-changing datasets. While many automatic methods have been proposed and achieve good accuracies, most of these focus on the English language, and are hard to apply directly to languages in which few labeled datasets exist. Recent work has therefore investigated the use of cross-lingual transfer learning to solve this problem, training a model in a well-resourced language and transferring to a less-resourced target language; but performance has so far been significantly less impressive. In this paper, we investigate the reasons for this performance drop, via a systematic comparison of pre-trained models and intermediate training regimes on five different languages. We show that using a better pre-trained language model results in a large gain in overall performance and in zero-shot transfer, and that intermediate training on other languages is effective when little target-language data is available. We then use multiple analyses of classifier confidence and language model vocabulary to shed light on exactly where these gains come from and gain insight into the sources of the most typical mistakes."
4a67143584f41dd2508c2e52a5aba79c2ee6dac1,"We present a conversational management act (CMA) annotation schema for one-to-one tutorial dialogue sessions where a tutor uses an analogy to teach a student a concept. CMAs are more fine-grained sub-utterance acts compared to traditional dialogue act mark-up. The schema achieves an inter-annotator agreement (IAA) Cohen Kappa score of at least 0.66 across all 10 classes. We annotate a corpus of analogical episodes with the schema and develop statistical sequence models from the corpus which predict tutor content related decisions, in terms of the selection of the analogical component (AC) and tutor conversational management act (TCMA) to deploy at the current utterance, given the student’s behaviour. CRF sequence classifiers perform well on AC selection and robustly on TCMA selection, achieving respective accuracies of 61.9% and 56.3% on a cross-validation experiment over the corpus."
69254dbe0d17776456dde46cfae1f7cadc80fb74,"This paper presents tools and data sources collected and released by the EMBEDDIA project, supported by the European Union’s Horizon 2020 research and innovation program. The collected resources were offered to participants of a hackathon organized as part of the EACL Hackashop on News Media Content Analysis and Automated Report Generation in February 2021. The hackathon had six participating teams who addressed different challenges, either from the list of proposed challenges or their own news-industry-related tasks. This paper goes beyond the scope of the hackathon, as it brings together in a coherent and compact form most of the resources developed, collected and released by the EMBEDDIA project. Moreover, it constitutes a handy source for news media industry and researchers in the fields of Natural Language Processing and Social Science."
7c936b504abbec7b191a876cfcdb545cd21bc47a,
84b26030b648b6d79177bdafd3e896b1dda9f91e,"Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective."
902f6969ce6cb56bb730626d72b358d76760628c,"Moderation of reader comments is a significant problem for online news platforms. Here, we experiment with models for automatic moderation, using a dataset of comments from a popular Croatian newspaper. Our analysis shows that while comments that violate the moderation rules mostly share common linguistic and thematic features, their content varies across the different sections of the newspaper. We therefore make our models topic-aware, incorporating semantic features from a topic model into the classification decision. Our results show that topic information improves the performance of the model, increases its confidence in correct outputs, and helps us understand the model’s outputs."
98a9aaf65f93db824d2c36d9d2740879ad00c757,"This work revisits the task of detecting decision-related utterances in multi-party dialogue. We explore performance of a traditional approach and a deep learning-based approach based on transformer language models, with the latter providing modest improvements. We then analyze topic bias in the models using topic information obtained by manual annotation. Our finding is that when detecting some types of decisions in our data, models rely more on topic specific words that decisions are about rather than on words that more generally indicate decision making. We further explore this by removing topic information from the train data. We show that this resolves the bias issues to an extent and, surprisingly, sometimes even boosts performance."
b0e1eb19aa17dd058524982641415082e268dc0e,"Alzheimer’s disease (AD) is a progressive, neurodegenerative disorder mainly characterized by memory loss with deficits in other cognitive domains, including language, visuospatial abilities, and changes in behavior. Detecting diagnostic biomarkers that are noninvasive and cost-effective is of great value not only for clinical assessments and diagnostics but also for research purposes. Several previous studies have investigated AD diagnosis via the acoustic, lexical, syntactic, and semantic aspects of speech and language. Other studies include approaches from conversation analysis that look at more interactional aspects, showing that disfluencies such as fillers and repairs, and purely nonverbal features such as inter-speaker silence, can be key features of AD conversations. These kinds of features, if useful for diagnosis, may have many advantages: They are simple to extract and relatively language-, topic-, and task-independent. This study aims to quantify the role and contribution of these features of interaction structure in predicting whether a dialogue participant has AD. We used a subset of the Carolinas Conversation Collection dataset of patients with AD at moderate stage within the age range 60–89 and similar-aged non-AD patients with other health conditions. Our feature analysis comprised two sets: disfluency features, including indicators such as self-repairs and fillers, and interactional features, including overlaps, turn-taking behavior, and distributions of different types of silence both within patient speech and between patient and interviewer speech. Statistical analysis showed significant differences between AD and non-AD groups for several disfluency features (edit terms, verbatim repeats, and substitutions) and interactional features (lapses, gaps, attributable silences, turn switches per minute, standardized phonation time, and turn length). For the classification of AD patient conversations vs. non-AD patient conversations, we achieved 83% accuracy with disfluency features, 83% accuracy with interactional features, and an overall accuracy of 90% when combining both feature sets using support vector machine classifiers. The discriminative power of these features, perhaps combined with more conventional linguistic features, therefore shows potential for integration into noninvasive clinical assessments for AD at advanced stages."
ceb4b96216d4538589fd7dcd3c043e1cd365cdee,"Alzheimer’s Disease (AD) is a form of Dementia that man-ifests in cognitive decline including memory, language, and changes in behavior. Speech data has proven valuable for in-ferring cognitive status, used in many health assessment tasks, and can be easily elicited in natural settings. Much work focuses on analysis using linguistic features; here, we focus on non-linguistic features and their use in distinguishing AD patients from similar-age Non-AD patients with other health con-ditions in the Carolinas Conversation Collection (CCC) dataset. We used two types of features: patterns of interaction including pausing behaviour and ﬂoor control, and acoustic features including pitch, amplitude, energy, and cepstral coefﬁcients. Fusion of the two kinds of features, combined with feature selection, obtains very promising classiﬁcation results: classiﬁcation accuracy of 90% using standard models such as support vector machines and logistic regression. We also obtain promising results using interactional features alone (87% accuracy), which can be easily extracted from natural conversations in daily life and thus have the potential for future implementation as a non-invasive method for AD diagnosis and monitoring."
e172700d02fa58e45825850db3491139ac521c64,"Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the control and dementia groups."
e557a47096d57270338049a789226d691ebfaff5,"This paper concerns the evaluation of a workspace architecture for generating natural language descriptions, including methods for evaluating both its output and its own self-evaluation. Herein are details of preliminary results from evaluation of an early iteration of the architecture operating in the domain of weather. The domain is not typically seen as creative, but provides a simple testbed for the architecture and evaluation methodology. The program does not yet match humans in terms of fluency of language, factual correctness, and how completely the input is described, but human judges did find the program’s output easier to read than human generated texts. Planned improvements to the program also described in the paper will incorporate self-monitoring and better self-evaluation with the aim of producing descriptions that are more fluently written and more accurate."
ee4cb8dd73c90ab2b085c7dcfe8ba32b1baa0093,"Protocol standards, defined by the Internet Engineering Task Force (IETF), are crucial to the successful operation of the Internet. This paper presents a large-scale empirical study of IETF activities, with a focus on understanding collaborative activities, and how these underpin the publication of standards documents (RFCs). Using a unique dataset of 2.4 million emails, 8,711 RFCs and 4,512 authors, we examine the shifts and trends within the standards development process, showing how protocol complexity and time to produce standards has increased. With these observations in mind, we develop statistical models to understand the factors that lead to successful uptake and deployment of protocols, deriving insights to improve the standardisation process."
09307f588e2800b37c943194c84836b4811eb74e,"Abstract We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show that context induces a cognitive load for humans, which compresses the distribution of ratings. Moreover, in relevant contexts we observe a discourse coherence effect that uniformly raises acceptability. Next, we test unidirectional and bidirectional language models in their ability to predict acceptability ratings. The bidirectional models show very promising results, with the best model achieving a new state-of-the-art for unsupervised acceptability prediction. The two sets of experiments provide insights into the cognitive aspects of sentence processing and central issues in the computational modeling of text and discourse."
2f5b788189e36316dd64e4ca4628baf7bcd66acd,"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant-supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depression, supported by the literature. We propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decision-making."
37db68504202274a4353f5cf2d48e27d79c79063,"This article describes initial work into the automatic classification of user-generated content in news media to support human moderators. We work with real-world data — comments posted by readers under online news articles — in two less-resourced European languages, Croatian and Estonian. We describe our dataset, and experiments into automatic classification using a range of models. Performance obtained is reasonable but not as good as might be expected given similar work in offensive language classification in other languages; we then investigate possible reasons in terms of the variability and reliability of the data and its annotation."
43ea6b783e2fa9be4a932ba4dcfc254193431732,"Many existing models of narrative and language generation use rigid sequences of steps which are cognitively implausible and limit creativity. Iterative models based on Sharples’ cycle of engagement and reflection improve on this by incorporating self-evaluation but still have a rigid arrangement of parts. This paper outlines how a multi-agent approach could be used to break apart the cycle into a more fluid society of engagement and reflection, whose constituent agents interact with one another to produce a text. Our approach is to work in a simple domain in order to focus on the underlying processes, and to avoid the ELIZA effect during evaluation."
625cfa4170a12aee17e76c18b7165ae13e522c4f,"This paper presents the Graded Word Similarity in Context (GWSC) task which asked participants to predict the effects of context on human perception of similarity in English, Croatian, Slovene and Finnish. We received 15 submissions and 11 system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two different contexts. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect."
66340042199dbaf7f8c73eed93ef06924626bd67,"ABSTRACT In everyday conversation, no notion of “complete sentence” is required for syntactic licensing. However, so-called “fragmentary”, “incomplete”, and abandoned utterances are problematic for standard formalisms. When contextualised, such data show that (a) non-sentential utterances are adequate to underpin agent coordination, while (b) all linguistic dependencies can be systematically distributed across participants and turns. Standard models have problems accounting for such data because their notions of ‘constituency’ and ‘syntactic domain’ are independent of performance considerations. Concomitantly, we argue that no notion of “full proposition” or encoded speech act is necessary for successful interaction: strings, contents, and joint actions emerge in conversation without any single participant having envisaged in advance the outcome of their own or their interlocutors’ actions. Nonetheless, morphosyntactic and semantic licensing mechanisms need to apply incrementally and subsententially. We argue that, while a representational level of abstract syntax, divorced from conceptual structure and physical action, impedes natural accounts of subsentential coordination phenomena, a view of grammar as a “skill” employing domain-general mechanisms, rather than fixed form-meaning mappings, is needed instead. We provide a sketch of a predictive and incremental architecture (Dynamic Syntax) within which underspecification and time-relative update of meanings and utterances constitute the sole concept of “syntax”."
b1dd7744065713869f54e4e8756780b19139c949,"WikiSQL and Spider, the large-scale cross-domain text-to-SQL datasets, have attracted much attention from the research community. The leaderboards of WikiSQL and Spider show that many researchers propose their models trying to solve the text-to-SQL problem. This paper first divides the top models in these two leaderboards into two paradigms. We then present details not mentioned in their original paper by evaluating the key components, including schema linking, pretrained word embeddings, and reasoning assistance modules. Based on the analysis of these models, we want to promote understanding of the text-to-SQL field and find out some interesting future works, for example, it is worth studying the text-to-SQL problem in an environment where it is more challenging to build schema linking and also worth studying combing the advantage of each model toward text-to-SQL."
c77d14e649bc2a6a421676dc36c44da26a400f66,"This paper is a submission to the Alzheimer’s Dementia Recognition through Spontaneous Speech (ADReSS) challenge, which aims to develop methods that can assist in the automated prediction of severity of Alzheimer’s Disease from speech data. We focus on acoustic and natural language features for cognitive impairment detection in spontaneous speech in the context of Alzheimer’s Disease Diagnosis and the mini-mental state examination (MMSE) score prediction. We proposed a model that obtains unimodal decisions from different LSTMs, one for each modality of text and audio, and then combines them using a gating mechanism for the final prediction. We focused on sequential modelling of text and audio and investigated whether the disfluencies present in individuals’ speech relate to the extent of their cognitive impairment. Our results show that the proposed classification and regression schemes obtain very promising results on both development and test sets. This suggests Alzheimer’s Disease can be detected successfully with sequence modeling of the speech data of medical sessions."
cf9548e7c21959e0964ec347b1d614661b43dcd2,"This is the author accepted manuscript of McGregor, Purver, Wiggins (2020). Metaphor Generation through Context Sensitive Distributional Semantics. In Producing Figurative Expression: Theoretical, experimental and practical perspectives pp.419-448, DOI 10.1075/ftl.10.15mcg. Copyright 2020 John Benjamins Publishing Company, who should be contacted for permission to re-use or reprint the material in any form. Abstract: In this paper, we outline a preliminary methodology for generating metaphor based"
da86cbc2fe7f44b7641b5b306d38f62366d8c756,"We introduce an annotation scheme and corpus study to investigate the use of base and target components of analogies in tutorial dialogues. We present the development of the scheme and test its final form on a corpus of one-to-one tutorial dialogues on computer science, for which we achieve over 0.77 multirater inter-annotator agreement. We then annotate data from the same corpus to investigate the use of semantic wave structures from Legitimation Code Theory in tutoring, and we find a regular adherence to semantic wave structures in explanations which use analogies. We further identified different semantic wave shapes and show their distributions. We conclude that semantic waves and the novel characterisation of analogical explanations in tutorial dialogues reported in this investigation can be useful tools for both the analysis of human tutorial dialogue and future implementation of tutorial dialogue systems."
023f954745e84062c6190f08c9a250fab446fa35,"Semi-structured clinical interviews are frequently used diagnostic tools for identifying depression during an assessment phase. In addition to the lexical content of a patient’s responses, multimodal cues concurrent with the responses are indicators of their motor and cognitive state, including those derivable from their voice quality and gestural behaviour. In this paper, we use information from different modalities in order to train a classifier capable of detecting the binary state of a subject (clinically depressed or not), as well as the level of their depression. We propose a model that is able to perform modality fusion incrementally after each word in an utterance using a time-dependent recurrent approach in a deep learning set-up. To mitigate noisy modalities, we utilize fusion gates that control the degree to which the audio or visual modality contributes to the final prediction. Our results show the effectiveness of word-level multimodal fusion, achieving state-of-the-art results in depression detection and outperforming early feature-level and late fusion techniques."
2e94d9ba9199b874f966881949f57ea6c417773c,"State of the art natural language processing tools are built on context-dependent word embeddings, but no direct method for evaluating these representations currently exists. Standard tasks and datasets for intrinsic evaluation of embeddings are based on judgements of similarity, but ignore context; standard tasks for word sense disambiguation take account of context but do not provide continuous measures of meaning similarity. This paper describes an effort to build a new dataset, CoSimLex, intended to fill this gap. Building on the standard pairwise similarity task of SimLex-999, it provides context-dependent similarity measures; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far."
42935dca0480af822180510a2b364826dcd8a870,"This paper describes an initial corpus study of question-answer pairs in the Carolina Conversations Collection corpus of conversational interviews with older people. Our aim is to compare the behaviour of patients with and without Alzheimer’s Disease (AD) on the basis of types of question asked and their responses in dialogue. It has been suggested that questions present an interesting and useful phenomenon for exploring the quality of communication between patients and their interlocutors, and this study confirms this: questions are common, making up almost 14% of utterances from AD and Non-AD patients; and type distributions vary, interviewers asking many Yes-No questions (nearly 6%) from AD patients while more Wh-questions (5.4%) from Non-AD patients. We also find that processes of clarification and coordination (e.g. asking clarification questions, signalling non-understanding) are more common in dialogue with AD patients."
559624b3bafc4177b22ae6038eec2d9c4538cbf6,
63e01a8df3be98c0e7ac8054d14753458699679b,"We examine the benefit of a variety of discourse and semantic features for the identification of summary-worthy content in narrative stories. Using logistic regression models, we find that the most informative features are those that relate to the narrative structure of a text. We show that automatic methods for feature extraction perform significantly worse than full manual annotation, but that with optimization, a fully automatic approach can outperform a variety of existing extractive approaches to summarization."
6894b6ca5d408f6c72f704e60e64a0c06eed7ec8,"Vector models of meaning—both strictly distributional models derived directly from co-occurrence statistics, and embeddings learned using representations such as those learnt by a neural network—have revolutionised computational linguistics via their ability to reflect semantic similarities and regularities while providing flexibility to model dynamics and change. However, while there has been much recent interest in extending these models from the level of words to that of larger phrases and sentences, with its own scalability and transparency problems, and there is relatively little progress in understanding how they might apply beyond the sentence: in the realm of discourse and dialogue. This requires a shift in perspective, moving beyond the static word/sentence view of language as a jigsaw of pieces, to a dynamic perspective seeing language as a set of mechanisms for interaction in real time, encompassing a whole range of actions both suband supra-sentential. At a sub-sentential level, dialogue is highly incremental: individuals can interrupt, extend, correct, or request clarification midturn, in effect constructing joint utterances without any sense of breakdown in the dialogue exchange. And at the suprasentential level, we have the challenges not only of establishing coreference but of modelling the vast array of speech act effects in dialogue, and rhetorical effects in text discourse, and how they evolve. The inaugural Vector Semantics for Discourse and Dialogue workshop brings together researchers using vector space methods for distributional semantics, word and sentence embeddings, and dialogue and discourse, to discuss these challenges and fill this gap. We received 13 submissions, each receiving at least two reviews, and all contributions were accepted."
8b04c0d51390f98527d93081f3b611e4be350c83,"Research on language and gender has a long tradition, and large electronic text corpora and novel computational methods for representing word meaning have recently opened new directions. We explain how gender can be analysed using word embeddings: vector representations of words computationally derived from lexical context in large corpora and capturing a degree of semantics. Being derived from naturally-occurring text, these also capture human biases, stereotypes and reflect social inequalities. The relation between the English words man and programmer can correspond to that between woman and homemaker. In Slovene, the availability of male and female forms for many words for occupations means that such effects might be reduced; however, we study a range of such relations and show that some gender bias still persists (e.g. the relation between words woman and secretary is very similar to that between man and boss)."
94afc47d2643f8ea195650a7c3728710d42c752a,"In this paper, we present a novel context-dependent approach to modeling word meaning, and apply it to the modeling of metaphor. In distributional semantic approaches, words are represented as points in a high dimensional space generated from co-occurrence statistics; the distances between points may then be used to quantifying semantic relationships. Contrary to other approaches which use static, global representations, our approach discovers contextualized representations by dynamically projecting low-dimensional subspaces; in these ad hoc spaces, words can be re-represented in an open-ended assortment of geometrical and conceptual configurations as appropriate for particular contexts. We hypothesize that this context-specific re-representation enables a more effective model of the semantics of metaphor than standard static approaches. We test this hypothesis on a dataset of English word dyads rated for degrees of metaphoricity, meaningfulness, and familiarity by human participants. We demonstrate that our model captures these ratings more effectively than a state-of-the-art static model, and does so via the amount of contextualizing work inherent in the re-representational process."
a7b26a7e0932fc33aec50e97ad0329998d5b258b,"Computational creativity seeks to understand computational mechanisms that can be characterized as creative. The creation of new concepts is a central challenge for any creative system. In this article, we outline different approaches to computational concept creation and then review conceptual representations relevant to concept creation, and therefore to computational creativity. The conceptual representations are organized in accordance with two important perspectives on the distinctions between them. One distinction is between symbolic, spatial and connectionist representations. The other is between descriptive and procedural representations. Additionally, conceptual representations used in particular creative domains, such as language, music, image and emotion, are reviewed separately. For every representation reviewed, we cover the inference it affords, the computational means of building it, and its application in concept creation."
e0e7d30a1ee00a28504dc4cde1a4a128b29db4d6,"Computational creativity seeks to understand computational mechanisms that can be characterized as creative. The creation of new concepts is a central challenge for any creative system. In this paper, we outline different approaches to computational concept creation and then review conceptual representations relevant to concept creation, and therefore to computational creativity. The conceptual representations are organized in accordance with two important perspectives on the distinctions between them. One distinction is between symbolic, spatial and connectionist representations. The other is between descriptive and procedural representations. Additionally, conceptual representations used in particular creative domains, i.e. language, music, image and emotion, are reviewed separately. For every representation reviewed, we cover the inference it affords, the computational means of building it, and its application in concept creation."
0371e15eb97ad2cdd5a93555ecb477880cd5a39c,"One of the fundamental requirements for models of semantic processing in dialogue is incrementality: a model must reflect how people interpret and generate language at least on a word-by-word basis, and handle phenomena such as fragments, incomplete and jointly-produced utterances. We show that the incremental word-by-word parsing process of Dynamic Syntax (DS) can be assigned a compositional distributional semantics, with the composition operator of DS corresponding to the general operation of tensor contraction from multilinear algebra. We provide abstract semantic decorations for the nodes of DS trees, in terms of vectors, tensors, and sums thereof; using the latter to model the underspecified elements crucial to assigning partial representations during incremental processing. As a working example, we give an instantiation of this theory using plausibility tensors of compositional distributional semantics, and show how our framework can incrementally assign a semantic plausibility measure as it parses phrases and sentences."
0941b6aeb383753da8fd861ebdbdd291466343a6,"We present initial investigations for a diachronic study of lexical changes in financial reporting, looking at methods suitable for analysing semantic associations between financial terms and how these change across time. Our corpus consists of US 10-K annual reports of 30 companies included in the Dow Jones Industrial Average stock index over the years 1996-2015. We grouped the reports by the reported fiscal year and derived word embedding models for each year using both GloVe and a count-based PPMI method; these vectors were then used to calculate cosine similarity between pairs of words. We expect the resulting diachronic patterns of lexical contexts of financial terms to vary with the economic cycle; here we select pairs of terms with strong increasing association over time (e.g. dividend and shareholder) or strong decreasing association over time (e.g. dividend and gain), and suggest some qualitative explanations for these changes due to the economic crisis."
14301326e1998b06aebb7a9a11b3c2426820e865,"[Introduction/Motivation:] As well as memory loss and linguistic impairment, changes in behaviour and decreased interactional skills in conversation are also symptoms for Alzheimer’s Disease (AD). Recent work has shown that linguistic features can be used within natural language processing (NLP) and machine learning (ML) methods to provide computational tools with potential for automatic detection of AD [1]; however, few studies have applied these techniques to investigate the predictive power of interactional symptoms. Automatic diagnosis of AD aids assessment and allows for earlier diagnosis. Interactional features are linguistically and culturally independent, allowing the automation to be applied across languages and borders."
1db52b4e62bd7738bc3e383618faf8d74530964f,"Miscommunication phenomena such as repair in dialogue are important indicators of the quality of communication. Automatic detection is therefore a key step toward tools that can characterize communication quality and thus help in applications from call center management to mental health monitoring. However, most existing computational linguistic approaches to these phenomena are unsuitable for general use in this way, and particularly for analyzing human-human dialogue: Although models of other-repair are common in human-computer dialogue systems, they tend to focus on specific phenomena (e.g., repair initiation by systems), missing the range of repair and repair initiation forms used by humans; and while self-repair models for speech recognition and understanding are advanced, they tend to focus on removal of ""disfluent"" material important for full understanding of the discourse contribution, and/or rely on domain-specific knowledge. We explain the requirements for more satisfactory models, including incrementality of processing and robustness to sparsity. We then describe models for self- and other-repair detection that meet these requirements (for the former, an adaptation of an existing repair model; for the latter, an adaptation of standard techniques) and investigate how they perform on datasets from a range of dialogue genres and domains, with promising results."
2a9c38cb8f4aec659239463036b30c5f3d1ed68c,"It has been claimed that natural dialogue is an especially repetitive form of language use. Comparison of dialogues and monologues in a corpus of naturally occurring speech (the DCPSE) suggests the reverse; monologue is substantially more repetitive than dialogue. We dub this the bore effect: the more people talk the more they repeat themselves. Dialogue, it appears, may provide an important means of escape from our cognitive and communicative ruts. 1 Repetition and Interaction Work in psycholinguistics has sometimes characterised dialogue as an especially repetitive form of language use (Tannen, 2007; Pickering and Garrod, 2004; Pickering and Ferreira, 2008). However, previous research has indicated that, in free dialogue at least, repetition is rare. People repeat only 3% more of each other’s words than would be expected by chance and systematically diverge from each other in their syntactic choices (Howes et al., 2010; Healey et al., 2014). This is compatible with a view of dialogue as constructive engagement in which participants respond to one another by actively building on, e.g.: modifying, adapting or elaborating each other’s contributions rather than repeating them (Healey et al., 2014; Healey et al., 2018). The principal evidence against repetition in natural conversation comes from the analysis of otherrepetition (Howes et al., 2010; Healey et al., 2014). Spoken monologues, such as one-sided conversations or speeches, provide an interesting alternative test case that allows us to examine patterns of self-repetition. Do people repeat themselves more in monologues or dialogues? A constructive engagement view would predict that dialogue should reduce self-repetition, as people actively respond to each other’s contributions. This contrasts with priming models that claim that repetition in dialogue is typically either equivalent to or stronger than in monologue (Pickering and Garrod, 2004; Pickering and Ferreira, 2008)."
7af9722b1ef84e9f3ec5096da09b95cea94e7381,"Most of the recent researches have been carried out to analyse sentiment and emotions found in English texts, where few studies have been conducted on Arabic contents, which have been focused on analysing the sentiment as positive and negative, instead of the different emotions’ classes. Therefore this paper has focused on analysing different six emotions’ classes in Arabic contents, especially Arabic tweets which have unstructured nature that make it challenging task compared to the formal structured contents found in Arabic journals and books. On the other hand, the recent developments in the distributional sematic models, have encouraged testing the effect of the distributional measures on the classification process, which was not investigated by any other classification-related studies for analysing Arabic texts. As a result, the model has successfully improved the average accuracy to more than 86% using Support Vector Machine (SVM) compared to the different sentiments and emotions studies for classifying Arabic texts through the developed semi-supervised approach which has employed the contextual and the co-occurrence information from a large amount of unlabelled dataset. In addition to the different remarkable achieved results, the model has recorded a high average accuracy, 85.30%, after removing the labels from the unlabelled contextual information which was used in the labelled dataset during the classification process. Moreover, due to the unstructured nature of Twitter contents, a general set of pre-processing techniques for Arabic texts was found which has resulted in increasing the accuracy of the six emotions’ classes to 85.95% while employing the contextual information from the unlabelled dataset."
a74533e568df1e30479237768192dd47dc157392,
3dcc819e642beafccd3f6bffa434767d1a818eb1,"In this paper we present state-of-the-art results on the computational classification of semantic type coercion, accomplished using a novel geometric method which is both context-sensitive and generalisable. We show that this method improves accuracy on a SemEval dataset over previous work, and gives promising results on a new more challenging experimental setup involving the same data. In addition to a description of our distributional semantic methodology and the results obtained on an established dataset, we offer an overview of the linguistic phenomenon of coercion and an analysis of the geometric features by which our results are achieved."
67f810bef10dcb453d420362b71c9a4c64e79557,
8bd3ed004b7db19e481b32d61363d2bcbad2b4b2,"Managing disagreement in conversation requires subtle linguistic and pragmatics skills. One key dimension is the degree of ‘knowingness’ with which people present their stance on an issue. It has been hypothesised that framing stances as ‘knowing’, i.e. with higher implied levels of speaker certainty limits the potential for challenge by others. We present the first experimental test of this hypothesis. Using a text based chat-tool paradigm and a debating task we are able to systematically manipulate how ‘knowing’ people’s turns appear to one-another. The results show that ‘knowing’ stances tend to close off discussion leading to less carefully formulated, truncated turns, but do not reliably affect the range of solutions considered. Unknowing stances, by contrast, do not affect turn length or formulation but do encourage more deliberation and include more signals of certainty in the message contents."
d212ffd7d361e211152624f585e1e60ffaf8e70c,"This paper discusses the problem of incongruent headlines: those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as clickbait and sensationalism, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative methodologies that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel data set in this domain."
1e2b2f183ada4e2c8baefb30f6888caabdc0fde2,"One way to investigate creativity is to build artificial agents which might be capable of creative output -- perhaps basing them on theories of human cognition and creativity -- and see how well they work. This talk will describe recent and ongoing work on a range of projects in the Computational Creativity Lab at Queen Mary University of London, explain how they relate to general models of human cognition, and discuss what insights they can give us. We will examine general statistical models of sequential and hierarchical learning, discuss how they can be connected to higher-level conceptual structures, and show how they can be applied to model language and music while generating interesting, novel outputs. Matthew Purver is Reader in Computational Linguistics at Queen Mary University of London"
24640c13dcd36590e366cb1f340b44df6abe4f16,"Exposed disagreement is extremely rare in natural dialogue. Although informal argumentation features frequently in natural dialogue, the ways in which individuals make and evidence claims and position their opinions in relation to those of others is often achieved through more subtle and oblique methods. This makes natural dialogue distinct from more formal or institutionalised contexts. With increasing availability of natural dialogue datasets and with increasingly diverse contexts within which the application of argumentation modelling could be beneficial, being able to identify and interpret argumentation in natural dialogue becomes more important; so too does an understanding of why argumentation is enacted differently in natural dialogue and how factors such as politeness impact upon this. In this paper we highlight some of the ways in which argumentative content is produced differently in natural dialogue compared to formalised debate contexts and highly structured documents. We present some initial findings that demonstrate how existing models such as the Penn Discourse Treebank need further development if they are to adapt to the more dialogic data created on the social web."
3eac1268426825f55b80026487fa921237abfaad,"Previous optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on highdimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of a few hundreds; but dimensionality of a few thousands is often applied in compositional tasks as it is still computationally feasible and does not require the dimensionality reduction step. We present a systematic study of the interaction of the parameters of the association measure and vector dimensionality, and derive parameter selection heuristics that achieve performance across word similarity and relevance datasets competitive with the results previously reported in the literature achieved by highly dimensional or dense models."
49cc20ee378633b711f30ec7070fe7dc0cdfc2ce,"We present a novel hypothetical account of entrainment in music and language, in context of the Information Dynamics of Thinking model, IDyOT. The extended model affords an alternative view of entrainment, and its companion term, pulse, from earlier accounts. The model is based on hierarchical, statistical prediction, modeling expectations of both what an event will be and when it will happen. As such, it constitutes a kind of predictive coding, with a particular novel hypothetical implementation. Here, we focus on the model's mechanism for predicting when a perceptual event will happen, given an existing sequence of past events, which may be musical or linguistic. We propose a range of tests to validate or falsify the model, at various different levels of abstraction, and argue that computational modeling in general, and this model in particular, can offer a means of providing limited but useful evidence for evolutionary hypotheses."
64307e5e58fd95dddff2af353e3d19a643a8004c,"Abstract Empirical evidence from dialogue, both corpus and experimental, highlights the importance of interaction in language use – and this raises some questions for Christiansen & Chater's (C&C's) proposals. We endorse C&C's call for an integrated framework but argue that their emphasis on local, individual production and comprehension makes it difficult to accommodate the ubiquitous, interactive, and defeasible processes of clarification and repair in conversation."
6c56137ffb008ee5f94a482e0c74e494d7f7bc04,We sketch the basis of a categorical compositional distributional semantic approach to the analysis of verb phrase ellipsis.
89f329347735cb92a3698385e1f86f56f400bbdf,
9e3b7e27b11b4ca0662ffc4c9cd8da6ccf23457f,"This paper presents and evaluates a novel system for computer generated poetry. Framed within contemporary theoretical trends in the evaluation of computational creativity, we investigate how evidence of generative process influences readers’ opinions of computer generated textual output. In addition to a technical description of our system, we present results from a study asking respondents to evaluate short computer generated poems prefaced with different types of descriptions, in some cases objectively presenting the poem as the product of a statistical analysis of corpora and in some cases subjectively presenting the computer as a self-aware agent."
b243751d26ae4d1a22f237ef07e9c3038b0c73dd,"Single pulses preserve information about the pulsar radio emission and propagation in the pulsar magnetosphere, and understanding the behaviour of their variability is essential for estimating the fundamental limit on the achievable pulsar timing precision. Here we report the findings of our analysis of single pulses from PSR J1713+0747 with data collected by the Large European Array for Pulsars (LEAP). We present statistical studies of the pulse properties that include distributions of their energy, phase and width. Two modes of systematic sub-pulse drifting have been detected, with a periodicity of seven and three pulse periods. The two modes appear at different ranges of pulse longitude but overlap under the main peak of the integrated profile. No evidence for pulse micro-structure is seen with a time resolution down to 140 ns. In addition, we show that the fractional polarization of single pulses increases with their pulse peak flux density. By mapping the probability density of linear polarization position angle with pulse longitude, we reveal the existence of two orthogonal polarization modes. Finally, we find that the resulting phase jitter of integrated profiles caused by single pulse variability can be described by a Gaussian probability distribution only when at least 100 pulses are used for integration. Pulses of different flux densities and widths contribute approximately equally to the phase jitter, and no improvement on timing precision is achieved by using a subset of pulses with a specific range of flux density or width."
b35d14143e7cae0ee43102acf719c80f7f8a507e,"This paper presents a geometric approach to the problem of modelling the relationship between words and concepts, focusing in particular on analogical phenomena in language and cognition. Grounded in recent theories regarding geometric conceptual spaces, we begin with an analysis of existing static distributional semantic models and move on to an exploration of a dynamic approach to using high dimensional spaces of word meaning to project subspaces where analogies can potentially be solved in an online, contextualised way. The crucial element of this analysis is the positioning of statistics in a geometric environment replete with opportunities for interpretation."
de148c245c6378e0814b25f2a98a5ad9cb434421,"In this paper, we present a novel application of a computational model of word meaning to capture human judgments of the linguistic properties of metaphoricity, familiarity, and meaningfulness. We present data gathered from human subjects regarding their ratings of these properties over a set of word pairs specifically designed to exhibit varying degrees of metaphoricity. We then investigate whether these properties can be measured in terms of geometric features of a model of distributional lexical semantics. We compare the performance of two models, our own Concept Discovery Model which dynamically constructs context-sensitive subspaces, and a state-of-the-art static distributional semantic model, and find that our dynamic model performs significantly better in its measurement of metaphoricity."
f5a4d71b9afe858d57cc3af7b5fc86c03f0c34b7,
054189bfc692bc0f3313b0d6733970d3a4d8da97,
068b3283deef4f397845aaa9e742962e13124670,"We present computational experiments on language segmentation using a general information-theoretic cognitive model. We present a method which uses the statistical regularities of language to segment a continuous stream of symbols into “meaningful units” at a range of levels. Given a string of symbols—in the present approach, textual representations of phonemes—we attempt to find the syllables such as grea and sy (in the word greasy); words such as in, greasy, wash, and water ; and phrases such as in greasy wash water. The approach is entirely information-theoretic, and requires no knowledge of the units themselves; it is thus assumed to require only general cognitive abilities, and has previously been applied to music. We tested our approach on two spoken language corpora, and we discuss our results in the context of learning as a statistical processes."
30f2ac6b6fe07ccc074dccf62e63efbfb9c878e5,"This paper puts forth a method for discovering computationally-derived conceptual spaces that reflect human conceptualization of musical and poetic creativity. We describe a lexical space that is defined through co-occurrence statistics, and compare the dimensions of this space with human responses on a word association task. Participants’ responses serve as external validation of our computational findings, and frequent terms are also used as input dimensions for creating mappings from the linguistic to the conceptual domain. This novel method finds low-dimensional subspaces that represent particular conceptual regions within a vector space model of distributional semantics. Word-vectors from these discovered conceptual spaces are considered, and argued to be useful for the evaluation of creativity and creative artifacts within computational creativity."
4b419d35724b46d5c65ed2db37683108b57607ed,"This study explores the use of English colour names in large datasets from informal Twitter messages and the well-structured corpus of Google Books. Because colour names in text have no directly associated chromatic stimuli, the corresponding colour categories of colour words was assessed from responses in an online colour naming experiment. A comparison of the frequency in the three datasets revealed that the mapping of colour names to perceptually uniform colour spaces does not reflect natural language colour distributions."
6697ba1d690d2f3b03f394121d06432306e5fbac,"Taylor, S.R.; Mingarelli, C.M.F.; Gair, J.R.; Sesana, A.; Theureau, G.; Babak, S.; Bassa, C.G.; Brem, P.; Burgay, M.; Caballero, R.N.; Champion, D.J.; Cognard, I.; Desvignes, G.; Guillemot, L.; Hessels, J.W.T.; Janssen, G.H.; Karuppusamy, R.; Kramer, M.; Lassus, A.; Lazarus, P.; Lentati, L.; Liu, K.; Osłowski, S.; Perrodin, D.; Possenti, A.; Purver, M.B.; Rosado, P.A.; Sanidas, S.A.; Smits, R.; Stappers, B.; Tiburzi, C.; van Haasteren, R.; Vecchio, A.; Verbiest, J.P.W."
74691c5f2b979c0fb12d36d38d266b5dbdca8ff6,
7915a9d73f241b47c55e35d0bdc59b80a96332e1,
83bee08ca267c0e7a20bf013704086765cd732a6,"Zellig Harris proposed a method for grouping phonemes in an utterance into morphemes by simply using counts of each of the phonemes in a corpus relative to their position in sequences contained in the data set. Thus, using an n-gram model, one can model this process and see whether a computational model can actually group representations of phonemes into segments which correspond to morphemes. Here, we use a general n-gram modelling tool created for melodic grouping in music corpora and apply it to a natural language data set. We show that this method which approximates Harris’s can indeed find morphemes in a given language corpus by calculating the distributions of phonemes across a corpus."
8e5833fccaa01e66d29b6c158dd57118b434bb56,"Can structural priming help to explain language processing in dialogue? Dialogue is the environment in which language is first encountered, acquired and used and arguably represents the most important challenge for models of language processing. The literature on structural priming provides a large body of evidence that exposure to a particular syntactic structure, in production or comprehension, makes individuals more likely to repeat that structure. However, the strongest evidence for these effects comes from experimental studies of individual language processing, i.e. by individual participants who are not actively engaged with a conversational partner. In a typical experimental paradigm, participants are exposed to sequences of specific types of sentences. For example, participants are presented with sentences to read in one of two forms, such as a prepositional object (”A rock star sold some cocaine to an undercover agent.”) or double object construction (”A rock star sold an undercover agent some cocaine.”). The form of the prime sentence influences the form of an unrelated sentence produced on a subsequent picture description task (Bock, 1986). It has become common to claim that the structural priming effects found in these studies extend to dialogue, and even provide the basis for successful communication (Pickering and Garrod, 2004; Pickering and Ferreira, 2008; Jaeger and Snider, 2013, and the call for papers for this special issue a.o.). However, there are problems with this generalisation. Common experimental paradigms such as picture description tasks significantly restrict the naturalness of the interaction by limiting people’s responses (Branigan et al., 2000, 2006) and face the difficulties associated with using confederates in dialogue experiments (Kuhlen and Brennan, 2013). Corpus studies which are cited to support for the idea that syntactic priming is prevalent in naturally occurring dialogue either also focus on a few specific structures (Gries, 2005; Szmrecsanyi, 2005), or if they have broader coverage in terms of syntactic structures often focus on task-based dialogues (Reitter et al., 2006). Healey et al. (2014) examined patterns of repetition in unrestricted natural dialogues and found that when the effects of lexical repetition are taken into account people systematically avoid using the same constructions in the next turn i.e. a pattern of structural divergence. They concluded that in natural dialogue structural priming effects are overwhelmed by the demands of constructive engagement with a conversational partner. Repetition alone would quickly bring ordinary conversation to a halt, preventing conversations from moving forward. Productive conversation involves people providing answers to questions, responding to proposals, and, of course, by changing topics and adding new material. According to Healey et al. this leads to the reuse of lexical items in contrasting syntactic contexts."
a5d1eb48afb86b60544836fda3df595a73ece972,
a7f1308d6046da6f5c2762f7d2ca189396e455ce,"People tend to avoid exposed disagreement in conversation. This is normally attributed to politeness strategies that mitigate the potential face-threat created by direct disagreement with a conversational partner. In reported speech the pressure for mitigation of negative responses is removed, leading to the prediction that reported speech should contain more exposed disagreement. However, concerns about self-presentation may lead people to present their prior behaviour in such a way that demonstrates their understanding that disagreement is a sensitive matter; thus, differences in self-reported and other-reported disagreement would be anticipated. Finally, we predict that reported speech is used to highlight substantive differences in stance, and contains more explicit markers of stance to highlight newsworthiness. To test these ideas we compare the distribution of markers of agreement, disagreement and stance in four samples of conversation from the BNC: direct speech, self-reported speech (I said), other-reported speech (he / she said) and local dialogue context. Contrary to the prediction the results show that both direct and indirect markers of agreement and disagreement are more common in direct speech than reported speech. However, markers of contrast and emphasis including negations, swearwords and contrastive conjuncts are both more common in reported speech than direct speech and in self-reported speech than other-reported speech."
bf589b3345edeb8dfe6fbab5e75cb8d01ff2972a,"Abstract We investigate the relationship between lexical spaces and contextually-defined conceptual spaces, offering applications to creative concept discovery. We define a computational method for discovering members of concepts based on semantic spaces: starting with a standard distributional model derived from corpus co-occurrence statistics, we dynamically select characteristic dimensions associated with seed terms, and thus a subspace of terms defining the related concept. This approach performs as well as, and in some cases better than, leading distributional semantic models on a WordNet-based concept discovery task, while also providing a model of concepts as convex regions within a space with interpretable dimensions. In particular, it performs well on more specific, contextualized concepts; to investigate this we therefore move beyond WordNet to a set of human empirical studies, in which we compare output against human responses on a membership task for novel concepts. Finally, a separate panel of judges rate both model output and human responses, showing similar ratings in many cases, and some commonalities and divergences which reveal interesting issues for computational concept discovery."
c0bcdfc1136ca9e31f54b3d3de2509f002a339fd,"In conversation, interlocutors routinely indicate whether something said or done has been processed and integrated. Such feedback includes backchannels such as ‘okay’ or ‘mhm’, the production of a next relevant turn, and repair initiation via clarification requests. Importantly, such feedback can be produced not only at sentence/turn boundaries, but also sub-sententially. In this paper, we extend an existing model of incremental semantic processing in dialogue, based around the Dynamic Syntax (DS) grammar framework, to provide a low-level, integrated account of backchannels, clarification requests and their responses; demonstrating that they can be accounted for as part of the core semantic structure-building mechanisms of the grammar, rather than via higher level pragmatic phenomena such as intention recognition, or treatment as an “uno cial” part of the conversation. The end result is an incremental model in which words, not turns, are seen as procedures for contextual update and backchannels serve to align participant semantic processing contexts and thus ease the production and interpretation of subsequent conversational actions. We also show how clarification requests and their following responses and repair can be modelled within the same DS framework, wherein the divergence and re-alignment e ort in participants’ semantic processing drives conversations forward."
c3552ed15bebfca5bd902e9a099d9774f7616b93,
c96d86b044234ad9551587fb247f8044fc97fe3c,"Previous research has shown that political leanings correlate with various psychological factors. While surveys and experiments provide a rich source of information for political psychology, data from social networks can offer more naturalistic and robust material for analysis. This research investigates psychological differences between individuals of different political orientations on a social networking platform, Twitter. Based on previous findings, we hypothesized that the language used by liberals emphasizes their perception of uniqueness, contains more swear words, more anxiety-related words and more feeling-related words than conservatives’ language. Conversely, we predicted that the language of conservatives emphasizes group membership and contains more references to achievement and religion than liberals’ language. We analysed Twitter timelines of 5,373 followers of three Twitter accounts of the American Democratic and 5,386 followers of three accounts of the Republican parties’ Congressional Organizations. The results support most of the predictions and previous findings, confirming that Twitter behaviour offers valid insights to offline behaviour."
caa27456b4c7931b25f159436635e5fe27a07d02,"Disagreement is understood to be socially problematic; it also rarely surfaces in naturally occurring conversation. An experiment was designed to allow us to directly manipulate the occurrence of exposed (dis)agreement and track its effects on the subsequent dialogue. This is the first experiment to directly manipulate the occurrence of exposed agreement and disagreement in dialogue. Insertions of exposed disagreement disrupt dialogues, bringing the topic of disagreement directly into the conversation, provoking clarification requests and resulting in a greater number of self-edits when formulating turns. The insertion of disagreement also led to more instances of exposed agreement, suggesting that dialogue partners co-operate to redress the face-threat of disagreement. Conversely, exposed agreement insertions were not as incongruous and had less disruptive impact on the ensuing dialogues; however, introducing agreement into the dialogue did lead to greater deliberation, with more alternative scenarios considered by participants during the task."
edfb07a6da72ac676b8592f18461e46f8363ebe2,"Millisecond pulsars (MSPs) are known as highly stable celestial clocks. Nevertheless, recent studies have revealed the unstable nature of their integrated pulse profiles, which may limit the achievable pulsar timing precision. In this article, we present a case study on the pulse-profile variability of PSR J1022+1001. We have detected approximately 14 000 subpulses (components of single pulses) in 35-h long observations, mostly located in the trailing component of the integrated profile. Their flux densities and fractional polarization suggest that they represent the bright end of the energy distribution in ordinary emission mode and are not giant pulses. The occurrence of subpulses in the leading and trailing components of the integrated profile is shown to be correlated. For subpulses from the latter, a preferred pulse width of approximately 0.25 ms has been found. Using simultaneous observations from the Effelsberg 100-m telescope and the Westerbork Synthesis Radio Telescope, we have found that the integrated profile varies on a time-scale of a few tens of minutes. We show that improper polarization calibration and diffractive scintillation cannot be the sole reason for the observed instability. In addition, we demonstrate that timing residuals generated from averages of the detected subpulses are dominated by phase jitter and we place an upper limit of similar to 700 ns on jitter noise, based on continuous 1-min integrations."
fe044446626c007e59522d42bb85600bf6abf8a9,This paper seeks to situate the computational modelling of metaphor within the context of questions about the relationship between the meaning and use of language. The results of this pragmatic assessment are used as the theoretical basis for a proposed computational implementation that seeks metaphor in the geometry of a vector space model of distributional semantics. This statistical approach to the analysis and generation of metaphor is taken as a platform for a consideration of the fraught relationship between computational models of cognitive processes and the study of consciousness.
027d1b5a888aa54420a1f48789ba1d68d02ded05,"Howes was supported by the EPSRC-funded 
PPAT project grant number EP/J501360/1 during this work. Hough is supported by the DUEL project financially supported by the Agence Nationale de la Research (grant number ANR-13-FRAL-0001) and the Deutsche Forschungsgemainschaft. Much of the work was carried out under an EPSRC DTA scholarship at Queen Mary University of London. Purver is partly supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET grant number 611733."
0ab972317105871858f41a631e25fdcd2e2b1d7d,"This paper seeks to situate computational creativity in relation to philosophy and in particular philosophy of mind. The goal is to investigate issues relevant to both how computational creativity can be used to explore philosophical questions and how philosophical positions, whether they are accepted as accurate or not, can be used as a tool for evaluating computational creativity. First, the possibility of symbol manipulating machines acting as creative agents will be examined in terms of its ramifications for historic and contemporary theories of mind. Next a philosophically motivated mechanism for evaluating creative systems will be proposed, based on the idea that an intimation of dualism, with its inherent mental representations, is a thing that typical observers seek when evaluating creativity. Two computational frameworks that might adequately satisfy this evaluative mechanism will then be described, though the implementation of such systems in a creative context is left for future work. Finally, the kind of audience required for the type of evaluation proposed will be briefly discussed."
1ffac56810c299cf8e54a1898d9ceaf77972e6f0,
2b77deccf50b9de70c4f0433966101cde5e727f9,"In this study we work with a multi-arts organisation to assess the appropriateness of natural language processing methods for the analysis of audience behaviours and interests via Twitter. We investigate supervised and unsupervised topic modelling methods to investigate whether they can: i) capture the nuanced differences between art genres/forms; ii) characterise users according to their cultural interests; and iii) help to surface the more culturally mobile members of the audience. We show promising results: supervised methods showed high accuracy (95%) in filtering data for relevance and pre-defined topics; unsupervised methods provided novel topic discovery, showing sophisticated genre groupings and discovering audience members with wider interests."
2e91bc4b98c36056c479f6d2b308667d75dcd91c,"One of the best known claims about human communication is that people's behaviour and language use converge during conversation. It has been proposed that these patterns can be explained by automatic, cross-person priming. A key test case is structural priming: does exposure to one syntactic structure, in production or comprehension, make reuse of that structure (by the same or another speaker) more likely? It has been claimed that syntactic repetition caused by structural priming is ubiquitous in conversation. However, previous work has not tested for general syntactic repetition effects in ordinary conversation independently of lexical repetition. Here we analyse patterns of syntactic repetition in two large corpora of unscripted everyday conversations. Our results show that when lexical repetition is taken into account there is no general tendency for people to repeat their own syntactic constructions. More importantly, people repeat each other's syntactic constructions less than would be expected by chance; i.e., people systematically diverge from one another in their use of syntactic constructions. We conclude that in ordinary conversation the structural priming effects described in the literature are overwhelmed by the need to actively engage with our conversational partners and respond productively to what they say."
3795fd6725cfe23e31543185c841680ca3c5ebdb,"This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings 
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/"
3f3d110cf78f759760c354bfde1b2ceb9883c544,"We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks."
54f6fd23d7becafd0278aa3497feec2baa5ca173,
5cadfff8069997715c5274931c6891b54fbafcd8,"In this chapter, we examine the phenomenon of compound contributions (CCs) and the implications for NLG in interactive systems. Compound contributions are contributions in dialogue which continue or complete an earlier contribution, thus resulting in a single syntactic or semantic (propositional) unit built across multiple contributions, provided by one speaker or more than one (1.1). The term as used here therefore includes more specific cases that have been referred to as expansions, (collaborative) completions, and split or shared utterances."
62c1f7b48e835508fda0d387e108c1bd51427cf4,"There is little attention given to forgetfulness in a healthy population. However, forgetfulness is not only associated with feelings of embarrassment and shame, but is also a cause for concern when it begins to affect our daily lives. Many people describe it as having an 'off' day. We explore augmenting everyday objects to assist us in our daily routines, ultimately to examine the question: Can a smart object alleviate those negative feelings and lead us to a less stressful life?"
67207fe72787579e21b6106e48be705aad3083f9,"Mental illnesses such as depression and anxiety are highly prevalent, and therapy is increasingly being offered online. This new setting is a departure from face-toface therapy, and offers both a challenge and an opportunity ‐ it is not yet known what features or approaches are likely to lead to successful outcomes in such a different medium, but online text-based therapy provides large amounts of data for linguistic analysis. We present an initial investigation into the application of computational linguistic techniques, such as topic and sentiment modelling, to online therapy for depression and anxiety. We find that important measures such as symptom severity can be predicted with comparable accuracy to face-to-face data, using general features such as discussion topic and sentiment; however, measures of patient progress are captured only by finergrained lexical features, suggesting that aspects of style or dialogue structure may also be important."
7d1c24d7b2d6b1b20da1108dab3921b5b43497dc,"New Media has not only revolutionized the way art is made but also how it is presented and curated. In this context, Virtual Reality offers exciting new possibilities, which can be encapsulated as 3D interactive features for the Web. We present two projects along these lines, and describe their effects on user engagement and interaction."
834f47ac856b4e541c57af65693ff6a1f55f5d4a,"While there has been substantial work on referential communication tasks in psycholinguistics, computational and formal modelling (Dale and Reiter (1995), Krahmer and Van Deemter (2012), Frank and Goodman (2012) inter alia), the element we discuss here is incremental processing. Motivated by work in incremental generation of referring expressions (Guhe, 2007; Fernandez, 2013) and incremental reference resolution in NLU (Kennington and Schlangen, 2014), we present a dialogue-motivated account which models the speaker and the hearer in reference identification games. A central desideratum of an incremental account of reference identification tasks can be found in the evidence from Brennan and Schober (2001)’s experiments; namely that people reason at an incredibly time-critical level from linguistic information. They demonstrated selfrepair can speed up semantic processing (or at least object reference) where an incorrect object being partly vocalized and then repaired in the instructions (e.g. “the yell-, uh, purple square”) yields quicker response times from the onset of the target (“purple”) than in the case of the fluent instructions (“the purple square”), with little effect on accuracy. We wish to model this faculty of repair processing, and also wish to model non-local repair processing of instructions such as “From yellow down to brown – no – thats red.” (Levelt, 1989, via Ginzburg et al. (2014)), here using a syntactically simpler but illustrative alternative “the yellow square, no, purple”. We build on Hough and Purver (2014)’s integration of Knuth (2005)’s lattice-theoretic characterization of probabilistic inference to model interpretation of repaired instructions in a small reference domain."
88636b47b79985ded833930b876e689ae7f91947,"We present an adaptation of recent work on probabilistic Type Theory with Records (Cooper et al., 2014) for the purposes of modelling the incremental semantic processing of dialogue participants. After presenting the formalism and dialogue framework, we show how probabilistic TTR type judgements can be integrated into the inference system of an incremental dialogue system, and discuss how this could be used to guide parsing and dialogue management decisions."
9491edf2a844e5d7e2481b7f5ab389a111f32de7,
9cb40382e1e7ae57fbb4dc8daa8100a757ea0183,
c626757e5cbb22180d2cd2ad5182876dd2ec15f2,
e39604527bcfe296b3e6b39a38e9c95e50e3c928,"We present STIR (STrongly Incremental Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs. Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards."
f6fd84f9eb41bb8a4e4d2ae4fe1339258e271bc0,"This paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification. In contrast to the widely used bag-ofwords approach, we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication. We investigate the contribution of word sequence, dialogue act sequence, and distributional information to the performance, and compare with the current state of the art approaches. Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence- or grammar-driven, such as categorical, word vector composition) are required."
11a680b574438ced8610a696094ba4c19684045e,"New forms of art have developed because of the possibilities that modern technology provides for artists. However, innovative technology can not only be used in the creation of new media art but also sets a range of new opportunities for the presentation and communication of visual art. In this paper, we want to introduce an approach for the presentation of visual artworks in interactive virtual environments. In this work, we showcase a project that focuses on the problem of exploring and implementing innovative approaches and technologies for displaying art in a 3D virtual environment. We wish to excite people about art by offering them an entertaining and educational virtual experience. Furthermore, we want to contribute towards escaping from the common exhibition space by creating a virtual application that augments the presented artwork by offering an interactive audience experience."
1497fcca00555ef5198531e153ebab974e3b15c7,"Information & communications Technology (ICT) have been increasingly important players in the healthcare industry. Use of ICT in healthcare started with monitoring devices such as heart rate sensors and alarms. Other computing and communication devices include pagers, networked monitoring stations, remote operation equipment and electronic patient records. An important theme, especially with relevance to individual monitoring, has been the use of sensors [18]. Body Sensor Networks {BSN} have been developed over the last decade as part of a large number of projects to monitor the health of individuals and their surrounding environment (e.g., by monitoring their CO2 level [13])."
198e826d307d41cf2bfa44604e1477c8b79158e9,"Social situations can be difficult for shy people. Computermediated communication (CMC) technologies have been reported as helpful to shy users, but can also encourage withdrawal from face-to-face activities. The current research investigates the benefits of ubiquitous computing and slow technology by designing wearable computing devices which can be integrated into face-to-face activities and help increase comfort for shy users. Initial proof-of-concept prototypes have shown potential; testing with self-reported socially anxious subjects in a speed-dating environment resulted in 73% identifying the shirts as helpful to their meeting with strangers."
2252da2c872306e36b92ddcf27ba378dcebfce6c,2 Introduction 2 2.1 Dynamic Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1.1 Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1.2 The parsing process [3] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1.3 Graph representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.4 Parsing in Context [3.2] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.5 Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Integrating Type-Theory with Records (TTR) . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Dialogue System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3.1 Mapping TTR record types to domain concepts incrementally . . . . . . . . . 10
3fdcf000f208984fc6a2f419b4cb9a4206436b3f,"Overview Incremental processing of both syntax and semantics, both in parsing and generation, is of significant interest for modelling the human language capability, and for build ing systems which interact with it. Formal linguistics has made significant contributions to this; one example is th framework Dynamic Syntax, which provides an inherently word-by-word incremental gr ammatical framework. However, making this practical for computational models or systems involves building grammars with broad coverage on real data – a significant challenge. Here, we describe a me thod for inducing such a grammar from a corpus in which sentences are paired with semantic logical forms. By taking a probabilistic view, we hypothesise possible lexical entries – including entries for anaphoric e lements – and learn a lexicon from their observed distributions without requiring annotation at the word le vel. The resulting grammar provides a resource for incremental semantic processing with good cove rage, while learning grammatical constraints similar to a hand-crafted version."
40cc1ecac23272d9ac63bbaeb2e08c61ac9dbe82,"We describe a method for learning an incremental semantic grammar from a corpus in which sentences are paired with logical forms as predicate-argument structure trees. Working in the framework of Dynamic Syntax, and assuming a set of generally available compositional mechanisms, we show how lexical entries can be learned as probabilistic procedures for the incremental projection of semantic structure, providing a grammar suitable for use in an incremental probabilistic parser. By inducing these from a corpus generated using an existing grammar, we demonstrate that this results in both good coverage and compatibility with the original entries, without requiring annotation at the word level. We show that this semantic approach to grammar induction has the novel ability to learn the syntactic and semantic constraints on pronouns."
4688f4f9167a28b5f1db1f83b16b977eba237281,"Forgetfulness can be a cause for concern when it begins affecting our daily lives. Forgetfulness is associated with feelings of embarrassment and shame [1] and yet there is little attention given to forgetfulness in a healthy population. Forgetfulness is a lived experience and something that happens in our day to day. Therefore we propose the ""message bag"", which will be carried throughout regular daily activities, with an aim to alleviate the cognitive load, in an effort to eliminate forgetfulness. We describe a prototype for a device that will be tested in the wild."
5492b2d3c88b2d60062d3d605bd820fd31838fe8,"Previous research shows that aspects of doctor-patient communication in therapy can predict patient symptoms, satisfaction and future adherence to treatment (a significant problem with conditions such as schizophrenia). However, automatic prediction has so far shown success only when based on low-level lexical features, and it is unclear how well these can generalize to new data, or whether their effectiveness is due to their capturing aspects of style, structure or content. Here, we examine the use of topic as a higher-level measure of content, more likely to generalize and to have more explanatory power. Investigations show that while topics predict some important factors such as patient satisfaction and ratings of therapy quality, they lack the full predictive power of lower-level features. For some factors, unsupervised methods produce models comparable to manual annotation."
664bfc3c1b58537bdc861af42098406af55aaa82,"This paper describes a statistical corpus study of self-repairs in the disfluencyannotated Switchboard corpus which examines the time-linear nature of self-repair processing for annotators and listeners in dialogue. The study suggests a strictly local detection and processing mechanism for self-repairs is sufficient, an advantage currently not used effectively under the bonnet of state-of-the-art automatic disfluency processing. We then show how simple local fluency measures using modified language models can be strongly indicative of repair onset detection, and how simple information theoretic measures could characterize different classes of repairs."
8726b8e4afb77598eb37ef444f9f6bc245ce865f,"Previous research shows that aspects of doctor-patient communication in therapy can predict patient symptoms, satisfaction and future adherence to treatment (a significant problem with conditions such as schizophrenia). However, automatic prediction has so far shown success only when based on low-level lexical features, and it is unclear how well these can generalise to new data, or whether their effectiveness is due to their capturing aspects of style, structure or content. Here, we examine the use of topic as a higher-level measure of content, more likely to generalise and to have more explanatory power. Investigations show that while topics predict some important factors such as patient satisfaction and ratings of therapy quality, they lack the full predictive power of lower-level features. For some factors, unsupervised methods produce models comparable to manual annotation."
9179f1a8d10fc5cf26504f82423cb3ce2f11de7c,"We describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning. Working in an inherently incremental framework, Dynamic Syntax, we show how words can be associated with probabilistic procedures for the incremental projection of meaning, providing a grammar which can be used directly in incremental probabilistic parsing and generation. We test this on child-directed utterances from the CHILDES corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax."
9d07226e84d117f890d697a9b3230044b0624334,"Social media have substantially altered the way brands and businesses advertise: Online Social Networks provide brands with more versatile and dynamic channels for advertisement than traditional media (e.g., TV and radio). Levels of engagement in such media are usually measured in terms of content adoption (e.g., likes and retweets) and sentiment, around a given topic. However, sentiment analysis and topic identication are both non-trivial tasks. In this paper, using data collected from Twitter as a case study, we analyze how engagement and sentiment in promoted content spread over a 10-day period. We nd that promoted tweets lead to higher positive sentiment than promoted trends; although promoted trends pay o in response volume. We observe that levels of engagement for the brand and promoted content are highest on the rst day of the campaign, and fall considerably thereafter. However, we show that these insights depend on the use of robust machine learning and natural language processing techniques to gather focused, relevant datasets, and to accurately gauge sentiment, rather than relying on the simple keyword- or frequency-based metrics sometimes used in social media research."
eeee15bfa0bb32d56bb324844cb03f49a09706a9,This paper discusses a design consideration and method in the early stage of development of Vibrosign: a wearable vibrotactile display used to facilitate social interaction in networking events. A pilot test was conducted to observe user’s ability to perceive and interpret meanings conveyed by vibratory stimuli. The test result informs viable techniques that lead to modifications of prototype and required parameters needed in an upcoming informative experiment.
00ca511358dbc33eca25187973743ad3e519654f,"Recent work on consultations between out-patients with schizophrenia and psychiatrists has shown that adherence to treatment can be predicted by patterns of repair -- specifically, the pro-activity of the patient in checking their understanding, i.e. patient clarification. Using machine learning techniques, we investigate whether this tendency can be predicted from high-level dialogue features, such as backchannels, overlap and each participant's proportion of talk. The results indicate that these features are not predictive of a patient's adherence to treatment or satisfaction with the communication, although they do have some association with symptoms. However, all these can be predicted if we allow features at the word level. These preliminary experiments indicate that patient adherence is predictable from dialogue transcripts, but further work is necessary to develop a meaningful, general and reliable feature set."
095ccdd4b9ac9be1530dca6fd6c967005c73dfc3,"Finishing each other’s . . . Responding to incomplete contributions in dialogue Christine Howes, Patrick G. T. Healey, Matthew Purver, Arash Eshghi {chrizba, ph, mpurver, arash}@eecs.qmul.ac.uk Queen Mary University of London Interaction, Media and Communication Research Group School of Electronic Engineering and Computer Science, London E1 4NS, UK Abstract Hypothesis 1 Cross-person completions likely at transition relevance places A distinguishing feature of dialogue is that contributions can be fragmentary or incomplete. Such incomplete ut- terances may be later completed by another interlocu- tor. These cross-person compound contributions (CCs) have been hypothesised to be more likely in predictable contexts but the contributions of diﬀerent sources of pre- dictability has not been systematically investigated. In this paper we present an experiment which artiﬁcially truncates genuine contributions in ongoing text-based dialogues, to investigate the eﬀects of lexical, syntac- tic and pragmatic predictability of the truncation point on the likelihood of one’s interlocutor supplying a con- tinuation. We show that what is critical is the actual and presumed accessibility of common ground, and that while people are sensitive to syntactic predictability, this alone is insuﬃcient to prompt a completion. Keywords: Dialogue; compound contributions; com- mon ground. are more Second, completions should tend to occur at syntacti- cally projectable points (e.g. compound turn construc- tional units Lerner, 1991). Hypothesis 2 Cross-person completions are likely when they are syntactically predictable. more A third source of predictability comes from the degree to which the speaker and hearer share, or can be assumed to share, common ground relevant to the CC. If the topic of the utterance is already in the common ground then the content of the completion is more predictable. Hypothesis 3 Cross-person completions are more likely when they address topics that are part of the common ground. Introduction The eﬀects of these diﬀerent forms of predictability are directly tested here for the ﬁrst time using a text chat experiment performed with the DiET experimental platform. The evidence points towards shared knowledge being a key factor with other sources of predictability also contributing. It is well known that contributions to dialogue are often fragmentary or in some sense unﬁnished Fern´andez and Ginzburg (2002). These incomplete utterances may be subsequently completed, either by the original speaker following some response or interruption from an inter- locutor, or, by another person (Purver et al., 2009). These compound contributions (CCs) are a paradig- matic feature of dialogue, and cross-person CCs in par- ticular are a key indicator of coordination between in- terlocutors. Although naturally occurring cross-person CCs and their interpretations have been studied (Lerner, 1996; Purver et al., 2009), there has not previously been a systematic, experimental, attempt to investigate the factors that inﬂuence how a completion for an incom- plete utterance may be produced. Intuitively, people’s willingness to ﬁnish another person’s incomplete utter- ance will depend (at least) on how predictable the rest of the utterance is. There are several sources of possible predictability. Expansions are CCs which add material (e.g. an ad- junct) to an already complete syntactic element; com- pletions are CCs which complete an incomplete element. Conversation analytic (CA) discussions of CCs suggest that they should preferably occur at transition relevance places (TRPs), points that are foreseeable by the partic- ipants. Expansions are CCs with split points at TRPs, and are more common in spoken dialogue (Howes et al., 2011) so ought to be more likely than completions. Method In this experiment, to see what factors inﬂuence how people respond to unﬁnished turns and their likelihood of producing a continuation, a number of genuine single contributions in dyadic text-based conversations were ar- tiﬁcially split into two parts, using the DiET chat tool. The DiET chat tool The Dialogue Experimental Toolkit (DiET) chat tool is a text-based chat interface into which interventions can be introduced into a dialogue in real time. These in- terventions can take a number of forms; turns may not be relayed, additional turns may be added, as in Healey et al. (2003), in which spoof clariﬁcation requests are added to the dialogue, or turns may be altered prior to transmission. As these manipulations occur as the dia- logue progresses, they cause a minimum of disruption to the ‘ﬂow’ of the conversation. The DiET chat tool is a custom built Java applica- tion, consisting of two main components: the server con- sole and the user interface. The server time-stamps and"
5d3be0136beda6414bf84b982c51c9dd0672c3cd,"We introduce a method for data-driven learning of lexical en tri s in an inherently incremental semantic grammar formalism, Dyn amic Syntax (DS). Lexical actionsin DS are constrained procedures for the incremental projec ti n of compositional semantic structure. Here, we show how thes e can be induced directly from sentences paired with their complete proposi ti nal semantic structures. Checking induced entries over an artificial dataset g enerated using a known grammar demonstrates that the method learns lexical entrie s compatible with those defined by linguists, with different versions of the DS framework induced by varying only general tree manipulation rules. This is ach ieved without requiring annotation at the level of individual words, via a method compatible with work on linguistic change and routinisation."
72afd497dde40d0e917f99ceeaf67f5b6618307b,"Repair is crucial in achieving and maintaining shared understanding in dialogue. Recent work on consultations between patients with schizophrenia and psychiatrists has shown that adherence to treatment correlates with patterns of repair. We show that distributions of repair in consultation dialogues are different to those in general conversation.We investigate whether particular types of repair can be detected from highlevel dialogue features and/or lexical content, with encouraging results. We further explore whether we can predict adherence directly from these features. The results indicate that prediction appears to be possible from low-level lexical content."
7493438a29f4372b616b50562123ee50296444a4,"We present a novel incremental approach to modelling self-repair phenomena in dialogue, using the grammar and parsing mechanism of Dynamic Syntax (DS) to construct Type Theory with Records (TTR) record type representations incrementally in both parsing and generation. We demonstrate how a DS-TTR hybrid implementation when integrated into an incremental dialogue system can be exploited to account for the semantic processing of selfrepair phenomena in a unified way and in line with psycholinguistic evidence."
b1cf302f952cf4c074055435ba9721c7838ab26a,"We describe a set of experiments using automatically labelled data to train supervised classifiers for multi-class emotion detection in Twitter messages with no manual intervention. By cross-validating between models trained on different labellings for the same six basic emotion classes, and testing on manually labelled data, we conclude that the method is suitable for some emotions (happiness, sadness and anger) but less able to distinguish others; and that different labelling conventions are more suitable for some emotions than others."
c6364f3467584bf54436d2d2e8140e027d659166,"Whose turn is it anyway? Same- and cross-person compound contributions in dialogue Christine Howes, Patrick G. T. Healey, Matthew Purver {chrizba, ph, mpurver}@eecs.qmul.ac.uk Queen Mary University of London Interaction, Media and Communication Research Group School of Electronic Engineering and Computer Science London E1 4NS, UK Abstract In natural conversation people sometimes build larger grammatical, semantic and pragmatic units out of mul- tiple turns or installments. The incremental and col- laborative character of these ‘compound contributions’ presents challenges for theories of natural language pro- cessing. Compounds produced over successive turns by one person have often been analysed in essentially the same way as compounds produced by multiple people. In some recent accounts this putative equivalence has been taken as evidence for the claim that within- and cross-person language processing are fundamentally in- terchangeable. However, in this paper we present an analysis of compound contributions in a corpus of or- dinary dialogues which shows that same- and cross- person compound contributions are constructed in dif- ferent ways and have diﬀerent semantic and pragmatic eﬀects on the organisation of dialogue. In particular, we show that they diﬀer in the pragmatic environments in which they occur and that they have diﬀerent conse- quences for subsequent turn-taking and interpretation. This asymmetry highlights the need for models of dia- logue that account for not just the inherent incremen- tality of dialogue, but the diﬀerent status of each con- tributor towards a turn-in-progress. Keywords: Dialogue; compound contributions. Introduction Compound contributions (CCs) – dialogue contributions that continue or complete an earlier contribution, see e.g. (1) – are the paradigm case of coordination in dialogue and constitute a critical test case for theories of natural language processing. (1) Daughter: Dad: Daughter: Oh here dad, a good way to get those corners out is to stick yer ﬁnger inside. well, that’s one way. [from Lerner (1991)] CCs are of interest to dialogue theorists because they provide evidence about how contributions can cohere with each other at multiple levels – syntactic, semantic and pragmatic (though of course they are not the only way). They also indicate the radical context-dependency of conversational contributions, which can, in general, be highly elliptical without disrupting the ﬂow of the dia- logue. CCs are a dramatic illustration of this: speakers must rely on the dynamics of the unfolding context (lin- guistic and extra-linguistic) in order to guarantee suc- cessful processing and production. Much of the work on CCs has studied cross-person cases, in diﬀerent disciplines and under a variety of dif- ferent names, including collaborative completions (Clark, 1996; Poesio and Rieser, 2010), co-constructions (Sacks, 1992), joint productions (Helasvuo, 2004), and split ut- terances (Purver et al., 2006). Linguistic studies show that grammatical constraints are respected across speaker and hearer (see e.g. Gre- goromichelaki et al., 2009). In Finnish (which has rich inﬂectional morphology), and Japanese (a verb-ﬁnal lan- guage), cross-person CCs within a single clause conform to the strict syntactic constraints of the language, de- spite the change in speaker (Helasvuo, 2004; Hayashi, From a psycholinguistic point of view, the phe- nomenon of CCs seems compatible with mechanistic ap- proaches as exempliﬁed by the Interactive Alignment model of Pickering and Garrod (2004), which claims that, all things being equal, it should be as easy to com- plete another’s sentence as one’s own. According to this model, speaker and listener ought to be interchangeable at any point. A similar stance is taken by the gram- matical framework of Dynamic Syntax (DS: Cann et al., 2005). In DS, parsing and production are taken to em- ploy the same mechanisms, leading to a prediction that CCs ought to be strikingly natural (Purver et al., 2006). From an organisational point of view, it has been claimed that turn-taking operates not on individual conversational participants, but on ‘parties’ (Schegloﬀ, 1995). For example, a couple talking to a third person may organise their turns as if they are one ‘party’, rather than two separate individuals. Lerner (1991) speculates, following Sacks (1992), that cross-person compound con- tributions can clarify the formation of such parties, as they reveal a relationship between syntactic mechanisms and social organisation. He claims that this provides evi- dence of one way in which syntax can be used to organise participants into “groups”. Because a sentence is obviously a prototypical instance of that thing which is done by a unit. Nor- mally, some single person. That then permits it – for those who have the wit to do it – to be a way that some non-apparent unit may be demonstrated to exist."
c8415267f3e1e51844d5561ea17f2580a9d32014,
eb00635932e39a17a10c325330ad8f4941317f5f,
03b745eadd5a328edd7fb83d148663606420fbcd,"Spoken contributions in dialogue often continue or complete earlier contributions by either the same or a different speaker. These compound contributions(CCs) thus provide a natural context for investigations of incremental processing in dialogue. We present a corpus study which confirms that CCs are a key dial ogue phenomenon: almost 20% of contributions fit our general definition of CCs, with ne arly 3% being the cross-person case most often studied. The results suggest that processing is word-by-word incremental, as splits can occur within syntactic ‘constituents’; however, some s ystematic differences between sameand cross-person cases indicate important dialogue-specific pragmatic effects. An experimental study then investigates these effects by artificially intro ducing CCs into multi-party text dialogue. Results suggest that CCs affect people’s expectations abou t who will speak next and whether other participants have formed a coalition or ‘party’. Together, these studies suggest that CCs require an incremental processing mechanism that can provide a resource for constructing linguistic constit uents that span multiple contributions and multiple participants. They also suggest the need to model higher-level dialogue units that have consequences for the organisation of turn-taking and for the development of a shared context."
0e524928ee516d60a90368d31004af52e001dca0,
4c2605be80f13f9916fd104f290989e0f8bdaade,
7995b0f73c960059913305878019ea07a9929d6f,"Ever since dialogue modelling first developed relative to br oadly Gricean assumptions about utterance interpretation (Clark, 1996), it has remained an open question whether the full complexity of higher-order intention computation is made use of in everyday conversation. In this paper we examine the phenomenon of split utterances, from the perspective of Dynamic Syntax, to further probe the necessity of full intention recognition/formation in c ommunication: we do so by exploring the extent to which the interactive coordination of dialogue exchange can be seen as emergent from low-level mechanisms of language processing, without needing representation by interlocutors of each other’s mental states, or fully developed intentions a s regards messages to be conveyed. We thus illustrate how many dialogue phenomena can be seen as direct consequences of the grammar architecture, as long as this is presented within an incremental, goal-directed/predictivemodel."
7aaed07ba7625daa8dcc2941367a50a0d2d86db1,"This paper describes recent work on the DynDial project* towards incremental semantic interpretation in dialogue. We outline our domain-general grammar-based approach, using a variant of Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based semantics. We describe a Java-based implementation of the parser, used within the Jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels."
90797323438fbd4c929a91601d9b148e14b6bfeb,
91cd5799f4cfc4eab8d30e36db6219db14144ea6,"PurposeThe purpose of this paper is to inform the development of mixed initiative systems for distributed digital communication of manual skills. In particular, manual skills that are essential in project production paradigms such as engineer‐to‐order.Design/methodology/approachFindings from survey research, which included literature review and interviews with practitioners, are reported. Literature review investigated media, strategies, and computation relevant to distributed digital communication of manual skills. Interviews investigated attitudes among industry practitioners towards distributed digital communication of manual skills.FindingsCommunication media, instructional strategies, and computational semantics techniques are available which can be integrated to address the limitations of human communication of manual skills.Research limitations/implicationsOnly ten organizations were involved in interviews investigating attitudes towards distributed digital communication of manual skills.Practical implicationsManual skills will continue to be important to project businesses involved in the production, refurbishment, and/or maintenance of large engineer‐to‐order products such as public buildings and process plants. The limitations of human communication can be addressed by using a variety media, such as augmented reality headsets, to enable new instructional strategies, such as just‐in‐time training. Further, combinations of media and strategies can be integrated with computational semantics in the development of mixed initiative systems which provide feedback as well as initial instruction.Originality/valueThe originality of the research reported in this paper is that it addresses a full range of enablers for distributed communication of manual skills. Further, an overview of computational semantics is presented which does not rely on prior specialist knowledge. The value of this paper is that it introduces a framework for enabling distributed communication of manual skills. In addition, a preliminary ontology for distributed communication of manual skills is introduced, together with recommendations for implementation."
9c5a0deba8e8cff44ad59a4c44ad6cf6c6b88b3d,"This chapter discusses the task of topic segmentation: automatically dividing single long recordings or transcripts into shorter, topically coherent segments. First, we look at the task itself, the applications which require it, and some ways to evaluate accuracy. We then explain the most influential approaches – generative and discriminative, supervised and unsupervised – and discuss their application in particular domains."
eab2e6746450225d15875260c905b4a20f284667,"Social Proximity Applications (SPAs) have facilitated social networking in the real world. However, most applications are deployed in mobile devices (i.e. smartphone, PDA, tablet) restricted to traditional input and output (I/O) interfaces i.e. button, keyboard and screen. For people with low interpersonal skills, the require- ments imposed by traditional interfaces can make their problems worse. In contrast, this paper describes the Icebreaker T-shirt, designed with natural interfaces to ease face-to-face communication when meeting new people. We discuss its design con- cept, technical requirements and user experience based on an experiment conducted with shy users in which 73% reported that the shirt was helpful to their meetings with strangers."
0b53a7292daaa319a47c341a7add31c2d4262be6,
0c3516df85162b5ac010ca6810ec0c24defe455b,"This paper discusses the problem of second-person pronoun resolution in dialogue: determining who (if anyone) the word ‘you’ refers to. We motivate the task, and break it down into three distinct subtasks – distinguishing generic from deictic uses, distinguishing singular from plural uses, and determining individual reference. We then describe a dataset and series of supervised classification experiments, and show that various linguistic and non-linguistic features can be used to achieve overall accuracies of up to 78%."
5934627113c8fff5358f3cc231983a43180cbdba,
675b1e5c5db693f281bb20bc26a6a116e1ba8b89,This paper presents an empirical study and analytical examination of the actual and possible co-occurrence of dialogue acts in dialogue units of various sorts. We formulate semantic and pragmatic constraints on dialogue act combinations for various types of dialogue unit.
6af77a4375ece656a6cea8c505d21872fccd60e5,"Tracking Lexical and Syntactic Alignment in Conversation Christine Howes, Patrick G. T. Healey and Matthew Purver {chrizba,ph,mpurver}@dcs.qmul.ac.uk Queen Mary University of London Interaction, Media and Communication Group Mile End Road, London E1 4NS Abstract As much empirical work attests, people have a reli- able tendency to match their conversational partner’s body movements, speech style, and patterns of language use – amongst other things. A specific version of this tendency, Structural priming, which occurs when prior exposure to a particular linguistic structure facilitates one’s subsequent processing of the same structure, has gained widespread acceptance. Pickering and Garrod (2004) propose that cross-person structural priming is a basic mechanism of conversational coordination – part of an automatic, resource-free alignment mechanism that is the basis for all successful human interaction. We present evidence to the contrary from two analyses of a corpus of ordinary conversation. The first suggests that the level of structural (syntactic) matching is no dif- ferent from chance, and the second that the observed statistical correlation between prime form and target form may be entirely associated with repetition of lexical form. Keywords: structural priming; alignment Introduction The apparent tendency for speakers to repeat their own or others syntactic or structural choices in conversation – a phenomenon referred to as structural or syntactic alignment – has been a subject of particular scrutiny (see Pickering and Ferreira (2008) for an overview). The evidence for such alignment in dialogue comes from two main sources: experimental studies of task- oriented dialogue (e.g. (Branigan, Pickering, & Cleland, 2000)), and corpus studies that track frequency of use of these same constructions in language use outside of the laboratory setting (e.g. (Gries, 2005)). In the basic experimental set-up of Branigan and col- leagues, there are two participants, one of whom is a confederate of the experimenter. The participants de- scribe picture cards to each other, the critical items of which require the use of ditransitive verbs in their de- scriptions. In English, there are two syntactic struc- tures which can be used; one a double object structure (“The thief giving the nurse the banana”), and the other using a preposition (“The thief giving the banana to the nurse”). The confederate uses a scripted descrip- tion of the ditransitive prime sentences, thus manipu- lating which type naive subjects are exposed to. Par- ticipants are more likely to use the type of structure that they have just used or been exposed to. This has been found to hold across comprehension and production (Branigan, Pickering, Stewart, & McLean, 2000; Bock, Dell, Chang, & Onishi, 2007), from main clauses to rel- ative clauses (Branigan, Pickering, McLean, & Stewart, 2006) and even across languages in bilingual speakers (Hartsuiker, Pickering, & Veltkamp, 2004). Different factors found to increase the strength of syntactic align- ment include the distance between the prime and the target, participant role (Branigan, Pickering, McLean, & Cleland, 2007) and, importantly for the interactive alignment model (see below), reuse of the same or se- mantically related lexical items (Branigan, Pickering, & Cleland, 2000). In a corpus study using the International Corpus of English (ICE-GB), Gries (2005) looked at the same syn- tactic alternation. His data show that there is a tendency to reuse the form of a ditransitive verb most recently en- countered (double object or prepositional), in line with the experimental results. Similar results have been found to hold with different constructions such as particle placement of phrasal verbs (Gries, 2005), future markers (“will” versus “going to”) and comparatives (“cleverer” versus “more clever than”) (Szmrecsanyi, 2005). Pickering and Garrod (2004) argue, in their Interac- tive Alignment model, that alignment is the basis for successful communication; “successful dialogue occurs when interlocutors construct similar situation models to each other” (Pickering & Garrod, 2006, p206). In or- der to do this, interlocutors align on situation models; however, as this alignment is not usually negotiated ex- plicitly, it is hypothesised to arise automatically from local alignment, via resource-free priming mechanisms. 1 Alignment at local levels, including lexical (repetition of words) and the syntactic alignment discussed above, “percolates”, leading to alignment at other levels. From priming to alignment? There are three problems with using these studies to support the claim that cross-speaker structural priming is ubiquitous in conversation. First, automatic priming predicts an increase in matching of all structures across turns, but this claim has not been directly tested. For practical reasons, experimental studies have focussed on situations in which specific syntactic alternatives can be used to describe the same situation. Similarly, corpus studies have tended to track the frequency of use of spe- Note that the observed effects are alignment effects; priming mechanisms are their hypothesised cause, leading to two distinct questions – does such alignment occur; and if it does, is it caused by priming?"
72ea8e68a8764039051ce3ec861faf2bdb3e599f,"The CALO Meeting Assistant (MA) provides for distributed meeting capture, annotation, automatic transcription and semantic analysis of multiparty meetings, and is part of the larger CALO personal assistant system. This paper presents the CALO-MA architecture and its speech recognition and understanding components, which include real-time and offline speech transcription, dialog act segmentation and tagging, topic identification and segmentation, question-answer pair identification, action item recognition, decision extraction, and summarization."
9e15bda987495c59651ffd10d084c0292827ca85,"The occurrence of split utterances (SUs) in dialogue raises many puzzles for grammar formalisms, from formal to pragmatic and even philosophical issues. This paper presents an account of some of the formal details that grammars need to incorporate in order to accommodate them. Using Dynamic Syntax(DS), we illustrate how by incorporating essential interaction with the context into the grammar itself, we can deal with speaker change in SUs: not only its effects on indexicals I andyou, but also the multiple illocutionary forces that can arise. We also introduce a Split Turn Taking Puzzle(STTP) showing that the current speaker and the agent of the resulting speech act are not necessarily the same."
a70bfc96d89467b4d4e9cc449ed5766fa14c9a40,"It is often observed that people tend to match each others body movements, speech style, and patterns of language use during conversation (Giles Coupland and Coupland, 1991). Recently, it has been proposed that ‘structural priming’ should be added to this list (Pickering and Ferreira, 2008). Structural priming occurs when processing of a linguistic structure is facilitated by prior exposure to the same structure. The main evidence for these effects comes from experimental studies of individuals processing sequences of sentences. However, the Interactive Alignment Model has proposed that cross-person structural priming is a part of an automatic, resource-free priming mechanism that helps to underpin all successful human interaction (Pickering and Garrod, 2004, 2006). We present evidence from a corpus analysis that, in fact, people tend to diverge in their use of syntactic structures in ordinary conversation. There are three problems with the corpus evidence normally provided for cross-speaker structural priming in conversation:"
03172042f79510c850637dbd599f0c06b1c051be,
0b9fc4e81d7c1591fd164f676fb93359b4c2f5d4,"This paper presents a preliminary English corpus study of split utterances (SUs), single utterances split between two or more dialogue turns or speakers. It has been suggested that SUs are a key phenomenon of dialogue, which this study confirms: almost 20% of utterances were found to fit this general definition, with nearly 3% being the between-speaker case most often studied. Other claims/assumptions in the literature about SUs' form and distribution are investigated, with preliminary results showing: splits can occur within syntactic constituents, apparently at any point in the string; it is unusual for the separate parts to be complete units in their own right; explicit repair of the antecedent does not occur very often. The theoretical consequences of these results for claims in the literature are pointed out. The practical implications for dialogue systems are mentioned too."
55d06a2a3431657c73f1ee4fbd188ec740ff8d30,"A substantial body of empirical work suggests people have a reliable tendency to match, amongst other things, their conversational partner’s body movements, speech style, and patterns of language use (Giles, Coupland and Coupland, 1991). Recently, a more specific ‘structural priming’ version of this claim has gained prominence (e.g. Pickering and Ferreira, 2008). Structural priming occurs when people’s processing of a particular linguistic structure is facilitated by prior exposure to the same structure. Much of the psycholinguistic evidence for these effects comes from experimental studies of single individuals processing a sequence of sentences. However, Pickering and Garrod (2004, 2006) propose that cross-person structural priming is a basic mechanism of conversational co-ordination. They claim that it is part of an automatic, resource-free priming mechanism that helps to underpin all successful human interaction. We present evidence from a corpus analysis of ordinary conversation which suggests that this claim is incorrect."
858e67d38e2f2e125078bdf45efe91955a647609,"This paper examines the resolution of the second person English pronoun you in multi-party dialogue. Following previous work, we attempt to classify instances as generic or referential, and in the latter case identify the singular or plural addressee. We show that accuracy and robustness can be improved by use of simple lexical features, capturing the intuition that different uses and addressees are associated with different vocabularies; and we show that there is an advantage to treating referentiality and addressee identification as separate (but connected) problems."
a6269098ce0901dc555c12ec4ee47e403a338a2c,"We are happy to present the Proceedings of the SIGDIAL 2009 Conference, the 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue. This year the SIGDIAL meeting has been elevated from Workshop to Conference by ACL, its main sponsoring organization. That is an unmistakable recognition of the role that dialogue and discourse research play in the fields of computational linguistics, human-machine communication, and language technology in general."
21616992c2a37fad0d1192392830c638f96ba02f,"We describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party, human-human meetings. Extending our previous work on action item identification, we propose a structured approach that takes into account the different roles utterances play in the decision-making process. We show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on flat annotations, while enabling the extraction of more fine-grained information that can be used for summarization and reporting."
23182c4d892fc43676172d6e6c4debb132200313,
32718d2ff6e3363f7c8e00a2e9b351b2acf91f2f,"In order to determine the points at which meeting discourse changes from one topic to another, probabilistic models were used to approximate the process through which meeting transcripts were produced. Gibbs sampling was used to estimate the values of random variables in the models, including the locations of topic boundaries. This paper shows how discourse features were integrated into the Bayesian model and reports empirical evaluations of the benefit obtained through the inclusion of each feature and of the suitability of alternative models of the placement of topic boundaries. It demonstrates how multiple cues to segmentation can be combined in a principled way, and empirical tests show a clear improvement over previous work."
4cd11519f36d4a7a06a645f324af0758b6be41f6,"Global skill shortages are reported in many occupations. Existing strategies for addressing skill shortages are not successful and, as a result, skill shortages are an intractable problem. A new strategy, real-time communication of skill knowledge without human instructors, has the potential to bring about radical reductions in skill shortages. The goal of the study reported in this VTT Working Paper was to determine how skill knowledge could be communicated in real-time without reliance on access to a person with relevant existing skill knowledge. In particular, the communication of manual skills. The research involved literature review and field study. Literature review encompassed studies concerned with skill knowledge, communication media, and computational semantics. Field study involved interviews with industry practitioners seeking to address skill shortages and computational semantics scientists. The study revealed that many of the technologies and methods required for the realtime communication of skill knowledge without human instructors are already available. Further, the study revealed that computational semantics is essential to the successful application and integration of these technologies and methods. ISBN 978-951-38-7162-8 (URL: http://www.vtt.fi/publications/index.jsp) Series title and ISSN Project number VTT Working Papers 1459-7683 (URL: http://www.vtt.fi/publications/index.jsp) 21791 Date Language Pages June 2008 English 85 p. Name of project Commissioned by Rapid Economic Production of Special Products VTT, Tekes, companies"
8443a110b29286027c85dcd84e49103f0cdc5252,"Upcoming technologies will automatically identify and extract certain types of general information from meetings, such as topics and the tasks people agree to do. We explore interfaces for presenting this information to users after a meeting is completed, using two post-meeting interfaces that display information from topics and action items respectively. These interfaces also provide an excellent forum for obtaining user feedback about the performance of classification algorithms, allowing the system to learn and improve with time. We describe how we manage the delicate balance of obtaining necessary feedback without overburdening users. We also evaluate the effectiveness of feedback from one interface on improvement of future action item detection."
85d8c4bc2979a66581f85e95ac97d90befced552,
d5a08efdd946714182f4639663343bf8aaf3a5b6,"The CALO meeting assistant provides for distributed meeting capture, annotation, automatic transcription and semantic analysis of multiparty meetings, and is part of the larger CALO personal assistant system. This paper summarizes the CALO-MA architecture and its speech recognition and understanding components, which include real-time and offline speech transcription, dialog act segmentation and tagging, question-answer pair identification, action item recognition, decision extraction, and summarization."
055e8d684e721112e994556284cc2f05aec7dc88,We describe an algorithm for a novel task: disambiguating the pronoun you in conversation. You can be generic or referential; finding referential you is important for tasks such as addressee identification or extracting 'owners' of action items. Our classifier achieves 84% accuracy in two-person conversations; an initial study shows promising performance even on more complex multi-party meetings.
267657d3484c4251c59fc0149da7d10c4d7d0fd1,"In the past few years, we have been developing a robust, wide-coverage, and cognitive load-sensitive spoken dialog interface, CHAT (Conversational Helper for Automotive Tasks). New progress has been made to address issues related to dynamic and attention-demanding environments, such as driving. Specifically, we try to address imperfect input and imperfect memory issues through robust understanding, knowledge-based interpretation, flexible dialog management, sensible information communication, and user-adaptive responses. In addition to the MP3 player and restaurant finder applications reported in previous publications, a third domain, navigation, has been developed, where one has to deal with dynamic information, domain switch, and error recovery. Evaluation in the new domain has shown a good degree of success: including high task completion rate, dialog efficiency, and improved user experience."
3df3658384f620a10a04352dc58be58bdce204cd,"This paper presents experiments into the resolution of “you” in multi-party dialog, dividing this process into two tasks: distinguishing between generic and referential uses; and then, for referential uses, identifying the referred-to addressee(s). On the first task we achieve an accuracy of 75% on multi-party data. We achieve an accuracy of 47% on determining the actual identity of the referent. All results are achieved without the use of visual information."
406d2aaae6a6f2bdd7d9ccc8791f434d33f7252d,
5da387fd54375141daa8574181242c41ec685969,
5dc6dd8d88dd35f3b74295ac1c65422e43919de8,
807c15c1be8d55125a0276f0477e3bac5b81e118,"We present a system for extracting useful information from multi-party meetings and presenting the results to users via a browser. Users can view automatically extracted discussion topics and action items, initially seeing high-level descriptions, but with the ability to click through to meeting audio and video. Users can also add value by defining and searching for new topics and editing, correcting, deleting, or confirming action items. These feedback actions are used as implicit supervision by the understanding agents, retraining classifier models for improved or user-tailored performance."
b7e6cec1701c297a01887d6a2e99b21eb0ed112c,"In this demonstration we present a conversational dialog system for automobile drivers. The system provides a voice-based interface to playing music, finding restaurants, and navigating while driving. The design of the system as well as the new technologies developed will be presented. Our evaluation showed that the system is promising, achieving high task completion rate and good user satisfation."
bca28863b1725f108efbef56fcbaa189eecd6373,
dc54ec5cb84b0a4d29fa86ec36b73bcf8f79bf2a,
e381e352f9a5e2673ccfd6e3ec51f21838f7b250,"This paper addresses the problem of identifying action items discussed in open-domain conversational speech, and does so in two stages: firstly, detecting the subdialogues in which action items are proposed, discussed and committed to; and secondly, extracting the phrases that accurately capture or summarize the tasks they involve. While the detection problem is hard, we show that we can improve accuracy by taking account of dialogue structure. We then describe a semantic parser that identifies potential summarizing phrases, and show that for some task properties these can be more informative than plain utterance transcriptions."
34ac74df5c9ffef68c87183e8aa0f93d3e86666d,
46ad11c37f234d5bf9d77e490d9fb3e881e0eb22,"We present a system for extracting useful information from multi-party meetings and presenting the results to users via a browser. Users can view automatically extracted discussion topics and action items, initially seeing high-level descriptions, but with the ability to click through to meeting audio and video. Users can also add value: new topics can be defined and searched for, and action items can be edited or corrected, deleted or confirmed. These feedback actions are used as implicit supervision by the understanding agents, retraining classifier models for improved or user-tailored performance."
60e4c79d2a4ec0c9c425048655d3e38d3446690b,"Ever since the the early 1980’s, it has been a corner-stone of work in natural-language semantics and pragmatics that any model of interpretation of natural-language has to reflect the way in which understanding of words systematically depends on aspects of the context in which they are produced. Throughout this period, however, syntacticians, semanticists and pragmatists alike have sought to retain a clear competence-performance distinction in which a grammar formalism is seen as a pairing of wellformed strings of the language with interpretation to be defined in some sense independent of considerations of use; and pragmatic theories and performance theories of parsing or production have had to be presumed to take some core formalism as the point of departure for their own explanations of aspects of language use. Even those individuals who have played a central role in promoting the importance of articulating concepts of underspecification in understanding the relation between natural language expressions and their interpretation in context have in general remained committed to the view that the grammar formalism articulates some pairing of syntactic structure for a sentential string and some representation of content for that string, in some sense sui generis. Amongst these are relevance theorists, and they have consistently advocated that the grammar articulates a pairing of sentences with logical forms as output, these being incomplete representations"
846fe959418bfc7e13ba349f084444e5a39d6f4d,"We present an approach to dialogue management and interpretation that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple interpretation methods can be combined, multiple speech recognition and parsing hypotheses tested, and multiple candidate dialogue moves considered to choose the highest scoring hypothesis overall. We integrate hypotheses generated from shallow slot-filling methods and from relatively deep parsing, using pragmatic information. We show that this gives more robust performance than using either approach alone, allowing n-best list reordering to correct errors in speech recognition or parsing."
923194c2c5983c292c2137268862c5598a0153d3,
b591f2c7feecc788022c9275b7d6a764f37abbf9,"Spoken dialogue interfaces, mostly command-and-control, become more visible in applications where attention needs to be shared with other tasks, such as driving a car. The deployment of the simple dialog systems, instead of more sophisticated ones, is partly because the computing platforms used for such tasks have been less powerful and partly because certain issues from these cognitively challenging tasks have not been well addressed even in the most advanced dialog systems. This paper reports the progress of our research effort in developing a robust, wide-coverage, and cognitive load-sensitive spoken dialog interface called CHAT: Conversational Helper for Automotive Tasks. Our research in the past few years has led to promising results, including high task completion rate, dialog efficiency, and improved user experience. Index Terms: dialog systems, cognitive load, robustness"
b60042eb4363e9043e9d90ae4d5853636590e552,"Spoken dialogue systems have to deal with imperfect speech recognition. We describe how we address the resulting robustness challenges both on the interpretation side (via combining complementary parsing and classification techniques) and on the generation side (via robustness to ill-formed generation input, and use of clarification questions). We describe empirical results obtained from a user study involving 20 subjects."
b95c0a4fe935559904862c535a48e7e5e41ee745,"We present an approach to dialogue management and interpretation that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple interpretation methods can be combined, multiple speech recognition and parsing hypotheses tested, and multiple candidate dialogue moves considered to choose the highest scoring hypothesis overall. We integrate hypotheses generated from shallow slot-filling methods and from relatively deep parsing, using pragmatic information. We show that this gives more robust performance than using either approach alone, allowing n-best list reordering to correct errors in speech recognition or parsing. Index Terms: dialogue management, robust interpretation"
e6b1b41ade015e2ea3eec1a7d84afdb64af555c5,"We present a method for unsupervised topic modelling which adapts methods used in document classification (Blei et al., 2003; Griffiths and Steyvers, 2004) to unsegmented multi-party discourse transcripts. We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification: automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods (Galley et al., 2003) while simultaneously extracting topics which rate highly when assessed for coherence by human judges. We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors."
e7e6d40eeae01749cd5d3facdbf0a17274a4ab01,
fcb83a509f5d5c340bd13c99a599caac4c6cc890,"We investigated automatic action item detection from transcripts of multi-party meetings. Unlike previous work (Gruenstein et al., 2005), we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting."
1937ead0274995c11f48e4784ffa870f4a3a81d6,"We present a set of annotations of hierarchical topic segmentations and action item subdialogues collected over 65 meetings from the ICSI and ISL meeting corpora, designed to support automatic meeting understanding and analysis. We describe an architecture for representing, annotating, and analyzing multi-party discourse, including: an ontology of multimodal discourse, a programming interface for that ontology, and an audiovisual toolkit which facilitates browsing and annotating discourse, as well as visualizing and adjusting features for machine learning tasks."
1d79ade569b51172184e66cbf0ddada7dd0da407,"In this paper, we present research toward ontology-based understanding of discourse in meetings and describe an ontology of multimodal discourse designed for this purpose. We investigate its application in an integrated but modular architecture which uses semantically annotated knowledge of communicative meeting activity as well as discourse subject matter. We highlight how this approach assists in improving system performance over time and supports understanding in a changing and persistent environment. We also describe current and future plans for ontology-driven robust naturallanguage understanding in the presence of the highly ambiguous and errorful input typical of the meeting domain."
40cc9cba39e49d6c30c719a70dc3b423e88e4388,"This paper describes current and planned research efforts towards developing multimodal discourse understanding for an automated personal office assistant. The research is undertaken as part of a project called The Cognitive Agent that Learns and Organizes (CALO) (see http://www.ai.sri.com/project/CALO). The CALO assistant is intended to aid users both personally and as a group in performing office-related tasks such as coordinating schedules, providing relevant information for completing tasks, making a record of meetings, and assisting in fulfilling decisions. Our focus within this enterprise is on understanding, describing, and automatically participating in multimodal human-human and human-computer discourse amongst CALO users and the system itself. This aspect is functionally realized by the system in its role as a persistent presence before, during, and after meetings; firstly by helping to set up and coordinate meetings and meeting agendas; secondly by extracting detailed information about what was discussed, what the participants’ actions were, and what decisions were reached; thirdly by interactively reporting on this extracted information; and eventually by interactively providing relevant and useful information or responding to queries during the meeting itself. Currently, the system is not interactive during meetings (though this is a plan for future versions), which al-"
4652793bba1870809522dc022ad1d3b509ffeba9,"Contrary to what for years has been a widespread belief that conversational data are riddled with mistakes, false starts, and simply ungrammatical data, Pickering and Garrod (2004) extended a challenge primarily to psycholinguists, but also to theoretical linguists that their models should be evaluatable by how well they provide a basis for characterizing the patterns systematically displayed in conversational dialogue, this being the core language data. In this paper we argue that by adopting a Dynamic Syntax perspective, with the time-linear dynamics of building up structured semantic representations relative to context built in to the architecture of the grammar formalism, these patterns are directly predicted, while still retaining the assumption of grammar as a system which underpins language use but does not fully dictate it, i.e. without writing conversational rules or a grammar of conversation."
5448c631d21b369a3d126f3940abf8b6335722dc,
70941fde16df704e5fac3a27cbfe60ad8e74ad7d,"We present an approach to multi-device dialogue that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple sources of information can be combined, multiple speech recognition and parsing hypotheses tested, and multiple devices and moves considered to choose the highest scoring hypothesis overall. The approach has the added benefit of potentially re-ordering n-best lists of inputs, effectively correcting errors in speech recognition or parsing. A current application includes conversational interaction with a collection of in-car devices."
0edf94d1218d67d402a8db5e11b91edc481e3d96,
17c71f58d0353e6684075211e39d6cf5d4ad345b,"Clarification requests are an important, relatively common and yet under-studied dialogue device allowing a user to ask about some feature (e.g. the mea ning or form) of an utterance, or part thereof. They can take many different forms (often hi ghly elliptical) and can have many different meanings (requesting various types of infor mation). This thesis combines empirical, theoretical and implementational work to provi de a study of the various types of clarification request that exist, give a theoretical analys is thereof, and show how the results can be applied to add useful capabilities to a prototype comp utational dialogue system. A series of empirical studies (corpus-based and experiment al) are described which establish a taxonomy of the possible types of clarification reques t together with information about their meaning and usage, about the phrase types and conditio ns that trigger them and their particular forms and interpretations, and about the likely methods of responding to them. A syntactic and semantic analysis using the HPSG framework i s given which extends the work of (Ginzburg and Cooper, 2004) to cover the main classes of the above taxonomy, and to account for the clarificational potential of those word an d phrase types which commonly cause clarification requests. This is shown to have interest ing implications for the semantics of various lexical and phrasal types, in particular suggest ing hat noun phrases be given a simple witness-set based representation. Finally, the theoretical analysis and empirical findings ar e pplied within a HPSG grammar and a prototype text-based dialogue system, CLARIE. Imp le ented in Prolog using the TrindiKit, the system combines the information-state-bas ed dialogue management of GoDiS (Larsson et al., 2000) and the HPSG-based ellipsis resoluti on of SHARDS (Ginzburg et al., 2001a) and adds the capability to interpret and respond to us er clarification requests, and generate its own clarifications where necessary to deal with inc omprehensible or contradictory input, resolve unknown or ambiguous reference, and learn ou t-of-vocabulary words."
25362283c3c9fc11aa0650b772ebdb4630ea83fb,"This paper describes an implemented prototype dialogue model within the Dynamic Syntax (DS) framework (Kempson et al., 2001) which directly reflects dialogue phenomena such as alignment, routinization and shared utterances. In DS, word-by-word incremental parsing and generation are defined in terms of actions on semantic tree structures. This paper proposes a model of dialogue context which includes these trees and their associated actions, and shows how alignment and routinization result directly from minimisation of lexicon search (and hence speaker's effort), and how switch of speaker/hearer roles in shared utterances can be seen as a switch between incremental processes directed by different goals, but sharing the same (partial) data structures.Article"
8120d2b2999b63c6e433b1bf4b712f667dbcdeda,
8bd94b388a8e4320feb3c37eb5467c676704e625,"Reprise questions are a common dialogue device allowing a conversational participant to request clarification of the meaning intended by a speaker when uttering a word or phrase. As such they can act as semantic probes, providing us with information about what meaning can be associated with word and phrase types and thus helping to sharpen the principle of compositionality. This paper discusses the evidence provided by reprise questions concerning the meaning of nouns, noun phrases and determiners. Our central claim is that reprise questions strongly suggest that quantified noun phrases denote (situation-dependent) individuals - or sets of individuals - rather than sets of sets, or properties of properties. We outline a resulting analysis within the HPSG framework, and discuss its extension to such phenomena as quantifier scope, anaphora and monotone decreasing quantifiers."
cbe232748328431075ec6c24c842d385acb02fe1,"Standard grammar formalisms are defined without reflection of the incremental and serial nature of language processing, and incrementality must therefore be reflected by independently defined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straight-forward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment."
14ef9d58581c67aa83a43e8cf6ee13b582c4f186,"We present an approach to utterance representation which views utterances and their sub-constituents as instructions for contextual update: programs in a dynamic logic defined with respect to the dialogue gameboard of (Ginzburg, 1996; Fernández, 2003b). This approach allows utterance processing protocols to be represented within the grammar rather than postulated as separate dialogue processes: it also allows a view of incremental grounding and clarification which reflects the insights of dynamic semantics and allows us to treat salience and information structure in a coherent manner."
15c9e822a6ff3f111d5c3a57ccda5952905f8998,"Author(s): Healey, Patrick G.T.; Purver, Matthew; King, James; Ginzburg, Jonathan; Mills, Greg J."
268578c98d1de8628c7f5ec10c9fbfcdfaf6e569,"The paper shows how an incremental tactical generator can be constructed based on the incremental parsing framework described in Dynamic Syntax (DS)(Kempson et al., 2001), without adding a generator-specific vocabulary or intermediate levels of representation. The resulting generator is defined purely in terms of the parsing process, together with a notion of tree subsumption. This is shown to have various advantages including easy self-monitoring and psycholinguistic plausibility. A simple Prolog implementation is described, together with various possible improvements in efficiency."
38469d0e00ebc31132c70c3c0b4a1f0c80a16d99,We argue for an approach which treats the compositional semantic content of an utterance as including its basic dialogue update effects ‐ those which can be derived entirely from its semantic and syntactic properties. This allows us to capture the distinction between these integral semantic contextual effects and those pragmatic effects which can only be determined from the interaction between features of the utterance and the context itself.
52d78f19f52fef313ff1efa68d13481fce87763a,"
This paper examines reprise questions: questions which request clarification of
the meaning intended by a speaker when uttering a word or phrase. As such they
can act as semantic probes, providing information about what meaning can be
associated with word and phrase types. We present corpus evidence regarding the
meaning of nouns and noun phrases, and argue that this evidence runs contrary
to the usual treatments of semantics in HPSG, and to the traditional
generalised quantifier view of NPs as sets of sets. Instead we outline an
analysis of NPs as (possibly functional) sets of individuals."
8e3f6f8b3d4d0b074c29517ad0e0e7b83f447c04,
931ffb49c2e250bde5c23befa4eea3a3a6a973e1,
976a155db97bfd4189c89a9ad4ed7d61202f51b4,"This paper describes the results of corpus and experimental investigation into the factors that affect the way clarification questions in dialogue are interpreted, and the way they are responded to. We present some results from an investigation using the BNC which show some general correlations between clarification request type, likelihood of answering, answer type and distance between question and answer. We then describe a new experimental technique for integrating manipulations into text-based synchronous dialogue, and give more specific results concerning the effect of word category and level of grounding on interpretation and response type."
c8c78a06d382d3dafae627192c3cca0caea38f8a,
f12a71f4d43d2301ec75d01bc98ac8c876893887,"The paper shows how an incremental generator can be constructed based on the incremental parsing framework described in Dynamic Syntax (DS)(Kempson et al., 2001), without adding a generator-specific vocabulary or intermediate levels of representation. The resulting generator is defined purely in terms of the parsing process, together with a notion of tree subsumption. This is shown to have various advantages including easy self-monitoring and psycholinguistic plausibility. A simple Prolog implementation is described, together with various possible improvements in efficiency."
0cab72524d384fceb1325a24a51b918990437f27,
1cd0a37a33f880c0adb4df05c7b05af013ef40af,
1e8c1b29fb63f405f96b792e69f7f37f10b7617b,"This paper describes a method of processing unknown words in a HPSG-based dialogue system, with acquisition of lexical semantics via clarification questions answered by the user. Use of a highly contextualized semantic representation, together with an utterance-anaphoric view of clarification, allows the clarificational dialogue to be integrated within the grammar and governed by standard rules of conversation."
110cee46c612c83464c61d716bc2af0d36002094,
331200582e1fcb4663ab3eed72ebd5856b7854e7,
c713ee35e22d16dd99e4b2bbbb54cf9618942b1d,"The ability to request clarification of utterances is a vital part of the communicative process. In this paper we discuss the range of possible forms for clarification requests, together with the range of readings they can convey. We present the results of corpus analysis which show a correlation between certain forms and possible readings, together with some indication of maximum likely distance between request and the utterance being clarified. We then explain the implications of these results for a possible HPSG analysis of clarification requests and for an ongoing implementation of a clarification-capable dialogue system.1"
22129285f522a1bb81400f4fd60a95c460756ea7,
15cbd5a70075ce3a534b3bba084a80e7553a61cf,"This paper describes the CLARIE system, a prototype information-statebased text dialogue system designed to deal with many types of clarification requests (CRs) by using a highly contextualised semantic representation together with a suitable grounding process. This allows it to interpret and respond to user CRs, and generate its own CRs in order to clarify unknown reference and learn new words, with both integrated within the standard dialogue update processes."
52c0409a652f19d8e5e96085721c189d57ca1847,"Executive Summary In this review, we bring together conceptual representations relevant to concept creation in the scope of ConCreTe. The conceptual representations reviewed are organized in accordance with two important perspectives on the distinctions between them. One distinction is between symbolic, spatial, and connectionist representations. The other is between descriptive and procedural representations. These two distinctions are orthogonal. Moreover, conceptual representations used in particular creative domains, i.e., language, music, image, and emotion, are reviewed separately. For each representation reviewed, we also cover the inference it affords, the computational means of building it, and its application in concept creation. In the end, we propose a high-level categorization of concept formation, and indicate directions of future research, as identified during this review, according to the proposed categories. Dissemination Level PU Public X PP Restricted to other programme participants (including the Commission Services)-RE Restricted to a group specified by the Consortium (including the Commission Services)-CO Confidential, only for members of the Consortium (including the Commission Services)-The ConCreTe Consortium has addressed all comments received, making changes as necessary. Changes to this document are detailed in the change log table below."
0e77ece386d76a4b99e1f2eafae583f810a39dd7,"Based on the localization packaging design as a starting point,combined with the packaging culture building national brand strategy,it expounded the localization packaging design and national brand of the relationship.And then through the brand differentiation,the formation of the brand the accumulation of assets and brand image promotion of several aspects of the argument,explored the analysis of packaging design concept in localization national brand in the construction of the value of existence and the necessity and feasibility of implementation."
9d4c041726f23d4929884655a898c583bbe644b0,"Two wave coupling equations on anisotropic diffraction in photorefractive crystal is deri-ved according to coupling wave theory. Phase gratig diffraction results of a He-Ne laser beam (λ= 632.8 nm) passing through tungsten bronze crystals KNSBN doped respectively Ca, Ce and Co are given. The anisotropic diffraction patterns of these three kinds of photorefractive crystals is formally analyzed using these coupling formulae. It is concluded that the anisotro-pic diffraction ring pattern will present in one of the two kinds of diffraction (e→o or o→e diffraction) for different (positive or negtive) unixial crystal. Theory analysis and experi-mental results agree fair by well."
72d62690cac7359f4b76343d3048537e918f6571,
c1e95a8d4cfeadd84eabbd7f2c3e79fb7de18052,"Melatonin secretion follows a circadian pattern with a maximum level at night in many species. However, in zebrafish (Danio rerio, a diurnal fish species) large inter-individual variations in daily rhythmicity of melatonin levels are present and are associated with variation in behaviour. Melatonin secretion rhythm of proactive individuals that are more active and exploratory are of larger amplitude compared to reactive individuals. In threespine sticklebacks (Gasterosteus aculeatus), a nocturnal species, inter-individual variability of behaviour is well described. However, inter-individual variation of melatonin rhythm and its association with variation in behaviour has never been measured in this species, which would allow to test if patterns found in zebrafish can be generalized for diurnal and nocturnal species. We measured large inter-individual variation in melatonin levels and found that activity was positively correlated with plasma melatonin concentration measured at night. We did not observe any significant difference in nigh-day variation in melatonin concentration between very active and less active groups. However, we found that individuals classified as reactive based on their propensity to wall-hugging, a measure of anxiety in fish, showed large variation in melatonin between night and day, while this rhythm was not seen in proactive individuals that frequently used the centre of the aquarium. Overall, our study suggests that melatonin may directly modulate specific behaviours in wild sticklebacks, and that while interindividual variation in melatonin rhythm may be widespread in fish, different patterns of association with behaviours should be expected."
1b0d1637d6b28f0126be870eefcf64bf35072253,"The study of the evolution of sociality is closely associated with the study of the evolution of sensory systems. Indeed, group life and sociality necessitate that individuals recognize each other and detect outsiders, as seen in eusocial insects such as Hymenoptera. While we know that antennal sense organs that are involved in olfactory perception are found in greater densities in social species of that group compared to solitary hymenopterans, whether this among-species correlation represents the consequence of social evolution leading to sensory evolution, or the opposite, is still questioned. Knowing more about how sociality and sensory abilities covary within a species would help us understand the evolutionary sequence. Studying a species that shows social plasticity, that is facultatively social, would further allow disentangling the cause and consequence of social evolution and sensory systems and the implication of plasticity in the process."
414821671fd1bb2f5c07906445fd9e47be1cf2fd,
4c919a1b117ce470b6e41daa30570d1003f74c79,
5da5da7670743dd3d4559f2334e744e1a7ba9ea9,"The circadian clock is an internal timekeeping system shared by most organisms, and knowledge about its functional importance and evolution in natural environments is still needed. Here, we investigated the circadian clock of wild-caught threespine sticklebacks (Gasterosteus aculeatus) at the behavioural and molecular levels. While their behaviour, ecology, and evolution are well studied, information on their circadian rhythms are scarce. We quantified the daily locomotor activity rhythm under a light-dark cycle (LD) and under constant darkness (DD). Under LD, all fish exhibited significant daily rhythmicity, while under DD, only 18% of individuals remained rhythmic. This interindividual variation suggests that the circadian clock controls activity only in certain individuals. Moreover, under LD, some fish were almost exclusively nocturnal, while others were active around the clock. Furthermore, the most nocturnal fish were also the least active. These results suggest that light masks activity (i.e. suppresses activity without entraining the internal clock) more strongly in some individuals than others. Finally, we quantified the expression of five clock genes in the brain of sticklebacks under DD using qPCR. We did not detect circadian rhythmicity, which could either indicate that the clock molecular oscillator is highly light-dependent, or that there was an oscillation but that we were unable to detect it. Overall, our study suggests that a strong circadian control on behavioural rhythms may not necessarily be advantageous in a natural population of sticklebacks and that the daily phase of activity varies greatly between individuals because of a differential masking effect of light. Summary statement We found that in wild-caught threespine sticklebacks, the circadian clock does not control locomotor activity in most, but not all, individuals. Sticklebacks are mostly nocturnal, although interindividual variation exists."
6311ceffb2bc174e941f835cb84db1784e1394a5,"The early social environment an animal experiences may have pervasive effects on its behaviour. The social decision‐making network (SDMN), consisting of interconnected brain nuclei from the forebrain and midbrain, is involved in the regulation of behaviours during social interactions. In species with advanced sociality such as cooperative breeders, offspring are exposed to a large number and a great diversity of social interactions every day of their early life. This diverse social environment may have life‐long consequences on the development of several neurophysiological systems within the SDMN, although these effects are largely unknown. We studied these life‐long effects in a cooperatively breeding fish, Neolamprologus pulcher, focusing on the expression of genes involved in the monoaminergic and stress response systems in the SDMN. N. pulcher fry were raised until an age of 2 months either with their parents, subordinate helpers and same‐clutch siblings (+F), or with same‐clutch siblings only (−F). Analysis of the expression of glucocorticoid receptor, mineralocorticoid receptor, corticotropin releasing factor, dopamine receptors 1 and 2, serotonin transporter and DNA methyltransferase 1 genes showed that early social experiences altered the neurogenomic profile of the preoptic area. Moreover, the dopamine receptor 1 gene was up‐regulated in the preoptic area of −F fish compared to +F fish. −F fish also showed up‐regulation of GR1 expression in the dorsal medial telencephalon (functional equivalent to the basolateral amygdala), and in the dorsolateral telencephalon (functional equivalent to the hippocampus). Our results suggest that early social environment has life‐long effects on the development of several neurophysiological systems within the SDMN."
ae000b91ded10555d2f8076366ea604c0ba49959,
d87949d6114023c0dee274ffa8c9afe1c6a73345,"The circadian clock is an internal timekeeping system shared by most organisms, and knowledge about its functional importance and evolution in natural environments is still needed. Here, we investigated the circadian clock of wild-caught threespine sticklebacks (Gasterosteus aculeatus) at the behavioural and molecular levels. While their behaviour, ecology, and evolution are well studied, information on their circadian rhythms are scarce. We quantified the daily locomotor activity rhythm under a light-dark cycle (LD) and under constant darkness (DD). Under LD, all fish exhibited significant daily rhythmicity, while under DD, only 18% of individuals remained rhythmic. This interindividual variation suggests that the circadian clock controls activity only in certain individuals. Moreover, under LD, some fish were almost exclusively nocturnal, while others were active around the clock. Furthermore, the most nocturnal fish were also the least active. These results suggest that light masks activity more strongly in some individuals than others. Finally, we quantified the expression of five clock genes in the brain of sticklebacks under DD using qPCR. We did not detect circadian rhythmicity, which could either indicate that the clock molecular oscillator is highly light-dependent, or that there was an oscillation but that we were unable to detect it. Overall, our study suggests that a strong circadian control on behavioural rhythms may not necessarily be advantageous in a natural population of sticklebacks and that the daily phase of activity varies greatly between individuals because of a differential masking effect of light.We found that in wild-caught threespine sticklebacks, the circadian clock does not control locomotor activity in most, but not all, individuals. Sticklebacks are mostly nocturnal, although interindividual variation exists."
dcc8a24ab1fa38e62f867e59077de8fcf0725bbb,
f0353471c1b45759aea4d5070ba20f07afdfb708,"Several studies have quantified the differences in transcription between the sexes in mature individuals, showing the extent of this sex-bias and which functions are affected. There is, however, less data available on what occurs during the different phases of development leading to this phenotype, especially in species with specific developmental strategies, such as hemimetabolous insects. While many well-studied insects such as the honey bee, drosophila, and butterflies, exhibit an holometabolous development (""holo"" meaning ""complete"" in reference to their drastic metamorphosis from the juvenile to the adult stage), hemimetabolous insects have juvenile stages that look similar to the adult stage (the hemi prefix meaning ""half"", referring to the more tissue-specific changes during development), as seen in crickets, cockroaches, and stick insects. Learning more about what happens during development in terms of the identity Open Access"
36ccac229e3294310bc78d26b912681c97f6b1c4,
3aab320f61fe949cec23a1fe1a4802904a338d08,"Many parasites with complex life cycles modify their intermediate hosts’ behaviour, presumably to increase transmission to their final host. The threespine stickleback (Gasterosteus aculeatus) is an intermediate host in the cestode Schistocephalus solidus life cycle, which ends in an avian host, and shows increased risky behaviours when infected. We studied brain gene expression profiles of sticklebacks infected with S.solidus to determine the proximal causes of these behavioural alterations. We show that infected fish have altered expression levels in genes involved in the inositol pathway. We thus tested the functional implication of this pathway and successfully rescued normal behaviours in infected sticklebacks using lithium exposure. We also show that exposed but uninfected fish have a distinct gene expression profile from both infected fish and control individuals, allowing us to separate gene activity related to parasite exposure from consequences of a successful infection. Finally, we find that Selective Serotonin Reuptake Inhibitor (SSRI)-treated sticklebacks and infected fish do not have similarly altered gene expression, despite their comparable behaviours, suggesting that the serotonin pathway is probably not the main driver of phenotypic changes in infected sticklebacks. Taken together, our results allow us to predict that if S.solidus directly manipulates its host, it could target the inositol pathway."
5521b9f69c38845962cc733711c6e9cf2cc003ca,
9c2be8e95814addbebc949e6abd0209252c40d56,"Many parasites with complex life cycles modify their intermediate hosts' behaviour, presumably to increase transmission to their final host. The threespine stickleback (Gasterosteus aculeatus) is an intermediate host in the cestode Schistocephalus solidus life cycle, which ends in an avian host, and shows increased risky behaviours when infected. We studied brain gene expression profiles of sticklebacks infected with S. solidus to determine the proximal causes of these behavioural alterations. We show that infected fish have altered expression levels in genes involved in the inositol pathway. We thus tested the functional implication of this pathway and successfully rescued normal behaviours in infected sticklebacks using lithium exposure. We also show that exposed but uninfected fish have a distinct gene expression profile from both infected fish and control individuals, allowing us to separate gene activity related to parasite exposure from consequences of a successful infection. Finally, we find that selective serotonin reuptake inhibitor-treated sticklebacks and infected fish do not have similarly altered gene expression, despite their comparable behaviours, suggesting that the serotonin pathway is probably not the main driver of phenotypic changes in infected sticklebacks. Taken together, our results allow us to predict that if S. solidus directly manipulates its host, it could target the inositol pathway."
d48c61e8bfd59dc8096e7de164d061fcfeb879cb,"Parasites with complex life cycles have been proposed to manipulate the behaviour of their intermediate hosts to increase the probability of reaching their final host. The cause of these drastic behavioural changes could be manipulation factors released by the parasite in its environment (the secretome), but this has rarely been assessed. We studied a non-cerebral parasite, the cestode Schistocephalus solidus, and its intermediate host, the threespine stickleback (Gasterosteus aculeatus), whose response to danger becomes significantly diminished when infected. These altered behaviours appear only during late infection, when the worm is ready to reproduce in its final avian host. Sympatric host–parasite pairs show higher infection success for parasites, suggesting that the secretome effects could differ for allopatric host–parasite pairs with independent evolutionary histories. We tested the effects of secretome exposure on behaviour by using secretions from the early and late infection of S. solidus and by injecting them in healthy sticklebacks from a sympatric and allopatric population. Contrary to our prediction, secretome from late infection worms did not result in more risky behaviours, but secretome from early infection resulted in more cautious hosts, only in fish from the allopatric population. Our results suggest that the secretome of S. solidus contains molecules that can affect host behaviour, that the causes underlying the behavioural changes in infected sticklebacks are multifactorial and that local adaptation between host–parasite pairs may extend to the response to the parasite's secretome content."
3a864f81c480124199e395d51805c9769a1769a3,"Several species show diversity in reproductive patterns that result from phenotypic plasticity. This reproductive plasticity is found for example in mate choice, parental care, reproduction suppression, reproductive tactics, sex role, and sex reversal. Studying the genome-wide changes in transcription that are associated with these plastic phenotypes will help answer several questions, including those regarding which genes are expressed and where they are expressed when an individual is faced with a reproductive choice, as well as those regarding whether males and females have the same brain genomic signature when they express the same behaviors, or if they activate sex-specific molecular pathways to output similar behavioral responses. The comparative approach of studying transcription in a wide array of species allows us to uncover genes, pathways, and biological functions that are repeatedly co-opted (“genetic toolkit”) as well as those that are unique to a particular system (“genomic signature”). Additionally, by quantifying the transcriptome, a labile trait, using time series has the potential to uncover the causes and consequences of expressing one plastic phenotype or another. There are of course gaps in our knowledge of reproductive plasticity, but no shortage of possibilities for future directions."
426205beb541bc830ba52f013a05fcddb12e9434,"Parasites with complex life cycles have been proposed to manipulate the behaviour of their intermediate hosts to increase the probability of reaching their final host. The cause of these drastic behavioural changes could be manipulation factors released by the parasite in its external environment (i.e. the secretome), but this has rarely been assessed. Here, we studied a non-cerebral parasite, the cestode Schistocephalus solidus, and its intermediate host, the threespine stickleback (Gasterosteus aculeatus), whose response to danger becomes significantly diminished when infected. These altered behaviours appear only when the worm is ready to reproduce in its final avian host. We tested the effects of secretome exposure on behaviour by using secretions from the early and late infection of S. solidus and by injecting it in healthy sticklebacks from a sympatric and allopatric population. Contrary to our prediction, secretome from late infection worms did not result in more risky behaviours in sticklebacks, but secretome from early infection parasite resulted in more cautious host, only in fish from the allopatric population. Our results suggest that the secretome of Schistocephalus solidus contains molecules that can affect host behaviour, that the causes underlying the behavioural changes in infected sticklebacks are multifactorial, and that local adaptation between host-parasite pairs may extend to the response to the parasite’s secretome content."
7218c757ba108c0e42440e06b971234eaaecb226,"Parasites with complex life cycles have been proposed to manipulate the behaviour of their intermediate hosts to increase the probability of reaching their final host. The cause of these drastic behavioural changes could be manipulation factors released by the parasite in its environment (the secretome), but this has rarely been assessed. We studied a non-cerebral parasite, the cestode Schistocephalus solidus, and its intermediate host, the threespine stickleback (Gasterosteus aculeatus), whose response to danger becomes significantly diminished when infected. These altered behaviours appear only during late infection, when the worm is ready to reproduce in its final avian host. Sympatric host-parasite pairs show higher infection success for parasites, suggesting that the secretome effects could differ for allopatric host-parasite pairs with independent evolutionary histories. We tested the effects of secretome exposure on behaviour by using secretions from the early and late infection of S. solidus and by injecting them in healthy sticklebacks from a sympatric and allopatric population. Contrary to our prediction, secretome from late infection worms did not result in more risky behaviours, but secretome from early infection resulted in more cautious hosts, only in fish from the allopatric population. Our results suggest that the secretome of Schistocephalus solidus contains molecules that can affect host behaviour, that the causes underlying the behavioural changes in infected sticklebacks are multifactorial, and that local adaptation between host-parasite pairs may extend to the response to the parasite’s secretome content."
798cf13bed5767c0b71f8e78cec048dad309671d,
91ae10bc484061fcc8a7af19610c479fbbf93fc4,"Background: The choice of model systems is important for research and researchers, and has been studied in the biomedical literature but not with regard to evolution and behaviour. Neither have changes in the use of the threespine stickleback, Gasterosteus aculeatus, as a model been examined quantitatively. Questions: What are the major vertebrate model systems in evolutionary biology and animal behaviour. How have they changed over the last 25 years, and how has work on the threespine stickleback model evolved in this period? Data: We collected publication rates from the Web of Science for candidate model organisms for three periods over the last 25 years. As a complement to this analysis, we conducted a more focused and inductive analysis on Gasterosteus, concerning which fields have used this model, by analysing the keywords co-occurring with ‘Gasterosteus’. To elucidate emerging trends involving stickleback, we analysed word frequencies in the abstracts of the current conference and of manuscripts in the stickleback special issues of Evolutionary Ecology Research, Volume 20. Results: For evolutionary biology, traditional biomedical models have declined in importance over the last 25 years whereas non-biomedical comparative models have shown the opposite trend. Patterns for behaviour are more complex, with some natural systems increasing in usage and newer biomedical models, such as Danio, replacing previously important ones. Salmonids proved unexpectedly important for both evolution and behaviour and Drosophila appeared in more publications than any vertebrate. Overall, model systems were stable or declining in usage, a pattern also reported in the biomedical literature. Our keyword analysis for Gasterosteus suggests that the stickleback has evolved as a model. At first it was used mainly in behaviour but now it is also being used extensively to study evolution as well as to address concerns over human-induced environmental changes, and to investigate other new topics. Abstracts from the conference and the special issues illustrate the diversity of stickleback research, including emphases on variation in phenotypes between habitats and between sexes and the evolutionary and ecological processes that lead to these differences, as well as on genome-level questions, interactions with parasites, and eco-evolutionary dynamics."
a7eb9952816fc4a2e2f672de0ece73415143c86a,"The social environment encountered early during development can temporarily or permanently influence life history decisions and behaviour of individuals and correspondingly shape molecular pathways. In the highly social cichlid fish Neolamprologus pulcher, deprivation of brood care permanently affects social behaviour and alters the expression of stress axis genes in juveniles and adults. It is unclear when gene expression patterns change during early life depending on social experience, and which genes are involved. We compared brain gene expression of N. pulcher at two time points during the social experience phase when juveniles were reared either with or without brood care, and one time point shortly afterwards. We compared (a) whole transcriptomes and (b) expression of 79 genes related to stress regulation, in order to define a neurogenomic state of stress for each fish. At developmental day 75, that is, after the social experience phase, 43 genes were down‐regulated in fish having experienced social deprivation, while two genes involved in learning and memory and in post‐translational modifications of proteins (PTM), respectively, were up‐regulated. Down‐regulated genes were mainly associated with immunity, PTM and brain function. In contrast, during the experience phase no genes were differentially expressed when assessing the whole transcriptome. When focusing on the neurogenomic state associated with the stress response, we found that individuals from the two social treatments differed in how their brain gene expression profiles changed over developmental stages. Our results indicate that the early social environment influences the transcriptional activation in fish brains, both during and after an early social experience, possibly affecting plasticity, immune system function and stress axis regulation."
ad20761ccae5af8aa0f05cf57ad43c2e560352f2,"Organism and background: Threespine stickleback (Gasterosteus aculeatus) from freshwater populations in Lake Témiscouata and Rond Lake (Québec, Canada) differ in predator defence morphology and behaviour. Individuals from Lake Témiscouata have more lateral plates, longer pelvic and dorsal spines, and a longer pelvic girdle than fish from Rond Lake. When raised in a common environment, Lake Témiscouata fish are also significantly less aggressive and more limited in their locomotor activity than those from Rond Lake. Neurological background: Several neuropeptides and their receptors are known to be key players in both the molecular networks that underlie variation in social behaviour, and those that govern the physiological response to stress. These molecules include arginine vasotocin (AVT), isotocin (IT), corticotropin-releasing factor (CRF), and their receptors. Thus individuals that differ in social (aggression, sociality) and stress-response behaviours (locomotor activity, exploration, response to predators) might also differ in the activity of these neuropeptides and their receptors. Question: Do the juveniles of Lake Témiscouata and Rond Lake diverge in the expression of these neuropeptides and their receptors, particularly in the context of a response to an acute stressor. Methods: We quantified the genomic reaction norm of common-environment-reared juveniles from each population by measuring expression in the brain of genes coding for AVT, IT, CRF, and receptor subtypes (AVTR1a, ITR, CRFR1, respectively) before and after an acute stress using quantitative PCR. Results: We found no significant effect of population of origin, stress treatment, or their interaction on the expression of the three neuropeptides studied (AVT, IT, CRF) or of the Correspondence: N. Aubin-Horth, Département de Biologie et Institut de Biologie Intégrative et des Systèmes (IBIS), 1030 Avenue de la Médecine, Université Laval, Québec, Québec G1V 0A6, Canada. email: Nadia.AubinHorth@bio.ulaval.ca Consult the copyright statement on the inside front cover for non-commercial copying policies. Evolutionary Ecology Research, 2019, 20: 331–347 © 2019 Nadia Aubin-Horth AVTR1a receptor. We found a significant difference in expression of the ITR receptor between the two populations, with Témiscouata fish exhibiting higher expression of that gene, both before and after a stress. We observed a tendency for Témiscouata fish to show a larger transcriptional stress response for the CRFR1 receptor. Thus receptors in these neuropeptide networks have evolved divergently in these two populations and might be functionally implicated in behavioural divergence."
f6cfa54f1e9f2fa9b725ddb17e6a05ec57916ddc,
1da98a457ff8191875bd681c6664e3da312eca97,
602daff8abf66efb00f5dc2cb7ac0f014f497f85,"Juveniles of the cooperatively breeding cichlid fish Neolamprologus pulcher either consistently provide help in form of alloparental egg care (“cleaners”) or consistently abstain from helping (“noncleaners”). These phenotypes are not based on heritable genetic differences. Instead, they arise during ontogeny, which should lead to differences in brain structure or physiology, a currently untested prediction. We compared brain gene expression profiles of cleaners and noncleaners in two experimental conditions, a helping opportunity and a control condition. We aimed to identify (a) expression differences between cleaners and noncleaners in the control, (b) changes in gene expression induced by the opportunity and (c) differences in plasticity of gene expression between cleaners and noncleaners. Control cleaners and noncleaners differed in the expression of a single gene, irx2, which regulates neural differentiation. During the opportunity, cleaners and noncleaners had three upregulated genes in common, which were implicated in neuroplasticity, hormonal signalling and cell proliferation. Thus, the stimulus in the opportunity was sufficiently salient. Cleaners also showed higher expression of seven additional genes that were unique to the opportunity. One of these cleaner‐specific genes is implicated in neuropeptide metabolism, indicating that this process is associated with cleaning performance. This suggests that the two types employed different pathways to integrate social information, preparing them for accelerated reaction to future opportunities. Interestingly, three developmental genes were downregulated between the control and the opportunity in cleaners only. Our results indicate that the two behavioural types responded differently to the helping opportunity and that only cleaners responded by downregulating developmental genes."
62f18424d6317517be6206bebad7346d693b1bfe,"ABSTRACT Many parasites with complex life cycles modify the behaviour of their intermediate host, which has been proposed to increase transmission to their definitive host. This behavioural change could result from the parasite actively manipulating its host, but could also be explained by a mechanical effect, where the physical presence of the parasite affects host behaviour. We created an artificial internal parasite using silicone injections in the body cavity to test this mechanical effect hypothesis. We used the Schistocephalus solidus and threespine stickleback (Gasterosteus aculeatus) system, as this cestode can reach up to 92% of its fish host mass. Our results suggest that the mass burden brought by this macroparasite alone is not sufficient to cause behavioural changes in its host. Furthermore, our results show that wall-hugging (thigmotaxis), a measure of anxiety in vertebrates, is significantly reduced in Schistocephalus-infected sticklebacks, unveiling a new altered component of behaviour that may result from manipulation by this macroparasite. Summary: The mechanical effect of the presence of Schistocephalus solidus parasite on behaviour in its host, the threespine stickleback, is tested using phenotypic engineering."
630d43608b6ed51b68028196848bf584ecf167b6,"Individuals within the same population generally differ among each other not only in their behavioral traits but also in their level of behavioral plasticity (i.e., in their propensity to modify their behavior in response to changing conditions). If the proximate factors underlying individual differences in behavioral plasticity were the same for any measure of plasticity, as commonly assumed, one would expect plasticity to be repeatable across behaviors and contexts. However, this assumption remains largely untested. Here, we conducted an experiment with sailfin mollies (Poecilia latipinna) whose behavioral plasticity was estimated both as the change in their personality traits or mating behavior across a social gradient and using their performance on a reversal-learning task. We found that the correlations between pairwise measures of plasticity were weak and non-significant, thus indicating that the most plastic individuals were not the same in all the tests. This finding might arise because either individuals adjust the magnitude of their behavioral responses depending on the benefits of plasticity, and/or individuals expressing high behavioral plasticity in one context are limited by neural and/or physiological constraints in the amount of plasticity they can express in other contexts. Because the repeatability of behavioral plasticity may have important evolutionary consequences, additional studies are needed to assess the importance of trade-offs between conflicting selection pressures on the maintenance of intra-individual variation in behavioral plasticity."
ece492f03c410f25c04018733e3bd032332e40d7,"ABSTRACT Detecting the presence of a parasite within its host is crucial to the study of host–parasite interactions. The Schistocephalus solidus–threespine stickleback pair has been studied extensively to investigate host phenotypic alterations associated with a parasite with a complex life cycle. This cestode is localized inside the stickleback's abdominal cavity and can be visually detected only once it passes a mass threshold. We present a non-lethal quantitative PCR (qPCR) approach based on detection of environmental DNA from the worm (eDNA), sampled in the fish abdominal cavity. Using this approach on two fish populations (n=151), 98% of fish were correctly assigned to their S. solidus infection status. There was a significant correlation between eDNA concentration and total parasitic mass. We also assessed ventilation rate as a complementary mean to detect infection. Our eDNA detection method gives a reliable presence/absence response and its future use for quantitative assessment of infection is promising. Summary: A non-lethal approach to detect an internal parasitic worm using environmental DNA extracted from fluids sampled from the fish host’s body cavity."
f28e0446cc3f6fa0edb55115f9bee5c4e7f35b32,"In vertebrates, the early social environment can persistently influence behaviour and social competence later in life. However, the molecular mechanisms underlying variation in animal social competence are largely unknown. In rats, high-quality maternal care causes an upregulation of hippocampal glucocorticoid receptors (gr) and reduces offspring stress responsiveness. This identifies gr regulation as a candidate mechanism for maintaining variation in animal social competence. We tested this hypothesis in a highly social cichlid fish, Neolamprologus pulcher, reared with or without caring parents. We find that the molecular pathway translating early social experience into later-life alterations of the stress axis is homologous across vertebrates: fish reared with parents expressed the glucocorticoid receptor gr1 more in the telencephalon. Furthermore, expression levels of the transcription factor egr-1 (early growth response 1) were associated with gr1 expression in the telencephalon and hypothalamus. When blocking glucocorticoid receptors (GR) with an antagonist, mifepristone (RU486), parent-reared individuals showed more socially appropriate, submissive behaviour when intruding on a larger conspecific's territory. Remarkably, mifepristone-treated fish were less attacked by territory owners and had a higher likelihood of territory takeover. Our results indicate that early social-environment effects on stress axis programming are mediated by an evolutionary conserved molecular pathway, which is causally involved in environmentally induced variation of animal social competence."
140ac895e771609d8cb898736905bde4112b619a,"The early social environment can have substantial, lifelong effects on vertebrate social behaviour, which can be mediated by developmental plasticity of brain gene expression. Early‐life effects can influence immediate behavioural responses towards later‐life social challenges and can activate different gene expression responses. However, while genomic responses to social challenges have been reported frequently, how developmental experience influences the shape of these genomic reaction norms remains largely unexplored. We tested how manipulating the early social environment of juvenile cooperatively breeding cichlids, Neolamprologus pulcher, affects their behavioural and brain genomic responses when competing over a resource. Juveniles were reared either with or without a breeder pair and a helper. Fish reared with family members behaved more appropriately in the competition than when reared without. We investigated whether the different social rearing environments also affected the genomic responses to the social challenge. A set of candidate genes, coding for hormones and receptors influencing social behaviour, were measured in the telencephalon and hypothalamus. Social environment and social challenge both influenced gene expression of egr‐1 (early growth response 1) and gr1 (glucocorticoid receptor 1) in the telencephalon and of bdnf (brain‐derived neurotrophic factor) in the hypothalamus. A global analysis of the 11 expression patterns in the two brain areas showed that neurogenomic states diverged more strongly between intruder fish and control fish when they had been reared in a natural social setting. Our results show that same molecular pathways may be used differently in response to a social challenge depending on early‐life experiences."
55ad2d58ec800843b09dd79b8603a1b356b0cfb8,"Parasites with complex life cycles have developed numerous phenotypic strategies, closely associated with developmental events, to enable the exploitation of different ecological niches and facilitate transmission between hosts. How these environmental shifts are regulated from a metabolic and physiological standpoint, however, still remain to be fully elucidated. We examined the transcriptomic response of Schistocephalus solidus, a trophically transmitted parasite with a complex life cycle, over the course of its development in an intermediate host, the threespine stickleback, and the final avian host. Results from our differential gene expression analysis show major reprogramming events among developmental stages. The final host stage is characterized by a strong activation of reproductive pathways and redox homoeostasis. The attainment of infectivity in the fish intermediate host—which precedes sexual maturation in the final host and is associated with host behaviour changes—is marked by transcription of genes involved in neural pathways and sensory perception. Our results suggest that un‐annotated and S. solidus‐specific genes could play a determinant role in host–parasite molecular interactions required to complete the parasite's life cycle. Our results permit future comparative analyses to help disentangle species‐specific patterns of infection from conserved mechanisms, ultimately leading to a better understanding of the molecular control and evolution of complex life cycles."
5a6e21cebd19f53b568584092f73f8ee635b9fab,
8a59faea777fa7ca4b52e3f985ea71cbd5cf7942,"To study parasite-host interactions, it is of major importance to detect the presence of the parasite during its development in the host. The Schistocephalus solidus-threespine stickleback pair has been used extensively to study host phenotypic alterations caused by a parasite with a complex life cycle. This cestode worm is localized inside the abdominal cavity of its intermediate fish host, which limits its detection to the time when it has grown substantially or to multiple infections. Here, we present a non-invasive species-specific quantitative real time PCR approach based on the detection of environmental DNA from the worm (eDNA), sampled directly in the abdominal cavity of the threespine stickleback. Using this approach on two fish populations (n=151), 98% of fish were correctly assigned to their S. solidus infection status. Among the 35 sticklebacks that harboured single or multiple infections, only 3 were not detected as infected. Furthermore, non-infected sticklebacks were never incorrectly identified as infected. There was a significant correlation between eDNA concentration and the total mass of parasites. We also assessed ventilation rates as a rapid and complimentary mean to detect infection (n=20). Ventilation rates were repeatable over 3 measurements, did not vary significantly between non-infected and infected fish, were not significantly correlated to the parasite index, and thus could not reliably be used to predict infection. Our eDNA approach could be used in this parasite-host system alone or in combination with morphological measurements to detect S. solidus."
f45bf7d61c8688f70da65cb734bb214c59bd3265,"Many parasites with complex life cycles modify their intermediate hosts’ behaviour, which has been proposed to increase transmission to their definitive host. This behavioural change could result from the parasite actively manipulating its host, but could also be explained by a mechanical effect, where the parasite’s physical presence affects host behaviour. We created an artificial internal parasite using silicone injections in the body cavity to test this mechanical effect hypothesis. We used the Schistocephalus solidus- threespine stickleback (Gasterosteus aculeatus) system, as this cestode can reach up to 92% of its fish host mass. Our results suggest that the mass burden brought by this macroparasite alone is not sufficient to cause behavioural changes in its host. Furthermore, our results show that wall-hugging (thigmotaxis), a measure of anxiety in vertebrates, is significantly reduced in Schistocephalus-infected sticklebacks, unveiling a new altered component of behaviour that may result from manipulation by this macroparasite."
fc52eb3af14cdeaa5217e0c09f477f858398f5ef,"ABSTRACT Sticklebacks infected by the parasitic flatworm Schistocephalus solidus show dramatic changes in phenotype, including a loss of species-typical behavioural responses to predators. The timing of host behaviour change coincides with the development of infectivity of the parasite to the final host (a piscivorous bird), making it an ideal model for studying the mechanisms of infection-induced behavioural modification. However, whether the loss of host anti-predator behaviour results from direct manipulation by the parasite, or is a by-product (e.g. host immune response) or side effect of infection (e.g. energetic loss), remains controversial. To understand the physiological mechanisms that generate these behavioural changes, we quantified the behavioural profiles of experimentally infected fish and attempted to replicate these in non-parasitized fish by exposing them to treatments including immunity activation and fasting, or by pharmacologically inhibiting the stress axis. All fish were screened for the following behaviours: activity, water depth preference, sociability, phototaxis, anti-predator response and latency to feed. We were able to change individual behaviours with certain treatments. Our results suggest that the impact of S. solidus on the stickleback might be of a multifactorial nature. The behaviour changes observed in infected fish might result from the combined effects of modifying the serotonergic axis, lack of energy and activation of the immune system. Summary: Hypotheses regarding causes of the behavioural modifications in threespine stickleback parasitized with the flatworm Schistocephalus solidus are tested by analyzing behaviour changes induced by experimental manipulations."
2dc088799c88a82605e2b9713a27a82159916e43,"Parasites with complex life cycles have developed numerous phenotypic strategies, closely associated with developmental events, to enable the exploitation of different ecological niches and facilitate transmission between hosts. How these environmental shifts are regulated from a metabolic and physiological standpoint, however, still remain to be fully elucidated. We examined the transcriptomic response of Schistocephalus solidus, a trophically-transmitted parasite with a complex life cycle, over the course of its development in an intermediate host, the threespine stickleback, and the final avian host. Results from our differential gene expression analysis show major reprogramming events among developmental stages. The final host stage is characterized by a strong activation of reproductive pathways and redox homeostasis. The attainment of infectivity in the fish intermediate host – which precedes sexual maturation in the final host and is associated with host behaviour changes – is marked by transcription of genes involved in neural pathways and sensory perception. Our results suggest that un-annotated and S. solidus-specific genes could play a determinant role in host-parasite molecular interactions required to complete the parasite’s life cycle. Our results permit future comparative analyses to help disentangle species-specific patterns of infection from conserved mechanisms, ultimately leading to a better understanding of the molecular control and evolution of complex life cycles."
5e331d017c90a84896877d12f00993b476d51f5d,"The environment experienced by females can have long-lasting effects on offspring phenotype. The objective of this study was to determine if maternal stress-induced behaviour reprogramming in offspring is found in brook char and to test whether cortisol is the main mediator, by separating the potential effects of cortisol from that of other potential maternal factors. We exposed female brook trout (Salvelinus fontinalis) to different parallel treatments during the oogenesis period: undisturbed as controls (1) fed cortisol through food (2) or physically stressed by handling once a week (3). Additionally, we exposed half of the control eggs to a cortisol suspension before fertilisation (4). Cortisol consumption and handling did not elevate either maternal plasma or egg cortisol, although egg cortisol level was significantly increased when eggs were bathed in the suspension. We measured spatial learning and memory, boldness and neophobia in 6 month-old offspring and found no effects of treatments on learning, memory or behaviour. Our results suggest that the relationship between maternal stress, circulating and egg cortisol levels, other maternal factors, and behavioural reprogramming is context and species-specific."
6429f204ba083e7c9727b957bea4f390177dabca,"Hypothesis: The stress response involves a series of endocrine cascades that allow an individual to cope with real or perceived stressors to maintain a normal state. Although differences in stress response have been reported between populations facing distinct ecological challenges, whether these responses originate from genetic variation or environmental effects remains unclear. If the variation is genetically based, it could be the result of natural selection. Question: Does variation in stress response reflect genetic variation? Organism: Threespine stickleback (Gasterosteus aculeatus) originating from two contrasting habitats (a freshwater and a marine population). Methods: We reared threespine stickleback originating from freshwater and marine populations in a common environment. We exposed them to a 30-minute confinement stress. We measured their ventilation rates during the first and thirtieth minutes of exposure. We also quantified whole-body cortisol concentration in fish from both populations assigned either to an undisturbed control group or to the 30-minute confinement stress. Results: Marine stickleback exhibited significantly higher stress reactivity as inferred from the ventilation rates. Marine fish also showed higher inter-individual variation in reactivity than freshwater individuals. For all fish there was a strong response in cortisol concentration to the confinement stress. However, the extent of the response differed between the two populations, with freshwater fish showing a more marked change in cortisol concentration. Conclusion: Genetic variation underlying divergence in stress reactivity is correlated with differences between marine and freshwater habitats."
78772a7479984c034662f6d447a23e466ea30fb6,"The molecular mechanisms underlying behavioural evolution following colonization of novel environments are largely unknown. Molecules that interact to control equilibrium within an organism form physiological regulatory networks. It is essential to determine whether particular components of physiological regulatory networks evolve or if the network as a whole is affected in populations diverging in behavioural responses, as this may affect the nature, amplitude and number of impacted traits. We studied the regulation of four physiological regulatory networks in freshwater and marine populations of threespine stickleback raised in a common environment, which were previously characterized as showing evolutionary divergence in behaviour and stress reactivity. We measured nineteen components of these networks (ligands and receptors) using mRNA and monoamine levels in the brain, pituitary and interrenal gland, as well as hormone levels. Freshwater fish showed higher expression in the brain of adrenergic (adrb2a), serotonergic (htr2a) and dopaminergic (DRD2) receptors, but lower expression of the htr2b receptor. Freshwater fish also showed higher expression of the mc2r receptor of the glucocorticoid axis in the interrenals. Collectively, our results suggest that the inheritance of the regulation of these networks may be implicated in the evolution of behaviour and stress reactivity in association with population divergence. Our results also suggest that evolutionary change in freshwater threespine stickleback may be more associated with the expression of specific receptors rather than with global changes of all the measured constituents of the physiological regulatory networks."
9724c8a021ae64b544380a2f8f29a106633f8c11,
b099867b37174e3d05e8c19930aec407c3db35bf,"In many species, under varying ecological conditions, social interactions among individuals result in the formation of dominance hierarchies. Despite general similarities, there are robust differences among dominance hierarchies across species, populations, environments, life stages, sexes, and individuals. Understanding the proximate mechanisms underlying the variation is an important step toward understanding the evolution of social behavior. However, physiological changes associated with dominance, such as gonadal maturation and somatic growth, often complicate efforts to identify the specific underlying mechanisms. Traditional gene expression analyses are useful for generating candidate gene lists, but are biased by choice of significance cut-offs and difficult to use for between-study comparisons. In contrast, complementary analysis tools allow one to both test a priori hypotheses and generate new hypotheses. Here we employ a meta-analysis of high-throughput expression profiling experiments to investigate the gene expression patterns that underlie mechanisms and evolution of behavioral social phenotypes. Specifically, we use a collection of datasets on social dominance in fish across social contexts, sex, and species. Using experimental manipulation to produce female dominance hierarchies in the cichlid Astatotilapia burtoni, heralded as a genomic model of social dominance, we generate gene lists, and assess molecular gene modules. In the dominant female gene expression profile, we demonstrate a strong pattern of up-regulation of genes previously identified as having male-biased expression and furthermore, compare expression biases between male and female dominance phenotypes. Using a threshold-free approach to identify correlation throughout ranked gene lists, we query previously published datasets associated with maternal behavior, alternative reproductive tactics, cooperative breeding, and sex-role reversal to describe correlations among these various neural gene expression profiles associated with different instances of social dominance. These complementary approaches capitalize on the high-throughput gene expression profiling from similar behavioral phenotypes in order to address the mechanisms associated with social dominance behavioral phenotypes."
bc83186bdff27c59c2811441aa34be9283e634ec,
ed68d08ee991ee445c61b05e0f8a392c6a57ca11,"In many species, under varying ecological conditions, social interactions among individuals result in the formation of dominance hierarchies. Despite general similarities, there are robust differences among dominance hierarchies across species, populations, environments, life stages, sexes, and individuals. Understanding the proximate mechanisms underlying the variation is an important step toward understanding the evolution of social behavior. However, physiological changes associated with dominance, such as gonadal maturation and somatic growth, often complicate efforts to identify the specific underlying mechanisms. Traditional gene expression analyses are useful for generating candidate gene lists, but are biased by choice of significance cut-offs and difficult to use for between-study comparisons. In contrast, complementary analysis tools allow one to both test a priori hypotheses and generate new hypotheses. Here we employ a meta-analysis of high-throughput expression profiling experiments to investigate the gene expression patterns that underlie mechanisms and evolution of behavioral social phenotypes. Specifically, we use a collection of datasets on social dominance in fish across social contexts, sex, and species. Using experimental manipulation to produce female dominance hierarchies in the cichlid Astatotilapia burtoni, heralded as a genomic model of social dominance, we generate gene lists, and assess molecular gene modules. In the dominant female gene expression profile, we demonstrate a strong pattern of up-regulation of genes previously identified as having male-biased expression and furthermore, compare expression biases between male and female dominance phenotypes. Using a threshold-free approach to identify correlation throughout ranked gene lists, we query previously published datasets associated with maternal behavior, alternative reproductive tactics, cooperative breeding, and sex-role reversal to describe correlations among these various neural gene expression profiles associated with different instances of social dominance. These complementary approaches capitalize on the high-throughput gene expression profiling from similar behavioral phenotypes in order to address the mechanisms associated with social dominance behavioral phenotypes."
f890e44fcfeb93623a7f64fa0c9c6667a668d62b,
c14c583b211eee3694ad376d712c64846b10c3ba,
ca412213a7d5162ee30864710e7ec69a1a334572,
ce614a44459c22f129599efa094387cd178376b0,
d7f207502c12b2243ba4a8f1a1bf3073fe4b34ae,
eb3387bef3abcc6c5fb89fc737476fb1eee535cc,"Behaviour is a central focus of interest in biology because it has an impact on several aspects of an organism's life. Evolutionary biologists have realised the advantage of an integrative approach that jointly studies the molecular, cellular and physiological levels of an individual to link them with the organismal behavioural phenotype. First, this mechanistic information helps in understanding physiological and evolutionary constraints acting on the behavioural response to the environment and its evolution. Second, it furthers our understanding of the process of molecular convergent evolution. Finally, we learn about natural variation in molecular, cellular and physiological traits present in wild populations and their underlying genetic basis, which can be a substrate for selection to act on. I illustrate these points using our work on behaviour variation in fishes. The information on the mechanistic bases of behaviour variation in various species and behaviours will contribute to an ecological annotation of genes and to uncover new mechanisms implicated in how this astonishing behavioural diversity arose, is maintained and will evolve."
1cfd5bf61bddacd4afb6aeb9f7b62fa28ecb27bf,
1d2b66be928a077ecc1aa4b0d4c97ee24ccf4345,"1. Recent advances in ecological genomics: from phenotypic plasticity to convergent and adaptive evolution and speciation Christian Landry and Nadia Aubin-Horth 2. Trait transitions in explicit ecological and genomic contexts: plant mating systems as case studies Vincent Castric, Sylvain Billiard and Xavier Vekemans 3. Revisiting Mortimer's genome renewal hypothesis: heterozygosity, homothallism, and the potential for adaptation in yeast Paul M. Magwene 4. Ecological genomics of adaptation and speciation in Fungi Jean-Baptiste Leducq 5. Integrating phenotypic plasticity within an ecological genomics framework: recent insights from the genomics, evolution, ecology, and fitness of plasticity Matthew Morris and Sean M. Rogers 6. Eco-evo-devo: the time has come Ehab Abouheif, Marie-Julie Fave, Ana Sofia Ibarraran-Viniegra, Maryna Lesoway, Ab Matteen Rafiqi and Rajendhran Rajakumar 7. Evolutionary and ecological genomics of developmental plasticity: novel approaches and first insights from the study of horned beetles Armin P. Moczek, Teiya Kijimoto, Emilie Snell-Rood , Guilherme Rocha, Melissa Pespeni and Karen Kafadar 8. Neurogenomics of behavioral plasticity Rayna M. Harris and Hans A. Hofmann 9. Ecological genomics of host behavior manipulation by parasites Francois Olivier Hebert and Nadia Aubin-Horth 10. Ecological epigenetics Holly J. Kilvitis, Mariano Alvarez, Christy M. Foust, Aaron W. Schrey, Marta Robertson and Christina L. Richards 11. The reproducibility of adaptation in the light of experimental evolution with whole genome sequencing Guillaume Achaz, Alejandra Rodriguez-Verdugo, Brandon S. Gaut and Olivier Tenaillon 12. Ecological Genomics of Host Shifts in Drosophila mojavensis Luciano M. Matzkin 13. The genomics of an adaptive radiation-insights across the Heliconius speciation continuum Megan Supple, Riccardo Papa, Brian Counterman and W. Owen McMillan 14. Merging ecology and genomics to dissect diversity in wild tomatoes and their relatives David C. Haak, Jamie L. Kostyun and Leonie C. Moyle 15. Integrated genomics approaches in evolutionary and ecological endocrinology Jun Kitano, Asano Ishikawa and Sean C. Lema 16. Evolutionary genomics of environmental pollution Andrew Whitehead 17. Signatures of natural selection and ecological differentiation in microbial genomes B. Jesse Shapiro"
51fe8f65fef0c32ae5cc5b18e5c4c4295c392d78,
52659e5a1fbb3506f82c4524e3b8bae390dd7194,
586a48e14fef5ebc8abacae56c9f3acd9b0b7340,"Colonisation of novel environments means facing new ecological challenges often resulting in the evolution of striking divergence in phenotypes. However, little is known about behavioural divergence following colonisation, despite the predicted importance of the role of behavioural phenotype-environment associations in adaptive divergence. We studied the threespine stickleback (Gasterosteus aculeatus), a model system for postglacial colonisation of freshwater habitats largely differing in ecological conditions from the ones faced by the descendants of the marine ancestor. We found that common-environment reared freshwater juveniles were less social, more active and more aggressive than their marine counterparts. This behavioural divergence could represent the result of natural selection that acted on individuals following freshwater colonisation, with predation as a key selection agent. Alternatively, the behavioural profile of freshwater juveniles could represent the characteristics of individuals that preferentially invaded freshwater after the glacial retreat, drawn from the standing variation present in the marine population."
6cae01c81080299229d3130380bb94c6bcdb2e0f,"Phenotypic plasticity is predicted to facilitate individual survival and/or evolve in response to novel environments. Plasticity that facilitates survival should both permit colonization and act as a buffer against further evolution, with contemporary and derived forms predicted to be similarly plastic for a suite of traits. On the other hand, given the importance of plasticity in maintaining internal homeostasis, derived populations that encounter greater environmental heterogeneity should evolve greater plasticity. We tested the evolutionary significance of phenotypic plasticity in coastal British Columbian postglacial populations of threespine stickleback (Gasterosteus aculeatus) that evolved under greater seasonal extremes in temperature after invading freshwater lakes from the sea. Two ancestral (contemporary marine) and two derived (contemporary freshwater) populations of stickleback were raised near their thermal tolerance extremes, 7 and 22 °C. Gene expression plasticity was estimated for more than 14 000 genes. Over five thousand genes were similarly plastic in marine and freshwater stickleback, but freshwater populations exhibited significantly more genes with plastic expression than marine populations. Furthermore, several of the loci shown to exhibit gene expression plasticity have been previously implicated in the adaptive evolution of freshwater populations, including a gene involved in mitochondrial regulation (PPARAa). Collectively, these data provide molecular evidence that highlights the importance of plasticity in colonization and adaptation to new environments."
f24aba74a2946fe91df5dc36c29678d3f0975f9b,"Abstract of a presentation that was presented at the Society for Integrative and Comparative Biology 2014 Annual Meeting, 3-7 January, Austin, TX, USA. Disciplines Medicine and Health Sciences | Social and Behavioral Sciences Publication Details O'Connor, C. M., Marsh-Rollo, S., Coirtz Ghio, S., Hick, K., Tan, J., Wong, M. Y. L., Reddon, A. R., AubinHorth, N. & Balshine, S. (2014). Two closely related cichlids with divergent social systems differ in socially relevant behaviours and molecular pathways. Integrative and Comparative Biology, 54 (Suppl. 1), e154-e154. Authors C M. O'Connor, S Marsh-Rollo, S Coirtz Ghio, K Hick, J Tan, Marian Y. L Wong, A R. Reddon, N AubinHorth, and S Balshine This journal article is available at Research Online: http://ro.uow.edu.au/smhpapers/1930 Meeting Abstract 28.1 Saturday, Jan. 4 13:30 Two closely related cichlids with divergent social systems differ in socially relevant behaviours and molecular pathways O'CONNOR, CM*; MARSH-ROLLO, S; CORTZ GHIO, S; HICK, K; TAN, J; WONG, MYL; REDDON, AR; AUBIN-HORTH, N; BALSHINE, S; McMaster University; McMaster University; Universite Laval; McMaster University; McMaster University; University of Wollongong; McMaster University; Universite Laval; McMaster University coconn@mcmaster.ca It is widely assumed that complex social behaviours arise from modifications to simpler behaviours and via small changes to the underlying molecular pathways. However, few empirical studies have explored variation between species in simple dimensions of social behaviour in conjunction with proximate mechanisms. Here, we make use of the diversity of social systems that have arisen through the radiation of African cichlid fishes to compare socially relevant behaviours and related molecular pathways between closely related cichlids with similar mating behaviour and ecological niches but divergent social systems. We found individuals of the groupliving species displayed higher social motivation and more sophisticated conflict resolution strategies than individuals of the non-grouping species. Furthermore, individuals of the group-living species had higher brain gene expression of the neuropeptides isotocin and vasotocin (the teleost fish homologues of oxytocin and vasopressin, respectively) and their receptors than did the non-grouping species. These results suggest that simple behaviours related to social motivation and conflict resolution are an important component of an overall group-living social system. Furthermore, we provide support for the notion that isotocin and vasotocin play a role in modulating social behaviour in cichlid fishes. Together, the combination of laboratory and field-based, behavioural and molecular results contribute to our understanding of how social systems evolve at the level of both simple behaviours and the underlying molecular mechanisms. Page 1 of 1 SICB 2014 meeting Abstract Details 18/07/2014 http://www.sicb.org/meetings/2014/schedule/abstractdetails.php?id=253"
931f7d949e652345d320f3baaf028af706b29ee9,"Extensive individual variation in spatial behaviour is a common feature among species that exhibit migratory life cycles. Nowhere is this more evident than in salmonid fishes; individual fish may complete their entire life cycle in freshwater streams, others may migrate variable distances at sea and yet others limit their migrations to larger rivers or lakes before returning to freshwater streams to spawn. This review presents evidence that individual variation in migratory behaviour and physiology in salmonid fishes is controlled by developmental thresholds and that part of the variation in proximal traits activating the development of alternative migratory tactics is genetically based. We summarize evidence that alternative migratory tactics co‐exist within populations and that all individuals may potentially adopt any of the alternative phenotypes. Even though intra‐specific genetic divergence of migratory tactics is uncommon, it may occur if female competition for oviposition sites results in spawning segregation of alternative phenotypes. Because of their polygenic nature, alternative migratory tactics are considered as threshold traits. Threshold traits have two characteristics: an underlying 'liability' trait that varies in a continuous fashion, and a threshold value which is responsible for the discreetness observed in phenotypic distribution. We review evidence demonstrating that body size is an adequate proxy for the liability trait controlling the decision to migrate, but that the same phenotypic outcome (anadromy or residency) may be reached by different developmental pathways. The evidence suggesting a significant heritable component in the development of alternative migratory tactics is subsequently reviewed, leading us to conclude that alternative migratory tactics have considerable potential to respond to selection and evolve. We review what is known about the proximal physiological mechanisms mediating the translation of the continuous value of the liability trait into a discontinuous migratory tactic. We conclude by identifying several avenues for future research, including testing the frequency‐dependent selection hypothesis, establishing the relative importance of adaptive phenotypic plasticity in explaining some geographic gradients in migratory behaviour and identifying the physiological and genetic basis of the switching mechanisms responsible for alternative migratory tactics."
ebabdf66027358c43b7ed2af1d0ef7a77e27d7d1,"Adult social behaviour can be persistently modified by early-life social experience. In rodents, such effects are induced by tactile maternal stimulation resulting in neuroendocrine modifications of the hypothalamic–pituitary–adrenal axis involved in stress responsiveness. Whether similar long-term alterations can occur in the hypothalamic–pituitary–interrenal (HPI) axis of poikilothermic vertebrates is unknown. We compared the expression of four genes of the HPI axis in adults of the cooperatively breeding cichlid Neolamprologus pulcher, which had been exposed to two early-life social treatments 1.5 years prior to brain sampling. Fish reared with parents and siblings had less brain expression of corticotropin-releasing factor and of the functional homologue of the mammalian glucocorticoid receptor (GR1) than individuals reared with same-age siblings only. Expression of the mineralocorticoid receptors (MR) did not differ between treatments, but the MR/GR1 expression ratio was markedly higher in fish reared with parents and siblings. Thus, we show here that early social experience can alter the programming of the stress axis in poikilothermic vertebrates, suggesting that this mechanism is deeply conserved within vertebrates. Moreover, we show for the first time that reprogramming of the stress axis of a vertebrate can be induced without tactile stimulation by parents."
5851c47b65755f1e21dd6ec3d4f24b2f9c97bf2c,
8761ddb9e22c6c1877bb06286f3e0e83de02ac8c,
95843f54b93bce3fa50336474e451d0d93b3f4ff,
a1e6a76276ba25b6ecde6de4e7876b5a5776df37,
c2982977ea291c4ce570c2796145bfddea714b80,
c9377b8683bb2e163683f505edcffd61a7abcd33,"Among-population differences in morphology and behaviors such as boldness have been shown to co-vary with eco- logical conditions, including predation regime. However, between- and within-population covariation of predator defense mor- phology with variation in behaviors relevant to ecology and evolution (boldness, exploration, activity, sociability and aggressive- ness, often defined as personality traits when they are consistent across time and contexts) have never been quantified together in a single study in juvenile fish from populations found in contrasting environments. We measured predator defense morphology differences between adults from two freshwater populations of threespine sticklebacks with different ecological conditions. We then quantified five behaviors in juveniles from both populations raised in a common environment. Wild-caught adults showed significant differences in predator defense morphology. One population had significantly lower lateral plate number, shorter dorsal spine, pelvic spine and pelvic girdle. Furthermore, 61% of individuals from that population showed an absence of pelvic spine and girdle. At the population level, we found that differences in defense morphology in adults between the two lakes were coupled with differences in behaviors in juveniles raised in a common environment. Levels of activity, aggressiveness and boldness were higher in juveniles from the population lacking predator defense structures. At the individual level, anti-predator morphology of adult females could not predict their offspring's behavior, but juvenile coloration predicted individual boldness in a popula- tion-specific manner. Our results suggest that ecological conditions, as reflected in adult predator defense morphology, also affect juvenile behavior in threespine sticklebacks, resulting in trait co-specialization, and that there is a genetic or epigenetic compo- nent to these behavioral differences (Current Zoology 58 (1): 53-65, 2012)."
e73effef6470963f128ddf772c89dc6b036725b5,
d386bc0948972d3a0fc65c93ad47a05bdd062035,"Consistent individual differences in behaviour, aka personality, pose several evolutionary questions. For example, it is difficult to explain within-individual consistency in behaviour because behavioural plasticity is often advantageous. In addition, selection erodes heritable behavioural variation that is related to fitness, therefore we wish to know the mechanisms that can maintain between-individual variation in behaviour. In this paper, we argue that whole genome expression data can reveal new insights into the proximate mechanisms underlying personality, as well as its evolutionary consequences. After introducing the basics of whole genome expression analysis, we show how whole genome expression data can be used to understand whether behaviours in different contexts are affected by the same molecular mechanisms. We suggest strategies for using the power of genomics to understand what maintains behavioural variation, to study the evolution of behavioural correlations and to compare personality traits across diverse organisms."
d87acfd1e2eb0efe6d22e74e4a317647e5c2f6dd,
ea1da1eb6b36ad1affe38cd82a73abba7438f7d5,"New technologies promise to revolutionize the field of molecular ecology. This technological progress comes with its own set of challenges. Among the most important ones is the analysis and interpretation of the data in a way that tells us about the molecular causes of the phenotype of interest and its consequences. In this issue, Whitehead et al. (2010) reveal part of the mechanistic basis of evolved pollution tolerance by studying the developmental and transcriptional response of tolerant and sensitive fish embryos to polychlorinated biphenyls (PCBs), a pollutant commonly found in coastal waters of the United States. By integrating their gene expression profiling data with phenotypic data on individuals along with what is known about pathways by which this pollutant acts in zebrafish and mammals, they are able to suggest detailed mechanisms that have evolved to allow a fish population to adapt to a very damaging pollutant and develop normally."
2c3ef2fc13b5eaa19ca0ef7d0b73e68cc54b5bfb,
38eee16054a08af1faacfeb34ff5c6298e7e9637,
d68085f8e34957a8193de266003bb64ec9581fe7,
e209511ec610eac6cdcd491f9b39f2938bc53005,"Phenotypic plasticity is the development of different phenotypes from a single genotype, depending on the environment. Such plasticity is a pervasive feature of life, is observed for various traits and is often argued to be the result of natural selection. A thorough study of phenotypic plasticity should thus include an ecological and an evolutionary perspective. Recent advances in large‐scale gene expression technology make it possible to also study plasticity from a molecular perspective, and the addition of these data will help answer long‐standing questions about this widespread phenomenon. In this review, we present examples of integrative studies that illustrate the molecular and cellular mechanisms underlying plastic traits, and show how new techniques will grow in importance in the study of these plastic molecular processes. These techniques include: (i) heterologous hybridization to DNA microarrays; (ii) next generation sequencing technologies applied to transcriptomics; (iii) techniques for studying the function of noncoding small RNAs; and (iv) proteomic tools. We also present recent studies on genetic model systems that uncover how environmental cues triggering different plastic responses are sensed and integrated by the organism. Finally, we describe recent work on changes in gene expression in response to an environmental cue that persist after the cue is removed. Such long‐term responses are made possible by epigenetic molecular mechanisms, including DNA methylation. The results of these current studies help us outline future avenues for the study of plasticity."
eb4b58911dd18a05edfca85ccfbed7009b999a78,"SUMMARY Behavior and physiology are regulated by both environment and social context. A central goal in the study of the social control of behavior is to determine the underlying physiological, cellular and molecular mechanisms in the brain. The African cichlid fish Astatotilapia burtoni has long been used as a model system to study how social interactions regulate neural and behavioral plasticity. In this species, males are either socially dominant and reproductively active or subordinate and reproductively suppressed. This phenotypic difference is reversible. Using an integrative approach that combines quantitative behavioral measurements, functional genomics and bioinformatic analyses, we examine neural gene expression in dominant and subordinate males as well as in brooding females. We confirm the role of numerous candidate genes that are part of neuroendocrine pathways and show that specific co-regulated gene sets (modules), as well as specific functional gene ontology categories, are significantly associated with either dominance or reproductive state. Finally, even though the dominant and subordinate phenotypes are robustly defined, we find a surprisingly high degree of individual variation in the transcript levels of the very genes that are differentially regulated between these phenotypes. The results of the present study demonstrate the molecular complexity in the brain underlying social behavior, identify novel targets for future studies, validate many candidate genes and exploit individual variation in order to gain biological insights."
42b756ac1c2b8c92a629220f712223f32ff51b58,"The molecular mechanisms underlying complex social behaviours such as dominance are largely unknown. Studying the cooperatively breeding African cichlid Neolamprologus pulcher, we show that dominant females were similar to dominant males in dominance behaviour, high testosterone levels and brain arginine vasotocin expression (a neuropeptide involved in vertebrate territorial, reproductive and social behaviours) compared to subordinate helpers, but had lower levels of 11‐ketotestosterone than males. Furthermore, brain gene expression profiles of dominant females were most similar to those of the males (independent of social rank). Dominant breeder females are masculinized at the molecular and hormonal level while being at the same time reproductively competent, suggesting a modular organization of molecular and endocrine functions, allowing for sex‐specific regulation."
791a14f20841922efe7b909d92d059fe8da2e2d6,"Here we critically review the scale and extent of adaptive genetic variation in Atlantic salmon (Salmo salar L.), an important model system in evolutionary and conservation biology that provides fundamental insights into population persistence, adaptive response and the effects of anthropogenic change. We consider the process of adaptation as the end product of natural selection, one that can best be viewed as the degree of matching between phenotype and environment. We recognise three potential sources of adaptive variation: heritable variation in phenotypic traits related to fitness, variation at the molecular level in genes influenced by selection, and variation in the way genes interact with the environment to produce phenotypes of varying plasticity. Of all phenotypic traits examined, variation in body size (or in correlated characters such as growth rates, age of seaward migration or age at sexual maturity) generally shows the highest heritability, as well as a strong effect on fitness. Thus, body size in Atlantic salmon tends to be positively correlated with freshwater and marine survival, as well as with fecundity, egg size, reproductive success, and offspring survival. By contrast, the fitness implications of variation in behavioural traits such as aggression, sheltering behaviour, or timing of migration are largely unkown. The adaptive significance of molecular variation in salmonids is also scant and largely circumstantial, despite extensive molecular screening on these species. Adaptive variation can result in local adaptations (LA) when, among other necessary conditions, populations live in patchy environments, exchange few or no migrants, and are subjected to differential selective pressures. Evidence for LA in Atlantic salmon is indirect and comes mostly from ecological correlates in fitness‐related traits, the failure of many translocations, the poor performance of domesticated stocks, results of a few common‐garden experiments (where different populations were raised in a common environment in an attempt to dissociate heritable from environmentally induced phenotypic variation), and the pattern of inherited resistance to some parasites and diseases. Genotype × environment interactions occurr for many fitness traits, suggesting that LA might be important. However, the scale and extent of adaptive variation remains poorly understood and probably varies, depending on habitat heterogeneity, environmental stability and the relative roles of selection and drift. As maladaptation often results from phenotype‐environment mismatch, we argue that acting as if populations are not locally adapted carries a much greater risk of mismanagement than acting under the assumption for local adaptations when there are none. As such, an evolutionary approach to salmon conservation is required, aimed at maintaining the conditions necessary for natural selection to operate most efficiently and unhindered. This may require minimising alterations to native genotypes and habitats to which populations have likely become adapted, but also allowing for population size to reach or extend beyond carrying capacity to encourage competition and other sources of natural mortality."
eb7651d878f68b60e9e342cbe3b68aa88c3b489b,"Ecological genomics is a research field that aims to determine how a genome or a population of genomes interacts with its environment across ecological and evolutionary timescales. This matter was the central theme of the symposium on Ecological Genomics that took place at the First meeting of the Canadian Society for Ecology and Evolution, held at the University of Toronto in May 2007. Through their research on a diverse array of organisms, the various speakers illustrated how ecology and evolution benefit from genomics, and indirectly how genomics can benefit from evolutionary ecology."
0887dbdb4c8680809758c9fd9dfc048d82053517,"Atlantic salmon (Salmo salar) males may mature early in life in freshwater, rather than maturing after a migration to sea, if their size is above a threshold value. We analyzed the spatiotemporal variation in size and incidence of the early maturity tactic among males over an 8-year period in six subpopulations on two branches of a river and collected environmental data on each site and across the river scape. A positive longitudinal trend in the frequency of early maturing males that was stable over the 8-year period occurred from the mouth to the head of the river. Threshold sizes for early maturation varied among subpopulations; size thresholds for male parr to mature were higher in downstream habitats and lowest upstream. This pattern was consistent in both river branches over the 8-year period and was not related to either the density of parr or site-specific abiotic habitat characteristics. However, the cumulative incidence of habitat features that could impede migration of large individuals increas..."
849e16daed342e0b1863e322c7ef1e1bbe688ded,
05ffbb56c210f0718a6093d24ea1b48c08a1f514,"Atlantic salmon (Salmo salar) undergo spectacular marine migrations before homing to spawn in natal rivers. However, males that grow fastest early in life can adopt an alternative ‘sneaker’ tactic by maturing earlier at greatly reduced size without leaving freshwater. While the ultimate evolutionary causes have been well studied, virtually nothing is known about the molecular bases of this developmental plasticity. We investigate the nature and extent of coordinated molecular changes that accompany such a fundamental transformation by comparing the brain transcription profiles of wild mature sneaker males to age-matched immature males (future large anadromous males) and immature females. Of the ca. 3000 genes surveyed, 15% are differentially expressed in the brains of the two male types. These genes are involved in a wide range of processes, including growth, reproduction and neural plasticity. Interestingly, despite the potential for wide variation in gene expression profiles among individuals sampled in nature, consistent patterns of gene expression were found for individuals of the same reproductive tactic. Notably, gene expression patterns in immature males were different both from immature females and sneakers, indicating that delayed maturation and sea migration by immature males, the ‘default’ life cycle, may actually result from an active inhibition of development into a sneaker."
95f2cfaf8c2e9d129526bff91d0366d698841d32,
a217e1e415dce314c7c2e02370fc0a0c4cf1a097,"Question: Does fluctuating natural selection on body size of fish among years (balancing selection) influence the frequency of an alternative male reproductive tactic? Hypothesis: When the surviving juveniles of a cohort are larger because of selection, a higher proportion of the population will develop as mature ‘sneaker’ males than expected in the absence of selection. In the case where selection favours smaller individuals, a lower proportion will develop as mature ‘sneaker’ males. Organisms: Juvenile Atlantic salmon (Salmo salar) from a naturally sustained population in the Ste-Marguerite River, Centre Interuniversitaire de Recherche sur le Saumon Atlantique, Province of Quebec, Canada. Methods: The presence of balancing size-selection was examined by measuring the proportional shift of the mean size and variance of juvenile salmon sampled in the autumn and following spring at multiple sites for each of three annual cohorts. The proportional shift in mean size of individuals over winter was then correlated with the incidence of early male maturity (sneakers) observed at the same sites for each cohort the following fall. Conclusions: Winter mortality decreased the size of surviving fish in one cohort by 7.8% on average, increased size by 2.7% on average in another and had little effect on the third. Proportionally more juvenile males adopted the sneaker tactic when juveniles surviving winter were larger, whereas fewer juvenile males adopted the sneaker tactic when surviving juveniles were smaller. The fluctuating nature of selection on body size indirectly maintains life-cycle divergence through a direct effect on size frequencies within a cohort."
e11e5596338916b0e694b3fbb02d4008a0c4ec14,"Organisms that share the same genotype can develop into divergent phenotypes, depending on environmental conditions. In Atlantic salmon, young males of the same age can be found either as sneakers or immature males that are future anadromous fish. Just as the organism-level phenotype varies between divergent male developmental trajectories, brain gene expression is expected to vary as well. We hypothesized that rearing environment can also have an important effect on gene expression in the brain and possibly interact with the reproductive tactic adopted. We tested this hypothesis by comparing brain gene expression profiles of the two male tactics in fish from the same population that were reared in either a natural stream or under laboratory conditions. We found that expression of certain genes was affected by rearing environment only, while others varied between male reproductive tactics independent of rearing environment. Finally, more than half of all genes that showed variable expression varied between the two male tactics only in one environment. Thus, in these fish, very different molecular pathways can give rise to similar macro-phenotypes depending on rearing environment. This result gives important insights into the molecular underpinnings of developmental plasticity in relationship to the environment."
1c00e5406cb3c214e38ab27bfe96d063f24b28bc,
83b9e194b7fa5e16cfdb93ebc3e303625be06774,"Abstract In the conditional strategy model, divergence in reproductive phenotypes depends on whether the individual's condition is above or below a genetically determined threshold. The relative contribution of the genetic and environmental components that lead to the expression of a reproductive tactic by an individual is not well understood. In the present field study, we determined when condition diverged between males that develop the mature parr phenotype and those that do not in Atlantic salmon (Salmo salar). We also investigated the uniformity of the threshold value in the population. We sampled mature parr and immature males at age one, of the same population at six different sites for four consecutive years. Our study provides an example of the interaction of genotype and environment on the expression of a reproductive tactic. Size was significantly greater for future mature parr than for future immature males as early as 20 days after hatching (emergence), suggesting that there may be a parental effect component in the tactic adopted, since no exogenous feeding takes place before this time. Size advantage at emergence was maintained through the next spring at age one to different degrees depending on the year, thus suggesting the presence of an environmental component of tactic expression. Our results support the contention that within the conditional strategy, the environment faced by a male and his condition at the moment of reproduction consistently predicts neither the environment faced by his offspring nor the fitness they will obtain by expressing the same tactic as their father. Furthermore, higher mean size at a site did not always translate into a higher proportion of mature parr, therefore supporting the hypothesis that thresholds vary across habitats within the same population."
85c73f65c450bcebd32cfca647f65ff571b70d4d,
d99070df49000063153888b25d7a1184ce49871c,
f97eda961f0c98b859770323ad688f0b1f489aff,
d1116f35b0e184fd4a4d23ed1913bb8cd73a64de,"We studied juvenile Atlantic salmon (Salmo salar) males that become precociously mature or not at age-1+ to test the hypothesis that differential energy allocation affects the relationship between otolith size and fish size and to validate the use of a back-calculation method to estimate size over 30 weeks. We used a longitudinal approach by re- peatedly measuring marked fish and obtaining corresponding otolith radius measurements. Differential energy allocation of mature males did not affect the proportionality ratio between otolith and somatic size. Short-term otolith growth var - ied with short-term somatic growth, but only weakly with temperature. Some correlation coefficients of the covariation of otolith growth estimated over a longer time interval with somatic growth were significantly greater than the short- term estimate. For mature and immature males, back-calculated lengths accurately estimated the observed individual length on practically all occasions. These results indicate that back-calculation can be used to estimate size for Atlantic salmon with different energy allocation patterns. Variable strength of coupling of otolith and somatic growth depending on time interval suggests that these processes are completed on different time scales. Resume : Nous avons etudie des juveniles mâles de saumon atlantique (Salmo salar) qui deviennent sexuellement ma- tures de facon precoce ou non a l'âge 1+ pour tester l'hypothese selon laquelle une allocation differentielle de l'energie affecte la relation entre la taille de l'otolithe et celle du poisson et pour valider l'utilisation d'une methode de retro- calcul permettant d'estimer la taille anterieure sur trente semaines. Nous avons utilise une approche longitudinale en mesurant de facon repetee des poissons marques et en obtenant les mesures correspondantes du rayon de l'otolithe. La presence d'une allocation differentielle de l'energie chez les mâles precoces n'a pas affecte le rapport de proportionna- lite entre la croissance de l'otolithe et du poisson. La croissance de l'otolithe a court terme variait avec la croissance somatique a court terme mais marginalement avec la temperature. Les coefficients de correlation calcules pour la cova- riation de la croissance de l'otolithe et somatique estimees sur de plus longues periodes etaient parfois significative- ment plus eleves que l'estime a court terme. Les tailles retro-calculees estimaient adequatement la taille individuelle observee a presque toutes les occasions, et ce pour les mâles matures et immatures. Ces resultats indiquent que le re - tro-calcul peut etre utilise pour estimer la taille de saumon atlantique presentant des patrons differents d'allocation d'energie et que la croissance otolithique et somatique sont des processus completes sur des echelles temporelles diffe - rentes. Aubin-Horth and Dodson 1583"
e7b0666c6f04d3bdf5a50827e2eac789063eec64,We compared the stereocinematographic (SCG) method to estimate activity rates of yellow perch (Perca flavescens) with the more traditional bioenergetic approach. We also compared activity rates of perch from two populations with contrasting growth rates to test the hypothesis that fish with lower growth rates are characterized by higher activity rates. We attempted to corroborate the SCG method by comparing values of energetic costs obtained with observations of fish movements with estimates obtained using the difference between field-derived consumption and growth rates (bioenergetic method). Independent estimates of consumption and activity rates were obtained for Lakes Hertel (average growth = 172 J·day-1) and Memphremagog (average growth = 595 J·day-1). Daily consumption rates averaged 720 J·day-1 in Lake Hertel and 1457 J·day-1 in Lake Memphremagog. SCG and bioenergetic methods provided similar activity estimates for Lake Hertel (<2.5% difference) and diverged by 0.4-82% for Lake Memphremagog dependi...
b13c36677624370c25caa8cd22cbb086e30bb857,
47160da6af391e96c65d2d6b4714dd9a2332af08,"Knowledge practices mirroring is one of the central elements of the Trialogical Learning Approach as it is supposed to be a driving force in processes of practice transformation and knowledge creation. The exploitation of historical logging data holds promise to provide a great deal of information about group activities without requiring additional efforts for recording of events by the users. Building on the Trialogical Learning Approach as well as related work in the fields of Data Mining and Knowledge Discovery, Computer-Supported Collaborative Learning, and Information Visualization, this paper suggests high-level requirements for mirroring tools in support of practice transformation and introduces a software tool called Timeline-Based Analyzer (TLBA) that was designed and developed in response to these requirements. One of the main TLBA features is the possibility to define patterns as sequences of relevant actions that resulted into critical moments in investigated practices. Such kind of patterns might also represent conceptually interesting practices that emerged within a particular context - either being positive (a sort of good practices), or negative (bad practices). The usability of the whole analytical solution has been tested through the first iteration of several practical experiments and case studies. One of them is described in this paper and illustrates how TLBA can be used to support collaborative analysis and mirroring. The results of these evaluations have been used for continued improvement of the TLBA in order to provide a stable and intuitive tool not only for researchers, but for daily use of teachers or other users."
bcf9593b78b06f12753e56423f989b6e9a6c5c4d,
285a850aac17fa87a8a98c9ba6ce9a5d761d531a,"This deliverable describes the M36 release of the End user applications for knowledge practices software and covers the technical development performed by M38 (March 2009) within WP6 according to Description of Work 3.1 and D6.6 M33 specification of end-user applications. - Knowledge Practices Environment (KPE) is a rich internet application that supports collaborative knowledge creation processes. It provides a shared knowledge space with versatile tools for developing and managing knowledge artefacts, organizing processes and people, and reflecting on practices from several perspectives. KPE includes a set of tools (e.g. real-time and history based awareness, wiki, note editor, commenting, chat, semantic tagging, semantic search and visual modeling tools) for working with the shared knowledge objects. - Semantic Multimedia Annotation Tool (SMAT) is a rich internet application that facilitates an individuals or a groups activity of assigning annotations to the document fragments. The tools allow users to plan and organise their annotation activity, structure any multimedia document by dividing it into fragments, annotate the document formally, informally or by linking external documents to specific anchors, as well as to analyse structurally and statistically annotations and visualize analysis results. SMAT is adaptable to the users domain. -Activity System Design Tools (ASDT) enable users to analyze the history, present and future of their work activity in a way that helps address issues critical for deliberate transformation of prevailing practices. ASDT is a plug-in to Knowledge Practices Environment, utilizing its views and functionalities. In addition, ASDT has a specific view, Virtual whiteboard, which is designed based on the key elements of a developmental work research process and its conceptual tools. -Map-It is a tool that supports preparation, execution and analysis of meetings (face-to-face and remote). Synchronous and asynchronous interactions are possible through the collaborative elaboration of ""discussion maps"". Map-It allows the use of meeting templates, advance individual preparations, share of artifacts, planning and follow-up of actions, automatic generation of meeting minutes in various formats. Through M2T, a rich internet application, it also provides analytical tools for exploring meeting practices and their integration in largerscope activities, by connecting them to other KP-Lab concepts, visualisations (KPEs Content Views) and tools (ToDo) - CASS Query is used for collecting process- and context-sensitive data. The CASS Query Client runs in a mobile telephone allowing participant to respond to queries. The CASS Query Admin tool is a web application for entering participant info and creating the queries. -CASS Memo is used for note-taking and multimedia recordings on mobile phones to transmit the data to the KPE. The users are also able to browse the content items of the separate shared spaces in KPE and download them into their mobile phone. This deliverable provides the general description of tools in terms of targeted users, requirements for the use, new features, known issues, as well as deviations from the D6.6 specifications. More details can be found through the material available in the project intranet at: http://www.kp-lab.org/intranet/testable-tools/kp-lab-tools/"
8b36077b332bc51b776124d809680f0005063fe4,"The present deliverable provides a high-level view on the new specifications of end user applications defined in the WPII during the M37-M46 period of the KP-Lab project. This is the last in the series of four deliverables that cover all the tools developed in the project, the previous ones being D6.1, D6.4 and D6.6. This deliverable presents specifications for the new functionalities for supporting the dedicated research studies defined in the latest revision of the KP-Lab research strategy. The tools addressed are: the analytic tools (Data export, Time-line-based analyser, Visual analyser), Clipboard, Search, Versioning of uploadable content items, Visual Model Editor (VME) and Visual Modeling Language Editor (VMLE). The main part of the deliverable provides the summary of tool specifications and the description of the Knowledge Practices Environment architecture, as well as an overview of the revised technical design process, of the tools relationship with the research studies, and of the driving objectives and the high-level requirements relevant for the present specifications. The full specifications of tools are provided in the annexes 1-9."
d44ce20b7687e47ae6c1e88e0d7e3cef8017503b,"This paper presents proposed solution for analyses of performed actions within a virtual intelligent and interactive environment. Investigated environment provides important functionalities that are based on principles of Ambient intelligence paradigm, i.e. context awareness, personalized and adaptive environment, or access through mobile devices. All activities and changes are monitored and stored into a database to provide one source of data for analytical purposes. The other separate (possibly distributed) databases are various knowledge and content repositories with integrated support services for access by different actors in the environment; and an user database with information about users. Data from these repositories are used for visualization of the whole evolution processes or its selected parts through time-line. The visualization contains information about performed events, relevant objects and actors, interactions between users or between users and environment, all described by semantic metadata. Users will have possibility to create potentially interesting working or critical patterns that will be evaluated through historical data. Results of this evaluation can be seen as new practices, ready for further re-use."
75e288e563b029c566019a44940fa5908697bceb,"This presentation was part of the session : Technologies That Support the Latest Pedagogical Frameworks, KP-Lab"
a2e7bbbae9d3473c61be5dd809b0213de307fd8f,"This deliverable describes the M24 release of the End user applications for knowledge practices software v2.0.0. The deliverable includes the technical development performed until M24 (January 2008) within WP6 according to Description of Work 2.1 and D6.4 M21 specification of end-user applications. The current release is comprised of two set of tools: 1. Shared Space Tool The shared space and the accompanying support material can be found on the Internet at: http://2d.mobile.evtek.fi:8080/shared-space 2. Map-It. The installer program for Map-It v2.0.0 is available at: http://www.kp-lab.org/intranet/testable-tools/kp-lab-tools/map-it/map-it-2-0.0 Please consult the ""Getting Started"" Note before installing and using Map-It: http://www.kp-lab.org/intranet/testable-tools/kp-lab-tools/map-it/getting-started-with-map-it 3. Change Laboratory tools The release targeted for the end users participating in the trials planned to be conducted in the CL Working Knot can be accessed via the following link: http://2d.mobile.evtek.fi:8080/shared-space/cl.html Anyone who wishes to try the software out but is not participating in the Change Laboratory trials should use the development deployment on: http://mielikki.mobile.evtek.fi/shared-space/cl.html The M24 release of Semantic Multimedia Annotation tools is still delayed. The release of CASS Memo Client has been postponed to be included in the M28 release in DoW3."
cdb3c164bd075827976b143fc61222ad42f91056,"This paper presents some aspects of the IST project called Knowledge Practices Laboratory (KP-Lab), which aims at facilitating innovative practices of working with knowledge (knowledge practices) in education and workplaces. The project is based on the idea of trialogical learning that refers to the process where learners are collaboratively developing shared objects of activity (such as conceptual artifacts, practices, products) in systematic fashion. It is important do develop relevant flexible tools and functionalities for supporting this new innovative approach like semantic middleware with collaborative working or learning environment, etc."
1ef407e5aea4c1f4cb399ba781bdf188d81979c4,"Background Scientific knowledge on disability pension (DP) after revascularization by coronary artery bypass grafting (CABG) and percutaneous coronary intervention (PCI) is scarce. The aim was to study the incidence of and risk factors for being granted DP in the 5 years following a first CABG or PCI, accounting for socio-demographic and medical factors. Methods This is a nationwide population-based study using Swedish registers including all patients 30–63 years of age (n = 34,643, 16.4% women) who had a first CABG (n = 14,107) or PCI (n = 20,536) during 1994–2003. All were alive and without reintervention 30 days after the procedure and were not on DP or old-age pension. Multivariable adjusted Cox proportional hazard ratios (HR) for DP were estimated with 95% confidence intervals (CI). Results In 5 years following revascularization, 32.4% had been granted DP and the hazard ratio (HR) was higher in women (HR 1.55, 95% CI 1.48–1.62), and in CABG patients compared with PCI patients (HR 1.35, 95% CI 1.30–1.40). Long-term sick leave in the year before intervention was the strongest predictor for DP following revascularization. After adjustments for socio-demographic factors and sick-leave days in the 12 months before revascularization, HR remained high in all patients with diabetes mellitus regardless of type of revascularization. Conclusions DP after coronary revascularization was common, especially among women and CABG patients. Most studied medical covariates, including mental and musculoskeletal disorders, were risk factors for future DP, especially long-term sickness absence."
865a7d928b4d07946e1d7e55cbfe4c102c349d52,"Background Although coronary revascularisation by coronary artery bypass graft surgery (CABG) and percutaneous coronary intervention (PCI) is well documented, scientific knowledge on disability pension (DP) at the time of revascularisation is lacking. The aim was to investigate the prevalence of all-cause and diagnosis-specific DP at the time of a first coronary revascularisation, accounting for socio-demographic and medical factors. Materials and Methods A population-based cross-sectional study using Swedish registers was conducted including all 65,676 patients (80% men) who when aged 30–63 years, within 1994–2006, had a first CABG (n = 22,959) or PCI (n = 42,717) and did not have old-age pension. Associations between socio-demographic and medical factors and the probability of DP were estimated by odds ratios (OR) with 95% confidence intervals (CI) using logistic regression analyses. Findings The prevalence of DP at time of revascularisation was 24%, mainly due to musculoskeletal diagnoses. Sixty-two percent had had DP for at least four years before the revascularisation. In the multivariable analyses, DP was more common in women (OR: 2.40; 95% CI: 2.29–2.50), older patients (50–63 years); especially men aged 60–63 years with CABG (OR: 4.91; 95% CI: 4.27–5.66), lower educational level; especially men with PCI (OR: 2.96; 95% CI: 2.69–3.26), patients born outside Sweden; especially men with PCI (OR: 2.11; 95% CI: 1.96–2.27), and in women with an indication of other diagnoses than acute coronary syndrome (ACS) or stable angina pectoris for PCI (OR: 1.72; 95% CI: 1.31–2.24). Conclusion About a quarter had DP at the time of revascularisation, often due to musculoskeletal diagnoses. More than half had had DP for at least four years before the intervention. DP was associated with female gender, older age, lower educational level, and being born outside Sweden."
dc8ba063121d56e6ef87f74cac71fe1888746435,"Background Although coronary revascularisation by coronary artery bypass grafting (CABG) and percutaneous coronary intervention (PCI) are common procedures, little is known regarding disability pension (DP) at the time of coronary revascularisation and its association with mortality. The aim was to investigate the five-year mortality following a first coronary revascularisation among women and men on DP, compared with those not on DP at the time of intervention, accounting for socio-demographic and medical factors. Material and Methods A nationwide prospective population-based cohort study was conducted, using national registers including 70,040 patients (80% men), aged 30–64 years, with a first CABG (n = 24,987; 36%) or PCI (n = 45,053; 64%) during 1994–2006 in Sweden, who were alive 30 days after the intervention. The main outcome was all-cause and cause-specific mortality within five years or through 31 December 2006, following CABG and PCI, and the exposure was DP at the time of a first coronary revascularisation. Information on DP, patient characteristics, date and cause of death was obtained from nationwide registers. Hazard ratios (HR) with 95% confidence intervals (CI) for the outcome were estimated, using Cox proportional hazard regression analyses. All analyses were stratified by type of intervention and gender. Findings Four percent died following coronary revascularisation. Cardiovascular disease was the most common cause of death (54%), followed by neoplasms (25%). Regardless of type of intervention, gender and after multivariable adjustments, patients on DP had a higher HR for five-year mortality compared with those not on DP at time of revascularisation (CABG: women HR 2.14; 95% CI 1.59–2.89, men HR 2.09; 1.84–2.38, PCI: women HR 2.25; 1.78–2.83, men HR 1.95; 1.72–2.21). Young women on DP at the time of PCI had a substantially higher HR (HR 4.10; 95% CI: 2.25–7.48). Conclusion Patients on DP at the time of first coronary revascularisation had a higher five-year risk of mortality compared with those not on DP."
dd91bf76d72f1b2666a8d87b37f76bed30e7eb5b,"Background 

Although coronary revascularization by coronary artery bypass grafting (CABG) and percutaneous coronary intervention (PCI) is well documented, scientific knowledge on disability pension (DP) at time of revascularization is sparse. The aim was to investigate the prevalence of all-cause and diagnosis-specific DP at time of a first coronary revascularization, accounting for sociodemographic and medical factors.

Methods 

A population-based, …"
6ff4cd25910c0ea3b1d6866f973f4e22264dd789,"Background Evidence based and gender specific knowledge about sickness absence following coronary revascularisation is lacking. The objective was to investigate sickness absence after a first coronary artery bypass grafting (CABG) or percutaneous coronary intervention (PCI) among women and men in a national Swedish study. Materials and Methods All patients 30–63 years of age, who underwent a first CABG (n = 22,985, 16% women) or PCI (40,891, 22% women) in Sweden between 1994 and 2006 were included. Information on sickness absence, co-morbidity, and other patient characteristics was obtained from national registers. Long-term sickness absence (LTSA) was defined as >180 and >90 sick-leave days in the first sick-leave spell following CABG and PCI, respectively. Prevalence ratio (PR) and 95% confidence interval (CI) of LTSA were calculated. Findings LTSA followed the interventions in 41% and 36% for CABG and PCI patients, respectively. Women had more often LTSA compared with men, (CABG PR = 1.23: 95% CI 1.19–1.28 and PCI PR = 1.19; 95% CI 1.16–1.23). A history of sickness absence the year before the intervention increased the risk for LTSA after the intervention in both genders. Among women, older age, or being self employed or unemployed was associated with a lower risk for LTSA. Among men previous cardiovascular disease, diabetes and low socio-economic position increased the risk. During the observation period, there was no change in sickness absence rates among PCI patients but an increase among CABG patients adjusting for patient characteristics. Conclusion This national study covering a 13-year period shows that long-term sickness absence following coronary revascularisation is common in Sweden, especially among women, and is associated with socio-economic position, co-morbidity, and sickness absence during the year before the intervention. Gender specific scientific knowledge about use and effects of sickness absence following coronary revascularisation is warranted for the patients, the treating physicians, the healthcare sector, and the society."
ad374fd959dfde94ddf591de35ea31fe12925d28,"Abstract.  Elmberg M, Hultcrantz R, Simard JF, Stål P, Pehrsson K, Askling J (Karolinska University Hospital and Karolinska Institutet, Solna; Karolinska Institutet, Huddinge; Karolinska Institutet, Solna; Karolinska Institutet, Huddinge, Karolinska University Hospital; Karolinska Institutet, Solna; Sweden). Risk of ischaemic heart disease and cardiomyopathy in patients with haemochromatosis and in their first‐degree relatives: a nationwide, population‐based study. J Intern Med 2012; 272: 45–54."
d667bc95d6f4e9668f76b606bada6b5fa033e93f,
30a10ec7563eafa2d80c88b7ca96f830aa393a02,
6d739ab13fe7ba9958b0ef74a3e53723662c6777,
f3c2922b2165433bf4e8eb5e01a5287678b9ddf1,
79fd325f1cf9d25932b5cdda934e7e77a48c5392,"AIMS
To evaluate the impact of renal insufficiency (RI) on long-term mortality and incident myocardial infarction (MI) in patients undergoing coronary artery bypass grafting (CABG).


METHODS AND RESULTS
All patients (n = 6575) without dialysis-dependent RI undergoing a first isolated CABG during 1980-1995 at the Karolinska hospital who survived 30 days post-operatively were included. Estimated glomerular filtration rate (eGFR) was related to the incidence of MI and all-cause mortality within 5 years. There were 628 deaths and 496 incident MIs during follow-up. After multivariable adjustment, patients with mild (eGFR 60-90 mL/min), moderate (eGFR 30-60 mL/min), and severe (eGFR <30 mL/min) RI had an increased mortality within 5 years post-CABG; hazard ratio (HR) 1.2 [95% confidence interval (CI) 1.0-1.6], HR 1.8 (95% CI 1.3-2.4), and HR 5.2 (95% CI 3.1-8.6), respectively, compared with patients with normal renal function (eGFR >90 mL/min). In patients with moderate and severe RI, there was an increased incidence of MI; HR 1.5 (95% CI 1.1-2.1) and HR 3.5 (95% CI 1.8-6.8), respectively. There were no gender differences.


CONCLUSION
Already mild RI predicts late all-cause mortality after coronary artery bypass grafting (CABG), and moderate and severe RI is associated with an increased long-term incidence of MI post-CABG."
a38b4ee32c2a5b191b671f0fd757d5b02bc8bc72,
ed510e749244a0f53e6bd6cb8d8433cc6e2f6e23,
7acb8ad5d9dd2550188d86ad8ca319acda54790c,
bee5840c5335ba843af7229f5892a0aff37551f2,
361fb56afb172f8977d20549801ecc34645fb283,"Abstract. Edvardsson N, Juul‐Möller S, Ömblus R, Pehrsson K (Sahlgrenska University Hospital, Malmö University Hospital, Bristol‐Myers Squibb Bromma; and Karolinska University Hospital; Stockholm, Sweden). Effects of low‐dose warfarin and aspirin versus no treatment on stroke in a medium‐risk patient population with atrial fibrillation. J Intern Med 2003; 254: 95–101."
b7bfcd2b3f974ad504616f70386dd8ad75acda1f,
f96d8e139752052e0dda7bfa28be8884a59e96ac,
fffdeb7dcdf2d4e1606bb23e703ac50d3eba8f29,
1db49eeebe4d4cf3719c2896c5815f67ffe911bf,Outcome of myocardial infarction in the unselected population is vastly different from samples of eligible patients in large-scale clinical trials
1a7bd09d9b78bfca2712d0fad02160827294b872,"Rapid reperfusion of an infarct‐related artery reduces the extent of myocardial damage and improves survival in acute myocardial infarction (AMI). Currently, anticoagulant treatment with unfractionated heparin (UFH) is used as adjuvant therapy to fibrinolytic treatment. The low‐molecular‐weight heparin (LMWH) dalteparin is at least as effective as UFH in unstable coronary artery disease. The ASSENT PLUS trial was carried out to evaluate whether dalteparin is as effective as UFH as an adjunct to recombinant tissue‐plas‐minogen activator (rt‐PA) and aspirin in obtaining patency and Thrombolysis in Myocardial Infarction (TTMI)‐3 flow in patients with AMI. The primary assessment of this phase II trial was TTMI flow, determined by coronary angiography. Patients with ST‐elevation MI were randomized to receive aspirin and either rt‐PA and UFH for 48 h, or rt‐PA and dalteparin for 4 to 7 days. Evaluation was by TTMI flow after 4 to 7 days and clinical events (death, reinfarction, or revascularization) up to 30 days. There was a clear trend toward greater TTMI 3 flow with dalteparin compared with UFH. There was significantly less TIMI0‐1 flow or thrombus in the dalteparin group. Bleeding rates were similar. The occurrence of reinfarction was reduced during dalteparin treatment. These findings suggest that dalteparin could be substituted for UFH as an adjunct to rt‐PA/aspirin in the management of patients with AMI."
387a386d5d2ba6646f8a4a69fced82fce7776305,"BACKGROUND
Fibrinolytic therapy increases the risk of bleeding events. TNK-tPA (tenecteplase) is a variant of rt-PA with greater fibrin specificity and reduced plasma clearance that can be given as a single bolus. We compared the incidence and predictors of bleeding events after treatment with TNK-tPA and rt-PA.


METHODS AND RESULTS
In the Assessment of the Safety and Efficacy of a New Thrombolytic (ASSENT)-2 trial, 16 949 patients with acute myocardial infarction were randomly assigned a single weight-adjusted bolus of TNK-tPA or a 90-min infusion of rt-PA. A total of 4.66% of patients in the TNK-tPA group experienced major non-cerebral bleeding, in comparison with 5.94% in the rt-PA group (P=0.0002). This lower rate was associated with a significant reduction in the need for blood transfusion (4.25% vs 5.49%, P=0.0003) and was consistent across subgroups. Independent risk factors for major bleeding were older age, female gender, lower body weight, enrolment in the U.S.A. and a diastolic blood pressure <70 mmHg. Females at high risk (age >75 years and body weight <67 kg) were less likely to have major bleeding when treated with TNK-tPA even after other risk factors were taken into account. A total of 0.93% of patients in the TNK-tPA and 0.94% of patients in the rt-PA group experienced an intracranial haemorrhage. Female patients >75 years of age who weighed <67 kg tended to have lower rates of intracranial haemorrhage when treated with TNK-tPA (3/264, 1.14% vs 8/265, 3.02%).


CONCLUSIONS
The increased fibrin specificity and single bolus administration of TNK-tPA do not increase the risk of intracranial haemorrhage but are associated with less non-cerebral bleeding, especially amongst high-risk patients."
6623ccb8c641a374a42c96298727e82ce0e47f96,"This report describes a 60-year old man who three years earlier, due to tricuspid endocarditis, had undergone surgery entailing insuturation of a Carbomedic valve prosthesis. He was admitted via the emergency room with clinical signs of right heart failure, and he reported that he had not heard the valve sound for two to three weeks. Cineradiography revealed a dysfunction of the tricuspid valve prosthesis, with the bileaflet tilting disc closed in the opening position. Thrombolytic therapy was successful. We review the literature on obstructed mechanical prosthetic valves and on the use of thrombolysis."
b563b395949536909f842f8627d80c2a9ec8c780,"AIMS
To determine the prevalence of left ventricular systolic dysfunction in 75-year-old men and women.


METHODS AND RESULTS
In a population-based random sample of 75-year-old subjects (n=433; response rate 70.1%) the left ventricular systolic function was determined using two echocardiographic methods: (1) wall motion in nine left ventricular segments was visually scored and wall motion index was calculated as the mean value of the nine segments and (2) ejection fraction as measured by the disc summation method. Presence of heart failure was determined by a cardiologist's clinical evaluation. Wall motion index was achievable in 95% of the participants while ejection fraction was measurable in 65%. Normal values were obtained from a healthy subgroup (n=108) and left ventricular systolic dysfunction was defined as the 0.5th percentile of the wall motion index (i.e. <1.7). In participants in whom both ejection fraction and wall motion index were achievable, wall motion index <1.7 predicted ejection fraction <43% with a sensitivity and specificity of 84.0% and 99.6%, respectively. The prevalence of left ventricular systolic dysfunction was 6.8% (95% CI, 5.6--8.0%) and was greater in men than in women (10.2% vs 3.4%, P=0.006). Clinical evidence of heart failure was absent in 46% of the participants with left ventricular systolic dysfunction.


CONCLUSIONS
Left ventricular systolic dysfunction is common among 75-year-olds with a prevalence of 6.8% in our estimate. The condition is more likely to affect men than women. In nearly half of 75-year-olds with left ventricular systolic dysfunction there is no clinical evidence of heart failure."
0b89907a69b41208c2834c7d383c8aa805d7cfaa,
3862a060fe921a4955212855e82f48fbf68e5cf6,
cb687fec3e0efd5fb6118bfe3fb97caa781da038,
3847ba4756bad5720c4721ca57b3298d5f6e32ec,
6e327b48cccc85cc7cdef8d653d89d78277c8193,"Effect of protriptyline, 10 mg daily, on chronic hypoxaemia in chronic obstructive pul- monary disease. K. Strom, G. Boman, K. Pehrsson, M. Alton, J. Singer, P-O. Rydstrom, M. Uddenfeldt, C-H. Ericsson, B. Ostholm C. Morlin. ERS Journals Ltd 1995. ABSTRACT: A daily dose of 20 mg of protriptyline can improve daytime arterial blood gas tensions in chronic obstructive pulmonary disease (COPD). Its useful- ness is limited by anticholinergic side-effects. This study examined whether a daily dose of 10 mg of protriptyline improved daytime arterial oxygen tension (PaO2) and quality of life in patients with stable mild or moderate hypoxaemia caused by COPD. Twenty six patients were randomized to receive protriptyline or placebo in a double-blind parallel-group trial for 12 weeks, following a run-in period of 4 weeks, in order to assess the stability of hypoxaemia. Patients with a change in PaO2 of >0.7 kPa during the run-in were excluded. Spirometry, quality of life and dysp- noea score were measured at randomization and after 12 weeks, whilst arterial blood gas tensions were also measured 2 and 6 weeks after randomization. No improvement in arterial blood gas tensions, spirometry values, dyspnoea score, or quality of life was found in either the protriptyline or the placebo group. The majority of patients receiving protriptyline experienced anticholinergic side-effects, which necessitated the withdrawal of the drug in one patient. We conclude that there was no evidence that a daily dose of 10 mg of protri- ptyline had a significant effect on daytime arterial oxygen tension in stable mild and moderate hypoxaemia caused by COPD. Despite the low dose, anticholinergic side-effects occurred in most patients."
9e9e75d9c1d33df9f29a52d37b0db9e7d4c1eb99,"A daily dose of 20 mg of protriptyline can improve daytime arterial blood gas tensions in chronic obstructive pulmonary disease (COPD). Its usefulness is limited by anticholinergic side-effects. This study examined whether a daily dose of 10 mg of protriptyline improved daytime arterial oxygen tension (PaO2) and quality of life in patients with stable mild or moderate hypoxaemia caused by COPD. Twenty six patients were randomized to receive protriptyline or placebo in a double-blind parallel-group trial for 12 weeks, following a run-in period of 4 weeks, in order to assess the stability of hypoxaemia. Patients with a change in PaO2 of > 0.7 kPa during the run-in were excluded. Spirometry, quality of life and dyspnoea score were measured at randomization and after 12 weeks, whilst arterial blood gas tensions were also measured 2 and 6 weeks after randomization. No improvement in arterial blood gas tensions, spirometry values, dyspnoea score, or quality of life was found in either the protriptyline or the placebo group. The majority of patients receiving protriptyline experienced anticholinergic side-effects, which necessitated the withdrawal of the drug in one patient. We conclude that there was no evidence that a daily dose of 10 mg of protriptyline had a significant effect on daytime arterial oxygen tension in stable mild and moderate hypoxaemia caused by COPD. Despite the low dose, anticholinergic side-effects occurred in most patients."
06c08ff620c2a35132d32cabea6f0846034fe405,"Objective: The outcome of patients with acute myocardial infarction who received thrombolytic therapy was assessed in relation to the size and comprehensiveness of cardiovascular services in the admitting hospitals. Methods: Two characteristics were obtained for each of the 438 hospitals: number of beds and in-house availability of cardiovascular services (coronary catheterization laboratory and coronary angioplasty or bypass surgery). Hospitals were grouped into four categories on the basis of size (≤300 vs >300 beds) and availability of cardiovascular services. Baseline and outcome variables were compared by χ2analysis and logistic regression. Patients were followed up for 6 months. Results: Baseline variables were comparable among hospital categories except for significant differences in the distribution of antecedent angina and time to treatment. Significantly more coronary angioplasties and bypass surgeries were performed in patients first treated in hospitals with coronary revascularization services (4.1% and 4.2% vs 1.0% and 1.9%,P Conclusions: Patients with acute myocardial infarction treated with thrombolytic therapy have the same mortality in small centers without in-house coronary revascularization services as in larger centers with such services. (Arch Intern Med. 1994;154:2237-2242)"
4e7a36742a4185d0436f457eaa5bb200bac6fdeb,"BackgroundIn the prethrombolytic era, women with myocardial infarction were reported to have a worse outcome than men. This analysis evaluates the association of sex with morbidity and mortality after thrombolytic therapy. Methods and ResultsData were analyzed from 8261 of the 8387 randomized patients with acute myocardial infarction who received thrombolytic therapy in the International Tissue Plasminogen Activator/Streptokinase Mortality Study (baseline data were missing for 126 patients) and were followed for 6 months. Women made up 23% (n= 1944) of the study population. Baseline characteristics were worse in women: they were 6 years older, were more likely to have a history of previous infarction (P<.01), antecedent angina (P<.01), hypertension (P<.0001), or diabetes (P<.0001); were in a higher Killip class on admission(P<.0002); and received thrombolytic therapy 18 minutes later than men (P<.0001). Fewer women were smokers (P<.0001). Women had a higher hospital (12.1% versus 7.2%, P<.0001) and 6-month mortality (16.6% versus 10.4%, P<.0001) and were more likely to develop cardiogenic shock (9.1% versus 6.3%, P<.0001), bleeding (7.2% versus 5.3%, P<.01), and hemorrhagic (1% versus 0.3%, P<.001) or total stroke (2.2% versus 1.1%, P<.0001) during hospitalization. Reinfarction rates and requirement for angioplasty or surgery did not differ. After correction for worse baseline characteristics, women had similar morbidity and mortality apart from a significantly higher incidence of hemorrhagic stroke, which remained significant even after accounting for weight and treatment allocation (odds ratio, 2.90; P<.01). ConclusionsAfter thrombolytic therapy for acute myocardial infarction, women have similar morbidity and mortality to men but suffer from a higher incidence of hemorrhagic stroke."
6c4f533f9a563a42072c86651135e068d98eeb54,"BackgroundDespite the fact that smoking is a well-established risk factor for the development of coronary artery disease, some investigators have noted that hospital mortality after acute myocardial infarction is lower in patients who smoke than in nonsmoking patients. To evaluate the association of smoking with mortality during hospitalization after thrombolytic therapy and 6 months afterward, we analyzed the results of the International Tissue Plasminogen Activator/Streptokinase Mortality Trial. Methods and ResultsPatients were divided into three groups: nonsmokers (those who never smoked), ex-smokers, and active smokers. Multivariate and univariate comparisons were made with respect to baseline characteristics and clinical outcome. There were 2,366 nonsmokers, 2,244 ex-smokers, and 3,649 active smokers. The baseline characteristics of nonsmoking patients differed significantly from the ex-smokers and active smokers. The nonsmoking group included more women than the ex-smokers or active smokers (45% versus 10.6% and 17.6%, respectively), was older (67±10 years versus 64±10 years and 58±11 years), had a higher rate of diabetes mellitus (16.3% versus 11.1% and 7.5%), and had a worse Killip class at admission. Nonsmoking patients and ex-smokers experienced more in-hospital reinfarction than active smokers (4.7% and 5% versus 2.7%, p<0.0001, respectively). Nonsmokers experienced more in-hospital shock than the ex-smokers or active smokers (9.2% versus 6.4% and 5.8%, P<0.0001), stroke (1.9% versus 1.8% and 0.8%, p<0.0001), and bleeding (7.2% versus 6.5% and 4.4%, p<0.0001). They also experienced a higher in-hospital and 6-month mortality (12.8% and 17.6%) than ex-smokers (8.2% and 12.1%) or active smokers (5.4% and 7.8%) (p<0.0001). A multivariate analysis accounting for all baseline characteristics demonstrated a significant association between nonsmoking and increased hospital mortality, with an odds ratio of 1.42 (confidence limits, 1.15–1.72). Among active smokers, there was a nonsignificant trend for mortality rates to decrease with increasing numbers of cigarettes smoked per day. ConclusionsThis retrospective analysis indicates that smokers receiving thrombolytic therapy after acute myocardial infarction have significantly better hospital and 6-month outcome than nonsmokers or ex-smokers. However, smokers sustained their infarction at a significantly earlier age than nonsmokers, and strenuous efforts should continue to be made to decrease the incidence of new and continued smoking."
333e4cf5ed4e1819913c506fe9321dcb730a39a9,
9db79c08f6bf903eaa28d3d3075b7e7808a1a475,"The effect of beta‐adrenergic blockade (propranolol) on exercise performance was studied in 15 patients (12 men and 3 women, mean age 70 years) with complete heart block treated with a ventricular‐inhibited pacemaker (VVI). In a double‐blind procedure, the patients were randomly given either 0.1 mg/kg of propranolol or saline solution i.v. before a first exercise test and vice versa before a second test. The interval between the tests was 24 hours. Nine patients were in sinus rhythm, 4 patients had atrial flutter, and 2 others had atrial fibrillation. The exercise capacity was on an average 11% lower with propranolol than with placebo (p<0.001). The most marked reductions (20 and 33%) were found in the two patients with atrial fibrillation. The atrial rate in patients with sinus rhythm was significantly lower with propranolol than placebo both at rest (68 vs. 83 beats/min, p<0.001) and at maximal work load (91 vs. 141 beats/min, p<0.001). The present findings show that beta blockade has negative effects on exercise capacity in patients with complete heart block treated with VVI pacemakers. This finding should be considered in the selection of drug treatment in patients with fixed rate pacing and concomitant hypertension and/or ischemic heart disease."
eae4c98a704ea01e6c48cb49321d80e5d95d939d,"Five cases of neurogenic pulmonary oedema (NPE) are described. The causes were mechanical trauma to the skull, subarachnoid haemorrhage and epileptic seizure. In every case a frank pulmonary oedema was diagnosed that resolved within a few days. Treatment of the underlying disease resulted in a favourable outcome. The literature has been reviewed. The basic mechanism seems to be an increased intracranial pressure (ICP) precipitating an increased central sympathetic nerve activity mediated via peripheral alpha- or beta-adrenergic discharge. NPE results from a predominant alpha-receptor stimulation with massive increase in pre- and afterload. The major therapeutic efforts should be directed towards the underlying cause and, in addition, mechanical ventilation with passive hyperventilation is vital. High positive end-expiratory pressure should not be used without strict monitoring of ICP."
039041a1562352176cf5ffb4a2b7e30d329749b9,"Ten patients with severe effort angina and with left ventricular dysfunction during exercise before operation underwent haemodynamic and angiographic studies in average 20 months after coronary artery bypass surgery. Five patients (50%) were completely asymptomatic after operation(group I). The other five (group II) were still limited physically because of anginal pain, although two were much improved. Pre-operatively there was no significant difference in the severity of the disease, as judged from case histories, work tests and haemodynamic and angiographic findings between the two groups. The working capacity of the patients in group II was not increased significantly post-operatively. Their coronary arteriograms revealed unsatisfactory surgical results. In two patients, one significantly stenosed vessel was not bypassed because of poor run-off. In the other three patients, one graft was closed. Left ventricular function curves showed no significant improvement of left ventricular pump function. In group I, working capacity increased significantly, all stenoses of major coronary vessels were bypassed and all grafts were patent. Left ventricular function showed an almost normal response during exercise. These findings suggest that left ventricular dysfunction due to ischaemia can be significantly improved by coronary bypass and that there is a good correlation between clinical, haemodynamic and angiographic findings."
f2242fd593a8c7a134f700c2c9c0fcf342c34935,"In order to avoid the potential risk of a short refractory period and the inconvenience of the electronic blocking mechanism at the highest synchronous rate with the conventional atrial triggered pulse generator, two more functions have been added to the normal atrial triggered (VAT) pulse generator. First, it has been designed to be ventricular inhibited; second, a highest synchronous rate detector has been added, When the highest synchronous rate of about 140 is exceeded, the atrial signals are blocked and the frequency of the basic rate generator is increased to about 130. Two patients could hardly feel the change from atrial triggered pacing to ventricular inhibited stimulation, and both were able to work at higher loads with this type of pacing compared with the conventional atrial triggered pulse generator."
3eef8a8dfc81827f35cc68f900cdd27bda776d49,"An atrial detector electrode was introduced by mediastinoscopy in 82 patients requiring permanent cardiac pacing. There were no complications. An adequate P wave was recorded in 80 patients. During the following week, the P wave became ineffective in 5 patients; angina occurred in 2 and atrial arrhythmias in 2. Atrially triggered ventricular pacing established in 73 patients and was followed in 71 patients for a period of 1 to 113 months. In 17 cases, it had to be terminated because of an ineffective or unstable P wave,in 6 cases because of atrial arrhythmias, and in 4 cases because of advanced age and recurrent infections. The method is technically simple and place little stress on the patient."
898455d6b726078f2c0ab10f9dff5f1bbdccd261,
3f6945ce9d046da62522dd86bdb5ca6b50248b5b,"Adaptive building envelope systems can manage energy and mass transformation between indoor and outdoor environments, which contributes to the achievement of environmental benefits via reducing energy consumption and greenhouse gas emission while maintaining human comfort and well-being. However, the market penetration of adaptive façades (AFs) is far from sufficient, even though their capabilities have been recognized in research. Hence, this paper explores the factors hindering the growth of the market share of AFs, based on an exhaustive examination of designs, evaluation criteria and tools, and control systems. Insufficient commercial technology, inaccurate and incomplete performance data, and inconsistent evaluation criteria are demonstrated to be the factors that have hindered the widespread utilization of AFs thus far. Future research tendencies, including reducing costs, retrofitting existing building façades, developing building performance measurement tools, and building consensus evaluation criteria that favor the wide applicability of such façades in actual practice are identified."
b53279dbf65e894938764939fe022f16d533ffe7,
cdf3534200cd8229d7188b1ccc94acc3854cdd90,
c622985b8f25cbb89304d2a2f79530bc2537ca6f,
6b3445c9d9fe61db887a216516061ffaac180e0c,This paper examines the metaethical dimensions of the computing community’s efforts to program ethical decision-making abilities into robots. Arguments for and against that endeavor are outlined along with brief recommendations for the human-robot interaction realm.
88e3c0a5dc4e1d8b5236335f75d9cc83002e9cc2,
d15b4532350b77280d170fc0bfcf89770271c441,"As robots are becoming more intelligent and more commonly used, it is critical for robots to behave ethically in human-robot interactions. However, there is a lack of agreement on a correct moral theory to guide human behavior, let alone robots. This paper introduces a robotic architecture that leverages cases drawn from different ethical frameworks to guide the ethical decision-making process and select the appropriate robotic action based on the specific situation. We also present an architecture implementation design used on a pill sorting task for older adults, where the robot needs to decide if it is appropriate to provide false encouragement so that the adults continue to be engaged in the training task."
e61bf75507db4478ce14cc9bf1732a3f5b8804c4,
f2853a6be8c305dca98ebea508300abe345c3e56,"This paper describes current progress on developing an ethical architecture for robots that are designed to follow human ethical decision-making processes. We surveyed both regular adults (folks) and ethics experts (experts) on what they consider to be ethical behavior in two specific scenarios: pill-sorting with an older adult and game playing with a child. A key goal of the surveys is to better understand human ethical decision-making. In the first survey, folk responses were based on the subject’s ethical choices (“folk morality”); in the second survey, expert responses were based on the expert’s application of different formal ethical frameworks to each scenario. We observed that most of the formal ethical frameworks we included in the survey (Utilitarianism, Kantian Ethics, Ethics of Care and Virtue Ethics) and “folk morality” were conservative toward deception in the high-risk task with an older adult when both the adult and the child had significant performance deficiencies."
3aea48cbdad57f392de9d9eb208125888922faa4,"In response to widespread calls for computer scientists to better engage with the ethical dimensions of their work, there has been a surge of interest to embed ethics across the computer science (CS) curriculum. Yet one key set of barriers to doing so can be broadly described as scaling challenges -- in the number and breadth of courses in a curriculum and in the number of students in the CS major. Our paper describes and makes available a novel activity for teaching ethics using role-play that has advantages for scaling across different courses and in different delivery modes, including synchronous and asynchronous online course offerings. We describe our design process and early findings from developing the activity in a large first year seminar course, a senior-level computing and society class, and three different online graduate level courses. Further, we describe an evaluation survey that instructors can use to assess the short-term impact of the activity. We analyze survey results and our direct observations to reflect on the strengths and challenges of the activity. Our experiences suggest that role-play as a pedagogical tool can be particularly useful to broaden student perspectives and meaningfully incorporate ethics into CS courses."
61678593a208b1322728689baf1719120a640203,
8349034f31db26cd197e07674a4ad136896d42af,"In recent years, numerous public, private, and non-governmental organizations (NGOs) have produced documents addressing the ethical implications of artificial intelligence (AI). These normative documents include principles, frameworks, and policy strategies that articulate the ethical concerns, priorities, and associated strategies of leading organizations and governments around the world. We examined 112 such documents from 25 countries that were produced between 2016 and the middle of 2019. While other studies identified some degree of consensus in such documents, our work highlights meaningful differences across public, private, and NGO sectors. We analyzed each document in terms of how many of 25 ethical topics were covered and the depth of discussion for those topics. As compared to documents from private entities, NGO and public sector documents reflect more ethical breadth in the number of topics covered, are more engaged with law and regulation, and are generated through processes that are more participatory. These findings may reveal differences in underlying beliefs about an organization’s responsibilities, the relative importance of relying on experts versus including representatives from the public, and the tension between prosocial and economic goals."
9f363f24a80a32c33dcbf091127239652acd18a3,"During World War II, a Mass achusetts Institute of Tec hnology professor named Norbert Wiener worked on the automatic control of a cannon. In 1948, Wiener1 coined the term cybernetics and wrote about computers: ... we are already in a position to construct artificial machines of almost any degree of elaborateness of performance. Long before Nagasaki and the public awareness of the atomic bomb, it had occurred to me that we were here in the presence of another social potentiality of unheard-of importance for good and for evil."
161cbe2570680f39636945549c4c14d5b6605a6a,
2cdb5783214616fd759858c6751f6589226685da,
4dcfc6995d86a02a4d77353b4064b4bff581d754,Developing social responsibility attitudes in future engineers and computer scientists is of critical and rising importance. Yet research shows that prosocial attitudes decline during undergraduate engineering education.
7846f97260dc8b1e333294de2cdb654b0965cf47,"The attribution of human-like characteristics onto humanoid robots has become a common practice in Human-Robot Interaction by designers and users alike. Robot gendering, the attribution of gender onto a robotic platform via voice, name, physique, or other features is a prevalent technique used to increase aspects of user acceptance of robots. One important factor relating to acceptance is user trust. As robots continue to integrate themselves into common societal roles, it will be critical to evaluate user trust in the robot’s ability to perform its job. This paper examines the relationship among occupational gender-roles, user trust and gendered design features of humanoid robots. Results from the study indicate that there was no significant difference in the perception of trust in the robot’s competency when considering the gender of the robot. This expands the findings found in prior efforts that suggest performance-based factors have larger influences on user trust than the robot’s gender characteristics. In fact, our study suggests that perceived occupational competency is a better predictor for human trust than robot gender or participant gender. As such, gendering in robot design should be considered critically in the context of the application by designers. Such precautions would reduce the potential for robotic technologies to perpetuate societal gender stereotypes. CCS CONCEPTS Human-centered computing → Empirical studies in HCI. ACM Reference Format: De’Aira Bryant, Jason Borenstein and Ayanna Howard. 2020. Why Should We Gender? The Effect of Robot Gendering and Occupational Stereotypes on Human Trust and Perceived Competency. In Proceedings of 2020 ACM Conference on Human-Robot Interaction (HRI’20), March 23-26, 2020, Cambridge, UK. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3319502.3374778"
aa92adc4b42cdada4fc4453d9fa3f3473aa42425,
b1f3c750adcc1560c5b1dc37cf2c8b6716f8d2c9,"The purpose of this article was to survey the perspectives of clinicians regarding pediatric robotic exoskeletons and compare their views with the views of parents of children with disabilities. A total of 78 clinicians completed the survey; they were contacted through Children’s Healthcare of Atlanta, the American Academy for Cerebral Palsy and Developmental Medicine, and group pages on Facebook. Most of the clinicians were somewhat concerned to very concerned that a child might not use the device safely outside of the clinical setting. Most clinicians reported that the child would try to walk, run, and climb using the exoskeleton. The parents reported higher trust (i.e., lower concern) in the child using an exoskeleton outside of the clinical setting, compared to the clinician group. Prior experience with robotic exoskeletons can have an important impact on each group’s expectations and self-reported level of trust in the technology."
c463e0a0b9a4c53ad6a90d5254bb9bc03de14033,
ce2b56b728ab63c53414849f4e0b76cdd38f34df,"Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals."
d9d6cd4a7f26656b1a5b6980e2eee5762ec99b02,"Using the COVID-19 pandemic to frame the discussion, this paper explores the potential ethical impacts of a greater reliance on robots during a public health emergency. Through an examination of various uses of different kinds of robots across the prevention, response, and recovery phases of the pandemic, this paper considers ethical pitfalls as well as the possible benefits of expanded deployment and use of robots to facilitate human management of public health emergencies. Alongside consideration of pandemic-related uses of robots, this paper also explores ethical concerns related to their use in public health practice and health care beyond the context of a public health emergency."
29362c541532fbd8fdd91f8b39ebb568fa2b64fe,"This paper describes ongoing research for a three-year NSF-funded project on ethical architectures for robots; specifically striving to understand how robots can reconcile differing outcomes produced by alternative ethical frameworks (e.g., Kantianism, Utilitarianism, and Ross’s moral duties). The process of determining the correct action is mediated by context and the moral emotional state of the robot. This paper describes the motivation, background, and approach to the project."
39f9ea401b6f6348a701b8ecc86aead824b9f899,
46ab0afc1e7c23d19f3c6ac1f2b4fe7e9b859a24,"Presented on January 24, 2019 from 11:00 a.m.-12:00 p.m. in the Caddell Building, Flex Space, Georgia Tech."
571673ede081ac7f9c25010f62e867fc19d985e5,and
8dcc660dae79b09415f97f4dfc25ccaa9f53f73f,
bac96da20e2bb14b91d0d6654898eb05b8569d1e,
c129d712fa315894545ca1d2ad32bdaf3bade97d,"Since 2016, more than 80 AI ethics documents – including codes, principles, frameworks, and policy strategies – have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents’ creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals."
c87ea103438b6098eb79ab993412c68fe58a34c3,"This commentary responds to a hypothetical case involving an assistive artificial intelligence (AI) surgical device and focuses on potential harms emerging from interactions between humans and AI systems. Informed consent and responsibility-specifically, how responsibility should be distributed among professionals, technology companies, and other stakeholders-for uses of AI in health care are discussed."
fb1372d18a35ddb18008ae1f01eb7e46a2e208c8,"Autonomous vehicle manufacturers, people inside an autonomous vehicle (occupants), and people outside the vehicle (non-occupants) are among the distinct stakeholders when addressing ethical issues inherent in systems that include autonomous vehicles. As responses to recent tragic cases illustrate, advocates for autonomous vehicles tend to focus on occupant safety, sometimes to the exclusion of non-occupant safety. Thus, we aim to examine ethical issues associated with non-occupant safety, including pedestrians, bicyclists, motorcyclists, and riders of motorized scooters. We also explore the ethical implications of technical and policy ideas that some might propose to improve non-occupant safety. In addition, if safety (writ large) is truly the paramount priority for autonomous vehicle advocates, we contend that autonomous public transportation should be considered as a more effective and less expensive way to improve public safety."
23d88502dc3f9a718b5a155d3b78f6dac11b3d30,
331dcb4085f2bb40e4a66490adf7a8578c52cdb5,A contemporary ethical challenge.
350f688ecaae9a0e0d89d10825e873124e0a6ee0,"Numerous types of robots are being interwoven into the health-care system, including rehabilitative devices for use with pediatric populations. Yet a key ethical concern is that pediatric patients, their parents, and other caregivers might begin to overtrust robotic technology, possibly resulting in a patient being harmed or the technology adopted prematurely. To gain insight into the issue, our research team conducted a study examining the potential of overtrust in pediatric robotics. This article discusses results from a survey of parents who have at least one child with a movement disability. The survey's focus is on robotic exoskeletons, which represent the most viable of the currently available robotic technologies in terms of being adopted into the home as a clinically validated rehabilitative device for both adults and children. More than 62% of respondents indicated they would typically or completely trust their child to handle risky situations with an exoskeleton, even though the technology may not be designed for such situations. We conclude with suggestions for future research directions on the problem of overtrust in health-care robotics."
5de24aa5fdc437f0e9d739ee7e61a598fac9bc41,"Many of us, roboticists and those who collaborate with them, experience delight, excitement, and sometimes deep-seated, but rarely unvoiced, fears as we witness our robotic systems begin to impact human lives in countless ways. From automating driving to reshaping various facets of health care delivery, robotic systems are growing in their prevalence and intrusiveness into our daily lives. In combination with our siblings in the Artificial Intelligence (AI) community, scholars continue to predict a wide range of benefits from robotics and AI systems but also serious harms, including potential existential threats to humanity. Recognized pillars of science and engineering, including Elon Musk and the late great Stephen Hawking, have given voice to the apocalyptic kinds of fears that the public may have about an increasingly automated future. Whether these fears should be taken seriously is an issue that has divided scholars for awhile now, as illustrated by debates between Bill Joy [7] and Ray Kurzweil [8] at the beginning of the 21st century. On a different scale of granularity, a category of harms that users and others are more likely to experience on a day-to-day basis is from the various types of bias encoded in, or learned by, AI systems. This category of harms is especially troublesome in the world of physical robotics. Nonembodied AI systems can obviously make decisions that have effects on human beings, such as a chatbot determining what to say in response to a customer’s question on a company’s helpline. Yet it will need to rely on an embodied entity (often a human) to have a direct impact on the physical world. Typically, a nonembodied AI agent provides input to humans who may then execute a physical action – whether those humans are making an employment decision to hire or fire or deciding on a health care intervention for a patient. By definition, it lacks the capability of acting on the world without assistance. Robots that have a physical form, on the other hand, can perform actions on their own. This can raise the ethical stakes in terms of the potential benefits and harms that may result from the technology. The benefits and harms that we are particularly concerned about here are related to bias."
6be4201a440740064d14ca0aa625d09de055c38a,"Dr. Jason Borenstein is the Director of Graduate Research Ethics Programs and Associate Director of the Center for Ethics and Technology at the Georgia Institute of Technology. His responsibilities include administering a Responsible Conduct of Research (RCR) policy for all new doctoral students at Georgia Tech and instructing undergraduate and graduate courses on topics at the intersection of science, engineering, and ethics. Dr. Borenstein is also Editor for Research Ethics for the National Academy of Engineering’s Online Ethics Center for Engineering and Science. He is an assistant editor of the journal Science and Engineering Ethics and co-editor of the Stanford Encyclopedia of Philosophy’s Ethics and Information Technology section. His research interests include bioethics, engineering ethics, robot ethics, and research ethics. His work has appeared in various journals including AI & Society, Communications of the ACM, the Journal of Academic Ethics, Ethics and Information Technology, IEEE Technology & Society, Accountability in Research, and the Columbia Science and Technology Law Review."
cebbd7d735c948cd9058ec4477a4b803ecd1b736,
6c93e8a0b5da8e265a0e9ff4a3adfa61fed150c2,"In the wake of the exposure of Volkswagen's diesel engine test-rigging, a Bloomberg Business journalist described the company as ""driven by engineering-crazed executives"" [2] and The New York Times ran a story noting how with today's complex computer systems in automobiles, there are numerous opportunities for misdeeds both by automakers and hackers [3]. With the advent of so-called autonomous or self-driving cars, such issues may become even more pervasive and problematic. From a legal perspective, a key focal point is who would be at fault if and when an accident occurs [4]. Much also has been written about the ethical complexities posed by self-driving cars [5]-[6]. In accordance with Moore's Law, ""[a]s technological revolutions increase their social impact, ethical problems increase"" [7]. Yet relatively little has been said about the ethical responsibilities of the designers of self-driving cars."
9351b78dc214f992ab1c6fc78767e5918591655a,"Robots are being introduced into the U.S. healthcare system with growing frequency [1]. From surgical robotic systems to medication delivery devices, medical services are being transformed through the integration of diverse intelligent agents and platforms. Robotic rehabilitative devices are also gaining traction, including those for use with pediatric populations. The primary intent of such robots is to improve the quality of life for children. Yet a key ethical concern with the intended utilization of robots in pediatric healthcare settings is the prospect that children, their parents, and other caregivers might begin to overtrust the technology. This concern stems from studies indicating that placing too much trust in automated healthcare systems may result in unintended negative consequences. For example, when physicians overtrust automated systems for detecting cancer, it may contribute to certain types of cancers being overlooked [2]. In fact, a systematic review of clinical support systems discusses the potential overreliance on automated healthcare systems and how the occasional incorrect advice derived from these systems may cause expert users to reverse decisions they had already made [3]. In the context of healthcare robotics, overtrust might result in improper usage or premature adoption of the technology; individuals may be granted access before they have had sufficient training or a clear understanding of the technology’s capabilities. Based on the growing use of robots for healthcare-related applications, it thus becomes appropriate to explore the implications and potential for overtrust within healthcare settings in the hopes of mitigating or preventing possible negative effects."
a34a05eb4eceaadf3321475faf2deb4b935d6650,
d1c3949f26df5d1de1390f24c0f91653f3739a7a,
e8709305f27913dc926f164d4c38224a33b8f236,
3c9648be8c97ad7a2687d1e650e5d8fe0b459f5f,
872794afed5a1c35a2bd7112f2ef1aa4f6e32313,
ba31708d432753150d4810c3a3cfa7eaf627b508,
0d0f3b712fb58969796edf3f3f8668a99105538e,"The size and complexity of research teams continues to grow, especially within the realms of science and engineering. This has intensified already existing concerns about relying on traditional authorship schemes as the way to allocate credit for a contribution to a research project. In this paper, we examine current authorship problems plaguing research communities and provide suggestions for how those problems could potentially be mitigated. We recommend that research communities, especially those involved in large scale collaborations, revisit the contributor model and embrace it as means for allocating credit more authentically and transparently."
12075f6f239b0f8000afb789ab5b8306ef1ce209,"Individuals who are deemed to be experts are often asked to provide testimony before legislative committees, regulatory bodies, and the courts (both civil and criminal). Their testimony can greatly influence the decision-making of these bodies. Yet before deciding what role such testimony should play, we should determine what makes someone an expert, what bounds an area of expertise, and how a non-expert can effectively evaluate an expert's claims. Intertwined with the epistemological issues of what expertise is and how to identify an expert are ethical considerations. Considering how important an expert's influence can be on a criminal or civil case and a government's deliberations, many ethical issues related to expert witness testimony emerge, including whether, and to what degree, we should accept their testimony. Although the scope of these issues is quite broad, this article focuses on the use of expert testimony in the courtroom. 
 
 
Keywords: 
 
competence; 
conflict of interest; 
deception; 
ethics; 
expert testimony; 
expertise; 
justice; 
legal ethics"
769b5607677ab1d0236dbaae7228f028ff4de355,
9e76b60273b3096bf9412f21fd00e5090ebaead9,
d4bf49aa1ee7f927a512d7c2de9f5ef2aa2524fe,
d67793ffe28a723e18c11b95c3e00fa9ad8bb4f4,"The Daubert Case Daubert, a civil case, hinged on the validity of expert opinion on the scientific methodology used to determine the alleged causal link between an anti-nausea drug, Bendectin, and fetal birth defects. A key issue before the Court was whether the socalled “Frye test,” which emerged from Frye v. United States (1923), was the appropriate means for determining the admissibility of expert testimony or whether the Federal Rules of Evidence enacted in 1975 superseded Frye."
0b02535227780f692b38375145148ea950a9910b,"There remain many opportunities to enhance how the ethical dimensions of science and engineering are taught and assessed. In fact, current trends suggest growing demand for STEM professionals who can deftly navigate the wide range of moral and ethical issues that might be faced during their careers, whether in the private, academic, government, and/or non-profit sectors. Yet even when high quality instructional interventions are developed and delivered, there often remain unanswered questions about their relative effectiveness. Valid and reliable assessment tools are sorely needed to better evaluate whether our students and practitioners not only have sufficient background understanding related to science and engineering ethics, but can apply this knowledge in real-world situations. This highly interactive panel session is organized around presentations from three complementary efforts to develop high quality assessment instruments focused on science and engineering ethics. Background details for each instrument will be provided, including sample questions that the audience will be invited to complete. The primary audience for this panel includes educators and researchers looking for new ways to teach, assess, and investigate ethics outcomes in a wide variety of science and engineering fields."
410bc11a37dccda1ba0bc5ea656adc276bdc53d4,"There remain many opportunities to enhance how the ethical dimensions of science and engineering are taught and assessed. In fact, current trends suggest growing demand for STEM professionals who can deftly navigate the wide range of moral and ethical issues that might be faced during their careers, whether in the private, academic, government, and/or non-profit sectors. Yet even when high quality instructional interventions are developed and delivered, there often remain unanswered questions about their relative effectiveness. Valid and reliable assessment tools are sorely needed to better evaluate whether our students and practitioners not only have sufficient background understanding related to science and engineering ethics, but can apply this knowledge in real-world situations. This highly interactive panel session is organized around presentations from three complementary efforts to develop high quality assessment instruments focused on science and engineering ethics. Background details for each instrument will be provided, including sample questions that the audience will be invited to complete. The primary audience for this panel includes educators and researchers looking for new ways to teach, assess, and investigate ethics outcomes in a wide variety of science and engineering fields."
e6ec5347cfe9f4494454f816a59e2af621df5073,
e89691047761b1e0268e78cbfed8921f0aa6a70d,
1563e2c2769a104d07c2aaeb0c87d612083820e9,
3879e0c1072716f4e0885beed5e70a9967d50315,"In this paper, we examine the design facets of a companion robot’s behaviour that may have a bearing on whether children will become well-adjusted adults. Though we will examine several types of design features and their potential effects on children, the unifying theme of our exploration will be our focus on the impact of child-robot interaction (CRI) on children’s development and their future welfare. To a large degree, we will analyse how encoding robots with a particular type of ‘personality’ may affect the emotional development of children. We will integrate into our discussion commonly embraced ethical principles such as beneficence and nonmaleficence; our discussion is also influenced by elements of the capabilities approach as articulated by Martha Nussbaum and Amartya Sen. Robots designed to serve as companions for children can range from rather simple devices that a child can use as a toy during playtime, such as My Keepon, to those that can engage in conversation or other relatively sophisticated activities. Included at the latter end of this spectrum is Kismet, a robot with an expressive voice and face that encourages interaction with humans and is capable of learning through this interaction. Embedded in the term ‘companion’ is the notion that at least some portion of a child’s, or other user’s, care will be entrusted to the robot. Arguably, there are advantages to pairing a child with a robotic companion during playtime or while interacting"
578f8cc4973fe3b73d4aa248981e3f0424047933,
cb85ab10aec5358b7c39da2452d9c357bf8fad39,
e123ec42698ca82242bd21520eaec7185d541c0e,"Dr. Jason Borenstein is the director of Graduate Research Ethics Programs and co-director of the Center for Ethics and Technology at the Georgia Institute of Technology. His responsibilities include administering a Responsible Conduct of Research (RCR) policy for all new doctoral students at Georgia Tech and instructing undergraduate and graduate courses on topics at the intersection of science, engineering, and ethics. Dr. Borenstein is also an assistant editor of the journal Science and Engineering Ethics and co-editor of the Stanford Encyclopedia of Philosophy’s Ethics and Information Technology section. His research interests include bioethics, engineering ethics, robot ethics, and research ethics. His work has appeared in various journals including AI & Society, Communications of the ACM, the Journal of Academic Ethics, Ethics and Information Technology, IEEE Technology & Society, Accountability in Research, and the Columbia Science and Technology Law Review."
fce536ed22e44fa6fc2defe6bc95275b36ac2eaf,"Reacting to the turbulent events of the last decade or so, the U.S. government has focused much attention on developing strategies to curtail the misuse of technology, especially biological agents. In principle, any technology can be exploited and used to achieve malicious ends. This is an especially poignant issue now that we have reached the digital age and that an individual person¿s actions can have an impact that spans across the globe. One way in which a malicious actor could cause serious harm is by manipulating how a robot functions."
23a43cfca74e4895d27234b3fa946263de099bf9,"As a committee organized in 2009 by the National Academy of Engineering recognized, ethics education should foster the ability to analyze complex decision situations and illstructured problems. This presentation aims to build on the NAE‘s insights and reports about an innovative teaching approach that has two main features: first, it places the emphasis on deliberation and on self-directed, problem-based learning in small groups of students; and second, it focuses on understanding ill-structured problems. The first innovation is motivated by an abundance of scholarly research that supports the value of deliberative learning practices. The second results from a critique of the traditional case-study approach in engineering ethics. A key problem with standard cases is that they are usually described in such a fashion that renders the ethical problem as being too obvious and simplistic. Any description that already ―frames‖ a case in this kind of way tends to trivialize the ethical challenge. The practitioner, by contrast, will mostly face problems that are ill-structured and for which it is not even clear if they include a real ethical challenge. In the collaborative learning environment described here, groups of students use interactive and web-based argument visualization software called ―AGORAnet: Participate – Deliberate!‖. The function of the software is to structure communication and problem solving in small groups. The software guides students step by step through a process of argument mapping. Students are confronted with the task of identifying possible stakeholder positions and reconstructing their legitimacy by constructing justifications for these positions in the form of graphically represented logical argument maps. The argument maps are then presented in class so that these stakeholder positions and their respective justifications become visible and can be brought into a reasoned dialogue and deliberative process. Argument mapping in engineering ethics courses provides an exciting opportunity for students to collaborate in teams and to develop critical thinking and argumentation skills. A New Focus in Engineering Ethics Education Traditionally, the main objective of engineering ethics courses has been to foster awareness of and to stimulate reflection on the special responsibilities of professionals in technological fields. A well-established method to pursue this learning objective is to provide students with case studies from engineering practice. The case studies typically focus on common ethical issues such as taking a bribe from a vendor. However, a key problem with standard cases is that they usually describe the ethical problem in such a fashion that renders it as being something that is too simplistic. The more obvious the wrongdoing is, the easier it is to determine what should have been done. Thus, there may be no true ethical ―challenge‖ presented in the case. Clearly, the simplicity of ethics cases stands in contrast to the complexities of the real-life situations students will encounter after graduation. Aristotle astutely recognized in the first sentence of his Nicomachean Ethics that ―every action and undertaking seems to seek something good‖ [1]. No professional wants something bad to happen. At times, the problem is not the engineer‘s intentions but his or her inability to predict a bad outcome in spite of all P ge 25300.2 the good intentions. The most fundamental challenge from an ethical perspective is thus the fact that we need to realize, first of all, that there is an ethical challenge connected to one‘s decisions. Engineering ethics education needs to better prepare students for this kind of challenge. This conviction is conveyed within a workshop report on ―Ethics Education and Scientific and Engineering Research‖ that the National Academy of Engineering (NAE) organized in 2009. The report emphasizes that the following skills should be developed in ethics education [2]: 1  Recognizing and defining ethical issues.  Identifying relevant stakeholders and socio-technical systems.  Collecting relevant data about the stakeholders and systems.  Understanding relevant stakeholder perspectives.  Identifying value conflicts.  Constructing viable alternative courses of action or solutions and identifying constraints.  Assessing alternatives in terms of consequences, public defensibility, institutional barriers, etc.  Engaging in reasoned dialogue or negotiations.  Revising options, plans, or actions. This list highlights the complexity of the issues that engineers confront. An engineer‘s actions can have effects on stakeholders whose existence, perspectives, and values she does not necessarily see. An engineer does not always directly interact with the people whose lives are being altered as result of her decisions. Obviously, engineering students need to refine their technical competence. But it is crucially important that they develop ―soft skills‖ as well [3]. Among these skills is the ability to identify hidden ethical challenges. Ill-Structured Problems A key intellectual challenge is acquiring the ability to identify and structure complex situations. This ability is an important precondition for problem solving, for decision making, for designing, and for planning. Several decades ago, Horst Rittel and Melvin Webber recognized this as ―one of the most intractable problems‖ in their seminal paper ―Dilemmas in a General Theory of Planning‖ [4]. Rittel and Webber came to the conclusion that the real challenge is not ―tame‖ or ―benign‖ problems that are clearly specified and that allow for a clear determination as to whether a solution has been achieved—for example, a standard ethical problem in a textbook. The real challenge is what they called ―wicked problems‖ or what we refer to as ―ill-structured problems,‖ a term that is also used by other authors in the engineering ethics literature [5, 6]. We prefer ―ill-structured‖ because in common parlance, the term ―wicked‖ carries with it the connotation of something being ethically wrong and this could be misleading; it is not a feature that we intend to capture. However, even though we are using different terminology, ―ill-structured problems‖ is intended to mean the same thing as Rittel and Webber‘s ―wicked problems‖ (see also [7, 8]). Among the ten defining characteristics of a wicked problem that Rittel and Webber delineated, the most important for our purposes is the first one: ―There is no definitive 1 See also National Research Council. The Engineer of 2020: Visions of Engineering in the New Century. Washington, DC: The National Academies Press, 2004. P ge 25300.3 formulation of a wicked problem‖ [4]. Any sufficiently detailed description of what the problem ―is‖ is already predetermined by a certain vision of its solution—a vision that is often biased by diverse values and interests. This results from the fact that in pluralist societies, in which a multitude of world views and values compete, the determination and formulation of a problem as well as the assessment of its ―solution‖ are in themselves controversial and open to discussion. Based on differing belief and value systems, problems and solutions can be ―framed‖ in a variety of ways, and it is contentious whether anyone can legitimately claim the authority to decide who is right and who is wrong. We call this the ―perspectivity‖ of ill-structured problems. It depends on the perspective, the vantage point, of who is involved in a complex situation how exactly a problem is perceived and framed. Further characteristics of wicked problems are a direct consequence of perspectivity. As Rittel and Webber state, ―Solutions to wicked problems are not true-or-false, but good-orbad‖ [4]. This is because there are many parties with potentially varying interests, value-sets, and ideological predilections who are more likely to assess a solution as ―better or worse‖ or ―satisfying‖ or ―good enough.‖ In addition [4]: Wicked problems do not have an enumerable (or an exhaustively describable) set of potential solutions, nor is there a well-described set of permissible operations that may be incorporated into the plan. The fact that certain perspectives on an ill-structured problem might be overlooked is important because of two other characteristics of Rittel and Webber‘s wicked problems that are crucial for engineering in particular [4]. There is ―no immediate and no ultimate test of a solution‖ to it because any ―solution, after being implemented, will generate waves of consequences‖ which ―may yield utterly undesirable repercussions which outweigh the intended advantages.‖ And: ―Every solution to a wicked problem is a ̳one-shot operation‘...It leaves ̳traces‘ that cannot be undone. One cannot build a freeway to see how it works, and then easily correct it after unsatisfactory performance.‖ The concept of an ―ill-structured problem‖ presented here refers to the fact that engineers are often confronted with situations that require structuring. Most worrisome are those situations that seem to be straightforward but are not. For the professional, the biggest challenge is to realize, first of all, that there might be perspectives on a problem other than his or her own. As Coughlin notes, it can be difficult to imagine, and take seriously, a perspective that is in opposition to one‘s own ([9]; see also [5, 6]). The challenge is to identify the ethical dimensions of a decision especially in those situations where they are not obvious or are hidden, and where available descriptions do not contain any hint of the complexity and multiperspectivity of the problem. According to Rittel and Webber, the multi-perspectivity of wicked problems implies that they should be approached ―based on a model of planning as an argumentative process in the course of which an image of the problem and of the solution emerges gradually among the participants, as a product of incessant judgment, subjected"
37a6ca826054c709b4414a32ab37a665833ddb40,
8f4eb7078bc2ace5fe5eb79c5e5f85c0c9702419,
cfbf4ab1bb2252e54f10164251c8d85515ff37ce,
e7e66b30f0a46092d537f5351f4aac94d565a24d,"As robots become more pervasive and take on an ever-growing number of tasks, exploring ethical issues relating to the technology takes on increasing importance. Specifically, the manufacturing and sale of personal service robots could be severely detrimental to the environment. Ideally, members of the robotics community would develop a comprehensive awareness of the complex ethical and environmental consequences emerging from their design pathways before historical patterns are repeated."
81ad26fe408022aa4f432c2f2ee03297a5439a4c,
cac936c7f9a44878ba452557ce28b2c789a95940,
e2c5e41b2cb0c74050aadfe8a3530924a99e289d,
2fd1e4b94793afa313a90b9ab681da4dbbb56851,"Presented on October 26, 2010 from 5:00 pm-6:00 pm in the Skiles Building room 343 on the Georgia Tech campus."
5bf69583c436a9633d8a7e174558bea70a338508,
c0f84a692b5b289f23f8b954cd3aba703323e9ab,
ca6a67cc1bb3d94ea977abc3b7a6daa57a9e0d49,
cd9d31a665e3ff9b875e886388e8b1d2f8a225ca,
f7537948d394f0aac4b66770f944404f6a16f4e1,"Technological change results in changes in expectations, in this case affecting the workplace."
238b814af36eb21c38b24bea0e105c6c0fb0f07a,
6b579b3cc2f0a6d93b38e3c016f18a40dda96502,
9913566a35b274c3c7cf63ce08f496127d2998ed,"The debate about evolution continues as another category of critics seeks to challenge its merits. These critics put forward intelligent design (ID) as a scientific rival to evolution. For those familiar with the relevant history, this occurrence resurrects a cycle of debate about evolution that never seems to end. The purpose of this article is to identify key reasons why debate about evolution remains with us."
21a1f2be19042c39334c3d639aa6e221a73ec348,"The development and implementation of new technologies has led to a significant erosion of privacy. In this circumstance, it is an arduous struggle to control one's personal information. Now we need to determine the best ways to protect individuals from abuse when their personal information is collected and distributed."
48c588d564cab83941aac0db2c7987fb42f51a41,"The implications of the institutional review board (IRB) system's growing purview are examined. Among the issues discussed are whether IRBs are censoring research and whether the IRB review process fundamentally alters the research that is being conducted. The intersection between IRB review and free speech is also explored. In general, it is argued that the review system for human subjects research (HSR) should be modified in order to limit the scope of IRB review."
52fea28589ccd35ac6b10b89b3b7c28e97438ba9,
9d16d7887e9ef18ffc5643a6f648fe9940eda512,"The U.S. military has started to construct and deploy robotic weapons systems. Although human controllers may still be monitoring the functioning of the technology, the next logical step is to transfer incrementally more of the decision-making power to the robots themselves. Thus, this article seeks to examine the ethical implications of the creation and use of ""autonomous weapons systems."""
c5cf7b98b9f35ff50c94543fe5045ed2c5ed6994,
da9ada09a17c40369827c18f05884f57b8de4c44,
36a9233fcee31ebfb895df600670b72ad631335e,"If predictions are correct, the capacity to enhance children at the genetic level may be coming over the course of the future.1 Supporters of the technology suggest that it has the promise to fulfil some of the lofty hopes crafted by human imagination, the eradication of disease and the expansion of human capacities. Critics, offering a bleaker picture of the future, maintain that it is the height of human arrogance to presume that we will be able to control and master our genetics in the way in which scientists predict. They warn that if enhancing unborn children is put into practice, grave consequences may ensue. Although it may be misguided to condemn gene therapy categorically, there are compelling reasons to have reservations about using the technology for enhancement purposes."
f8f7f58f2a0e27f20058d42e325b0b271a1105d5,": We developed a new tool to assess ethics pedagogy in science and engineering. The Engineering and Science Issues Test (ESIT), measures moral judgment in a manner similar to the Defining Issues Test 2 but is built around technical dilemmas in science and engineering. We used a quasi-experimental approach with pre- and post-tests, and we compared the results those of a control group with no overt ethics instruction. Our findings are that several (but not all) stand-alone classes showed a significant improvement compared to the control group when the metric includes multiple stages of moral development. We also found the written test had a higher response rate and sensitivity to pedagogy than the electronic version. We do not find significant differences on pre-test scores with respect to age, education level, gender or political leanings, but we do on whether subjects were native English speakers and or had previous ethics instruction. results on to results test to and"
7bdd578ed55083d22f6503905d28c56e5fd4387b,
a019bc78650102582ee2974e0bee86e0f9ba9d1b,
51c4d97153dcd47d71a1a14c5e376379d746b75f,
9a14a3cd08cb65db085e4d8f578c7b5c5232a492,
01ca133c6fa85b700c70f066bb786618ce0754fa,
92256bd7835a084f912f42dbc7e4487c98e856f2,"Our courts are regularly confronted with the claims of expert witnesses. Since experts are permitted to present testimony in the courtroom, we have to assume that judges and juries understand what it means to have expertise and can consistently recognize someone who has it. Yet these assumptions need to be examined, for the legal system probably underestimates the difficulty of identifying expertise. In this paper, several philosophical issues pertaining to expertise will be discussed, including what expertise is, why we rely on experts, what measures can be taken to verify expertise, and how we determine whether a particular individual is an expert."
9029ed0404cee1eab1fe992781bcb8197a67bb1d,
5941f9614281c55dd3fa5af887db88eee33e5a61,
2a22d9098b0445e4194476f61b0eb7999d24d877,"generic Partitive (physical and theoretical) Partitive (habits, law and jurisdiction) Partitive (geographical, topographical) Instrumental Cause/effect Beneficial Detrimental Process applied. Derivative When the user enters a search term, the system uses synonym relationships to identify the corresponding concept and then displays other concepts in an array arranged by type of relationship, as in this abridged display (each referenced concept is a hyperlink): telecommuting is narrower concept of labor new ways of working and living is broader concept of mobile telecommuting alternating telecommuting is instrumental for organizing work effectively causes flexible work time energy conservation is beneficial for virtual organizations combining family and work is detrimental to face-to-face contacts by instruments telecommuting workplaces online technology The system displays results using its Basic Semantic Reference Structure, a frame whose slots can be seen form the following illustration: What? Who? Event? Where? When? How? Universal, concept, theme Person Corporate body Name of event Space Time Aspect General manager Mike Osborne Asia T rading Co., Vancouver Canada >1998-11-1 Definition Planting of St. John’s trees Ministry of Agriculture, Lima El Algarrobo project Peru >1984 Propagation Section 3 dealt with automated methods to create the knowledge structures necessary for good user support. Susanne Humphrey et al, Automatic indexing by discipline and high-level categories: Methodology and potential applications, developed a system for automatically indexing documents with broad descriptors that express the general nature and orientation of the document and thus are useful complements to specific descriptors. Two types of broad descriptors are assigned. A broad scheme used at NLM to categorize journals by subject (127 Journal Descriptors, such as Drug therapy, Antibiotics, or Pulmonary disease (specialty)) and the 134 semantic types defined in the Unified Medical Language System , such as Spatial concept, Therapeutic or preventive procedure, or Medical device. Rules for assigning journal descriptors were developed based on statistical association of document features, such as title words, with journal descriptors assigned to documents in a training set.. The rules for assigning semantic types rely on a more complex indirect method. Hidetsugu Nanba et al, Classification of research papers using citation links and citation types: Toward automatic review article generation, presented a tool box for the automated or computerassisted generation of reviews based on analyzing citation relationships.. The three tools would each be useful individually: A tool to identify and demarcate areas in a document that are concerned with reference to and discussion of a cited document; a tool for determining the type of citation relationships; and a tool for automatic classification of a document or document passages based on typed citation relationships. The citation area tool starts from the sentence containing the citation and adds sentences preceding or following based on the occurrence of cue words that indicate text cohesion. The citation type tool is also based on cue words to assign a citation to one of three types: show other researchers theories and methods, point out problems or gaps in related works, and other. The paper discusses both word-based and citation-based approaches to automatic classification. In the middle of the day, an “idea mart” was held. It was devoted to extensive discussion of emergent research ideas or projects in small groups in five parallel sessions covering two topics each.. This experiment turned out very well, producing many useful suggestions for the research of the presenters. The second part of this report discusses themes that emerged from the papers and discussions. Some themes are clearly tied to one paper while others emerged in several papers. We begin with an overview Some themes in classification research Theme 1. Expanded use of classifications Theme 2. Requirements for diversity in classification Theme 3. The quest for unity. Multi-purpose classifications, reuse Theme 4. Types of knowledge covered in classifications Theme 5. Orientation of classification: Users’ conceptual structures or intrinsic logic of the domain Theme 6. Types of relationships in a thesaurus / classification / ontology Theme 7. Display and user interaction issues Theme 8. Practical issues Theme 1. Expanded use of classifications Several presentations call into question the restricted uses that classification schemes have played, being used primarily for organization of information for retrieval. Other roles that need to be explored more fully include roles in learning (e.g. the use of the visual anatomist for training and education), exploration and browsing, creativity, discourse, problem solving, and information How to build classification systems that would enable us to discover and see relationships that have not yet been established.? Theme 2. Requirements for diversity in classification Classifications should serve a given purpose for a given user community. Language – terms and their relationships – is complex; it shows differences not only across domains but also across user groups in the same domain. This introduces many sources of diversity in the design of classifications. Sources of diversity Knowledge is complex (title of the first talk but an underlying theme of all). Many types of knowledge Many (discourse) communities / communities of use Multiple perspectives (for example, “standard” medicine and “alternative” medicine). Problem of our inability to incorporate all perspectives into one structure or scheme, no matter how richly articulated it is, yet we know that any one perspective limits what we see or learn, and perspectives evolve. Multiple situations/contexts Many different uses of knowledge Implications One scheme or many? One representations vs. multiple representations Limitations on mapping between schemes Role of classification in bridging diversity Classification should honor diversity by reflecting different perspective s etc. But classification should also bridge diversity by mediating between different points of view, different knowledge and cultural systems. For example, a classification of concepts in “alternative” medicine could include scope notes and relationships that relate its concepts to concepts in “standard” medicine. By elaborating concepts, concept relationships, and conceptual structure in different realms, classification can help identify commonalities and differences and the nature of differences, supporting an effort at sharing and mutual refinement of conceptual structures. Theme 3. The quest for unity. Multi-purpose classifications, reuse Classifications require considerable intellectual investment, so one would like to reuse them. Tension with diversity! Can a thesaurus be reorganized for multiple purposes? Classification modules that can be used in different schemes: How do we build modular ontologies to better represent dynamic domains? These would be ontologies that could flexibly extend the working ontology, for example extending the ontology of basic business processes by adding a module about auctions. How can we build classification schemes that store basic-level (mid-level) attributes that are neither too abstract nor overly specified so that they can be used effectively by people in a variety of contexts, when we know neither who the people are nor what the contexts are? The mapping of ontologies one to another must include more than just terms and their relationships, but must also include information about the context/situation. Is it possible to reorganize an existing thesaurus into a “navigational ontology” to support searching and browsing? Or does such a tool have to be created initially with these goals in mind (re question one)? Can one thesaurus be reorganized in different ways to serve multiple purposes, such as searching, navigation, instruction, “stimulation” (creativity)? Theme 4. Types of knowledge covered in classifications Role and importance of all knowledge types Most classifications deal with (static) domain knowledge Additional approaches are needed to support users, such as Problem schemas as organizing principle: A classification of problems by problem type, such as fix a device (fix a car, fix a washing machine), buy something, write a computer program, giving for each problem a schema that specifies aspects to be considered in solving the problem; information, people, material needed for solving the problem; procedural steps for solving the problem. Functions as organizing principle, for example, technical components classified by all the functions they could serve Classification of cases for case-based reasoning or for education and learning"
f8611502438a959624d7718af98fbb1d49055e26,
01a22ae0acf2387adb15e03cb3d91323681c0e40,
873c3846e26853d300ecf73a6e910ecf6a84e63c,"While coreference resolution typically involves various linguistic challenges, recent models are based on a single pairwise scorer for all types of pairs. We present LingMess, a new coreference model that defines different categories of coreference cases and optimize multiple pairwise scorers, where each scorer learns a specific set of linguistic challenges. Our model substantially improves pairwise scores for most categories and outperforms cluster-level performance on Ontonotes and 5 additional datasets. Our model is available in https://github.com/shon-otmazgin/lingmess-coref"
c9b0fcfc9470318a56e7dedf4503186e3a343408,"We introduce fastcoref, a python package for fast, accurate, and easy-to-use English coreference resolution. The package is pip-installable, and allows two modes: an accurate mode based on the LingMess architecture, providing state-of-the-art coreference accuracy, and a substantially faster model, F-coref, which is the focus of this work. F-coref allows to process 2.8K OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the LingMess model, and to 12 minutes of the popular AllenNLP coreference model) with only a modest drop in accuracy. The fast speed is achieved through a combination of distillation of a compact model from the LingMess model, and an efficient batching implementation using a technique we call leftover batching. https://github.com/shon-otmazgin/fastcoref"
1781517a91be8248dd0febad65211a1b6614e199,"Transformers have become a standard architecture for many NLP problems. This has motivated theoretically analyzing their capabilities as models of language, in order to understand what makes them successful, and what their potential weaknesses might be. Recent work has shown that transformers with hard attention are quite limited in capacity, and in fact can be simulated by constant-depth circuits. However, hard attention is a restrictive assumption, which may complicate the relevance of these results for practical transformers. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We show that saturated transformers transcend the limitations of hard-attention transformers. With some minor assumptions, we prove that the number of bits needed to represent a saturated transformer memory vector is O(log n), which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer’s effective circuit depth by a factor of O(log n)."
0abcbdf40f872e6baf1c082811d4ae93df787698,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators."
22a10776774da81d7e20599329c6e7a05cd2c6b7,"We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale code-switched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases."
32948ae25dbd35f2d94a59c27f6bee935bd602b8,"How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs’ syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order."
361904ad7588e9fbe133e1b5cf6f490bddcbd551,"The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process. We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods. We propose a model that accounts for noisy pairs. This is achieved by introducing a generative model with a compatible iterative EM algorithm. The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness of our proposed algorithm on two alignment problems: bilingual word embedding translation, and mapping between diachronic embedding spaces for recovering the semantic shifts of words across time periods."
3a33b55a9a8d495205682da767e74f3f23020dbc,"We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. 
The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. 
We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively."
7c593f8b5525851c2b453253835ddac6b1104a3b,"Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system’s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure."
94cf3f2c4410fcb06a90abebd99f7113c69e1ed9,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling."
a439b10042185646d50e7ab532a6482b71d59339,"Word sense induction (WSI) is the task of unsupervised clustering of word usages within a sentence to distinguish senses. Recent work obtain strong results by clustering lexical substitutes derived from pre-trained RNN language models (ELMo). Adapting the method to BERT improves the scores even further. We extend the previous method to support a dynamic rather than a fixed number of clusters as supported by other prominent methods, and propose a method for interpreting the resulting clusters by associating them with their most informative substitutes. We then perform extensive error analysis revealing the remaining sources of errors in the WSI task. 
Our code is available at this https URL."
b373f8249e5a45c89c38e0b45f499f38697b68a6,"When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must “guess” this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method."
b6ffd8ed6a0d35bce6339492fb0e776fe75a04c8,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling."
c508f8ce3c84de19e41325be10ae6f9118534084,"Distributed learning is central for large-scale training of deep-learning models. However, they are exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models and their corresponding defenses assume that the rogue participants are (a) omniscient (know the data of all other participants), and (b) introduce large change to the parameters. We show that small but well-crafted changes are sufficient, leading to a novel non-omniscient attack on distributed learning that go undetected by all existing defenses. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (backdooring). We show that 20% of corrupt workers are sufficient to degrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into MNIST and CIFAR10 models without hurting their accuracy"
ca5814084dc4333e472fbaecef7ea7cf3be56ab8,"Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics."
d5ff69bdd782101bc4ad11633d02af5ec8f9387c,"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun’s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While “embedding debiasing” methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words’ context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results."
efeab0dcdb4c1cce5e537e57745d84774be99b9a,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases."
f3b73161822f52e8917f7490e23e5f1f7767aa14,"Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model."
f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1,"Abstract We provide the first computational treatment of fused-heads constructions (FHs), focusing on the numeric fused-heads (NFHs). FHs constructions are noun phrases in which the head noun is missing and is said to be “fused” with its dependent modifier. This missing information is implicit and is important for sentence understanding. The missing references are easily filled in by humans but pose a challenge for computational models. We formulate the handling of FHs as a two stages process: Identification of the FH construction and resolution of the missing head. We explore the NFH phenomena in large corpora of English text and create (1) a data set and a highly accurate method for NFH identification; (2) a 10k examples (1 M tokens) crowd-sourced data set of NFH resolution; and (3) a neural baseline for the NFH resolution task. We release our code and data set, to foster further research into this challenging problem."
06354570d5f6be803d4a79bf59ecbb097bca8755,"While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism."
084c5f64e91fe190ea02201cffebe4419f3c3889,"An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors. We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin."
0d38284025bd0eabf92399b35df189ae1e034236,"Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in this https URL and this https URL ."
37cc20381657bac28e68445d08432bb7da3cd087,"We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. SetExpander implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at this https URL (some images were blurred for privacy reasons)."
413a03a146e6f7b16c11e73243d83e6f1a6627a3,"We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences."
8444ebd873ee6274567403a2b47f1cde936be394,"We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions)."
ab13dc188d2408c6c66636bf37c438536cf85d2d,"We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. SetExpander implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv ."
d5629135ec1f29c6dc1ffd5cc5a65fe67445eee0,"Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in—and can be recovered from—the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to—and likely condition on—demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features."
dd2a14ca5a678695c2ab2ee23069321db86c9846,"Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task."
ff9318366692bdf16b31b0b74ff71c0deeb4b227,"Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates."
08d2663374bfbe467acbc576595bad76f95513af,
3615c89fee83759f465ff09e6ca68258bd81886d,"The tremendous success of word embeddings in improving the ability of computers to perform natural language tasks has shifted the research on language representation from word representation to focus on sentence representation. This shift introduced a plethora of methods for learning vector representations of sentences, many of them based on compositional methods over word embeddings. These vectors are used as features for subsequent machine learning tasks or for pretraining in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they encapsulate. Recent studies analyze the encoded representations and the kind of information they capture. In this paper, we analyze results from a previous study on the ability of models to encode basic properties such as content, order, and length. Our analysis led to new insights, such as the effect of word frequency or word distance on the ability to encode content and order."
480d545ac4a4ffff5b1bc291c2de613192e35d91,"We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL."
4e4cae6d93a1c2bdc2ac0fa2810a270052378fe2,"We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin's L* algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation."
57133ef4c4de4d54a57686b8a914b06e4ff4aab5,"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."
709f0a4229e40339b595072ae9fbd3a1ae1fd93e,"Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually."
849fec1fd6ca709b28f24d5c825342ae3b6047f1,
a8cc07f4285864b899d8d9193d7378900a664834,"Neural network (“deep learning”) models are taking over machine learning approaches for language by storm. In particular, recurrent neural networks (RNNs), which are flexible non-markovian models of sequential data, were shown to be effective for a variety of language processing tasks. Somewhat surprisingly, these seemingly purely sequential models are very capable at modeling syntactic phenomena, and using them result in very strong dependency parsers, for a variety of languages."
af9d67bad068a77d165e145368e98bf7bd7cce72,"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). 
 
This release contains the test data used in the CoNLL 2017 shared task on parsing Universal Dependencies. Due to the shared task the test data was held hidden and not released together with the training and development data of UD 2.0. Therefore this release complements the UD 2.0 release (http://hdl.handle.net/11234/1-1983) to a full release of UD treebanks. In addition, the present release contains 18 new parallel test sets and 4 test sets in surprise languages. The present release also includes the development data already released with UD 2.0. Unlike regular UD releases, this one uses the folder-file structure that was visible to the systems participating in the shared task."
b64a0511b5e802b4dafef63b81800cf64f359eb1,"Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models."
c41092e1eed1ec554d0de37d6379cd5ac6fca4fe,
df80d1d53e8c8e17eb65174addeec668b8a59b71,"We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks—the stack long short-term memory unit (LSTM). Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: (i) standard word vectors based on look-up tables and (ii) character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in morphologically rich languages. Finally, we discuss the use of dynamic oracles in training the parser. During training, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with dynamic oracles yields a linear-time greedy parser with very competitive performance."
e75b3c12da067552fda910a5bbed8b4d0e82dbcb,"Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.

The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning."
f0f16c6f377408e9176351413cf296abf2c30fd7,This release contains errors in several files. Please use http://hdl.handle.net/11234/1-1983 instead.
fb39923f6a4cdb486da5b579c5f8e2c500f36a35,"Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content."
fc8eb74acd25169672b3b85517da3d66eb0c47cf,"We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system."
027ced58a1fb3600f438c856b9cc4e1ac24cb0a6,"Semantic NLP applications often rely on dependency trees to recognize major elements of the proposition structure of sentences. Yet, while much semantic structure is indeed expressed by syntax, many phenomena are not easily read out of dependency trees, often leading to further ad-hoc heuristic post-processing or to information loss. To directly address the needs of semantic applications, we present PropS -- an output representation designed to explicitly and uniformly express much of the proposition structure which is implied from syntax, and an associated tool for extracting it from dependency trees."
19a88698a3f0c16af480f4f1a0f4045b4f2a2857,"Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation."
1c7174bd2b01831920217088e2b48cb151691110,"We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account."
6789e0dbd294cccb3b7dd4e001c9e8ba4813f334,"We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network."
71cd024b29488c2ea1f83db1c20990a0adde05ab,"Morphological reinflection is the task of generating a target form given a source form and the morpho-syntactic attributes of the target (and, optionally, of the source). This work presents the submission of Bar Ilan University and the Massachusetts Institute of Technology for the morphological reinflection shared task held at SIGMORPHON 2016. The submission includes two recurrent neural network architectures for learning morphological reinflection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction. The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data."
779ea6e2ae1ebcca69db2739dab487f1608c00e6,"Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised preposition-sense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets."
94960fcdf0ea3b346fca77ae8c63ae7943eb0d28,"We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings. The parser’s implementation is available for download at the first author’s webpage."
9a5765f10f8cab7b3ebdac29ab38922a3be7e102,
a032bbeb523215a9462afafc17a047fa37b7d735,"Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modified trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive x2.7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees."
a32763adef1ef22cc27d4d67ef7ac1490d23ce0b,"We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract."
a782d0fc34c0ed5cd94a1e4237cb40900a248836,"Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task."
d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46,"Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages."
da797d293203ebe7c95edb98184955a9b92990a4,"We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task."
e44da7d8c71edcc6e575fa7faadd5e75785a7901,"There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations."
eec3a236ecd185712ce65fb336141f8656eea13d,"We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese."
f7e37cd317e037e05e30198b4259032d5f5e4e9e,"We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus."
1972e54dae2484475af2f0f9cbcdf9b350950db3,"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."
55d675398668de706f97314fba9e64611a1ce90a,
56edaa1368ff4dfa45388e4be24fdfbded7d88a7,"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation."
6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2,"Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others."
897eed0892f40d546e84465ed13a6bdc1ad0159b,"We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets."
0183b3e9d84c15c7048e6c2149ed86257ccdc6cb,"While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings."
09d5446fd2cb488e9cf0663dcd9f41ca4869e292,"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages."
0cf827a622083d9c46faf45a5778aa11050dd886,"We show that it is possible to automatically detect machine translated text at sentence level from monolingual corpora, using text classification methods. We show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it. Finally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences."
2012f32199adc88747d5a1b47c7b4ba1cb3cb995,"The word2vec software of Tomas Mikolov and colleagues (this https URL ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. 
This note is an attempt to explain equation (4) (negative sampling) in ""Distributed Representations of Words and Phrases and their Compositionality"" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean."
500d570ce02abf42bc1bc535620741d4c5665e6a,"Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.’s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations."
5ed3e63c10974e84a9cba51d5630a6ce5996714f,"Arc-eager dependency parsers process sentences in a single left-to-right pass over the input and have linear time complexity with greedy decoding or beam search. We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser."
ac4537cbd61ce0c0d24c17ca7a6a7943463a267f,"We propose an intermediary-level semantic representation, providing a higher level of abstraction than syntactic parse trees, while not committing to decisions in cases such as quantification, grounding or verbspecific roles assignments. The proposal is centered around the proposition structure of the text, and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees, such as copular relations introduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing."
e3ef3a4f695de9817688ba3cf6f34b30a7dfc9df,"Arc-eager dependency parsers process sentences in a single left-to-right pass over the input and have linear time complexity with greedy decoding or beam search. We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser."
f4c018bcc8ea707b83247866bdc8ccb87cd9f5da,"We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization."
b80caf0492cbbe00310c1f4867aa8512b6f73320,"We adapt the dynamic-oracle training method of Goldberg and Nivre (2012; 2013) to train classifiers that produce probabilistic output. Evaluation of an Arc-Eager parser on 6 languages shows that the AdaGrad-RDA based training procedure results in models that provide the same high level of accuracy as the averagedperceptron trained models, while being sparser and providing well-calibrated probabilistic output."
d32c62d9709a10fc75096246971edc36a5855b53,"Previous incremental parsers have used monotonic state transitions. However, transitions can be made to revise previous decisions quite naturally, based on further information. We show that a simple adjustment to the Arc-Eager transition system to relax its monotonicity constraints can improve accuracy, so long as the training data includes examples of mistakes for the nonmonotonic transitions to repair. We evaluate the change in the context of a stateof-the-art system, and obtain a statistically significant improvement (p < 0.001) on the English evaluation and 5/10 of the CoNLL languages."
df791ad4ae9c8ca2c26c93c12b4525ff255fefe2,"We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books. The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided."
e775b768c6566c6deb3be07a9346d7439abbd0cc,"We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. 2006, which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model. We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsing methodology is useful in any case where the input is uncertain. Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank."
ed8d90fe462f27dde0bfe30c0584c780b73aae79,"This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios."
f90fc459c305f13b3fab0abf42b8dd1801c59511,"Greedy transition-based parsers are very fast but tend to suffer from error propagation. This problem is aggravated by the fact that they are normally trained using oracles that are deterministic and incomplete in the sense that they assume a unique canonical path through the transition system and are only valid as long as the parser does not stray from this path. In this paper, we give a general characterization of oracles that are nondeterministic and complete, present a method for deriving such oracles for transition systems that satisfy a property we call arc decomposition, and instantiate this method for three well-known transition systems from the literature. We say that these oracles are dynamic, because they allow us to dynamically explore alternative and nonoptimal paths during training — in contrast to oracles that statically assume a unique optimal path. Experimental evaluation on a wide range of data sets clearly shows that using dynamic oracles to train greedy parsers gives substantial improvements in accuracy. Moreover, this improvement comes at no cost in terms of efficiency, unlike other techniques like beam search."
fe6e30c0da4837c97f0092b79a3ed375c618e5ed,"Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n 2 ), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of 2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences."
3b6a160361e9180256477780983a5f2c8ad334b0,"We introduce precision-biased parsing: a parsing task which favors precision over recall by allowing the parser to abstain from decisions deemed uncertain. We focus on dependency-parsing and present an ensemble method which is capable of assigning parents to 84% of the text tokens while being over 96% accurate on these tokens. We use the precision-biased parsing task to solve the related high-quality parse-selection task: finding a subset of high-quality (accurate) trees in a large collection of parsed text. We present a method for choosing over a third of the input trees while keeping unlabeled dependency parsing accuracy of 97% on these trees. We also present a method which is not based on an ensemble but rather on directly predicting the risk associated with individual parser decisions. In addition to its efficiency, this method demonstrates that a parsing system can provide reasonable estimates of confidence in its predictions without relying on ensembles or aggregate corpus counts."
86a0c081b11275806cd675759d6876ac7972f489,"When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain. To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline."
d4bd0035fe14832626279e6c3c72b73c21c7f5d8,"The standard training regime for transition-based dependency parsers makes use of an oracle, which predicts an optimal transition sequence for a sentence and its gold tree. We present an improved oracle for the arc-eager transition system, which provides a set of optimal transitions for every valid parser configuration, including configurations from which the gold tree is not reachable. In such cases, the oracle provides transitions that will lead to the best reachable tree from the given configuration. The oracle is efficient to implement and provably correct. We use the oracle to train a deterministic left-to-right dependency parser that is less sensitive to error propagation, using an online training procedure that also explores parser configurations resulting from non-optimal sequences of transitions. This new parser outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets."
fbb4ad663c233a8fb33d34598f6e8343de564dcc,"While the use of cluster features became ubiquitous in core NLP tasks, most cluster features in NLP are based on distributional similarity. We propose a new type of clustering criteria, specific to the task of part-of-speech tagging. Instead of distributional similarity, these clusters are based on the beha vior of a baseline tagger when applied to a large corpus. These cluster features provide similar gains in accuracy to those achieved by distributional-similarity derived clusters. Using both types of cluster features together further improve tagging accuracies. We show that the method is effective for both the in-domain and out-of-domain scenarios for English, and for French, German and Italian. The effect is larger for out-of-domain text."
53bab7c386fe995ba1120b01f84a5ce1789dbba6,
6a4b296ed7102cde2da400fac7ce6f5f78d6d2de,"The averaged-perceptron learning algorithm is simple, versatile and effective. However, when used in NLP settings it tends to produce very dense solutions, while much sparser ones are also possible. We present a simple modification to the perceptron algorithm which allows it to produce sparser solutions while remaining accurate and computationally efficient. We test the method on a multiclass classification task, a structured prediction task, and a guided learning task. In all of the experiments the method produced models which are about 4-5 times smaller than the averaged perceptron, while remaining as accurate."
92de46eae233b296d242379785514943e75dcced,"We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs."
bd87a452b4a2530b6c22eadec1d7bede990ad36f,"We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese."
f2410f3d4805e64e183a9d3ab7035f9018344063,
3ce0f00d6c949192107f1bd6a167c03e1fb7393a,"We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models."
5f3d297e0974955b81f2dadda27bc6c8ad494cd8,"We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to under- and over- produce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and first- and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses."
a4005f3cecb4ec2e55cea90ecff5709ebce29923,"Consider the following (simple) English sentences: “I driv e a car.”, “I don’t know how to drive”, “I wash the car”, “I wash the floor”. Translating them to Hebrew using Goo gle’s statistical MT system, yields: zipekna bdep ip` (I drive(masculine) a car);bedpl zr ei `l ip` (I don’t know(feminine) how to drive); ugex ip` zipeknd z` (I wash(masculine) the car); and dtvxd z` zthey ip` (I wash(feminine) the floor). While amusing and not quite politically correct, these are al l arguably very good translations: without explicit gender marking, the translator can not know if the speaker is ma culine or feminine, and he (she?) resorts to deciding based on her (his?) cultural knowledge. This does, however, highlight a class of problems which aris e when attempting to translate from a morphologically clean language (e.g. English) into a morphologic ally rich one (e.g. Hebrew): many words in the target language are morphologically marked for gender and number, and the translator should be able to generate these markings correctly, based on little, elusive or sometimes n o evidence in the source language. These issues are orthogonal to the data sparsity issues associated with high ly inflected languages. Can current state-of-the-art statistical MT systems handl e this? In what follows we present a few cases where the target language output should be morphologically marke d for either gender or number, with varying amounts (and sources) of information available on the source langua ge text, and discuss the suitability of current translation models to handle these phenomena. We show that correct handling of morphological agreement is beyond the reach of current systems as it requires better syntactic models, looking beyond a single sentence, and performing accurate anaphora resolution. However, while phrase-based models can not model even the simplest ca ses, syntax based models already posses most of the necessary machinery. While we demonstrate using English ⇒ Hebrew translations, similar issues will occur when transl ting into practically any morphologically rich language. Moreover, the issues discussed remain relevant also when the source language is also morphologically rich."
afba150b66d486ba86f8bf3aba0e96599135defc,"The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite language-specific idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations."
dd2fe6c8594916965375240009ab865bae504f71,"s of the Israeli Seminar on Computational Linguistics Wednesday, 16 June 2010"
f6d8c5f676622bd17df686bcc39357e68972b2e8,"The phonetic transcription of a word from a source language using a different script is called transli teration. Transliterations affect Information Extract ion (IE) in two ways. First, it takes time for a tr nsliterated word to make it into a technical lexicon, m aking recognition difficult. A second problem is th e variability of ways a foreign word can be rendered phonetically, leading in most cases (except for ver y short words) to many possible spellings of the word an , therefore, making lexicon-based recognition difficult. In this paper, we present a method for a utomatically acquiring transliterated words and the ir source word in order to improve a technical lexicon , addressing both problems: spelling variants and unknown tokens."
fd7269689a79f788add950f54342cab9350e26e8,"We investigate the performance of an easy-first, non-directional dependency parser on the Hebrew Dependency treebank. We show that with a basic feature set the greedy parser's accuracy is on a par with that of a first-order globally optimized MST parser. The addition of morphological-agreement feature improves the parsing accuracy, making it on-par with a second-order globally optimized MST parser. The improvement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used."
ff4006c112812cef905889c00a55830843452896,"We specify here the features which are used by our models. Each feature description is composed of two parts: a description of a structural element, and a (possibly empty) description of a sequential context. All models discussed in the paper are obtained by combining a set of structural elements St with a set of sequential contexts Co, and producing all corresponding features (i.e. producing a feature for each structural element in St and a corresponding sequential context from Co). In Section 1 we define the different loop types, which are used for categorizing structural elements. In Section 2 we give the three sets of structural elements St, St, and St, and in Section 3 we give the three sets of sequential contexts Co, Co, and Co, which are used in our models. All examples in the text refer to the sequence-folding (x, y) depicted in Fig. 1."
1e45c5529d900fd5a38cd03731df8bcbfa615ef8,
1e941acf5e3d3339f51b52bcbf940dff1287fc28,"We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains. These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood. Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities -- but we find this contribution does not generalize outside the training corpus. As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources."
a64920f3b8788274d80aaec7e84bb91102e02f1a,"We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Tree-bank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than Malt-Parser, and (b) both parsers do not make good use of morphological information when parsing Hebrew."
e0ce36d3757761f8bb50d23082c3f366e3f6c84d,"Word associations are an important element of linguistic creativity. Traditional lexical knowledge bases such as WordNet formalize a limited set of systematic relations among words, such as synonymy, polysemy and hypernymy. Such relations maintain their systematicity when composed into lexical chains. We claim that such relations cannot explain the type of lexical associations common in poetic text. We explore in this paper the usage of Word Association Norms (WANs) as an alternative lexical knowledge source to analyze linguistic computational creativity. We specifically investigate the Haiku poetic genre, which is characterized by heavy reliance on lexical associations. We first compare the density of WAN-based word associations in a corpus of English Haiku poems to that of WordNet-based associations as well as in other non-poetic genres. These experiments confirm our hypothesis that the non-systematic lexical associations captured in WANs play an important role in poetic text. We then present Gaiku, a system to automatically generate Haikus from a seed word and using WAN-associations. Human evaluation indicate that generated Haikus are of lesser quality than human Haikus, but a high proportion of generated Haikus can confuse human readers, and a few of them trigger intriguing reactions."
ede484b7a4f6274394b916c709b24b40838ae99f,
f120ea4eca8a7d96ddb4e5073198e67f4a50d854,"We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank. This is achieved by defining a stochastic mapping layer between the two resources. Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora. We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens."
023214084fb8b56b4027e9bcfaa3b8f3cd0db829,"We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective initial estimations p(t|w). We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods."
40563e47df962014215dd768cf8974b4527cd7c2,"Morphological disambiguation proceeds in 2 stages: (1) an analyzer provides all possible analyses for a given token and (2) a stochastic disambiguation module picks the most likely analysis in context. When the analyzer does not recognize a given token, we hit the problem of unknowns. In large scale corpora, unknowns appear at a rate of 5 to 10% (depending on the genre and the maturity of the lexi"
49f531f0463739b5b1cf65e9b4d4ea1978656333,"Morphologically rich languages pose a challenge to the annotators of treebanks with respect to the status of orthographic (space-delimited) words in the syntactic parse trees. In such languages an orthographic word may carry various, distinct, sorts of information and the question arises whether we should represent such words as a sequence of their constituent morphemes (i.e., a Morpheme-Based annotation strategy) or whether we should preserve their special orthographic status within the trees (i.e., a Word-Based annotation strategy). In this paper we empirically address this challenge in the context of the development of Language Resources for Modern Hebrew. We compare and contrast the Morpheme-Based and Word-Based annotation strategies of pronominal clitics in Modern Hebrew and we show that the Word-Based strategy is more adequate for the purpose of training statistical parsers as it provides a better PP-attachment disambiguation capacity and a better alignment with initial surface forms. Our findings in turn raise new questions concerning the interaction of morphological and syntactic processing of which investigation is facilitated by the parallel treebank we made available."
4bc0c0785dd1af1706fa4fdeb25f0bf0526c5dd0,"We report on an effort to build a corpus of Modern Hebrew tagged with part-of-speech and morphology. We designed a tagset specific to Hebrew while focusing on four aspects: the tagset should be consistent with common linguistic knowledge; there should be maximal agreement among taggers as to the tags assigned to maintain consistency; the tagset should be useful for machine taggers and learning algorithms; and the tagset should be effective for applications relying on the tags as input features. In this paper, we illustrate these issues by explaining our decision to introduce a tag for beinoni forms in Hebrew. We explain how this tag is defined, and how it helped us improve manual tagging accuracy to a high-level, while improving automatic tagging and helping in the task of syntactic chunking."
7d53d45bb95c8f2e7f00341f6739c7792667035e,"Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far."
815fdf98dbbf01a62b0f7373a82e38e904bcd264,
cc6fef7e0c439b5c3098c77d80d3b4af93a7784a,"We present a fast, space efficient and non-heuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library."
ecc4db6a9051ac7209fcf9713e25788f109bd5f7,"We study the issue of porting a known NLP method to a language with little existing NLP resources, specifically Hebrew SVM-based chunking. We introduce two SVM-based methods ‐ Model Tampering and Anchored Learning. These allow fine grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus, distinguish the role and interaction of lexical features and eventually construct a model with ∼10% error reduction. The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text. The SVM analysis methods also provide general insight on SVM-based chunking."
09253b10530edccc9079f0fa1a298d319186bc0a,"We present a method for Noun Phrase chunking in Hebrew. We show that the traditional definition of base-NPs as non-recursive noun phrases does not apply in Hebrew, and propose an alternative definition of Simple NPs. We review syntactic properties of Hebrew related to noun phrases, which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English. As a confirmation, we apply methods known to work well for English to Hebrew data. These methods give low results (F from 76 to 86) in Hebrew. We then discuss our method, which applies SVM induction over lexical and morphological features. Morphological features improve the average precision by ~0.5%, recall by ~1%, and F-measure by ~0.75, resulting in a system with average performance of 93% precision, 93.4% recall and 93.2 F-measure."
4a2a2390242d7f94f7af35176b999f61d6d9bac6,
af0607ebbb98ae2e293d93f4f3b4759fdbb961d7,"I provide an example to illustrate the contention that there are cases in which a recipient can acquire testimonially based knowledge from false testimony. The example involves a case in which the proposition believed by the recipient, though derived in a testimonially based way, is not identical with the proposition attested to. I conclude by suggesting that if we are to make sense of such cases as cases of testimonially based knowledge, we need to make some revisions both in our conception of the ways in which testimonially-grounded warrant can accrue to a belief and in our conception of what ‘testimonial authority’ can apply to."
aa3ea2d5fc8139fe772764585219de8d372013e3,"The transcriptional activities of c-Ets-1 and v-Ets and their functional interaction with the AP-1 factor c-Jun were investigated. Several recombinant Ets proteins were produced and purified either from bacteria or from insect cells. Plasmid DNAs that contained the polyoma virus enhancer Ets/AP-1 element were used as templates for in vitro transcription assays in the presence of HeLa nuclear extract and various combinations of the Jun and Ets proteins. Under these conditions full-length c-Ets-1 on its own does not markedly influence transcription but abolishes the strong transcriptional stimulation normally elicited by Jun. This repression depends on the Ets-binding site and on specific features of c-Ets-1 structure, as both v-Ets and a natural splicing variant c-Ets-1 (delta VII) fail to inhibit Jun activity. These findings suggest that c-Ets may act both as a transcriptional repressor or activator depending on promoter context and splicing pattern."
648217d6ffa3090821775aa8da4d60a66e5f0443,"We report on the first year of Fermi γ -ray observations of pulsed high-energy emission from the old PSR J2043 + 2740. The study of the γ -ray efficiency of such old pulsars gives us an insight into the evolution of pulsars’ ability to emit in γ rays as they age. The γ -ray light curve of this pulsar above 0.1 GeV is clearly defined by two sharp peaks, 0.353 ± 0.035 periods apart. We have combined the γ -ray profile characteristics of PSR J2043 + 2740 with the geometrical properties of the pulsar’s radio emission, derived from radio-polarization data, and constrained the pulsar-beam geometry in the framework of a two-pole caustic (TPC) and an outer gap (OG) model. The ranges of magnetic inclination and viewing angle were determined to be {α, ζ } ∼ {52◦–57◦, 61◦–68◦} for the TPC model, and {α, ζ } ∼ {62◦–73◦, 74◦–81◦} and {α, ζ } ∼ {72◦–83◦, 60◦–75◦} for the OG model. Based on this geometry, we assess possible birth locations for this pulsar and derive a likely proper motion, sufficiently high to be measurable with VLBI. At a characteristic age of 1.2 Myr, PSR J2043 + 2740 is the third oldest of all discovered, non-recycled, γ -ray pulsars: it is twice as old as the next oldest, PSR J0357 + 32, and younger only than the recently discovered PSR J1836 + 5925 and PSR J2055 + 25, both of which are at least five and ten times less energetic, respectively."
09dd9b91dc90818ef8a663cff0a45a9f1da186fe,"We report on the observations of 14 dwarf spheroidal galaxie s with theFermi Gamma-Ray Space Telescope taken during the first 11 months of survey mod e operations. TheFermi telescope, which is conducting an all-sky γ-ray survey in the 20 MeV to>300 GeV energy range, provides a new opportunity to test particle dark matter mode ls through the expected γ-ray emission produced by pair annihilation of weakly interacting ma ssive particles (WIMPs). Local Group dwarf spheroidal galaxies, the largest galactic subs tr ctures predicted by the cold dark 31University of Alabama in Huntsville, Huntsville, AL 35899, USA 32Department of Physics, Center for Cosmology and Astro-Part icle Physics, The Ohio State University, Columbus, OH 43210 , USA 33Department of Physics, Royal Institute of Technology (KTH) , AlbaNova, SE-106 91 Stockholm, Sweden 34UCO/Lick Observatories, Santa Cruz, CA 95064, USA 35Department of Physics, Tokyo Institute of Technology, Megu ro City, Tokyo 152-8551, Japan 36Waseda University, 1-104 Totsukamachi, Shinjuku-ku, Toky o, 169-8050, Japan 37Centre d’́ Etude Spatiale des Rayonnements, CNRS /UPS, BP 44346, F-30128 Toulouse Cedex 4, France Center for Research and Exploration in Space Science and Tec hnology (CRESST), NASA Goddard Space Flight Center, Greenbelt, MD 20771, USA Istituto Nazionale di Fisica Nucleare, Sezione di Trieste, and Università di Trieste, I-34127 Trieste, Italy 40Istituto Nazionale di Fisica Nucleare, Sezione di Roma “Tor Vergata”, I-00133 Roma, Italy 41Department of Physics and Astronomy, University of Denver, Denver, CO 80208, USA 42Max-Planck Institut für extraterrestrische Physik, 8574 8 Garching, Germany 43Institut für Astround Teilchenphysik and Institut für T heoretische Physik, Leopold-Franzens-Universität Inns bruck, A-6020 Innsbruck, Austria 44Institut de Ciencies de l’Espai (IEEC-CSIC), Campus UAB, 08 193 Barcelona, Spain 45Space Sciences Division, NASA Ames Research Center, Mo ffett Field, CA 94035-1000, USA 46NYCB Real-Time Computing Inc., Lattingtown, NY 11560-1025 , USA Department of Chemistry and Physics, Purdue University Cal umet, Hammond, IN 46323-2094, USA 48Institute of Space and Astronautical Science, JAXA, 3-1-1 Y oshinodai, Sagamihara, Kanagawa 229-8510, Japan 49Institució Catalana de Recerca i Estudis Avançats (ICREA ), Barcelona, Spain 50Consorzio Interuniversitario per la Fisica Spaziale (CIFS ), I-10133 Torino, Italy 51University of Maryland, Baltimore County, Baltimore, MD 21 50, USA 52Dipartimento di Fisica, Università di Roma “Tor Vergata”, I-00133 Roma, Italy 53School of Pure and Applied Natural Sciences, University of K almar, SE-391 82 Kalmar, Sweden 54Center for Cosmology, Physics and Astronomy Department, Un iversity of California, Irvine, CA 92697-2575, USA"
754e4b12d9c0c6091bb65bb2e5de1c07fa632f37,
3ec086ed6a23fab683de201f7d3d387e004f22f4,"The Cygnus region hosts a giant molecular-cloud complex which actively forms massive stars. Interactions of cosmic rays with interstellar gas and radiation fields make it shine at gamma-ray energies. Several gamma-ray pulsars and other energetic sources are seen in this direction. In this paper we analyse the gamma-ray emission measured by the Fermi Large Area Telescope in the energy range from 100 MeV to 100 GeV in order to probe the gas and cosmic-ray content over the scale of the whole Cygnus complex. The signal from bright pulsars is largely reduced by selecting photons in their off-pulse phase intervals. We compare the diffuse gamma-ray emission with interstellar gas maps derived from radio/mm-wave lines and visual extinction data, and a global model of the region, including other pulsars and gamma-ray sources, is sought. The integral HI emissivity and its spectral energy distribution are both consistent within the systematics with LAT measurements in the interstellar space near the solar system. The average X=N(H2)/W(CO) ratio is consistent with other LAT measurements in the Local Arm. We detect significant gamma-ray emission from dark neutral gas for a mass corresponding to ~40% of that traced by CO. Despite the conspicuous star formation activity and large masses of the interstellar clouds, the cosmic-ray population in the Cygnus complex averaged over a few hundred parsecs is similar to that of the local interstellar space. (abridged)"
ac700398e3e7ba77ba2612eb02a936a914939ea0,"The Fermi Large Area Telescope (LAT) First Source Catalog (1FGL) provided spatial, spectral, and temporal properties for a large number of γ-ray sources using a uniform analysis method. After correlating with the most-complete catalogs of source types known to emit γ rays, 630 of these sources are “unassociated” (i.e., have no obvious counterparts at other wavelengths). Here, we employ two statistical analyses of the primary γ-ray characteristics for these unassociated sources in an effort to correlate their γ-ray properties with the active galactic nucleus (AGN) and pulsar populations in 1FGL. Based on the correlation results, we classify 221 AGN-like and 134 pulsar-like sources in the 1FGL unassociated sources. The results of these source “classifications” appear to match the expected source distributions, especially at high Galactic latitudes. While useful for planning future multiwavelength follow-up observations, these analyses use limited inputs, and their predictions should not be considered equivalent to “probable source classes” for these sources. We discuss multiwavelength results and catalog cross-correlations to date, and provide new source associations for 229 Fermi-LAT sources that had no association listed in the 1FGL catalog. By validating the source classifications against these new associations, we find that the new association matches the predicted source class in ∼80% of the sources."
ba7d2a674e711b31192da3f5073ecd0beb3c1158,
0a6789125b65dbd4fe40940e3904e9dab6f36e02,"During its first year of data taking, the Large Area Telescope (LAT) onboard the Fermi Gamma-Ray Space Telescope has collected a large sample of high-energy cosmic-ray electrons and positrons (CREs). We present the results of a directional analysis of the CRE events, in which we searched for a flux excess correlated with the direction of the Sun. Two different and complementary analysis approaches were implemented, and neither yielded evidence of a significant CRE flux excess from the Sun. We derive upper limits on the CRE flux from the Sun's direction, and use these bounds to constrain two classes of dark matter models which predict a solar CRE flux: (1) models in which dark matter annihilates to CREs via a light intermediate state, and (2) inelastic dark matter models in which dark matter annihilates to CREs."
2f7c8e9ca890e905865b29e8855611b4cc6c330b,
42496ab07b29a8a466e0fcd02d8b47de4e834c6e,"HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Erratum: the first fermilarge area telescope catalog of gamma-ray pulsars Aous A. Abdo, Markus Ackermann, Marco Ajello, William B. Atwood, Magnus Axelsson, Luca Baldini, Jean Ballet, Guido Barbiellini, Matthew G. Baring, D. Bastieri, et al."
535b7e0fad60be62412382ece170e30935ae344f,"The high-frequency peaked BL Lac object PKS 2005-489 was the target of a multi-wavelength campaign with simultaneous observations in the TeV γ-ray (H.E.S.S.), GeV γ-ray (Fermi/LAT), X-ray (RXTE, Swift), UV (Swift) and optical (ATOM, Swift) bands. This campaign was carried out during a high flux state in the synchrotron regime. The flux in the optical and X-ray bands reached the level of the historical maxima. The hard GeV spectrum observed with Fermi/LAT connects well to the very high energy (VHE, E > 100 GeV) spectrum measured with H.E.S.S."
5a36269802259dc70d426965aee4f76095cee511,"We report on the γ-ray activity of the blazar Mrk 501 during the first 480 days of Fermi operation. We find that the average Large Area Telescope (LAT) γ-ray spectrum of Mrk 501 can be well described by a single power-law function with a photon index of 1.78 ± 0.03. While we observe relatively mild flux variations with the Fermi-LAT (within less than a factor of two), we detect remarkable spectral variability where the hardest observed spectral index within the LAT energy range is 1.52 ± 0.14, and the softest one is 2.51 ± 0.20. These unexpected spectral changes do not correlate with the measured flux variations above 0.3 GeV. In this paper, we also present the first results from the 4.5 month long multifrequency campaign (2009 March 15—August 1) on Mrk 501, which included the Very Long Baseline Array (VLBA), Swift, RXTE, MAGIC, and VERITAS, the F-GAMMA, GASP-WEBT, and other collaborations and instruments which provided excellent temporal and energy coverage of the source throughout the entire campaign. The extensive radio to TeV data set from this campaign provides us with the most detailed spectral energy distribution yet collected for this source during its relatively low activity. The average spectral energy distribution of Mrk 501 is well described by the standard one-zone synchrotron self-Compton (SSC) model. In the framework of this model, we find that the dominant emission region is characterized by a size ≲0.1 pc (comparable within a factor of few to the size of the partially resolved VLBA core at 15–43 GHz), and that the total jet power (≃1044 erg s−1) constitutes only a small fraction (∼10−3) of the Eddington luminosity. The energy distribution of the freshly accelerated radiating electrons required to fit the time-averaged data has a broken power-law form in the energy range 0.3 GeV–10 TeV, with spectral indices 2.2 and 2.7 below and above the break energy of 20 GeV. We argue that such a form is consistent with a scenario in which the bulk of the energy dissipation within the dominant emission zone of Mrk 501 is due to relativistic, proton-mediated shocks. We find that the ultrarelativistic electrons and mildly relativistic protons within the blazar zone, if comparable in number, are in approximate energy equipartition, with their energy dominating the jet magnetic field energy by about two orders of magnitude."
71a68e6700f91e61e75b44e37b43be125745e426,"We report on the observation of the bright, long gamma-ray burst, GRB 090926A, by the Gamma-ray Burst Monitor and Large Area Telescope (LAT) instruments on board the Fermi Gamma-ray Space Telescope. GRB 090926A shares several features with other bright LAT bursts. In particular, it clearly shows a short spike in the light curve that is present in all detectors that see the burst, and this in turn suggests that there is a common region of emission across the entire Fermi energy range. In addition, while a separate high-energy power-law component has already been observed in other gamma-ray bursts, here we report for the first time the detection with good significance of a high-energy spectral break (or cutoff) in this power-law component around 1.4 GeV in the time-integrated spectrum. If the spectral break is caused by opacity to electron–positron pair production within the source, then this observation allows us to compute the bulk Lorentz factor for the outflow, rather than a lower limit."
88364cc0b592d3a43565349b0b5d83664f5ae695,"The BL Lacertae object 3C 66A was detected in a flaring state by the Fermi Large Area Telescope (LAT) and VERITAS in 2008 October. In addition to these gamma-ray observations, F-GAMMA, GASP-WEBT, PAIRITEL, MDM, ATOM, Swift, and Chandra provided radio to X-ray coverage. The available light curves show variability and, in particular, correlated flares are observed in the optical and Fermi-LAT gamma-ray band. The resulting spectral energy distribution can be well fitted using standard leptonic models with and without an external radiation field for inverse Compton scattering. It is found, however, that only the model with an external radiation field can accommodate the intra-night variability observed at optical wavelengths."
b50885b834cc8bcbb8f14b0b3432d10836242fbc,"Contrary to expectations, the γ-rays from a distant cluster of stars are dominated by emission from a single neutron star. We report on the Fermi Large Area Telescope’s detection of γ-ray (>100 mega–electron volts) pulsations from pulsar J1823–3021A in the globular cluster NGC 6624 with high significance (∼7 σ). Its γ-ray luminosity, Lγ = (8.4 ± 1.6) × 1034 ergs per second, is the highest observed for any millisecond pulsar (MSP) to date, and it accounts for most of the cluster emission. The nondetection of the cluster in the off-pulse phase implies that it contains <32 γ-ray MSPs, not ∼100 as previously estimated. The γ-ray luminosity indicates that the unusually large rate of change of its period is caused by its intrinsic spin-down. This implies that J1823–3021A has the largest magnetic field and is the youngest MSP ever detected and that such anomalous objects might be forming at rates comparable to those of the more normal MSPs."
d559c840b66cffc1e0d69c3d730c0eacc14f92ca,"Gamma-ray observations of the Crab Nebula by two different space telescopes challenge particle acceleration theory. A young and energetic pulsar powers the well-known Crab Nebula. Here, we describe two separate gamma-ray (photon energy greater than 100 mega–electron volts) flares from this source detected by the Large Area Telescope on board the Fermi Gamma-ray Space Telescope. The first flare occurred in February 2009 and lasted approximately 16 days. The second flare was detected in September 2010 and lasted approximately 4 days. During these outbursts, the gamma-ray flux from the nebula increased by factors of four and six, respectively. The brevity of the flares implies that the gamma rays were emitted via synchrotron radiation from peta–electron-volt (1015 electron volts) electrons in a region smaller than 1.4 × 10−2 parsecs. These are the highest-energy particles that can be associated with a discrete astronomical source, and they pose challenges to particle acceleration theory."
de3adadfb41cbdaad36ae821aa872c2825fd1b06,"We report the detection of high-energy γ-rays from the quiescent Sun with the Large Area Telescope on board the Fermi Gamma-Ray Space Telescope (Fermi) during the first 18 months of the mission. These observations correspond to the recent period of low solar activity when the emission induced by cosmic rays (CRs) is brightest. For the first time, the high statistical significance of the observations allows clear separation of the two components: the point-like emission from the solar disk due to CR cascades in the solar atmosphere and extended emission from the inverse Compton (IC) scattering of CR electrons on solar photons in the heliosphere. The observed integral flux (⩾100 MeV) from the solar disk is (4.6 ± 0.2[statistical error]+1.0 − 0.8[systematic error]) × 10−7 cm−2 s−1, which is ∼7 times higher than predicted by the “nominal” model of Seckel et al. In contrast, the observed integral flux (⩾100 MeV) of the extended emission from a region of 20° radius centered on the Sun, but excluding the disk itself, (6.8 ± 0.7[stat.]+0.5 − 0.4[syst.]) × 10−7 cm−2 s−1, along with the observed spectrum and the angular profile, is in good agreement with the theoretical predictions for the IC emission."
0ba50d3b6ffd74c59bbf999216e6669f6dfc56b0,"We report on gamma-ray observations in the off-pulse window of the Vela pulsar PSR B0833−45 using 11 months of survey data from the Fermi Large Area Telescope (LAT). This pulsar is located in the 8° diameter Vela supernova remnant, which contains several regions of non-thermal emission detected in the radio, X-ray, and gamma-ray bands. The gamma-ray emission detected by the LAT lies within one of these regions, the 2° × 3° area south of the pulsar known as Vela-X. The LAT flux is significantly spatially extended with a best-fit radius of 0.°88  ±   0.°12 for an assumed radially symmetric uniform disk. The 200 MeV to 20 GeV LAT spectrum of this source is well described by a power law with a spectral index of 2.41  ±   0.09  ±   0.15 and integral flux above 100 MeV of (4.73  ±   0.63  ±   1.32) × 10−7 cm−2 s−1. The first errors represent the statistical error on the fit parameters, while the second ones are the systematic uncertainties. Detailed morphological and spectral analyses give strong constraints on the energetics and magnetic field of the pulsar wind nebula system and favor a scenario with two distinct electron populations."
0ea310dc63da4ba700d48c6c93f03de5ffc7ec62,"We report on the observations of 14 dwarf spheroidal galaxies with the Fermi Gamma-Ray Space Telescope taken during the first 11 months of survey mode operations. The Fermi telescope, which is conducting an all-sky {gamma}-ray survey in the 20 MeV to >300 GeV energy range, provides a new opportunity to test particle dark matter models through the expected {gamma}-ray emission produced by pair annihilation of weakly interacting massive particles (WIMPs). Local Group dwarf spheroidal galaxies, the largest galactic substructures predicted by the cold dark matter scenario, are attractive targets for such indirect searches for dark matter because they are nearby and among the most extreme dark matter dominated environments. No significant {gamma}-ray emission was detected above 100 MeV from the candidate dwarf galaxies. We determine upper limits to the {gamma}-ray flux assuming both power-law spectra and representative spectra from WIMP annihilation. The resulting integral flux above 100 MeV is constrained to be at a level below around 10{sup -9} photons cm{sup -2}s{sup -1}. Using recent stellar kinematic data, the {gamma}-ray flux limits are combined with improved determinations of the dark matter density profile in 8 of the 14 candidate dwarfs to place limits on the pair annihilation cross-section ofWIMPs inmore » several widely studied extensions of the standard model, including its supersymmetric extension and other models that received recent attention. With the present data, we are able to rule out large parts of the parameter space where the thermal relic density is below the observed cosmological dark matter density and WIMPs (neutralinos here) are dominantly produced non-thermally, e.g. in models where supersymmetry breaking occurs via anomaly mediation. The {gamma}-ray limits presented here also constrain some WIMP models proposed to explain the Fermi and PAMELA e{sup +}e{sup -} data, including low-mass wino-like neutralinos and models with TeV masses pair-annihilating into muon-antimuon pairs.« less"
10495fdf9701095f327877ca19f66d059fdd9061,"We report on the multi-wavelength observations of PKS 1510-089 (a flat spectrum radio quasar (FSRQ) at z = 0.361) during its high activity period between 2008 September and 2009 June. During this 11 month period, the source was characterized by a complex variability at optical, UV, and γ-ray bands, on timescales down to 6–12 hr. The brightest γ-ray isotropic luminosity, recorded on 2009 March 26, was ≃2 × 1048 erg s−1. The spectrum in the Fermi Large Area Telescope energy range shows a mild curvature described well by a log-parabolic law, and can be understood as due to the Klein–Nishina effect. The γ-ray flux has a complex correlation with the other wavelengths. There is no correlation at all with the X-ray band, a weak correlation with the UV, and a significant correlation with the optical flux. The γ-ray flux seems to lead the optical one by about 13 days. From the UV photometry, we estimated a black hole mass of ≃5.4 × 108 M☉ and an accretion rate of ≃0.5 M☉ yr−1. Although the power in the thermal and non-thermal outputs is smaller compared to the very luminous and distant FSRQs, PKS 1510-089 exhibits a quite large Compton dominance and a prominent big blue bump (BBB) as observed in the most powerful γ-ray quasars. The BBB was still prominent during the historical maximum optical state in 2009 May, but the optical/UV spectral index was softer than in the quiescent state. This seems to indicate that the BBB was not completely dominated by the synchrotron emission during the highest optical state. We model the broadband spectrum assuming a leptonic scenario in which the inverse Compton emission is dominated by the scattering of soft photons produced externally to the jet. The resulting model-dependent jet energetic content is compatible with a scenario in which the jet is powered by the accretion disk, with a total efficiency within the Kerr black hole limit."
157bbbb4b6f428ca5957660ca7ca4c07a0d2b418,
15cb204e90546fb98212335e73c5b1622b3a36f2,"Cosmic rays (CRs) can be studied through the galaxy-wide gamma-ray emission that they generate when propagating in the interstellar medium. The comparison of the diffuse signals from different systems may inform us about the key parameters in CR acceleration and transport. We aim to determine and compare the properties of the CR-induced gamma-ray emission of several Local Group galaxies. We use 2 years of nearly continuous sky-survey observations obtained with the Large Area Telescope aboard the Fermi Gamma-ray Space Telescope to search for gamma-ray emission from M31 and M33. We compare the results with those for the Large Magellanic Cloud, the Small Magellanic Cloud, the Milky Way, and the starburst galaxies M82 and NGC253. We detect a gamma-ray signal at 5sigma significance in the energy range 200 MeV-20 GeV that is consistent with originating from M31. The integral photon flux above 100MeV amounts to 9.1 +/- 1.9 (stat) +/- 1.0 (sys) x10e-9 ph/cm2/s. We find no evidence for emission from M33 and derive an upper limit on the photon flux >100MeV of 5.1 x10e-9 ph/cm2/s (2sigma). Comparing these results to the properties of other Local Group galaxies, we find indications for a correlation between star formation rate and gamma-ray luminosity that also holds for the starburst galaxies. The gamma-ray luminosity of M31 is about half that of the Milky Way, which implies that the ratio between the average CR densities in M31 and the Milky Way amounts to 0.35 +/- 0.25. The observed correlation between gamma-ray luminosity and star formation rate suggests that the flux of M33 is not far below the current upper limit from the LAT observations."
17f926fc317eb2cd2d1591fdc5569057e24844a0,"We report the detection of pulsed γ-rays for PSRs J0631+1036, J0659+1414, J0742–2822, J1420–6048, J1509–5850, and J1718–3825 using the Large Area Telescope on board the Fermi Gamma-ray Space Telescope (formerly known as GLAST). Although these six pulsars are diverse in terms of their spin parameters, they share an important feature: their γ-ray light curves are (at least given the current count statistics) single peaked. For two pulsars, there are hints for a double-peaked structure in the light curves. The shapes of the observed light curves of this group of pulsars are discussed in the light of models for which the emission originates from high up in the magnetosphere. The observed phases of the γ-ray light curves are, in general, consistent with those predicted by high-altitude models, although we speculate that the γ-ray emission of PSR J0659+1414, possibly featuring the softest spectrum of all Fermi pulsars coupled with a very low efficiency, arises from relatively low down in the magnetosphere. High-quality radio polarization data are available showing that all but one have a high degree of linear polarization. This allows us to place some constraints on the viewing geometry and aids the comparison of the γ-ray light curves with high-energy beam models."
2bdad2c3bba6da349857681eed6d1dd819b7dd2e,"We present a detailed analysis of the GeV gamma-ray emission toward the supernova remnant (SNR) G8.7−0.1 with the Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope. An investigation of the relationship between G8.7−0.1 and the TeV unidentified source HESS J1804−216 provides us with an important clue on diffusion process of cosmic rays if particle acceleration operates in the SNR. The GeV gamma-ray emission is extended with most of the emission in positional coincidence with the SNR G8.7−0.1 and a lesser part located outside the western boundary of G8.7−0.1. The region of the gamma-ray emission overlaps spatially connected molecular clouds, implying a physical connection for the gamma-ray structure. The total gamma-ray spectrum measured with LAT from 200 MeV–100 GeV can be described by a broken power-law function with a break of 2.4 ± 0.6 (stat) ± 1.2 (sys) GeV, and photon indices of 2.10 ± 0.06 (stat) ± 0.10 (sys) below the break and 2.70 ± 0.12 (stat) ± 0.14 (sys) above the break. Given the spatial association among the gamma rays, the radio emission of G8.7−0.1, and the molecular clouds, the decay of π0s produced by particles accelerated in the SNR and hitting the molecular clouds naturally explains the GeV gamma-ray spectrum. We also find that the GeV morphology is not well represented by the TeV emission from HESS J1804−216 and that the spectrum in the GeV band is not consistent with the extrapolation of the TeV gamma-ray spectrum. The spectral index of the TeV emission is consistent with the particle spectral index predicted by a theory that assumes energy-dependent diffusion of particles accelerated in an SNR. We discuss the possibility that the TeV spectrum originates from the interaction of particles accelerated in G8.7−0.1 with molecular clouds, and we constrain the diffusion coefficient of the particles."
31c084d652acaeff2d4a4163271bd6375b6e16f1,"We report on the first Fermi Large Area Telescope (LAT) measurements of the so-called ""extragalactic"" diffuse gamma-ray emission (EGB). This component of the diffuse gamma-ray emission is generally considered to have an isotropic or nearly isotropic distribution on the sky with diverse contributions discussed in the literature. The derivation of the EGB is based on detailed modeling of the bright foreground diffuse Galactic gamma-ray emission, the detected LAT sources, and the solar gamma-ray emission. We find the spectrum of the EGB is consistent with a power law with a differential spectral index gamma = 2.41 +/- 0.05 and intensity I(>100 MeV) = (1.03 +/- 0.17) x 10(-5) cm(-2) s(-1) sr(-1), where the error is systematics dominated. Our EGB spectrum is featureless, less intense, and softer than that derived from EGRET data."
321c4332bf3a5d61346a009282e0d3e537656d92,"Supernova Remnant Observations The sources of the cosmic rays that bombard Earth have been mysterious, but more recent explanations invoke high-energy acceleration of protons by the remnants of stellar explosions. Using the Fermi Large Area Telescope, Abdo et al. (p. 1103, published online 5 January) obtained an image of the supernova remnant W44, which shows associated gamma-ray emissions in the order of gigaelectronvolts, conforming with models indicating local proton and nuclei acceleration. Satellite observations suggest that protons are accelerated in the shell of a supernova remnant. Recent observations of supernova remnants (SNRs) hint that they accelerate cosmic rays to energies close to ~1015 electron volts. However, the nature of the particles that produce the emission remains ambiguous. We report observations of SNR W44 with the Fermi Large Area Telescope at energies between 2 × 108 electron volts and 3 ×1011 electron volts. The detection of a source with a morphology corresponding to the SNR shell implies that the emission is produced by particles accelerated there. The gamma-ray spectrum is well modeled with emission from protons and nuclei. Its steepening above ~109 electron volts provides a probe with which to study how particle acceleration responds to environmental effects such as shock propagation in dense clouds and how accelerated particles are released into interstellar space."
381e0471e85eb2c8636a74ccd4f55f46597a2d72,"We report on the observations of 14 dwarf spheroidal galaxies (dSphs) with the Fermi Gamma-Ray Space Telescope taken during the first 11 months of survey mode operations. The Fermi telescope, which is conducting an all-sky γ-ray survey in the 20 MeV to >300 GeV energy range, provides a new opportunity to test particle dark matter models through the expected γ-ray emission produced by pair annihilation of weakly interacting massive particles (WIMPs). Local Group dSphs, the largest galactic substructures predicted by the cold dark matter scenario, are attractive targets for such indirect searches for dark matter because they are nearby and among the most extreme dark matter dominated environments. No significant γ-ray emission was detected above 100 MeV from the candidate dwarf galaxies. We determine upper limits to the γ-ray flux assuming both power-law spectra and representative spectra from WIMP annihilation. The resulting integral flux above 100 MeV is constrained to be at a level below around 10−9 photons cm−2 s−1. Using recent stellar kinematic data, the γ-ray flux limits are combined with improved determinations of the dark matter density profile in eight of the 14 candidate dwarfs to place limits on the pair-annihilation cross section of WIMPs in several widely studied extensions of the standard model, including its supersymmetric extension and other models that received recent attention. With the present data, we are able to rule out large parts of the parameter space where the thermal relic density is below the observed cosmological dark matter density and WIMPs (neutralinos here) are dominantly produced non-thermally, e.g., in models where supersymmetry breaking occurs via anomaly mediation. The γ-ray limits presented here also constrain some WIMP models proposed to explain the Fermi and PAMELA e+e− data, including low-mass wino-like neutralinos and models with TeV masses pair annihilating into muon–antimuon pairs."
3e5e261dcaba3654aae3f46dbb0d73a05053cdd8,"We report on the detailed analysis of the high-energy extended emission from the short gamma-ray burst (GRB) 081024B detected by the Fermi Gamma-ray Space Telescope. Historically, this represents the first clear detection of temporal extended emission from a short GRB. The light curve observed by the Fermi Gamma-ray Burst Monitor lasts approximately 0.8 s whereas the emission in the Fermi Large Area Telescope lasts for about 3 s. Evidence of longer lasting high-energy emission associated with long bursts has been already reported by previous experiments. Our observations, together with the earlier reported study of the bright short GRB 090510, indicate similarities in the high-energy emission of short and long GRBs and open the path to new interpretations."
41a3491958df9056441aa9f1d52231e633ae73d2,"Analysis is presented for 15 months of data taken with the Large Area Telescope (LAT) on the Fermi Gamma-ray Space Telescope for 11 non-blazar active galactic nuclei (AGNs), including seven FRI radio galaxies and four FRII radio sources consisting of two FRII radio galaxies and two steep spectrum radio quasars. The broad line FRI radio galaxy 3C 120 is reported here as a γ-ray source for the first time. The analysis is based on directional associations of LAT sources with radio sources in the 3CR, 3CRR, and MS4 (collectively referred to as 3C-MS) catalogs. Seven of the eleven LAT sources associated with 3C-MS radio sources have spectral indices larger than 2.3 and, except for the FRI radio galaxy NGC 1275 that shows possible spectral curvature, are well described by a power law. No evidence for time variability is found for any sources other than NGC 1275. The γ-ray luminosities of FRI radio galaxies are significantly smaller than those of the BL Lac objects detected by the LAT, whereas the γ-ray luminosities of the FRII sources are quite similar to those of FSRQs, which could reflect different beaming factors for the γ-ray emission. A core dominance (CD) study of the 3CRR sample indicates that sources closer to the jet axis are preferentially detected with the Fermi LAT, insofar as the γ-ray-detected misaligned AGNs have larger CD at a given average radio flux. The results are discussed in view of the AGN unification scenario."
61ca3285c7fe3d24a12ed71c64447fbe425bbd6b,"The BL Lacertae object 3C 66A was detected in a flaring state by the Fermi Large Area Telescope (LAT) and VERITAS in 2008 October. In addition to these gamma-ray observations, F-GAMMA, GASP-WEBT, PAIRITEL, MDM, ATOM, Swift, and Chandra provided radio to X-ray coverage. The available light curves show variability and, in particular, correlated flares are observed in the optical and Fermi-LAT gamma-ray band. The resulting spectral energy distribution can be well fitted using standard leptonic models with and without an external radiation field for inverse Compton scattering. It is found, however, that only the model with an external radiation field can accommodate the intra-night variability observed at optical wavelengths."
63db1534f9522d213c99d4b8b5344ebd0878f877,"We report on the first year of Fermi γ-ray observations of pulsed high-energy emission from the old PSR J2043 + 2740. The study of the γ-ray efficiency of such old pulsars gives us an insight into the evolution of pulsars’ ability to emit in γ rays as they age. The γ-ray light curve of this pulsar above 0.1 GeV is clearly defined by two sharp peaks, 0.353 ± 0.035 periods apart. We have combined the γ-ray profile characteristics of PSR J2043 + 2740 with the geometrical properties of the pulsar's radio emission, derived from radio-polarization data, and constrained the pulsar-beam geometry in the framework of a two-pole caustic (TPC) and an outer gap (OG) model. The ranges of magnetic inclination and viewing angle were determined to be {α, ζ} ∼ {52°–57°, 61°–68°} for the TPC model, and {α, ζ} ∼ {62°–73°, 74°–81°} and {α, ζ} ∼ {72°–83°, 60°–75°} for the OG model. Based on this geometry, we assess possible birth locations for this pulsar and derive a likely proper motion, sufficiently high to be measurable with VLBI. At a characteristic age of 1.2 Myr, PSR J2043 + 2740 is the third oldest of all discovered, non-recycled, γ-ray pulsars: it is twice as old as the next oldest, PSR J0357 + 32, and younger only than the recently discovered PSR J1836 + 5925 and PSR J2055 + 25, both of which are at least five and ten times less energetic, respectively."
6978761cf092eb9624293376e3eb8beb2224fc34,"The Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope detected a γ-ray source that is spatially consistent with the location of Eta Carinae. This source has been persistently bright since the beginning of the LAT survey observations (from 2008 August to 2009 July, the time interval considered here). The γ-ray signal is detected significantly throughout the LAT energy band (i.e., up to ∼100 GeV). The 0.1–100 GeV energy spectrum is well represented by a combination of a cutoff power-law model (<10 GeV) and a hard power-law component (>10 GeV). The total flux (>100 MeV) is 3.7+0.3−0.1 × 10−7 photons s−1 cm−2, with additional systematic uncertainties of 10%, and consistent with the average flux measured by AGILE. The light curve obtained by Fermi is consistent with steady emission. Our observations do not confirm the presence of a γ-ray flare in 2008 October, as reported by Tavani et al., although we cannot exclude that a flare lasting only a few hours escaped detection by the Fermi LAT. We also do not find any evidence for γ-ray variability that correlates with the large X-ray variability of Eta Carinae observed during 2008 December and 2009 January. We are thus not able to establish an unambiguous identification of the LAT source with Eta Carinae."
6be033b65a6d5b0384ae318ad35f6a203922c2c6,"Nearby clusters and groups of galaxies are potentially bright sources of high-energy gamma-ray emission resulting from the pair-annihilation of dark matter particles. However, no significant gamma-ray emission has been detected so far from clusters in the first 11 months of observations with the Fermi Large Area Telescope. We interpret this non-detection in terms of constraints on dark matter particle properties. In particular for leptonic annihilation final states and particle masses greater than ~200 GeV, gamma-ray emission from inverse Compton scattering of CMB photons is expected to dominate the dark matter annihilation signal from clusters, and our gamma-ray limits exclude large regions of the parameter space that would give a good fit to the recent anomalous Pamela and Fermi-LAT electron-positron measurements. We also present constraints on the annihilation of more standard dark matter candidates, such as the lightest neutralino of supersymmetric models. The constraints are particularly strong when including the fact that clusters are known to contain substructure at least on galaxy scales, increasing the expected gamma-ray flux by a factor of ~5 over a smooth-halo assumption. We also explore the effect of uncertainties in cluster dark matter density profiles, finding a systematic uncertainty in the constraints of roughly a factor of two, but similar overall conclusions. In this work, we focus on deriving limits on dark matter models; a more general consideration of the Fermi-LAT data on clusters and clusters as gamma-ray sources is forthcoming."
75ce3aaf803413c7d588b10e897c131b69c31821,We report the detection of high-energy gamma-ray emission from the young and energetic pulsar PSR B1509-58 and its pulsar wind nebula (PWN) in the composite supernova remnant G320.4-1.2 (aka MSH 15 ...
75cf9ea4e414d82e426aeb4d1c68a160b0a53be1,The Large Area Telescope on board the Fermi satellite (Fermi LAT) detected more than 1.6 x 10(6) cosmic-ray electrons/positrons with energies above 60 GeV during its first year of operation. The ar ...
867647c14d49dc184be7f33bea096233bac76445,"The gamma-ray energy spectra of bright blazars of the LAT Bright AGN Sample (LBAS) are investigated using Fermi-LAT data. Spectral properties (hardness, curvature, and variability) established using a data set accumulated over 6 months of operation are presented and discussed for different blazar classes and subclasses: flat spectrum radio quasars (FSRQs), low-synchrotron peaked BLLacs (LSP-BLLacs), intermediate-synchrotron peaked BLLacs (ISP-BLLacs), and high-synchrotron peaked BLLacs (HSP-BLLacs). The distribution of photon index (Γ, obtained from a power-law fit above 100 MeV) is found to correlate strongly with blazar subclass. The change in spectral index from that averaged over the 6 months observing period is < 0.2–0.3 when the flux varies by about an order of magnitude, with a tendency toward harder spectra when the flux is brighter for FSRQs and LSP-BLLacs. A strong departure from a single power-law spectrum appears to be a common feature for FSRQs. This feature is also present for some high-luminosity LSP-BLLacs, and a small number of ISP-BLLacs. It is absent in all LBAS HSP-BLLacs. For 3C 454.3 and AO 0235+164, the two brightest FSRQ source and LSP-BLLac source, respectively, a broken power law (BPL) gives the most acceptable of power law, BPL, and curved forms. The consequences of these findings are discussed."
8a4530f96a9c18e487c0e7516dff0ef8e404b61a,"We report the observations of PG 1553+113 during the first ∼ 200 days of Fermi Gamma-ray Space Telescope science operations, from 2008 August 4 to 2009 February 22 (MJD 54682.7–54884.2). This is the first detailed study of PG 1553+113 in the GeV gamma-ray regime and it allows us to fill a gap of three decades in energy in its spectral energy distribution (SED). We find PG 1553+113 to be a steady source with a hard spectrum that is best fit by a simple power law in the Fermi energy band. We combine the Fermi data with archival radio, optical, X-ray, and very high energy (VHE) gamma-ray data to model its broadband SED and find that a simple, one-zone synchrotron self-Compton model provides a reasonable fit. PG 1553+113 has the softest VHE spectrum of all sources detected in that regime and, out of those with significant detections across the Fermi energy bandpass so far, the hardest spectrum in that energy regime. Thus, it has the largest spectral break of any gamma-ray source studied to date, which could be due to the absorption of the intrinsic gamma-ray spectrum by the extragalactic background light (EBL). Assuming this to be the case, we selected a model with a low level of EBL and used it to absorb the power-law spectrum from PG 1553+113 measured with Fermi (200 MeV–157 GeV) to find the redshift, which gave the best fit to the measured VHE data (90 GeV–1.1 TeV) for this parameterization of the EBL. We show that this redshift can be considered an upper limit on the distance to PG 1553+113."
8d7e0bf25bfd13bc482712941d92bf3f40519aa9,"We present a catalog of high-energy gamma-ray sources detected by the Large Area Telescope (LAT), the primary science instrument on the Fermi Gamma-ray Space Telescope (Fermi), during the first 11 months of the science phase of the mission, which began on 2008 August 4. The First Fermi-LAT catalog (1FGL) contains 1451 sources detected and characterized in the 100 MeV to 100 GeV range. Source detection was based on the average flux over the 11 month period, and the threshold likelihood Test Statistic is 25, corresponding to a significance of just over 4σ. The 1FGL catalog includes source location regions, defined in terms of elliptical fits to the 95% confidence regions and power-law spectral fits as well as flux measurements in five energy bands for each source. In addition, monthly light curves are provided. Using a protocol defined before launch we have tested for several populations of gamma-ray sources among the sources in the catalog. For individual LAT-detected sources we provide firm identifications or plausible associations with sources in other astronomical catalogs. Identifications are based on correlated variability with counterparts at other wavelengths, or on spin or orbital periodicity. For the catalogs and association criteria that we have selected, 630 of the sources are unassociated. Care was taken to characterize the sensitivity of the results to the model of interstellar diffuse gamma-ray emission used to model the bright foreground, with the result that 161 sources at low Galactic latitudes and toward bright local interstellar clouds are flagged as having properties that are strongly dependent on the model or as potentially being due to incorrectly modeled structure in the Galactic diffuse emission."
909b47563e778d408ee688e5d67e4cd9bc4bb8ab,"Dark matter (DM) particle annihilation or decay can produce monochromatic gamma rays readily distinguishable from astrophysical sources. gamma-ray line limits from 30 to 200 GeV obtained from 11 months of Fermi Large Area Space Telescope data from 20-300 GeV are presented using a selection based on requirements for a gamma-ray line analysis, and integrated over most of the sky. We obtain gamma-ray line flux upper limits in the range 0.6-4.5x10{-9} cm{-2} s{-1}, and give corresponding DM annihilation cross-section and decay lifetime limits. Theoretical implications are briefly discussed."
912b856134b9a84bdb0914a93ad73ee1de9c03a8,"The discovery of the γ-ray pulsar PSR J1836+5925, powering the formerly unidentified EGRET source 3EG J1835+5918, was one of the early accomplishments of the Fermi Large Area Telescope (LAT). Sitting 25° off the Galactic plane, PSR J1836+5925 is a 173 ms pulsar with a characteristic age of 1.8 million years, a spindown luminosity of 1.1 × 1034 erg s−1, and a large off-peak (OP) emission component, making it quite unusual among the known γ-ray pulsar population. We present an analysis of one year of LAT data, including an updated timing solution, detailed spectral results, and a long-term light curve showing no indication of variability. No evidence for a surrounding pulsar wind nebula is seen and the spectral characteristics of the OP emission indicate it is likely magnetospheric. Analysis of recent XMM-Newton observations of the X-ray counterpart yields a detailed characterization of its spectrum, which, like Geminga, is consistent with that of a neutron star showing evidence for both magnetospheric and thermal emission."
91703313b60a8f9b095408054487c25737e9ef09,"We present the light curves and spectral data of two exceptionally luminous gamma-ray outbursts observed by the Large Area Telescope experiment on board the Fermi Gamma-ray Space Telescope from 3C 273 in 2009 September. During these flares, having a duration of a few days, the source reached its highest γ-ray flux ever measured. This allowed us to study, in some details, their spectral and temporal structures. The rise and the decay are asymmetric on timescales of 6 hr, and the spectral index was significantly harder during the flares than during the preceding 11 months. We also found that short, very intense flares put out the same time-integrated energy as long, less intense flares like that observed in 2009 August."
ac88dc77899f68e8fc20577cae47cb10932614ed,"We present the analysis of the interstellar γ-ray emission measured by the Fermi Large Area Telescope toward a region in the second Galactic quadrant at 100° ⩽ l ⩽ 145° and −15° ⩽ b ⩽ +30°. This region encompasses the prominent Gould Belt clouds of Cassiopeia, Cepheus, and the Polaris flare, as well as atomic and molecular complexes at larger distances, like that associated with NGC 7538 in the Perseus arm. The good kinematic separation in velocity between the local, Perseus, and outer arms, and the presence of massive complexes in each of them, make this region well suited to probe cosmic rays (CRs) and the interstellar medium beyond the solar circle. The γ-ray emissivity spectrum of the gas in the Gould Belt is consistent with expectations based on the locally measured CR spectra. The γ-ray emissivity decreases from the Gould Belt to the Perseus arm, but the measured gradient is flatter than expectations for CR sources peaking in the inner Galaxy as suggested by pulsars. The XCO = N(H2)/WCO conversion factor is found to increase from (0.87 ± 0.05) × 1020 cm−2 (K km s−1)−1 in the Gould Belt to (1.9 ± 0.2) × 1020 cm−2 (K km s−1)−1 in the Perseus arm. We derive masses for the molecular clouds under study. Dark gas, not properly traced by radio and microwave surveys, is detected in the Gould Belt through a correlated excess of dust and γ-ray emission: its mass amounts to ∼50% of the CO-traced mass."
b1ee95b914dec695773b4549c78739c20efc7ee1,"We present detailed observations of the bright short–hard gamma-ray burst GRB 090510 made with the Gamma-ray Burst Monitor (GBM) and Large Area Telescope (LAT) on board the Fermi observatory. GRB 090510 is the first burst detected by the LAT that shows strong evidence for a deviation from a Band spectral fitting function during the prompt emission phase. The time-integrated spectrum is fit by the sum of a Band function with Epeak = 3.9 ± 0.3 MeV, which is the highest yet measured, and a hard power-law component with photon index −1.62 ± 0.03 that dominates the emission below ≈20 keV and above ≈100 MeV. The onset of the high-energy spectral component appears to be delayed by ∼0.1 s with respect to the onset of a component well fit with a single Band function. A faint GBM pulse and a LAT photon are detected 0.5 s before the main pulse. During the prompt phase, the LAT detected a photon with energy 30.5+5.8−2.6 GeV, the highest ever measured from a short GRB. Observation of this photon sets a minimum bulk outflow Lorentz factor, Γ≳ 1200, using simple γγ opacity arguments for this GRB at redshift z = 0.903 and a variability timescale on the order of tens of ms for the ≈100 keV–few MeV flux. Stricter high confidence estimates imply Γ ≳ 1000 and still require that the outflows powering short GRBs are at least as highly relativistic as those of long-duration GRBs. Implications of the temporal behavior and power-law shape of the additional component on synchrotron/synchrotron self-Compton, external-shock synchrotron, and hadronic models are considered."
b94f22bb92369b69cb0c09d1e76b853e46dff5ca,"The detection of diffuse radio emission associated with clusters of galaxies indicates populations of relativistic leptons infusing the intracluster medium (ICM). Those electrons and positrons are either injected into and accelerated directly in the ICM, or produced as secondary pairs by cosmic-ray ions scattering on ambient protons. Radiation mechanisms involving the energetic leptons together with the decay of neutral pions produced by hadronic interactions have the potential to produce abundant GeV photons. Here, we report on the search for GeV emission from clusters of galaxies using data collected by the Large Area Telescope on the Fermi Gamma-ray Space Telescope from 2008 August to 2010 February. Thirty-three galaxy clusters have been selected according to their proximity and high mass, X-ray flux and temperature, and indications of non-thermal activity for this study. We report upper limits on the photon flux in the range 0.2–100 GeV toward a sample of observed clusters (typical values (1–5) ×10−9 photon cm−2 s−1) considering both point-like and spatially resolved models for the high-energy emission and discuss how these results constrain the characteristics of energetic leptons and hadrons, and magnetic fields in the ICM. The volume-averaged relativistic-hadron-to-thermal energy density ratio is found to be <5%–10% in several clusters."
ba9f82e9690174ef2dc7eca4b9fa91b32156fa76,"The high sensitivity of the Fermi-LAT (Large Area Telescope) offers the first opportunity to study faint and extended GeV sources such as pulsar wind nebulae (PWNe). After one year of observation the LAT detected and identified three PWNe: the Crab Nebula, Vela-X, and the PWN inside MSH 15–52. In the meantime, the list of LAT detected pulsars increased steadily. These pulsars are characterized by high energy loss rates () from ∼3 × 1033 erg s−1 to 5 × 1038 erg s−1 and are therefore likely to power a PWN. This paper summarizes the search for PWNe in the off-pulse windows of 54 LAT-detected pulsars using 16 months of survey observations. Ten sources show significant emission, seven of these likely being of magnetospheric origin. The detection of significant emission in the off-pulse interval offers new constraints on the γ-ray emitting regions in pulsar magnetospheres. The three other sources with significant emission are the Crab Nebula, Vela-X, and a new PWN candidate associated with the LAT pulsar PSR J1023−5746, coincident with the TeV source HESS J1023−575. We further explore the association between the HESS and the Fermi source by modeling its spectral energy distribution. Flux upper limits derived for the 44 remaining sources are used to provide new constraints on famous PWNe that have been detected at keV and/or TeV energies."
c24b24e99488f2465e426929110e35c02c5ba6e3,"We present an analysis of the gamma-ray data obtained with the Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope in the direction of SNR W49B (G43.3−0.2). A bright unresolved gamma-ray source detected at a significance of 38σ is found to coincide with SNR W49B. The energy spectrum in the 0.2–200 GeV range gradually steepens toward high energies. The luminosity is estimated to be 1.5 × 1036 (D/8 kpc)2 erg s−1 in this energy range. There is no indication that the gamma-ray emission comes from a pulsar. Assuming that the supernova remnant (SNR) shell is the site of gamma-ray production, the observed spectrum can be explained either by the decay of neutral π mesons produced through the proton–proton collisions or by electron bremsstrahlung. The calculated energy density of relativistic particles responsible for the LAT flux is estimated to be remarkably large, Ue,p>104 eV cm−3, for either gamma-ray production mechanism."
c66391390750eb70c12613190bf625fdcaa35702,"The flux of gamma rays with energies >100MeV is dominated by diffuse emission from CRs illuminating the ISM of our Galaxy through the processes of Bremsstrahlung, pion production and decay, and inverse-Compton scattering. The study of this diffuse emission provides insight into the origin and transport of CRs. We searched for gamma-ray emission from the SMC in order to derive constraints on the CR population and transport in an external system with properties different from the Milky Way. We analysed the first 17 months of continuous all-sky observations by the Large Area Telescope of the Fermi mission to determine the spatial distribution, flux and spectrum of the gamma-ray emission from the SMC. We also used past radio synchrotron observations of the SMC to study the population of CR electrons specifically. We obtained the first detection of the SMC in high-energy gamma rays, with an integrated >100MeV flux of (3.7 +/-0.7) x10e-8 ph/cm2/s, with additional systematic uncertainty of <16%. The emission is steady and from an extended source ~3{\deg} in size. It is not clearly correlated with the distribution of massive stars or neutral gas, nor with known pulsars or SNRs, but a certain correlation with supergiant shells is observed. The observed flux implies an upper limit on the average CR nuclei density in the SMC of ~15% of the value measured locally in the Milky Way. The population of high-energy pulsars of the SMC may account for a substantial fraction of the gamma-ray flux, which would make the inferred CR nuclei density even lower. The average density of CR electrons derived from radio synchrotron observations is consistent with the same reduction factor but the uncertainties are large. From our current knowledge of the SMC, such a low CR density does not seem to be due to a lower rate of CR injection and rather indicates a smaller CR confinement volume characteristic size."
c766d62ab69fdbf7bf189b0b27369b8455290e6a,We present the results of our analysis of cosmic-ray electrons using about 8 x 10(6) electron candidates detected in the first 12 months on-orbit by the Fermi Large Area Telescope. This work extend ...
cabe71ceeec24817912b082e5917d348e549cfe5,"The first published Fermi large area telescope (Fermi-LAT) measurement of the isotropic diffuse gamma-ray emission is in good agreement with a single power law, and is not showing any signature of a dominant contribution from dark matter sources in the energy range from 20 to 100 GeV. We use the absolute size and spectral shape of this measured flux to derive cross section limits on three types of generic dark matter candidates: annihilating into quarks, charged leptons and monochromatic photons. Predicted gamma-ray fluxes from annihilating dark matter are strongly affected by the underlying distribution of dark matter, and by using different available results of matter structure formation we assess these uncertainties. We also quantify how the dark matter constraints depend on the assumed conventional backgrounds and on the Universe's transparency to high-energy gamma-rays. In reasonable background and dark matter structure scenarios (but not in all scenarios we consider) it is possible to exclude models proposed to explain the excess of electrons and positrons measured by the Fermi-LAT and PAMELA experiments. Derived limits also start to probe cross sections expected from thermally produced relics (e.g. in minimal supersymmetry models) annihilating predominantly into quarks. For the monochromatic gamma-ray signature, the current measurement constrains only dark matter scenarios with very strong signals."
cd6d51ace1749598c6f44b83296d610ddbfea5be,"This is the first of a series of papers aimed at characterizing the populations detected in the high-latitude sky of the Fermi-LAT survey. In this work, we focus on the intrinsic spectral and flux properties of the source sample. We show that when selection effects are properly taken into account, Fermi sources are on average steeper than previously found (e.g., in the bright source list) with an average photon index of 2.40 ± 0.02 over the entire 0.1–100 GeV energy band. We confirm that flat spectrum radio quasars have steeper spectra than BL Lacertae objects with an average index of 2.48 ± 0.02 versus 2.18 ± 0.02. Using several methods, we build the deepest source count distribution at GeV energies, deriving that the intrinsic source (i.e., blazar) surface density at F100 ⩾ 10−9 ph cm−2 s−1 is 0.12+0.03−0.02 deg−2. The integration of the source count distribution yields that point sources contribute 16(±1.8)% (±7% systematic uncertainty) of the GeV isotropic diffuse background. At the fluxes currently reached by LAT, we can rule out the hypothesis that pointlike sources (i.e., blazars) produce a larger fraction of the diffuse emission."
df9c0d9f92e9a8728ba9b32ce3a95cb7e620c9d4,"The Fermi Gamma-ray Space Telescope has detected the gamma-ray glow emanating from the giant radio lobes of the radio galaxy Centaurus A. The resolved gamma-ray image shows the lobes clearly separated from the central active source. In contrast to all other active galaxies detected so far in high-energy gamma-rays, the lobe flux constitutes a considerable portion (greater than one-half) of the total source emission. The gamma-ray emission from the lobes is interpreted as inverse Compton-scattered relic radiation from the cosmic microwave background, with additional contribution at higher energies from the infrared-to-optical extragalactic background light. These measurements provide gamma-ray constraints on the magnetic field and particle energy content in radio galaxy lobes, as well as a promising method to probe the cosmic relic photon fields."
e2265ca6467ca7d0387c5492aa7facdf655c14d2,"Millisecond pulsars (MSPs) have been firmly established as a class of γ-ray emitters via the detection of pulsations above 0.1 GeV from eight MSPs by the Fermi Large Area Telescope (LAT). Using 13 months of LAT data, significant γ-ray pulsations at the radio period have been detected from the MSP PSR J0034−0534, making it the ninth clear MSP detection by the LAT. The γ-ray light curve shows two peaks separated by 0.274 ± 0.015 in phase which are very nearly aligned with the radio peaks, a phenomenon seen only in the Crab pulsar until now. The ⩾0.1 GeV spectrum of this pulsar is well fit by an exponentially cutoff power law with a cutoff energy of 1.8 ± 0.6 ± 0.1 GeV and a photon index of 1.5 ± 0.2 ± 0.1, first errors are statistical and second are systematic. The near-alignment of the radio and γ-ray peaks strongly suggests that the radio and γ-ray emission regions are co-located and both are the result of caustic formation."
e5c87bb3cd7a6daf76841a77912969e3a365153f,
e99a86fbbee79dc85e522de1a0ee7adc411b9da5,"The Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope discovered a rapid (∼5 days duration), high-energy (E > 100 MeV) gamma-ray outburst from a source identified with the blazar PKS 1502+106 (OR 103, S3 1502+10, z = 1.839) starting on 2008 August 5 (∼23 UTC, MJD 54683.95), and followed by bright and variable flux over the next few months. Results on the gamma-ray localization and identification, as well as spectral and temporal behavior during the first months of the Fermi all-sky survey, are reported here in conjunction with a multiwaveband characterization as a result of one of the first Fermi multifrequency campaigns. The campaign included a Swift ToO (followed up by a 16 day observation on August 7–22, MJD 54685–54700), VLBA (within the MOJAVE program), Owens Valley Radio Observatory (OVRO) 40 m, Effelsberg-100 m, Metsähovi-14 m, RATAN-600, and Kanata–Hiroshima radio/optical observations. Results from the analysis of archival observations by INTEGRAL, XMM-Newton, and Spitzer space telescopes are reported for a more complete picture of this new gamma-ray blazar. PKS 1502+106 is a sub-GeV peaked, powerful flat spectrum radio quasar (luminosity at E > 100 MeV, Lγ, is about 1.1 × 1049 erg s−1, and black hole mass likely close to 109 M☉), exhibiting marked gamma-ray bolometric dominance, in particular during the asymmetric outburst (Lγ/Lopt ∼ 100, and 5 day averaged flux FE > 100 MeV = 2.91 ± 1.4 × 10−6 ph cm−2 s−1), which was characterized by a factor greater than 3 of flux increase in less than 12 hr. The outburst was observed simultaneously from optical to X-ray bands (F0.3 − 10 keV = 2.18+0.15−0.12 × 10−12 erg cm−2 s−1, and hard photon index ∼1.5, similar to past values) with a flux increase of less than 1 order of magnitude with respect to past observations, and was likely controlled by Comptonization of external-jet photons produced in the broad-line region (BLR) in the gamma-ray band. No evidence of a possible blue bump signature was observed in the optical–UV continuum spectrum, while some hints for a possible 4 day time lag with respect to the gamma-ray flare were found. Nonetheless, the properties of PKS 1502+106 and the strict optical/UV, X-, and gamma-ray cross-correlations suggest the contribution of the synchrotron self-Compton (SSC), in-jet, process should dominate from radio to X-rays. This mechanism may also be responsible for the consistent gamma-ray variability observed by the LAT on longer timescales, after the ignition of activity at these energies provided by the BLR-dissipated outburst. Modulations and subsequent minor, rapid flare events were detected, with a general fluctuation mode between pink-noise and a random-walk. The averaged gamma-ray spectrum showed a deviation from a simple power law, and can be described by a log-parabola curved model peaking around 0.4–0.5 GeV. The maximum energy of photons detected from the source in the first four months of LAT observations was 15.8 GeV, with no significant consequences on extragalactic background light predictions. A possible radio counterpart of the gamma-ray outburst can be assumed only if a delay of more than three months is considered on the basis of opacity effects at cm and longer wavelengths. The rotation of the electric vector position angle observed by VLBA from 2007 to 2008 could represent a slow field ordering and alignment with respect to the jet axis, likely a precursor feature of the ejection of a superluminal radio knot and the high-energy outburst. This observing campaign provides more insight into the connection between MeV–GeV flares and the moving, polarized structures observed by the VLBI."
f144dcac2c2058f5fc88bba535e4ac5cd3c6e854,"The high-frequency-peaked BL Lacertae object RGB J0710+591 was observed in the very high-energy (VHE; E > 100 GeV) wave band by the VERITAS array of atmospheric Cherenkov telescopes. The observations, taken between 2008 December and 2009 March and totaling 22.1 hr, yield the discovery of VHE gamma rays from the source. RGB J0710+591 is detected at a statistical significance of 5.5 standard deviations (5.5σ) above the background, corresponding to an integral flux of (3.9 ± 0.8) × 10−12 cm−2 s−1 (3% of the Crab Nebula's flux) above 300 GeV. The observed spectrum can be fit by a power law from 0.31 to 4.6 TeV with a photon spectral index of 2.69 ± 0.26stat ± 0.20sys. These data are complemented by contemporaneous multiwavelength data from the Fermi Large Area Telescope, the Swift X-ray Telescope, the Swift Ultra-Violet and Optical Telescope, and the Michigan–Dartmouth–MIT observatory. Modeling the broadband spectral energy distribution (SED) with an equilibrium synchrotron self-Compton model yields a good statistical fit to the data. The addition of an external-Compton component to the model does not improve the fit nor brings the system closer to equipartition. The combined Fermi and VERITAS data constrain the properties of the high-energy emission component of the source over 4 orders of magnitude and give measurements of the rising and falling sections of the SED."
ffca114128fb35a70f22cd01e2fb8530dd383e07,"Novae are thermonuclear explosions on a white dwarf surface fueled by mass accreted from a companion star. Current physical models posit that shocked expanding gas from the nova shell can produce x-ray emission, but emission at higher energies has not been widely expected. Here, we report the Fermi Large Area Telescope detection of variable gamma-ray emission (0.1 to 10 billion electron volts) from the recently detected optical nova of the symbiotic star V407 Cygni. We propose that the material of the nova shell interacts with the dense ambient medium of the red giant primary and that particles can be accelerated effectively to produce pi(0) decay gamma-rays from proton-proton interactions. Emission involving inverse Compton scattering of the red giant radiation is also considered and is not ruled out."
056beffa101aab7a21f49d3c435f9afdaf561d2b,"Gamma-Ray Pulsar Bonanza Most of the pulsars we know about were detected through their radio emission; a few are known to pulse gamma rays but were first detected at other wavelengths (see the Perspective by Halpern). Using the Fermi Gamma-Ray Space Telescope, Abdo et al. (p. 840, published online 2 July; see the cover) report the detection of 16 previously unknown pulsars based on their gamma-ray emission alone. Thirteen of these coincide with previously unidentified gamma-ray sources, solving the 30-year-old mystery of their identities. Pulsars are fast-rotating neutron stars. With time they slow down and cease to radiate; however, if they are in a binary system, they can have their spin rates increased by mass transfer from their companion stars, starting a new life as millisecond pulsars. In another study, Abdo et al. (p. 845) report the detection of gamma-ray emission from the globular cluster 47 Tucanae, which is coming from an ensemble of millisecond pulsars in the cluster's core. The data imply that there are up to 60 millisecond pulsars in 47 Tucanae, twice as many as predicted by radio observations. In a further companion study, Abdo et al. (p. 848, published online 2 July) searched Fermi Large Area Telescope data for pulsations from all known millisecond pulsars outside of stellar clusters, finding gamma-ray pulsations for eight of them. Their properties resemble those of other gamma-ray pulsars, suggesting that they share the same basic emission mechanism. Indeed, both sets of pulsars favor emission models in which the gamma rays are produced in the outer magnetosphere of the neutron star. Most of these identifications correspond to gamma-ray sources long suspected to be pulsars. Pulsars are rapidly rotating, highly magnetized neutron stars emitting radiation across the electromagnetic spectrum. Although there are more than 1800 known radio pulsars, until recently only seven were observed to pulse in gamma rays, and these were all discovered at other wavelengths. The Fermi Large Area Telescope (LAT) makes it possible to pinpoint neutron stars through their gamma-ray pulsations. We report the detection of 16 gamma-ray pulsars in blind frequency searches using the LAT. Most of these pulsars are coincident with previously unidentified gamma-ray sources, and many are associated with supernova remnants. Direct detection of gamma-ray pulsars enables studies of emission mechanisms, population statistics, and the energetics of pulsar wind nebulae and supernova remnants."
06f10a512d59fc436f5118d37e1dacd6b38a69ce,"The diffuse galactic gamma-ray emission is produced by cosmic rays (CRs) interacting with the interstellar gas and radiation field. Measurements by the Energetic Gamma-Ray Experiment Telescope (EGRET) instrument on the Compton Gamma-Ray Observatory indicated excess gamma-ray emission greater, > or approximately equal to 1 GeV relative to diffuse galactic gamma-ray emission models consistent with directly measured CR spectra (the so-called ""EGRET GeV excess""). The Large Area Telescope (LAT) instrument on the Fermi Gamma-Ray Space Telescope has measured the diffuse gamma-ray emission with improved sensitivity and resolution compared to EGRET. We report on LAT measurements for energies 100 MeV to 10 GeV and galactic latitudes 10 degrees < or = |b| < or = 20 degrees. The LAT spectrum for this region of the sky is well reproduced by a diffuse galactic gamma-ray emission model that is consistent with local CR spectra and inconsistent with the EGRET GeV excess."
0953cf06933f1ab521929c1a2daca3dbe0ca92bc,"We report the discovery of γ-ray pulsations (⩾0.1 GeV) from the young radio and X-ray pulsar PSR J0205 + 6449 located in the Galactic supernova remnant 3C 58. Data in the γ-ray band were acquired by the Large Area Telescope aboard the Fermi Gamma-ray Space Telescope (formerly GLAST), while the radio rotational ephemeris used to fold γ-rays was obtained using both the Green Bank Telescope and the Lovell telescope at Jodrell Bank. The light curve consists of two peaks separated by 0.49 ± 0.01 ± 0.01 cycles which are aligned with the X-ray peaks. The first γ-ray peak trails the radio pulse by 0.08 ± 0.01 ± 0.01, while its amplitude decreases with increasing energy as for the other γ-ray pulsars. Spectral analysis of the pulsed γ-ray emission suggests a simple power law of index −2.1 ± 0.1 ± 0.2 with an exponential cutoff at 3.0+1.1−0.7 ± 0.4 GeV. The first uncertainty is statistical and the second is systematic. The integral γ-ray photon flux above 0.1 GeV is (13.7 ± 1.4 ± 3.0) × 10−8 cm−2 s−1, which implies for a distance of 3.2 kpc and assuming a broad fan-like beam a luminosity of 8.3 × 1034 erg s−1 and an efficiency η of 0.3%. Finally, we report a 95% upper limit on the flux of 1.7 × 10−8 cm−2 s−1 for off-pulse emission from the object."
20652dd6bb9f41a9047b870958bd68aa948538b5,"The Fermi Gamma-Ray Space Telescope was launched in June 2008 and the onboard Large Area Telescope (LAT) has been collecting data since August of that same year. The LAT is currently being used to study a wide range of science topics in high-energy astrophysics, one of which is the study of high-energy cosmic rays. The LAT has recently demonstrated its ability to measure cosmic-ray electrons, and the Fermi LAT Collaboration has published a measurement of the high-energy cosmic-ray electron spectrum in the 20 GeV to 1 TeV energy range. Some methods for performing a similar analysis to measure the cosmic-ray proton spectrum using the LAT will be presented with emphasis on unfolding the reconstructed proton energy."
28b1579d409ad9b4711bb298c3d7724fa12c84da,
34fba8f34f6a1bc62fb5e6e4beadd340df4fb209,"The discovery of bright gamma-ray emission coincident with supernova remnant (SNR) W51C is reported using the Large Area Telescope (LAT) onboard the Fermi Gamma-ray Space Telescope. W51C is a middle-aged remnant (∼104 yr) with intense radio synchrotron emission in its shell and known to be interacting with a molecular cloud. The gamma-ray emission is spatially extended, broadly consistent with the radio and X-ray extent of SNR W51C. The energy spectrum in the 0.2–50 GeV band exhibits steepening toward high energies. The luminosity is greater than 1 × 1036 erg s−1 given the distance constraint of D > 5.5 kpc, which makes this object one of the most luminous gamma-ray sources in our Galaxy. The observed gamma-rays can be explained reasonably by a combination of efficient acceleration of nuclear cosmic rays at supernova shocks and shock–cloud interactions. The decay of neutral π mesons produced in hadronic collisions provides a plausible explanation for the gamma-ray emission. The product of the average gas density and the total energy content of the accelerated protons amounts to . Electron density constraints from the radio and X-ray bands render it difficult to explain the LAT signal as due to inverse Compton scattering. The Fermi LAT source coincident with SNR W51C sheds new light on the origin of Galactic cosmic rays."
3923c6fb1aa6c041542bc012445c950610e09390,"The first results from observations of the high-mass X-ray binary LS 5039 using the Fermi Gamma-ray Space Telescope data between 2008 August and 2009 June are presented. Our results indicate variability that is consistent with the binary period, with the emission being modulated with a period of 3.903 ± 0.005 days; the first detection of this modulation at GeV energies. The light curve is characterized by a broad peak around superior conjunction in agreement with inverse Compton scattering models. The spectrum is represented by a power law with an exponential cutoff, yielding an overall flux (100 MeV–300 GeV) of 4.9 ± 0.5(stat) ± 1.8(syst) ×10−7 photon cm−2 s−1, with a cutoff at 2.1 ± 0.3(stat) ± 1.1(syst) GeV and photon index Γ = 1.9 ± 0.1(stat) ± 0.3(syst). The spectrum is observed to vary with orbital phase, specifically between inferior and superior conjunction. We suggest that the presence of a cutoff in the spectrum may be indicative of magnetospheric emission similar to the emission seen in many pulsars by Fermi."
583b9797d1c456cef90c5514a093c6b264f4a300,"Microquasar Spotted Microquasars are binary star systems where a normal star sheds matter onto a neutron star or a black hole, generating x-ray radiation and jets of material moving at relativistic speeds. Microquasars have proved difficult to detect in high-energy gamma rays (> 100 megaelectron volts). Using the Fermi Large Area Telescope, Abdo et al. (p. 1512, published online 26 November; see the Perspective by Bignami) now report the detection of variable gamma-ray emission from the microquasar Cygnus X-3. The gamma-ray flux is modulated at the orbital period of Cygnus X-3, and its variation is correlated with the radio emission originating from the microquasar's relativistic jets. Gamma-ray emission from the jet of an accreting binary star system is correlated with the jet’s radio emission. Microquasars are accreting black holes or neutron stars in binary systems with associated relativistic jets. Despite their frequent outburst activity, they have never been unambiguously detected emitting high-energy gamma rays. The Fermi Large Area Telescope (LAT) has detected a variable high-energy source coinciding with the position of the x-ray binary and microquasar Cygnus X-3. Its identification with Cygnus X-3 is secured by the detection of its orbital period in gamma rays, as well as the correlation of the LAT flux with radio emission from the relativistic jets of Cygnus X-3. The gamma-ray emission probably originates from within the binary system, opening new areas in which to study the formation of relativistic jets."
5cc5603e4ac382252d5402bba51425bccd151da6,"We report the discovery of gamma-ray pulsations from the nearby isolated millisecond pulsar (MSP) PSR J0030+0451 with the Large Area Telescope on the Fermi Gamma-ray Space Telescope (formerly GLAST). This discovery makes PSR J0030+0451 the second MSP to be detected in gamma rays after PSR J0218+4232, observed by the EGRET instrument on the Compton Gamma-Ray Observatory. The spin-down power erg s−1 is an order of magnitude lower than the empirical lower bound of previously known gamma-ray pulsars. The emission profile is characterized by two narrow peaks, 0.07 ± 0.01 and 0.08 ± 0.02 wide, respectively, separated by 0.44 ± 0.02 in phase. The first gamma-ray peak falls 0.15 ± 0.01 after the main radio peak. The pulse shape is similar to that of the “normal” gamma-ray pulsars. An exponentially cutoff power-law fit of the emission spectrum leads to an integral photon flux above 100 MeV of (6.76 ± 1.05 ± 1.35) × 10−8 cm−2 s−1 with cutoff energy (1.7 ± 0.4 ± 0.5) GeV. Based on its parallax distance of (300 ± 90) pc, we obtain a gamma-ray efficiency for the conversion of spin-down energy rate into gamma-ray radiation, assuming isotropic emission."
6b78054cd8df8212bd6cb3baf6addb4cde298286,"We report the detection of pulsed gamma-rays from the young, spin-powered radio pulsar PSR J2021+3651 using data acquired with the Large Area Telescope (LAT) on the Fermi Gamma-ray Space Telescope (formerly GLAST). The light curve consists of two narrow peaks of similar amplitude separated by 0.468 ± 0.002 in phase. The first peak lags the maximum of the 2 GHz radio pulse by 0.162 ± 0.004 ± 0.01 in phase. The integral gamma-ray photon flux above 100 MeV is (56 ± 3 ± 11) × 10−8 cm−2 s−1. The photon spectrum is well described by an exponentially cut-off power law of the form , where the energy E is expressed in GeV. The photon index is Γ = 1.5 ± 0.1 ± 0.1 and the exponential cut-off is Ec = 2.4 ± 0.3 ± 0.5 GeV. The first uncertainty is statistical and the second is systematic. The integral photon flux of the bridge is approximately 10% of the pulsed emission, and the upper limit on off-pulse gamma-ray emission from a putative pulsar wind nebula is < 10% of the pulsed emission at the 95% confidence level. Radio polarization measurements yield a rotation measure of RM = 524 ± 4 rad m−2 but a poorly constrained magnetic geometry. Re-analysis of Chandra X-ray Observatory data enhanced the significance of the weak X-ray pulsations, and the first peak is roughly phase aligned with the first gamma-ray peak. We discuss the emission region and beaming geometry based on the shape and spectrum of the gamma-ray light curve combined with radio and X-ray measurements, and the implications for the pulsar distance. Gamma-ray emission from the polar cap region seems unlikely for this pulsar."
6b7f254cee6e4784034a4fca9054d66ae2bf45f1,
6e7df36841f52441f785e071831f0914ad96ad31,"We study the coalescence of two penumbra filaments driven by neutral-hydrogen flows. We investigate the effects of changing the initial collision velocity and axial current intensity on the magnetic reconnection rates. We use a two-fluid (ion–neutral) numerical code to model the three-dimensional dynamics of protons and neutral-hydrogen particles, which couple through proton/neutral-hydrogen collisions. It is found that the magnetic reconnection rate of the coalescing penumbral filaments is strongly enhanced by an initial collision velocity. An initial collision velocity corresponding to 10% of the sound speed increases the magnetic reconnection rate by a factor of 50 when compared to spontaneous coalescence. We conclude that the magnetic reconnection rate of coalescing penumbra filaments can be strongly enhanced by weak photospheric neutral-hydrogen flows."
9bff8f2ead3944bc56e232f1a34d61ab1710ba9a,"Gamma-Ray Pulsar Bonanza Most of the pulsars we know about were detected through their radio emission; a few are known to pulse gamma rays but were first detected at other wavelengths (see the Perspective by Halpern). Using the Fermi Gamma-Ray Space Telescope, Abdo et al. (p. 840, published online 2 July; see the cover) report the detection of 16 previously unknown pulsars based on their gamma-ray emission alone. Thirteen of these coincide with previously unidentified gamma-ray sources, solving the 30-year-old mystery of their identities. Pulsars are fast-rotating neutron stars. With time they slow down and cease to radiate; however, if they are in a binary system, they can have their spin rates increased by mass transfer from their companion stars, starting a new life as millisecond pulsars. In another study, Abdo et al. (p. 845) report the detection of gamma-ray emission from the globular cluster 47 Tucanae, which is coming from an ensemble of millisecond pulsars in the cluster's core. The data imply that there are up to 60 millisecond pulsars in 47 Tucanae, twice as many as predicted by radio observations. In a further companion study, Abdo et al. (p. 848, published online 2 July) searched Fermi Large Area Telescope data for pulsations from all known millisecond pulsars outside of stellar clusters, finding gamma-ray pulsations for eight of them. Their properties resemble those of other gamma-ray pulsars, suggesting that they share the same basic emission mechanism. Indeed, both sets of pulsars favor emission models in which the gamma rays are produced in the outer magnetosphere of the neutron star. The Fermi Large Area Telescope reveals up to 60 millisecond pulsars in this globular cluster, twice as many as predicted by radio observations. We report the detection of gamma-ray emissions above 200 megaelectron volts at a significance level of 17σ from the globular cluster 47 Tucanae, using data obtained with the Large Area Telescope onboard the Fermi Gamma-ray Space Telescope. Globular clusters are expected to emit gamma rays because of the large populations of millisecond pulsars that they contain. The spectral shape of 47 Tucanae is consistent with gamma-ray emission from a population of millisecond pulsars. The observed gamma-ray luminosity implies an upper limit of 60 millisecond pulsars present in 47 Tucanae."
ad82678f99f1af0260e7a3db1d29855d36bcb719,"The Large Area Telescope (LAT) on–board the Fermi Gamma–ray Space Telescope began its on–orbit operations on June 23, 2008. Calibrations, defined in a generic sense, correspond to synchronization of trigger signals, optimization of delays for latching data, determination of detector thresholds, gains and responses, evaluation of the perimeter of the South Atlantic Anomaly (SAA), measurements of live time, of absolute time, and internal and spacecraft boresight alignments. Here we describe on–orbit calibration results obtained using known astrophysical sources, galactic cosmic rays, and charge injection into the front-end electronics of each detector. Instrument response functions will be described in a separate publication. This paper demonstrates the stability of calibrations and describes minor changes observed since launch. These results have been used to calibrate the LAT datasets to be publicly released in August 2009."
b9c8fa536bb3aa274d7e994c5e5e900f2a4cb949,The Large Area Telescope (LAT) instrument on the Fermi mission will reveal the rich spectral and temporal gamma-ray burst (GRB) phenomena in the >100 MeV band. The synergy with Fermi's Gamma-ray Burst Monitor detectors will link these observations to those in the well explored 10–1000 keV range; the addition of the >100 MeV band observations will resolve theoretical uncertainties about burst emission in both the prompt and afterglow phases. Trigger algorithms will be applied to the LAT data both onboard the spacecraft and on the ground. The sensitivity of these triggers will differ because of the available computing resources onboard and on the ground. Here we present the LAT's burst detection methodologies and the instrument's GRB capabilities.
c39ce74d799e4ad673e0b1b0b69096091dcb1c6b,
c5b7f4e8f1caed777e22dad4484d1bf999520e55,"The Fermi Gamma-Ray Space Telescope was launched in June 2008 and the onboard Large Area Telescope (LAT) has been collecting data since August of that same year. The LAT is currently being used to study a wide range of science topics in high-energy astrophysics, one of which is the study of high-energy cosmic rays. The LAT has recently demonstrated its ability to measure cosmic-ray electrons, and the Fermi LAT Collaboration has published a measurement of the high-energy cosmic-ray electron spectrum in the 20 GeV to 1 TeV energy range. This talk will discuss the prospects for using the LAT to perform a similar analysis to measure cosmic-ray proton events. The instrument response for cosmic-ray protons will be characterized and an assessment of the potential to measure the cosmic-ray proton energy spectrum will be presented."
cd07bc78cb98f6ad487f7a71a90f11199e083156,"Radio pulsar PSR J1028−5819 was recently discovered in a high-frequency search (at 3.1 GHz) in the error circle of the Energetic Gamma-Ray Experiment Telescope (EGRET) source 3EG J1027−5817. The spin-down power of this young pulsar is great enough to make it very likely the counterpart for the EGRET source. We report here the discovery of γ-ray pulsations from PSR J1028−5819 in early observations by the Large Area Telescope (LAT) on the Fermi Gamma-Ray Space Telescope. The γ-ray light curve shows two sharp peaks having phase separation of 0.460 ± 0.004, trailing the very narrow radio pulse by 0.200 ± 0.003 in phase, very similar to that of other known γ-ray pulsars. The measured γ-ray flux gives an efficiency for the pulsar of ∼10–20% (for outer magnetosphere beam models). No evidence of a surrounding pulsar wind nebula is seen in the current Fermi data but limits on associated emission are weak because the source lies in a crowded region with high background emission. However, the improved angular resolution afforded by the LAT enables the disentanglement of the previous COS-B and EGRET source detections into at least two distinct sources, one of which is now identified as PSR J1028−5819."
d6cc79140515f38c63be2f4d464de6d91dcca64f,"The Fermi observatory is advancing our knowledge of gamma-ray bursts (GRBs) through pioneering observations at high energies, covering more than seven decades in energy with the two on-board detectors, the Large Area Telescope (LAT) and the Gamma-ray Burst Monitor (GBM). Here, we report on the observation of the long GRB 090217A which triggered the GBM and has been detected by the LAT with a significance greater than 9σ. We present the GBM and LAT observations and on-ground analyses, including the time-resolved spectra and the study of the temporal profile from 8 keV up to ∼1 GeV. All spectra are well reproduced by a Band model. We compare these observations to the first two LAT-detected, long bursts GRB 080825C and GRB 080916C. These bursts were found to have time-dependent spectra and exhibited a delayed onset of the high-energy emission, which are not observed in the case of GRB 090217A. We discuss some theoretical implications for the high-energy emission of GRBs."
e0ed6c4c54670ad75f19acbe107a85562becee17,"We report on the observation of the bright, long gamma-ray burst (GRB), GRB 090902B, by the Gamma-ray Burst Monitor (GBM) and Large Area Telescope (LAT) instruments on-board the Fermi observatory. This was one of the brightest GRBs to have been observed by the LAT, which detected several hundred photons during the prompt phase. With a redshift of z = 1.822, this burst is among the most luminous detected by Fermi. Time-resolved spectral analysis reveals a significant power-law component in the LAT data that is distinct from the usual Band model emission that is seen in the sub-MeV energy range. This power-law component appears to extrapolate from the GeV range to the lowest energies and is more intense than the Band component, both below ∼50 keV and above 100 MeV. The Band component undergoes substantial spectral evolution over the entire course of the burst, while the photon index of the power-law component remains constant for most of the prompt phase, then hardens significantly toward the end. After the prompt phase, power-law emission persists in the LAT data as late as 1 ks post-trigger, with its flux declining as t−1.5. The LAT detected a photon with the highest energy so far measured from a GRB, 33.4+2.7−3.5 GeV. This event arrived 82 s after the GBM trigger and ∼50 s after the prompt phase emission had ended in the GBM band. We discuss the implications of these results for models of GRB emission and for constraints on models of the extragalactic background light."
e20350533675bfd61650700c166743c9ee985c43,"Designed as a high-sensitivity gamma-ray observatory, the Fermi Large Area Telescope is also an electron detector with a large acceptance exceeding 2 m;{2} sr at 300 GeV. Building on the gamma-ray analysis, we have developed an efficient electron detection strategy which provides sufficient background rejection for measurement of the steeply falling electron spectrum up to 1 TeV. Our high precision data show that the electron spectrum falls with energy as E-3.0 and does not exhibit prominent spectral features. Interpretations in terms of a conventional diffusive model as well as a potential local extra component are briefly discussed."
e940d2a5783a25f4ced5e98bd25c3d41267daf91,We report on measurements of the cosmic-ray induced {gamma}-ray emission of Earth's atmosphere by the Large Area Telescope onboard the Fermi Gamma-ray Space Telescope. The LAT has observed the Earth during its commissioning phase and with a dedicated Earth-limb following observation in September 2008. These measurements yielded {approx} 6.4 x 10{sup 6} photons with energies > 100 MeV and {approx} 250 hours total livetime for the highest quality data selection. This allows the study of the spatial and spectral distributions of these photons with unprecedented detail. The spectrum of the emission - often referred to as Earth albedo gamma-ray emission - has a power-law shape up to 500 GeV with spectral index {Lambda} = 2.79 {+-} 0.06.
e9702784b2c3b0f1e98ceeebf39fcd231fc90f98,"Gamma-Ray Pulsar Bonanza Most of the pulsars we know about were detected through their radio emission; a few are known to pulse gamma rays but were first detected at other wavelengths (see the Perspective by Halpern). Using the Fermi Gamma-Ray Space Telescope, Abdo et al. (p. 840, published online 2 July; see the cover) report the detection of 16 previously unknown pulsars based on their gamma-ray emission alone. Thirteen of these coincide with previously unidentified gamma-ray sources, solving the 30-year-old mystery of their identities. Pulsars are fast-rotating neutron stars. With time they slow down and cease to radiate; however, if they are in a binary system, they can have their spin rates increased by mass transfer from their companion stars, starting a new life as millisecond pulsars. In another study, Abdo et al. (p. 845) report the detection of gamma-ray emission from the globular cluster 47 Tucanae, which is coming from an ensemble of millisecond pulsars in the cluster's core. The data imply that there are up to 60 millisecond pulsars in 47 Tucanae, twice as many as predicted by radio observations. In a further companion study, Abdo et al. (p. 848, published online 2 July) searched Fermi Large Area Telescope data for pulsations from all known millisecond pulsars outside of stellar clusters, finding gamma-ray pulsations for eight of them. Their properties resemble those of other gamma-ray pulsars, suggesting that they share the same basic emission mechanism. Indeed, both sets of pulsars favor emission models in which the gamma rays are produced in the outer magnetosphere of the neutron star. These objects appear to share a common emission mechanism with standard gamma-ray pulsars. Pulsars are born with subsecond spin periods and slow by electromagnetic braking for several tens of millions of years, when detectable radiation ceases. A second life can occur for neutron stars in binary systems. They can acquire mass and angular momentum from their companions, to be spun up to millisecond periods and begin radiating again. We searched Fermi Large Area Telescope data for pulsations from all known millisecond pulsars (MSPs) outside of globular clusters, using rotation parameters from radio telescopes. Strong gamma-ray pulsations were detected for eight MSPs. The gamma-ray pulse profiles and spectral properties resemble those of young gamma-ray pulsars. The basic emission mechanism seems to be the same for MSPs and young pulsars, with the emission originating in regions far from the neutron star surface."
f4c1a20409e45091253240912fe2649ce071c6e9,"We report on observations of TeV-selected active galactic nuclei (AGNs) made during the first 5.5 months of observations with the Large Area Telescope (LAT) on-board the Fermi Gamma-ray Space Telescope (Fermi). In total, 96 AGNs were selected for study, each being either (1) a source detected at TeV energies (28 sources) or (2) an object that has been studied with TeV instruments and for which an upper limit has been reported (68 objects). The Fermi observations show clear detections of 38 of these TeV-selected objects, of which 21 are joint GeV–TeV sources, and 29 were not in the third EGRET catalog. For each of the 38 Fermi-detected sources, spectra and light curves are presented. Most can be described with a power law of spectral index harder than 2.0, with a spectral break generally required to accommodate the TeV measurements. Based on an extrapolation of the Fermi spectrum, we identify sources, not previously detected at TeV energies, which are promising targets for TeV instruments. Evidence for systematic evolution of the γ-ray spectrum with redshift is presented and discussed in the context of interaction with the extragalactic background light."
09fb3d095d659fbfcaed78475df680764748b40d,"We investigate how penumbral microjets, recently observed by the Hinode satellite, can be produced within sunspot penumbra. We consider two penumbral filaments with axial currents and axial flows. We assume that a vertical magnetic flux tube exists between two horizontal penumbral filaments. We also assume that the axial flows are not steady; a high-velocity axial flow is imposed on the background slow axial flow. We find that this high-velocity axial flow can trigger magnetic reconnection between one penumbral filament and the vertical flux tube. As a result, inclined bidirectional jetlike flows, driven by the magnetic reconnection, propagate along the vertical magnetic flux tube. Strong proton heating, up to 25 times their original temperature, is observed in these generated jets. Conversely, the neutral-hydrogen particles are only very weakly heated. We propose that these plasma jets explain the phenomenon of penumbral microjets, recently observed by the Hinode satellite."
0d9a71092f4ca9fb6973c5df148899d33c84b2d8,"We investigate the sensitivity of the Gamma-ray Large Area Space Telescope (GLAST) for indirectly detecting weakly interacting massive particles (WIMPs) through the γ-ray signal that their pair annihilation produces. WIMPs are among the favorite candidates for explaining the compelling evidence that about 80% of the mass in the Universe is non-baryonic dark matter (DM). They are serendipitously motivated by various extensions of the standard model of particle physics such as supersymmetry and universal extra dimensions (UED). With its unprecedented sensitivity and its very large energy range (20 MeV to more than 300 GeV) the main instrument on board the GLAST satellite, the Large Area Telescope (LAT), will open a new window of discovery. As our estimates show, the LAT will be able to detect an indirect DM signature for a large class of WIMP models given a cuspy profile for the DM distribution. Using the current state of the art Monte Carlo and event reconstruction software developed within the LAT collaboration, we present preliminary sensitivity studies for several possible sources inside and outside the Galaxy. We also discuss the potential of the LAT to detect UED via the electron/positron channel. Diffuse background modeling and other background issues that will be important in setting limits or seeing a signal are presented."
430eee58f68b950161c9c519e59e6ce554393b68,"Abstract This paper explores the cross-fertilization between science and literature in the 1930s, at key moments in atomic physics and in the development of the atomic bomb. In 1932, the centenary of Goethe's death, physicists attending an international conference at Niels Bohr's Institute of Theoretical Physics in Copenhagen performed a parody of Goethe's Faust. Goethe's critique of science in the play made this a significant choice at the dawn of nuclear physics. James Chadwick's discovery of the neutron that year was high lighted in the performance. In 1933 while in Bloomsbury, London, the physicist Leo Szilard realized how to use a self-sustaining neutron chain reaction to release the energy of the atom. The previous year Szilard had read H. G. Wells's novel The World Set Free (1914) in which the phrase 'atomic bomb' was coined. As well as considering the Faustian themes in the novel, I explore parallels between Wells's scientist, Holsten, and Leo Szilard himself. I argue that this is a clear example of fiction influencing science, and that Goethe's notion that scientific knowledge and self-knowledge should evolve hand in hand, remains a valuable insight when considering the role of scientists in the creation of weapons of mass destruction."
4a127a2de73331e1f99ba6e7589ea04164e55e5c,"Aims. We investigate magnetic reconnection rates during the coalescence of two current loops in the solar chromosphere, by altering the neutral-hydrogen to proton density ratio, ionisation/recombination coefficients, collision frequency, and relative helicity of the loops. Methods. We used a newly developed two-fluid (ion-neutral) numerical code to perform 2.5D simulations of coalescing chromospheric current loops. Developed from the Artificial Wind scheme, the numerical code includes the effects of ion-neutral collisions, ionisation/recombination, thermal/resistive diffusivity, and collisional/resistive heating. Results. It was found that the rates of magnetic reconnection strongly depend on the neutral-hydrogen to proton density ratio: increasing the density ratio a thousandfold decreased the rate of magnetic reconnection twentyfold. This result implies that magnetic reconnection proceeds significantly faster in the upper chromosphere, where the density of ions (protons) and neutral-hydrogen is comparable, than in the lower chromosphere, where the density of neutral-hydrogen is over a thousand times the ion density. This result also implies that jets associated with fast magnetic reconnection tends to occur in the upper chromosphere / lower corona. The inclusion of ionisation/recombination, an important physical effect in the chromosphere, increases the total reconnected magnetic flux, but does not alter the rate of magnetic reconnection. Reductions in the ion-neutral collision frequency result in small increases to the rates of magnetic reconnection. The relative helicity of the two current loops was not observed to have any significant effect on the rates of magnetic reconnection. Comparisons of two-fluid and MHD (Magnetohydrodynamic) simulations show significant differences in the measured rates of magnetic reconnection, particularly for the higher neutral density cases which represent the lower chromosphere. This demonstrates that MHD is not an appropriate model for simulating magnetic reconnection in the solar chromosphere. Conclusions. The magnetic reconnection rates of coalescing current loops are strongly affected by the inclusion of neutral-hydrogen particles. It is therefore essential that ion-neutral collisions are included in future analytical/numerical models of chromospheric magnetic reconnection."
5c069091729f19093a9e2c7e6b337a72171bbb07,"Energetic young pulsars and expanding blast waves [supernova remnants (SNRs)] are the most visible remains after massive stars, ending their lives, explode in core-collapse supernovae. The Fermi Gamma-Ray Space Telescope has unveiled a radio quiet pulsar located near the center of the compact synchrotron nebula inside the supernova remnant CTA 1. The pulsar, discovered through its gamma-ray pulsations, has a period of 316.86 milliseconds and a period derivative of 3.614 × 10–13 seconds per second. Its characteristic age of 104 years is comparable to that estimated for the SNR. We speculate that most unidentified Galactic gamma-ray sources associated with star-forming regions and SNRs are such young pulsars."
db00892f2eb707a6237ce88c75bf007e3278ee67,
2e5d61318b919297f886484586ef0ecd92bef8e2,"The main goal of the Large Area Telescope (LAT) onboard science program is to provide quick identification and localization of Gamma Ray Bursts (GRB) onboard the LAT for follow-up observations by other observatories. The GRB identification and localization algorithm will provide celestial coordinates with an error region that will be distributed via the Gamma ray burst Coordinate Network (GCN). We present results that show our sensitivity to bursts as characterized using Monte Carlo simulations of the GLAST observatory. We describe and characterize the method of onboard track determination and the GRB identification and localization algorithm. Onboard track determination is considerably different than in the onground case, resulting in a substantially altered point spread function. The algorithm contains tunable parameters which may be adjusted after launch when real bursts characteristics at very high energies have been identified."
446fb3821ef65b5432e199ced6a76739070a50c9,"It was the weapon to end all weapons: the doomsday device. A huge nuclear bomb so powerful that it could envelop the entire planet in a cloud of radioactive dust, and bring about instant extinction. This is the untold story of the Cold War's most insane plan, the men behind it and how it nearly happened. It is also the history of humanity's nightmare vision of a superweapon, showing how popular culture, from the stories of H. G. Wells and Jules Verne to films such as ""Planet of the Apes"", ""Mad Max"" and ""Dr Strangelove"" itself have both shaped and reflected our darkest dreams."
4793d64008da656c5b3a3139c3e4648a1e501c69,"Aims. We explore the solar coronal heating enigma by an analytical and numerical study of the enhanced phase mixing of harmonic Alfven waves propagating in gravitationally stratified coronal structures of varying magnetic field divergence. Methods. Corrected analytical solutions are derived to model the dissipation of Alfven waves propagating in divergent and stratified coronal structures. These analytical solutions are validated and further explored using a newly developed 2.5D visco-resistive linear MHD code. Results. Corrected analytical solutions describing the enhanced phase mixing of Alfven waves in divergent and stratified coronal structures are presented. These show that the enhanced phase mixing mechanism can dissipate Alfven waves at heights less than half that is predicted by the previous analytical solutions. In divergent and stratified coronal structures, the enhanced phase mixing effect occurs only when the ratio of the magnetic and density scale heights, Hb/Hρ < 2. The enhanced phase mixing of 0.1 Hz harmonic Alfven waves propagating in strongly divergent, Hb = 5 Mm, stratified coronal structures, Hρ = 50 Mm, can fulfill 100% of an active region heating requirement, by generating viscous heating fluxes of FH ≈ 2100 J m −2 s −1 . The Alfven waves in this configuration are fully dissipated within 20 Mm, which is six times lower than would occur as a result of standard phase mixing in uniform magnetic fields. This results in the heating length scale, LH, defined as the height at which 95% of the Alfven wave poynting flux has dissipated, being lowered by a factor of six, to less than half of an active region density scale height. Using the corrected analytical solutions it was found that, for a given wave frequency, the generation of a heating length scale of LH ≤ 50 Mm, by enhanced phase mixing in strongly divergent magnetic fields, requires a shear viscosity eight orders of magnitude lower, than required by standard phase mixing in uniform magnetic fields. It was also found that the enhanced phase mixing of observable, ω ≈ 0.01 rads s −1 ,A lfven waves, in strongly divergent magnetic fields, Hb = 5 Mm, can generate heating length scales within a density scale height, Hρ = 50 Mm, using classical Braginskii viscosity. It is therefore not necessary to invoke anomalous viscosity in corona, if phase mixing takes place in strongly divergent magnetic fields. This study shows that the importance of enhanced phase mixing as a mechanism for dissipating Alfven waves in the solar corona (a stratified and divergent medium), has been seriously underestimated."
99546dbb78a8c2daf77dc74bea8282b9bd636b39,
b550931cae2614199068415dc695b3cb31920df6,"The GLAST Large Area Telescope (LAT) will measure the cosmic gamma‐ray flux in the energy range 20 MeV to >300 GeV. The LAT will open a new and important window on a wide variety of high‐energy phenomena. Achieving the capability requires a hardware trigger and onboard software event filters that are robust and highly efficient for gamma rays while remaining powerful rejecters of the much larger fluxes of charged‐particle backgrounds. Because of the important discovery windows for science and the uncertainties in the background fluxes, configuration flexibility is a particularly important system feature. This paper describes the purposes and architecture of the system, the components and capabilities of the hardware trigger and onboard software filters, and the on‐orbit operations plan and expected performance."
542bba6ddb6f19158d7c7fc1894351fcb9284e89,
4692e8988c234e6c1f44584238385c1e8d3c55d2,"In assessing the potential hazard to buildings and people in a city landscape, it is usual to assume that building facades are perfectly rigid reflecting surfaces. This is an attractive assumption because it results in conservative estimations of impulse loads at locations remote from the explosion. Similarly, it enables both small-scale physical modelling and numerical simulation to be performed without the need for structural prototypes or response calculations. However, close to the source of an explosion, damage to building facades can be devastating. It is not unusual for all the glazing elements, light cladding and architectural features to be completely removed, leaving only the steel or reinforced concrete skeleton of the building. Because this region of devastation allows blast to enter the building, pressures and impulses experienced at locations further down the street in which the building is located will be reduced. The extent and significance, or otherwise, of any reduction is difficult to q..."
b17dd1936645549a5abe7ed9d117c56778258d4c,
e56195592ac42e52299c4d4e29f31556a7e91b3f,"An overview of methods of blast load quantification and aspects of building design intended to reduce the damaging effects of explosions by increasing robustness is presented. Possible blast loading scenarios range from simple free-air bursts, loading essentially isolated buildings, to complex urban environments, with road junctions and side streets, etc., where buildings have varying amounts of frangible facade. The blast loads resulting in these scenarios are radically different, with different aspects of the pressure load dominating the type and extent of building damage. Means of calculating these time-varying pressure loads are described. 
 
Achieving stand-off between a vulnerable building and the source of an explosion is the single most important factor in reducing the blast load experienced by the building. Methods of achieving stand-off in building design are discussed. The creation of adequate stand-off may often not be possible, and the need for robust glazing systems is also essential if blast is to be keep out of buildings and contamination of the work-place by broken glass is to be prevented. The use of laminated glass, anti-shatter film and other measures is discussed. 
 
Existing buildings pose particular problems to engineers concerned with their protection. Several methods of strengthening or increasing the effective mass of existing buildings are described."
f5f8fceab27cded7ee13a93933430ba9e8e7e7cd,
6a36fdb5ec95477abf431e9ad99312c25ceff6ab,"The use of large vehicle bombs to attack city centres has been a feature of campaigns by terrorist organisations throughout the world during the last few years. Recent United Kingdom examples include the bombs in the City of London in both 1992 and 1993, the Docklands bomb of January 1996 and the explosion in Manchester city centre in June 1996. Such events cause damage—some of it very severe—and widespread disruption to the function of businesses in the vicinity of the explosion. The radius of damage to buildings experiencing the blast from such bombs is difficult to predict using simple techniques. Sometimes buildings quite close to an explosion experience relatively little damage while those at relatively long range suffer proportionately much higher levels as a consequence of the ‘channelling’ effect of city streets. This paper presents the results of an experimental study using scale models of simple generic urban geometries to provide assessment, both qualitatively and quantitatively, of the effect ..."
0f1bb0ca3897421b7017b2875c9da47622f273af,"In the blast loading of buildings by high-explosive charges, the phenomenon of clearing has been overlooked recently-this study offers a revisit. Clearing occurs when a blast wave interacts with the facade of a building, generating a reflected overpressure. The top and sides of the building will be loaded by the lower-magnitude 'side-on' pressure and there will be a pressure difference between these surfaces. Pressure equalization occurs via clearing, with the pressure on the facade decaying rapidly. Contrastingly, for an infinitely large reflecting surface, the reduction will be slower because there will be no region of lower pressure accessible. In this experimental study, small spherical charges of plastic explosive were detonated above instrumented, rigid, circular steel plates carrying flush-mounted pressure transducers. The results are presented as graphs of peak overpressure and peak scaled impulse plotted against scaled distance and are compared with predictions from the established ConWep database. The study demonstrates how the effects of clearing are more pronounced towards the edges of the plates, particularly for the scaled impulse values. As the scaled distance increases, there is relatively little difference between the experimental and database predictions. The results reinforce the use of ConWep as a conservative design tool because it overestimates the scaled impulse."
36c3c6bfe3130b88899d6ca89b47c2e558e542a7,
3c065bd4f56df9d998401264aa3903def6b7993b,"It is well known that the dynamical evolution of the Brillouin precursor field in a single-resonance Lorentz model dielectric can be fully explained in terms of a pair of saddle points that evolve in a region of the complex -plane near the origin such that , where is the undamped resonance frequency of the dispersive medium. As time increases at a fixed propagation distance, these two near first-order saddle points first approach each other along the imaginary frequency axis, then coalesce into a second-order saddle point at the time , and finally separate from each other in the lower half-plane, one with an increasing real part and the other with a decreasing real part. The uniform asymptotic description of the Brillouin precursor provides an accurate description of the field evolution about the observation time , at which the saddle-point order changes discontinuously. However, previous approximate expressions for the phase behaviour in the region of the near saddle points have resulted in an inaccurate field evolution around . This inaccuracy is corrected in this paper. Numerical illustrations of the complete precursor evolution for the delta function pulse and the step function modulated signal are provided."
b57ea759229c504e0db6a5f2c6e668b8135c7887,
f18f4e6c8d3c2c632acab0372dbc86e8faf4b11e,"A programme of research was conducted at approximately one-tenth scale to make measurements of the blast environment behind a vertical blast wall when spherical charges were detonated at different stand-off distances from the wall. In contrast to the types of wall that are generally provided for protective purposes, the structures deployed in this study were only sufficiently robust to remain in place while blast wave interaction occurred. They are described as 'partially failing', meaning that they suffered damage as a consequence of the loading they received. Walls were constructed from a range of materials, including plain sand monoliths of different thickness, sand enclosed in scaled geotextile materials, wood, expanded foam plastic and water. Some experiments were conducted using sand enclosed in geotextile material but with the wall being zig-zagged in plan (rather than straight) in order to increase overall stability. Peak overpressures and specific impulses obtained from the pressure-time histories were compared with the results from earlier investigations involving undeforming plane steel cantilever walls. In almost every case the reduction in blast resultants was at least as good as for the undeforming walls. A relationship between the degree of attenuation and the areal density of the wall structure was apparent The results indicate that, for the rapid provision of protection, non-permanent structures can provide a high degree of blast wave attenuation."
e1455fab4365903a5546c565c0ffa2510197d293,
e1a9be5c23afaef39854500f26956e049297f160,An extensive programme of research was conducted to make detailed measurements of the blast environment at different horizontal distances and heights above the ground behind a vertical cantileve blast wall when spherical charges were detonated at different heights above the ground and at different stand-off overpressures and peak specific side-on overpressures and peak side-on impulses obtained from the pressure-time histories were plotted in 'scaled space' that allowed a wide range of different terrorist threat scenarios to be represented. Comprehensive contour plots of overpressure and impulse behind the wall were developed. This information was then recast in the form of a series of design charts. These were accompanied by a simple and effective method of data extraction to provide engineers with information necessary to assist in the design for blast resistance of structures located behind the wall.
e949672c45684053eb7dea2713953014de0a3e6c,
255cc33aae316033fec1fd172b2c771a16b627c9,
3d9238e29443cde0384ce8680a51941c6e7b2edf,
b0862f2557a56bff50b9ab5a9e3e892a978866ff,This paper describes a programme of research in which detailed measurements of the blast environment were made behind a one-tenth scale vertical blast wall when scaled realistic threats were detonated at appropriate distances from the wall. A grid for measurement was established out to six wall heights behind the wall and up to three wall heights above the ground.
51b90e45fafde8a455ac81ad54d5b89adc8bf3be,"The shoe can be thought of as a powerful tool for controlling human movement. A well-designed shoe can assist in reducing the number of lower limb injuries arising from sport and training activities. The purpose of this paper is to present a summary of the main thrusts of research in this field by means of a digest of current thinking and practice. The paper initially presents a survey of work in the biomechanical field with particular reference to the design of footwear. A review of the types of injury acquired by sportsmen in both training and playing is then followed by a discussion of aspects of footwear design and their role in both contributing to and preventing lower limb injury. Finally, the paper considers support and shock absorption techniques in the context of footwear design. It is concluded that research has been wide-ranging and thorough. However, the complexity of the biomechanical system being studied has prevented definitive recommendations for the prevention of injury being made in every case. Nonetheless, it is clear that a number of guiding principles have been established which should be the basis for future developments in footwear design to minimize the chance of lower limb injury."
65ecbee74774e97aee68257fc138193fda4eb097,
508b75bdb3d6cd8d9dcd4318f6c0f7d78a6579d3,"Following an analysis of TEM structures, a brief discussion of the transient sources used is given, and a review of the biconic properties outlined. The effect of resistive loading is then considered. Next, some successful designs of planar fin and planar horn antennas radiating fast transient pulsed fields are presented. Finally, arrays of such horns are considered. >"
2843b399ee5563458633f16735f76808b59d44d5,"The objective of this study was to develop a better quantitative understanding of explosive behavior under skid impact conditions. We evaluated the effects of sample weight, impact velocity, contact surface area at impact, target surface roughness, and target material on the skid impact HE ignition threshold. We also quantified the effects of two parameters that had never been fully investigated in the standard skid impact sensitivity test: explosive sample size and angle of incidence. These parameters were studied experimentally by conducting a series of tests, and analytical, with a number of one-, two-, and three-dimensional computer models. This study is the first phase in a program to measure the transient heat produced in the ignition of a high explosive sample as it impacts an infrared (IR) transmissive target. We will use the experimentally derived data to enhance our ability to predict the onset of ignition in impact-heated high explosives. 12 refs., 9 figs., 2 tabs."
ea2872fc52260cc5b2aab3c5dc7b1066764ba740,
14c7d52b62b95566f2a65d58893e641f9c27c046,
71b338e4daa38f41bd936267f27205e2751032ab,
26bc792f09180294875eb5953c06b9f6499a0644,"It is useful to be able to predict both the E-field and monopole current time histories produced for a given pulse stimulating a radiating antenna, for both theoretical purposes and experimental measurements. Typical problems that arise in the remote measurements of E–fields and mono-pole currents involve background radiation and spurious responses from reflecting objects, as well as detector malfunction. With prior knowledge of the expected waveform, which can be obtained very quickly, serious discrepancies can be easily identified, and the causes eliminated or the time windows adjusted if required."
3ce83e24f7a02176e29b8ae3d869d3f668efd775,"The objective of this study is to develop seismic adequacy criteria for the visual identification of any features of aboveground welded steel safety-related piping systems in nuclear plants which could cause loss of function in an earthquake with a free-field ground surface acceleration of up to 0.5g. The criteria were developed by (1) collecting and assessing detailed data on the performance of piping at selected power and industrial facilities during past earthquakes and by (2) performing and evaluating a worldwide survey of earthquake-induced damage to and failure of piping over the past sixty years. Welded steel piping has exhibited superior performance in past earthquakes, even though it is rarely designed to resist earthquakes and is typically designed to resist only normal gravity and pressure loads. Seismic adequacy criteria are developed for the three areas where seismic-induced failure has been observed: seismic anchor movement, interaction and corrosion. It is noteworthy that inertia loads are not known to have caused failure to piping, and thus criteria for inertia loads are not proposed or apparently needed. This finding should be contrasted with the great emphasis placed on seismic design for inertia loads in nuclear plant piping systems. This study presents conclusive evidence thatmore » design for inertia effects is over-emphasized on the basis of providing needed additional safety. 12 refs., 12 figs., 2 tabs.« less"
a7554f263dacc8cd0a655b2a83bae6ce8c844191,
b769af1bd3d2d25b3c89baba896daaa40722c190,The design and testing of a system for studying underwater explosions is described together with the design of a suitable charge for use in the facility. From the results obtained it is concluded that the system offers a relatively simple method of demonstrating underwater explosion phenomena.
7f2aa0dcf4fc1e4ddcd142bf57eac9c148a259dc,
c487e9d678e4c276aa460704dbd00a44e6058253,"It is planned to install about 1500 new armor tiles in the DIII-D tokamak. The armor tiles currently installed in DIII-D are made by brazing Poco AXF-5Q graphite onto Inconel X-750 stock. A small percentage of these have failed by breakage of graphite. These failures were believed to be related to significant residual stress remaining in graphite after brazing. Hence, an effort was undertaken to improve the design with all-graphite tiles. Three criteria must be satisfied by the armor tiles and the hardware used to attach the tiles to the vessel walls: tiles should not structurally fail, peak tile temperature must be less than 2500 K, and peak vessel stresses must be below acceptable levels. A number of alternate design concepts were first analyzed with the two-dimensional finite element codes TOPAZ2D and NIKE2D. Promising designs were optimized for best parameters such as thicknesses, etc. The two best designs were further analyzed for thermal stresses with the three-dimensional codes P/THERMAL and P/STRESS. Prototype tiles of a number of materials were fabricated by GA and tested at the Plasma Materials Test Facility of the Sandia National Laboratory at Albuquerque. The tests simulated the heat flux and cooling conditions in DIII-D. This papermore » describes the 2-D and 3-D thermal stress analyses, the test results and logic which led to the selected design of the DIII-D armor tiles. 5 refs., 7 figs., 3 tabs.« less"
0003ca890b2aff1cc2cad40edf29f1f8597fe1e0,"Lawrence Livermore National Laboratory (LLNL) has conducted a review of the Millstone Unit 3 (MP 3) Probabilistic Safety Study (PSS) for the Office of Nuclear Reactor Regulation, US Nuclear Regulatory Commission (NRC). This probabilistic safety study was performed by Northeast Utilities (NU) in response to a 1981 request from the NRC. The objective of LLNL's review was to review those aspects of the MP 3 PSS leading to estimates of the plant core damage frequency. LLNL estimated core damage frequency from internal events at MP 3 to be about 1 x 10/sup -4/ per year. LLNL reviewed major areas of the PSS, including initiating events, event trees, success criteria, fault trees, human factors, component and operating experience data, and treatment of uncertainty. The review of external events included earthquakes, fires, external and internal flooding, extreme winds, aircraft accidents, hazardous materials, and turbine missiles. The MP 3 PSS treated external events, other than seismic and fire, in a cursory manner. LLNL's seismic review effort was curtailed by the staff because of ongoing seismic analysis revisions by NU."
5692c650e40c0df95cedb659bb0d836e160be96f,"induced waveform in short biconic sensors and (long) monopoles is described. The aim of the work is to develop an antenna which faithfully radiates the waveform of an applied stimulus, and to recognize the waveforms obtained by the receivers. The radiating antenna is also required to be easily portable, so that for example it can be taken to the site of a target whose transient response is to be determined."
569b6017b5ecb6f03977988085e8702c7667d9cc,
8a891bd1b102545e4f4bf65ef9b4bc9252faa648,
bcef02831c1408622d689f20447c21f7d633f864,"The report summarizes the development of a simplified seismic probabilistic risk assessment (PRA) methodology. The purpose was to reduce the costs while adequately performing seismic probabilistic risk assessment of nuclear power plants. A simplified methodology for estimating seismic response directly from free-field ground acceleration was developed, including methods to account for the effects of soil-structure interaction, calibration of major structure response, and calibration of piping system response. Guidelines were developed for event/fault trees to be used in simplified seismic PRAs. 14 refs., 5 figs., 32 tabs. (ACR)"
e63fb9935bf51c51093e6e0c7d87a11e433a6c05,Summary A simple modification is described to the integral method of P. D. Smith for calculating turbulent boundary layers on infinite yawed wings to allow for wing planform taper. A comparison has been made between the modified and general methods for calculating three-dimensional boundary layers. This suggests that the modified method is useful for obtaining a rapid indication of the separate effects on the boundary layer of wing sweep and planform taper.
0725d297cf0d738964fe6802920af12412a8742d,
93637ddc607da2927da2e6e2e8828efca1b62b23,"It is both a privilege and a pleasure to be asked to contribute to an edition of the Aeronautical Journal to celebrate Professor Alec Young's 70th birthday. He has been my mentor for nearly a quarter of a century. The subject of integral prediction methods for boundary layers is one with which Alec Young has long been associated. Starting with a method for laminar compressible flow he has guided the development of, amongst others, integral methods for compressible laminar flow with heat transfer, three-dimensional laminar flow, three-dimensional turbulent flow and flows with pressure gradients normal to the surface. This paper attempts to describe the integral methods for compressible turbulent boundary layers which are most widely used in aerodynamic design and to discuss the application of inverse integral methods to three-dimensional separated flows."
a7af4e9a3e077eca2a55c469874d79e3d6d144b1,"In this paper we present observations on a number of issues related to seismic PRAs including: strengths and weaknesses of seismic PRAs; uncertainty, sensitivity and variability; common misconceptions; possible improvements in seismic safety acceptance criteria; recommended modifications to the NRC approach to safety goals; and major problems. Some specific examples are provided."
8df7c382abb2a9614c993d0b06d56cd27f3a17d7,"An iterative procedure is presented for the balancing of a flexible rotor. In addition to determining optimal correction weights, the number and axial positions of the balance planes are optimized. A linear programming solution is employed using a linearized approximation of the axial variation of the influence coefficients. First the correction weights are found and then the axial locations of the balance planes are adjusted. These computations are repeated until the optimal values that lead to minimum rotor runout are determined. Numerical results are discussed."
08b05943660bd69b09928a4bad56a48e146fe57b,"As part of technical assistance to the US Nuclear Regulatory Commission (NRC, 1), a systematic format for seismic equipment qualification (EQ) was initiated. This format consists of thirty issues associated with seismic EQ. Each issue was considered as a Category of Possible Seismic EQ Requirements and Criteria. That is, seismic EQ standards might be (but presently are not formulated in terms of requirements and criteria that address each of the thirty issues. Each of the thirty issues was ranked and a minimum set identified. The current requirements in existing NRC and national standards were also evaluated against this common set of issues, and they were estimated to score 60 out of 100 overall. It is believed that the systematic format exhibited in this paper can be of assistance in obtaining a broader and more complete perspective on seismic EQ issues. This format (but especially the technique) may also be of interest in non-seismic EQ since many of the issues are common."
4e26f63beb59a03bf6e7da48ed0d54f9062deb9d,"The Lawrence Livermore National Laboratory (LLNL) under the sponsorship of the US Nuclear Regulatory Commission (NRC) has developed this program plan for research in equipment qualification (EQA). In this report the research program which will be executed in accordance with this plan will be referred to as the Equipment Qualification Research Program (EQRP). Covered are electrical and mechanical equipment under the conditions described in the OBJECTIVE section of this report. The EQRP has two phases; Phase I is primarily to produce early results and to develop information for Phase II. Phase I will last 18 months and consists of six projects. The first project is program management. The second project is responsible for in-depth evaluation and review of EQ issues and EQ processes. The third project is responsible for detailed planning to initiate Phase II. The remaining three projects address specific equipment; i.e., valves, electrical equipment, and a pump."
7a42a363b12b7d9c83398d2de170720536f63cbe,"This article describes what happens when a car battery explodes, describes how to use jumper cables and recommends the use of goggles. There are many situations that can generate sparks at battery terminals: hooking up a battery charger, disconnecting cable clamps when there is an electrical load on the circuit, or inadvertently causing a short with a wrench when tightening clamps. A battery generates hydrogen gas which accumulates at the top of the battery. If this gas escapes through vents and comes across spark sources, it will ignite and carry back to the hydrogen pocket in the battery causing an explosion. It is noted that the timing light is a device that could ignite in an explosive atmosphere."
80b554abfe18bf81157ec7153b5f702ae2f61148,"A study on the feasibility of using experience data on the performance of equipment in nonnuclear facilities during earthquakes found that such data would be quite useful, if they were developed, in addressing issues concerning the seismic qualification of equipment in operating nuclear power plants located in the eastern United States. Guidelines and criteria for the use of these data were also developed. Thirty possible issues associated with seismic qualification of equipment were identified and ranked, and the current seismic equipment qualification requirements evaluated against each of these issues. These results and this technique are also believed to be of use in areas beyond experience data and seismic issues such as in general EQ."
873fd2da81c6b48175596ebe269f79e3f7139f04,"Abstract : An inverse integral prediction method for the development of separated turbulent boundary layers developed from the lag-entrainment method is described. The inverse method uses the concept of equilibrium separated boundary layer flows and the predicted characteristics of such flows will be compared with measurements which represent the first known demonstration that equilibrium separated boundary layers can be realised experimentally. In these experiments the data were obtained with a single-component laser Doppler anemometer usually set up to measure streamwise components of mean velocity and turbulence intensity; in addition, however, one pair of profiles of the mean velocity and turbulence intensity normal to the wall was obtained. The separated flow on a NACA 4412 aerofoil has been measured by Wadcock using the flying hot-wire technique. It is shown that predicted values of momentum thickness agree with the measured values but that the calculation predicts a pressure rise in the separated region whereas the pressure is almost constant in the experiment. The result of introducing second order effects into the calculation is shown. The equivalent inviscid flow is constructed and the matching of the equivalent and real flows is considered."
b4cc17b0d6d1b08565d358196d7089fcc814793d,
bc258952b1c842850e16e745777e178c6a46dc32,Abstract : The governing equations for compressible three-dimensional turbulent boundary layers in a general coordinate system are given. Methods for the solution of these equations and their integral counterparts are descibed and compared. The available finite difference techniques are discussed in detail. (Author)
37940eaab139cea50633bf19c40943f41e4314e0,The seismic qualification requirements of auxiliary feedwater systems (AFWS) of Pressurized Water Reactors (PWR) were developed over a number of years. These are formalized in the publication General Design Criteria (Appendix A to 10CFR50). The full recognition of the system as an engineered safety feature did not occur until publication of the Standard Review Plan (1975). Efforts to determine how to backfit seismic requirements to earlier plants has been undertaken primarily in the Systematic Evaluation Program (SEP) for a limited number of operating reactors. Nuclear Reactor Research (RES) and NRR have requested LLNL to perform a probabilistic study on the AFWS of San Onofre Nuclear Generating Station (SONGS) Unit 1 utilizing the tools developed by the Seismic Safety Margins Research Program (SSMRP). The main objectives of this project are to: identify the weak links of AFWS; compare the failure probabilities of SONGS 1 and Zion 1 AFWS: and compare the seismic responses due to different input spectra and design values.
5e699c88613099ac9f2c92e87e4383a86e984907,"A study of transient stress wave propagation in graphite HTGR fuel elements has been undertaken as a step toward developing techniques for the evaluation of seismic impact loads. The objectives of the study were to identify appropriate numerical methods, to understand the influence of the geometry and the multiple holes on the response, and to determine the relative importance of high frequency response and lower mode vibrations. A general review is made of the dynamic contact problem, and the methods available to model impact phenomena and stress wave propagation are evaluated."
7957b78fab192cf67bf93027c0f4715593a8e928,
85803376b52a546e078e7161f3ef5f1ade3f79e0,"Objective is to develop improved requirements for seismic safety assessment of nuclear power plants. Three phases are planned. Phase I, successfully completed in January 1981, was to develop and demonstrate a probabilistic computational procedure for our investigation of the seismic safety assessment process. Phase II will use this procedure to carry out the investigation, and Phase III will develop improved requirements for seismic safety assessment. This paper provides an overview of the SSMRP and the work performed in Phase I, which was organized into eight projects. Project I was to select a typical power plant and to collect necessary dta. The Zion Nuclear Power Plant, Unit 1, north of Chicago, Illinois was selected as the demonstration plant for the computational procedure. Project II developed the tools and models necessary to describe probabilistically the seismic hazard at the Zion Site. A hazard curve was developed and 30 time histories were generated for each of six acceleration ranges spanning from 0.15g to 1.8g. In Project III the models for soil-structure interaction were developed. Models for major structural response were generated in Project IV. Project V developed models for 13 piping systems of the Zion Plant, the ones deemed most important to seismicmore » safety. The development of fragility descriptions for components and structures is carried out in Project VI. Fragility curves are developed for 37 generic categories of compenents and five Zion structures. Project VII is to generate the systems analysis models for the Zion Plant and develop the computer code SEISIM for performing the probbilistic failure analyses. The computer code SMACS is developed in Project VIII to tie together the calculations of soil-structure interaction, major structural response, and subsystem response.« less"
c4d0c22e94d1e5587b1f378102d484021d41d835,"This document is a progress report on the Seismic Safety Margins Research Program (SSMRP) covering the period April 1, 1981 through June 30, 1981. The report gives a general description of the program, together with financial summaries and individual project details. Each project is summarized to show accomplishments, schedules, milestones and completion dates, budget and expenditures, and any concerns that may affect the project."
e93617bb4050f6a9cb8e9acc4e99f9690f8205bd,"This document is a progress report on the Seismic Safety Margins Research Program (SSMRP) covering the period January 1, 1981 through March 31, 1981. The report gives a general description of the program, together with financial summaries and individual project details. Each project is summarized to show accomplishments, schedules, milestones and completion dates, budget and expenditures, and any concerns that may affect the project."
edd6d156b78264e85c57bd2e349c27c6358cabd6,"This document is a progress report on the Seismic Safety Margins Research Program (SSMRP) covering the period October 1 through December 31, 1980. This report gives a general description of the program, together with financial summaries and individual project details. Each project is summarized to show accomplishments, schedules, milestones and completion dates, budget and expenditures, and any concerns that may affect the project. This report covers the concluding three months of Phase I of the program and includes a grand total cost projection in Phase II for FY 1981. Project summaries include Plant/Site Selection and Data Collection; Seismic Input; Soil-Structure Interaction; Structural Building Response; Subsystem Response; Component and Structural Fragilities; Systems Analysis; and SMACS and BE-EM."
ef6804e96a386b41e4acaa0af8419ae51dcf93e0,"The January 29-30, 1981, meeting of the Advisory Committee on Reactor Safeguards (ACRS), Subcommittee on Extreme External Phenomena, mark the close of Phase I efforts on the Seismic Safety Margins Research Program (SSMRP). Presentations at the meeting focused on results produced. These included computer codes, response computations, failure and release probabilities, data bases, and fragilities and parameter characteristics."
0ae116d029a479d736867d3ea1dcab6d3061a68a,"The concept of how two techniques, Best Estimate Method and Evaluation Method, may be applied to the tradditional seismic analysis and design of a nuclear power plant is introduced. Only the four links of the seismic analysis and design methodology chain (SMC)--seismic input, soil-structure interaction, major structural response, and subsystem response--are considered. The objective is to evaluate the compounding of conservatisms in the seismic analysis and design of nuclear power plants, to provide guidance for judgments in the SMC, and to concentrate the evaluation on that part of the seismic analysis and design which is familiar to the engineering community. An example applies the effects of three-dimensional excitations on the model of a nuclear power plant structure. The example demonstrates how conservatisms accrue by coupling two links in the SMC and comparing those results to the effects of one link alone. The utility of employing the Best Estimate Method vs the Evauation Method is also demonstrated."
9e0beb72610729cac17407d576c27f78e3b92ea2,"The concept of how two techniques, Best Estimate Method and Evaluation Method, may be applied to the traditional seismic analysis and design of a nuclear power plant is introduced. Only the four links of the seismic analysis and design methodology chain (SMC) - seismic input, soil-structure interaction, major structural response, and subsystem response - are considered. The objective is to evaluate the compounding of conservatisms in the seismic analysis and design of nuclear power plants, to provide guidance for judgments in the SMC, and to concentrate the evaluation on that part of the seismic analysis and design which is familiar to the engineering community. An example applies the effects of three-dimensional excitations on a model of a nuclear power plant structure. The example demonstrates how conservatisms accrue by coupling two links in the SMC and comparing those results to the effects of one link alone. The utility of employing the Best Estimate Method vs the Evaluation Method is also demonstrated."
ad420792e023f6e0e4897a2f0345dcba2c15d8f6,"A recent risk analysis of nuclear power plants without seismically qualified auxiliary feedwater systems is reviewed. That report suffers from several shortcomings, including the failure to account for the common-mode nature of the seismic threat, a failure to deal adequately with the complexity of the decay heat removal problem and to account for its uncertainties, and an attempt to compare results with the far more sophisticated Reactor Safety Study. Nonetheless, it is recommended that additional such probabilistic risk studies be undertaken, with the emphasis on best estimate methods and sensitivity studies rather than conservative estimates of risk. Experimental tests should also be part of reactor safety analyses."
bd493e628c3a886b46de4a222df4c4e5ed593d56,A facility for determining the effects of soil surface strains on full-scale test structures is described. Soil strains may be those resulting from surface subsidence caused by collapse of underground mines or similar phenomena. The facility will provide data for use in designing subsidence-resistant structures.
c5cf45207b3dfb3553446f38519e5e4d98b19095,"This report documents interim definitions of terms in the Seismic Safety Margins Research Program (SSMRP). Intent is to establish a common-based terminology integral to the probabilistic methods that predict more realistically the behavior of nuclear power plants during an earthquake. These definitions are a response to a request by the Nuclear Regulatory Commission Advisory Committee on Reactor Safeguards at its meeting held November 15-16, 1979."
5e4cbed85e0d09002469b17cb889057e44a3591e,
71994ceebba5d516629c52e8723de36af430e325,"An analytical method and associated computer program are described by which the probability of damaging critical components of a nuclear power station can be determined. Input information consists of parameters that define the turbine, barriers, and targets. Barriers are noncritical objects that impede the flight of fragments from the turbine to the target. Targets can be plant structures, systems, and components. The program produces a matrix that relates the probability of damaging each target to each turbine wheel individually, as well as to the aggregate of all turbine wheels in the plant. In addition, the probability of either striking or penetrating selected barriers can be obtained. Results of an example calculation are presented."
c94189326e1a8cc9f9b4fb093e68782863de28d6,"The proposed load combination project has the following overall objectives: develop a methodology for appropriate combination of dynamic loads for nuclear power plants under normal plant operation, transients, accidents, and natural hazards; establish design criteria, load factors, and component service levels for appropriate combinations of dynamic loads or responses to be used in nuclear power plant design; determine the reliability of typical piping systems, both inside and outside the containment structure, and provide the NRC with a sound technical basis for defining the criteria for postulating pipe breaks; and determine the probabilities of a large LOCA induced directly and indirectly by a range of earthquakes."
281da647ec84b15724b628c7fde5a9006403bb7b,"A special purpose computer program, TRAFIC, is presented for calculating the release of metallic fission products from an HTGR core. The program is based upon Fick's law of diffusion for radioactive species. One-dimensional transient diffusion calculations are performed for the coated fuel particles and for the structural graphite web. A quasi steady-state calculation is performed for the fuel rod matrix material. The model accounts for nonlinear adsorption behavior in the fuel rod gap and on the coolant hole boundary. The TRAFIC program is designed to operate in a core survey mode; that is, it performs many repetitive calculations for a large number of spatial locations in the core. This is necessary in order to obtain an accurate volume integrated release. For this reason the program has been designed with calculational efficiency as one of its main objectives. A highly efficient numerical method is used in the solution. The method makes use of the Duhamel superposition principle to eliminate interior spatial solutions from consideration. Linear response functions relating the concentrations and mass fluxes on the boundaries of a homogeneous region are derived. Multiple regions are numerically coupled through interface conditions. Algebraic elimination is used to reduce the equations as far asmore » possible. The problem reduces to two nonlinear equations in two unknowns, which are solved using a Newton Raphson technique.« less"
69ae2f7d62d1c25e578de68f6ff325da17a76d26,
73526f60aef5394cfc9e300b1b538aca1ad99f74,This paper introduces a polarization surface current approach to the computation of the equivalent circuit self and mutual inductances required to predict the surge response of transformer windings. The effect of the magnetic core is accounted for by means of polarization currents circulating on the core surface. The application of the proposed method can be easily extended to solve axially-symmetric magnetic field problems. The method's algorithm is suitable for digital computer programming. Accurate results are very efficiently obtained.
89601afc52913333e6dc7a2e6f1af79c41930389,"Since the first meeting on November 9, 1977, the objective of the Seismic Safety Margin Research Program (SSMRP) has been to develop mathematical models that realistically predict the probability of radioactive releases from seismically induced events in nuclear power plants. These models will be used to estimate the actual probability of release, to estimate the conservatisms in seismic design methodologies such as those embodied in the Standard Review Plant (SRP), and to develop a new seismic design methodology based on probability. This document describes specific subtasks to be performed on Phase I of the SSMRP. Phase I activity will take approximately 20 months to complete."
91d417aa0292cac2110a028b67fb6d83e0442ece,"AbstractThe character of the shear instability in the metal-cutting operation is discussed with reference to results from machining En58C austenitic stainless steel. It is shown that at certain cutting speeds a continuous chip which exhibits large serrations is formed. The sequence of events in a typical cycle of serration formation has been established from a series of photomicrographs of ‘quick-stop’ specimens, and the associated dynamic cutting forces have been recorded. It is shown that the serrated chip formation is not caused by machine-tool vibration but is related to the inherent metallurgical features of the steel, for the machining conditions used. The essential feature of the chip formation is a varying shear strength of the work material at the chip/tool interface: a phenomenon that is similar to the stick-slip conditions described in previous work on the discontinuous chip formation. In a typical cycle, compressive stresses build up ahead of the tool as material sticks on to the rake face; se..."
a2a8b1046a70450f618d3e9d46743be926f4f330,"The document has been prepared pursuant to the second meeting of the Senior Research Review Group of the Seismic Safety Margin Research Program (SSMRP), which was held on June 15, 16, 1978. The major portion of the material contained in the document is descriptions of specific subtasks to be performed on the SSMRP. This is preceded by a brief discussion of the objective of the SSMRP and the approach to be used. Specific subtasks to be performed in Phase I of the SSMRP are as follows: (1) plant/site selection, (2) seismic input, (3) soil structure interaction, (4) structural building response, (5) structural sub-system response, (6) fragility, (7) system analysis, and (8) Phase II task definition."
b9ae5cf9ee38d3a9100569f051889e952a10e194,"TRAFIC performs calculations of metallic fission product inventory of the fuel as function of time, and release from failed and intact fuel particles, diffusion through the fuel compact matrix and diffusion and sorption in the fuel element graphite and release to the coolant for HTGR cores under normal operating conditions."
c36434c4e39b8303e9f6d0949536cda917d523bf,"The NONSAP-C finite element code is described and its application to the nonlinear structural analysis of three-dimensional concrete containments under static, dynamic, and long-term loadings is reviewed. Features of this code that allow for easy application to realistic concrete structural problems are discussed, along with the various material models used to represent plain and reinforced concrete, for both time-dependent and time-independent behavior. Applications of the code to analysis of conventional reinforced concrete structures and to the structural analysis of prestressed concrete reactor vessels (PCRVs) and PCRV models are illustrated. Comparisons of the code predictions with previous numerical solutions to these problems or to experimental data are made. Input instructions for the NONSAP-C code are described."
e1f71a0c908e773f4e4c0ba2232c1779434d95b7,"The variable modulus-cracking model is capable of predicting the behavior of reinforced concrete structures (such as the reinforced plate under transverse pressure described previously) well into the range of nonlinear behavior including the prediction of the ultimate load. For unreinforced thick-walled concrete vessels under internal pressure the use of elastic--plastic concrete models in finite element codes enhances the apparent ductility of the vessels in contrast to variable modulus-cracking models that predict nearly instantaneous rupture whenever the tensile strength at the inner wall is exceeded. For unreinforced thick-walled end slabs representative of PCRV heads, the behavior predicted by finite element codes using variable modulus-cracking models is much stiffer in the nonlinear range than that observed experimentally. Although the shear type failures and crack patterns that are observed experimentally are predicted by such concrete models, the ultimate load carrying capacity and vessel-ductility are significantly underestimated. It appears that such models do not adequately model such features as aggregate interlock that could lead to an enhanced vessel reserve strength and ductility."
23a7bab9a1f5359c0db20d0d5ff51268f1c9ba18,"This paper describes hot-machining experiments on a 321-type stainless steel and austenitic manganese steel in which an electric heating current in the range 0 to 400 amps has been applied to the tool-work interface. The purpose of the work has been to determine which features of the metal-cutting operation should be considered in order to effectively evaluate the effects of hot machining on different work materials. In these studies the following parameters have been compared; cutting force, material shear strength, the mean normal stress acting on the cutting edge, the energy consumed in the two shear zones and the energy per unit volume in the secondary shear zone. The normal stress and energy per unit volume in secondary shear have been shown to be the most effective properties to compare for different cutting conditions and these are related to the local stress and temperature distribution at the cutting edge."
2f4e9e2c021403eaa9a71fc2467d6031a64e0fa4,"Two constitutive models for concrete are discussed. For short-term loads, the orthotropic variable modulus model is described, and for long-term loads a viscoelastic model utilizing a Dirichlet series approximation for the creep compliance function is summarized. The orthotropic variable modulus model is demonstrated in an analysis of a PCRV head with penetrations. The viscoelastic model is illustrated with a simulation of a prestressed concrete cylinder subject to non-uniform temperatures."
33e8ea71ba9600d78c147bacde6d95ae02d3b1aa,"A model simulating the in-pile release of metallic fission prducts from a batch of coated fuel particles is based on a solution of the transient Fick's diffusion equation in a nonhomogeneous medium. It is developed in two stages. First, some representative analytic solutions for a single birth pulse in a single particle are numerically tabulated as functions of nondimensional parameters. Second, the solution for a history of continuously varying source, temperature, and particle failure fraction is obtained by interpolation and superposition. This permits use of the method as an efficient source subroutine in full-core release problems. The large number of physical parameters in the model provides adaptability in correlating and extrapolating experimental results. By using numerical examples, the model was shown to account for the following phenomena: recoil, transient diffusion response, transition from the intact to the failed state, and the effect of various rate-limiting mechanisms on the release."
40f72570ee5021a3780a161a75e07f6ed9a03809,
4b2dedcee62651f135f923dc31bc3448d6a5f8c6,"PADLOC performs one-dimensional calculation of plateout in an arbitrary pipe network, e.g. an HTGR primary coolant circuit, a test loop, etc. The problem solved is one of mass transport of fission products in a fluid, including the effects pf sources in the fluid and in the plateout surfaces, convection along the flow paths, decay, adsorption on surfaces (plateout), and desorption from surfaces. These phenomena are governed by a system of coupled, nonlinear partial differential equations."
d1ebef0338935120c3c0f93fe958f13735efe54a,"Several present and proposed gas-cooled reactors use concrete pressure vessels. In addition, concrete is almost universally used for the secondary containment structures of water-cooled reactors. Regulatory agencies must have means of assuring that these concrete structures perform their containment functions during normal operation and after extreme conditions of transient overpressure and high temperature. The NONSAP nonlinear structural analysis program has been extensively modified to provide one analytical means of assessing the safety of reinforced concrete pressure vessels and containments. Several structural analysis codes were studied to evaluate their ability to model the nonlinear static and dynamic behavior of three-dimensional structures. The NONSAP code was selected because of its availability and because of the ease with which it can be modified. In particular, the modular structure of this code allows ready addition of specialized material models. Major modifications have been the development of pre- and post-processors for mesh generation and graphics, the addition of an out-of-core solver, and the addition of constitutive models for reinforced concrete subject to either long-term or short-term loads. Emphasis was placed on development of a three-dimensional analysis capability."
4162012c36b92a3f6f612bb58902918d78344400,"A simple and accurate method of measuring relative displacements along a borehole using circular magnets as markers and reed switches as sensors has been developed and is now widely used.This note describes a new precise sensing system which is easily assembled and installed, together with a modified method of placing magnets. Details are given of the performance of the new system over a 2 year period for an investigation into the deformation properties of an heavily loaded week rock."
8ae605bb75d05177a46328f30fa70d88176fd68c,"Limnological characteristics for 1971-1972 of culturally eutrophic Shagawa Lake, Minnesota, were compared to those of the immediately upstream, oligotrophic Burntside Lake to evaluate the effects of domestic waste-water discharge to Shagawa Lake. Typical characteristics of Shagawa Lake were extensive summer blue-green algal blooms (Anabaena and Aphanizomenon), anaerobic hypolimnion and large and rapid fluctuations in chemical constituents. In contrast, oligotrophic Burntside Lake was characterized by low numbers of algae (predominately diatoms and greens), aerobic hypolimnion and small changes in chemical constituents. Chlorophyll a concentrations in Shagawa Lake typically reached 50-60 jug/liter while in Burntside Lake the maximum observed was 6 ,ug/liter. Predominant benthic fauna in Shagawa Lake was Chaoborus at densities up to 28 x 103 organisms/M2, while in Burntside Lake Amphipoda predominated during the summer at a maximum density of about 1300 organisms/M2. Both inorganic nitrogen (IN) and orthophosphate phosphorus '(OP) in Shagawa Lake were depleted to undetectable levels in the epilimnion during the summer. In the epilimnion of Burntside Lake OP was at the detectable limit throughout most of the summer and IN was depleted to undetectable levels. The thermocline in Burntside Lake was relatively stable while in Shagawa Lake it was readily destroyed by passing cold fronts and strong winds, thus allowing nutrient-rich hypolimnetic water to mix with the nutrientlimited epilimnetic water which helped to sustain the algal biomass. INTRODUCTION Although oligotrophic lakes dominate northeastern Minnesota, Shagawa Lake, St. Louis Co., Minn., has attained high levels of algal productivity. This cultural eutrophication resulted primarily from the high amounts of nutrients discharged from the waste-water treatment plant operated by the City of Ely (Malueg et al., 1975). Pilot studies conducted from 1966 to 1970 demonstrated that removing phosphorus from the waste water entering the lake would limit the amount of algal production (Smith, 1973). Subsequently, in order to demonstrate lake restoration by removing one critical nutrient, the city constructed a tertiary treatment plant to remove more than 99% of the phosphorus from all municipal waste-water effluent. This paper compares the limnological characteristics of Shagawa Lake with the immediately upstream, oligotrophic Burntside Lake during 1971 and 1972, just prior to the January 1973 initiation of tertiary treatment. Because Burntside Lake remains minimally influenced by man, this nearly pristine lake will serve as a control for future evalua-"
080cf4df59928e43cecb908809224d3c335813d9,"The flightworthiness of the Apollo spacecraft structure was verified primarily through a rigorous, vehicle level, ground test program and flight tests. The failures and anomalies encountered during this testing were the major factors considered in determining necessary modifications to the basic design of the spacecraft structure. In this report, these failures, their causes, and their resolutions are discussed. A description of the spacecraft structure and discussions of the ground and flight test programs are presented."
d451474f29d03cd6669c125c915a187bf06343c8,
980eaaefe0465b991b33dc15b159af1ac0ed5241,
3dacbadf115c4d9446efef4c97f669b047de515b,"AS PART OF A RESEARCH PROGRAMME INTO GROUND MOVEMENTS, A SIMPLE AND ACCURATE METHOD OF MEASURING RELATIVE DISPLACEMENTS ALONG A T OREHOLE HAS BEEN DEVELOPED BY THE DUILDING RESEARCH STATION USING CIRCULAR MAGNETS AND REED SWITCHES. DETAILS ARE GIVEN OF THE AXIALLY MAGNETIZED MAGNET AND REED SWITCH USED, METHOD OF INSERTING THE MAGNET INTO THE BOREHOLE, SENSING SYSTEMS, ACCURACY OF MEASUREMENTS, AND FIELDS OF APPLICATION."
81428cc63d3836198b57cd5420ca9ccef50e3f06,
a7bc7895ce3c69186d2fff2cbf6acd2ea6bc20cd,
d051eedd4cb487443e249144f0b9cb79f936f805,
f3ca59b18407ce9a71915ffb618da7e49fbc2bc4,"The effect of Hertzian stresses resulting from loads acting at the points of contact of beryllia spheres was determined. As-drawn glass and glass ground with various grades of silicon carbide were indented with beryllia spheres, 1 in. in diameter. Also, pairs of these beryllia spheres were pressed together. Circular cracks due to Hertzian stresses were produced; an optical technique was developed for detecting them in a beryllia surface. The outer crack radius was proportional to the cube root of the maximum applied load. Average values obtained for the respective inner crack radii were used to determine average values of the minimum load to fracture these materials. The average minimum load to fracture 1 in. diameter beryllia spheres in air at room temperature under dynamic loading was 235 1b. The crack radius in glass and beryllia was about 20% greater than the radius of the contact surface. This was not significantly affected (in glass) by flaw density. It is considered that the Hertz analysis does not give the correct location and value of the maximum tensile stress when finite displacements of material occur."
68e3f90a643015fc760bec57c3b3ff063ce18cc3,
f2c0806223875c3cc14c7c421faa99f43b1e10ea,
7f2ca5492147103016078664f040b3b770871334,
e2c43bacc14db08d76b1cbf8bf136c217b59c607,
